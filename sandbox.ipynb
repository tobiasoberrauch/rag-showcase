{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langer Text, der in kleinere Abschnitte aufgeteilt werden soll...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"Langer Text, der in kleinere Abschnitte aufgeteilt werden soll...\"\n",
    "\n",
    "# Konfiguration des Splitters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # Maximale Abschnittsgröße\n",
    "    chunk_overlap=20,  # Überlappung zwischen den Abschnitten, um Kontext zu erhalten\n",
    "    separators=[\"\\n\\n\", \".\", \" \"]  # Trennzeichen in absteigender Priorität\n",
    ")\n",
    "\n",
    "# Aufteilung des Textes\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Ausgabe der Abschnitte\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: Langer Text, der in semantisch sinnvolle Abschnitte aufgeteilt werden soll Ein neues Thema wird hier vorgestellt Hier endet der Text mit einer Zusammenfassung\n",
      "Cluster 2: Hier kommt ein weiterer Satz Und noch ein Satz Vielleicht ein letzter Satz Ein weiterer Abschnitt beginnt hier\n",
      "Cluster 3: Das neue Thema bringt neue Herausforderungen\n",
      "Cluster 4: Dies ist der Abschluss dieses Abschnitts\n",
      "Cluster 5: Dieser Abschnitt enthält wichtige Informationen Die Informationen müssen erhalten bleiben Dieses Thema ist ebenso wichtig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT Modell und Tokenizer laden\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = (\"Langer Text, der in semantisch sinnvolle Abschnitte aufgeteilt werden soll. \"\n",
    "        \"Hier kommt ein weiterer Satz. Und noch ein Satz. Vielleicht ein letzter Satz. \"\n",
    "        \"Ein weiterer Abschnitt beginnt hier. Dieser Abschnitt enthält wichtige Informationen. \"\n",
    "        \"Die Informationen müssen erhalten bleiben. Dies ist der Abschluss dieses Abschnitts. \"\n",
    "        \"Ein neues Thema wird hier vorgestellt. Dieses Thema ist ebenso wichtig. \"\n",
    "        \"Das neue Thema bringt neue Herausforderungen. Hier endet der Text mit einer Zusammenfassung.\")\n",
    "\n",
    "# Text in Sätze aufteilen (oder andere logische Einheiten)\n",
    "sentences = text.split('.')  # Beispielhafte Vereinfachung\n",
    "\n",
    "# Entfernen leerer Sätze\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Berechne BERT-Embeddings für jeden Satz\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy())\n",
    "\n",
    "# Konvertiere die Liste der Embeddings in ein numpy-Array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Bestimme die Anzahl der Cluster basierend auf der Anzahl der Sätze\n",
    "num_clusters = min(len(sentences), 5)\n",
    "\n",
    "# KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Cluster-Zuweisungen erhalten\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Sätze basierend auf Cluster-Zuweisungen gruppieren\n",
    "clustered_sentences = [[] for _ in range(num_clusters)]\n",
    "for i, cluster_id in enumerate(clusters):\n",
    "    clustered_sentences[cluster_id].append(sentences[i])\n",
    "\n",
    "# Ausgabe der semantisch aufgeteilten Abschnitte\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(f\"Cluster {i + 1}: {' '.join(cluster)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-showcase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
