{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Version mit vollständigem Anhang\\n\\n3\\n\\nGrußwort Jörg Bienert\\n\\nIm Jahr 2002 erschien in der New York Times ein Artikel mit der Überschrift „Google\\'s toughest search is for a Business Model“ (Hansell, 2002). Der Autor des Artikels war überzeugt, dass Google sich gegen die damaligen Konkurrenten im Online- Advertising Geschäft nicht behaupten würde und dass das damalige Kerngeschäft, die Lizensierung der Suchmaschine an andere Web-Portale, kein hinreichendes Wachstum bringen wird.\\n\\nSeitdem hat sich viel getan. Innerhalb weniger Jahre beherrschte die Google-Suchmaschine das Internet, den Online-Werbemarkt und spielte eine dominierende Rolle in vielen weiteren Bereichen. Ohne massive Investments in Google-Ads lässt sich heute kein Online-Geschäftsmodell mehr starten, in vielen Autos ist die Navigation von Google-Maps integriert, der meistgenutzte Browser Chrome sammelt vielfältige Daten über unser Surf-Verhalten und mit Online-Diensten wie Gmail, Docs oder Drive vertrauen weltweit Millionen Menschen ihre Daten dem Alphabet Konzern an. Europa hat erfolglos versucht, mit einer eigenen Suchmaschine eine Alternative aufzubauen.\\n\\nJörg Bienert, Präsident KI Bundesverband\\n\\nDas alles sehen wir nur bedingt als problematisch an, weil Google es geschafft hat, einen enormen Vertrauensvorsprung aufzubauen. Was aber, wenn sich dies ändert? Denken wir doch nur einmal an das hypothetische Szenario, Elon Musk würde Alphabet übernehmen. Wären wir in der Lage, das Internet ohne die Google Services sinnvoll zu nutzen? Oder hat Alphabet es durch die Ansammlung von Daten und darauf aufbauenden Diensten bereits geschafft, eine Lock-in-Situation zu erzeugen, aus der wir uns kaum noch befreien können?\\n\\nKünstliche Intelligenz hat das Potential, die Welt ähnlich stark zu verändern, wie es das Internet getan hat. Es wird viele neue Funktionalitäten, Produkte und Geschäftsmodelle geben. Prozessverbesserungen werden zu Effizienzsteigerungen und zu disruptiven Veränderungen führen, die wir heute noch gar nicht absehen können. Im Jahr 2000 hätten wir bei der Vorhersage, dass das Internet zur Insolvenz der größten deutschen Versand- und Warenhäuser führen würde, wahrscheinlich nur den Kopf geschüttelt.\\n\\nWir erleben gerade den Beginn der zweiten Welle der KI-Revolution, die mit der Veröffentlichung von GPT-3 durch OpenAI im Sommer 2020 begonnen hat. Auf Basis riesiger Datenmengen und mit enormem Einsatz von Entwicklerressourcen, Geld und Rechenkapazität hat OpenAI ein Sprachmodell geschaffen, das eine bis dahin unerreichte Performance aufweisen konnte. In dieser Studie beleuchten wir die Entwicklung und den Status Quo genauer.\\n\\nGroße KI-Modelle für Deutschland\\n\\n4\\n\\nWas also wird sich in den nächsten 2-3 Jahren verändern? Wir werden eine Vielzahl von neuen Modellen und Anwendungen sehen. Der Umfang und die Auswirkungen sind derzeit kaum abzuschätzen, aber es werden eine Menge Fragen auftauchen wie z.B.\\n\\nWann und wie werden große Sprachmodelle eine nahezu hundertprozentige inhaltliche Korrektheit in den generierten Texten erreichen?\\n\\nWelche Auswirkungen hat dies für Unternehmen aus allen Branchen und mögliche Anwendungen in den unterschiedlichen Bereichen der Wertschöpfungskette? • Wie gut werden neben Sprachmodellen Bild- und Video-Generatoren sowie die\\n\\nKombination aus diesen? Was bedeutet dies für die Kreativbranche, den Journalismus und die Ausbildung?\\n\\nWie wird sich das Internet mit einer Flut von automatisch generierten Inhalten verändern? Werden die großen Sprachmodelle die Google-Suche ablösen und damit den Online-Werbemarkt auch für andere Akteure öffnen?\\n\\nKönnen wir durch die Demokratisierung von KI den Arbeitsmarkt so umgestalten, dass die demographisch immer kleineren Jahrgänge die Arbeit gesund bewältigen können und gleichzeitig den Fachkräftemangel insbesondere bei “menschlichen” Arbeitsplätzen ausgleichen?\\n\\nInwiefern können die drängendsten Probleme der Menschheit, etwa in Bezug auf die Energiewende, Klimawandel, Gesundheit, mit Hilfe großer Modelle gelöst werden?\\n\\nEine übergeordnete Fragestellung ist dabei von entscheidender Bedeutung. Wer besitzt die Technologie, die Daten und die Ressourcen, um große Modelle zu erstellen und die Entwicklung und revolutionären Durchbrüche zu steuern? Bleibt dies in der Hand weniger großer Konzerne, und werden diese die Nutzung, die Regulierung und auch die Behandlung von ethischen Fragestellungen bestimmen, so wie dies bereits im Internet, bei Suchmaschinen und Sozialen Netzwerken der Fall ist?\\n\\nGenau hier liegt aktuell eine große Herausforderung für Deutschland und Europa. Nur wer die Technologie beherrscht, wird in der Lage sein, deren Nutzung nach eigenem Ermessen zu gestalten und wirtschaftliche und gesellschaftliche Entwicklungen positiv zu beeinflussen. Hier geht es bei weitem nicht nur um die Künstliche Intelligenz als Plattform- Technologie. Große KI-Modelle werden komplett neue Anwendungen ermöglichen und bestehende Geschäftsmodelle und Wertschöpfung disruptiv verändern – in allen Industrien.\\n\\nWenn wir nicht in der Lage sind, diese Basis-Technologie eigenständig zu entwickeln und bereitzustellen, wird die deutsche Industrie auf ausländische Dienste ausweichen müssen, Teile der Wertschöpfungskette verlieren und an Wettbewerbsfähigkeit einbüßen. Wir werden schwierige Diskussionen um Datenschutz, Datensicherheit und die ethische Nutzung von KI-Modellen haben. Die aktuellen Kontroversen um Google und Facebook erscheinen dagegen trivial.\\n\\nGroße KI-Modelle für Deutschland\\n\\n5\\n\\nUm dies zu verhindern und nicht auch in der KI die digitale Souveränität zu verlieren, müssen wir uns in Deutschland in die Lage versetzen, auf internationalem Niveau zu forschen, Daten zu sammeln und zu veredeln, große Modelle zu trainieren und diese offen für die Anwendung durch die Wirtschaft, Konzerne, Mittelstand und Start-ups bereitzustellen.\\n\\nDies ist das Ziel unserer Initiative Large European AI Models, kurz LEAM. Ein Team von 40 Vertretern aus Wissenschaft, Wirtschaft und Gesellschaft hat im vergangenen Jahr zunächst erste Ideen für eine Infrastruktur zur Schaffung von großen Modellen entwickelt. Im Auftrag des Bundesministeriums für Wirtschaft und Klimaschutz wurde nun diese Machbarkeitsstudie erstellt, die wir Ihnen hier vorstellen.\\n\\nKernpunkt des Konzeptes ist der Aufbau einer dedizierten KI-Supercomputing- Infrastruktur. Ein Team von Spezialist:innen betreibt eine dedizierte Hardware- Infrastruktur, die auf große KI-Modelle spezialisiert ist. Es entwickelt diese KI-Modelle weiter und stellt diese anderen zur Verfügung. Darüber hinaus sammelt und veredelt das Team die zum Betrieb und den Anwendungen notwendigen Daten und implementiert Software und Services rund um diese KI-Modelle, die das Training und Tuning von großen Modellen vereinfachen und diese für unterschiedliche Zielgruppen einfach nutzbar machen.\\n\\nMit LEAM planen wir ein zentrales KI-Leuchtturmprojekt, um das sich ein leistungsfähiges Ökosystem aus Wissenschaft, Wirtschaft und Start-ups bilden wird – in enger Zusammenarbeit auch mit bestehenden Aktivitäten wie Open GPT-X, Aleph Alpha oder Bloom und als wichtiger Player im europäischen Kontext. Ein Schwerpunkt liegt dabei auf der Berücksichtigung europäischer Werte und kommender Standards und Regulierungen.\\n\\nWir sind sehr froh über den breiten Zuspruch aus Wissenschaft, Wirtschaft und Politik. Denn nur gemeinsam mit allen Beteiligten können wir die Herausforderung meistern, die Möglichkeiten der Künstlichen Intelligenz zum Wohle der Menschen einzusetzen, durch leistungsfähige Forschung und Produkte international wettbewerbsfähig zu bleiben und den Wohlstand in Deutschland zu sichern.\\n\\nGroße KI-Modelle für Deutschland\\n\\nKernergebnisse\\n\\n6\\n\\nGroße KI-Modelle für Deutschland\\n\\n7\\n\\nGroße KI-Modelle für Deutschland\\n\\n8\\n\\nInhaltsverzeichnis\\n\\nGrußwort Jörg Bienert .............................................................................................................. 3 Kernergebnisse ......................................................................................................................... 6 Inhaltsverzeichnis ..................................................................................................................... 8 Einleitung ................................................................................................................................. 11 Ziele der Machbarkeitsstudie ................................................................................................ 13 Autor:innen der Machbarkeitsstudie .................................................................................... 13 Leseanleitung und Dokumentenstruktur ............................................................................. 16 Das Paradigma der KI-Foundation-Modelle .............................................................. 19 1. Technologische Grundlagen ....................................................................................... 23 2. Die Bedeutung der Größe von KI-Foundation-Modellen ...................................... 31 Anwendungsgebiete von KI-Foundation-Modellen............................................... 33 Vertrauenswürdige KI-Foundation-Modelle .......................................................... 45 Offene Forschungsfragen, neueste Entwicklungen und Erwartungen................ 51 KI-Foundation-Modelle im internationalen Vergleich .............................................. 55 Bedarf der Wirtschaft an KI-Foundation-Modellen .................................................. 67 Unterstützung bei der Entwicklung durch Forschung und Wissenschaft ............... 76 Chancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle .... 85 Erste europäische multilinguale Foundation-Sprachmodelle .............................. 88 Vermeidung von Falschaussagen, Bias und Toxizität ........................................... 97 Verbindung von Foundation-Modellen mit großen Wissensbeständen ............. 99 Kombination von Sprache mit anderen Modi und Medien ................................ 100 Fragestellungen und Weiterentwicklungen ......................................................... 105 Foundation-Modelle in anderen Datendomänen ............................................... 106 Zusammenfassung ................................................................................................ 107 Voraussetzungen bei Software und Personal ......................................................... 109 Applikations-Layer: Trainings- & Inference-Technologien .................................. 112 Data-Storage & -Loading-Layer ............................................................................. 118 System-Layer .......................................................................................................... 120 Framework- & Service-Layer ................................................................................. 121 LEAM als Leuchtturmprojekt für die Zukunft des KI-Ökosystems ..................... 123 Zusammenfassung ................................................................................................ 125 Aufbau eines KI-Hochleistungsrechenzentrums ..................................................... 127 Definition Rechenzentrum .................................................................................... 127 Anforderungen an ein KI-Hochleistungsrechenzentrum ................................... 130 Nachhaltigkeitsaspekte ......................................................................................... 136 Infrastrukturanforderungen im Detail ................................................................. 137 Standortauswahl .................................................................................................... 139 Betrieb eines KI-Rechenzentrums ........................................................................ 156 Zusammenfassung und Empfehlung ................................................................... 164 Die organisatorische Struktur von LEAM ................................................................. 167 Zielgruppen des LEAM KI-Servicezentrums ......................................................... 167 Organisationseinheiten des LEAM KI-Servicezentrums ...................................... 168 Das LEAM-Board .................................................................................................... 181 Zusammenfassung ................................................................................................ 182\\n\\n2.1 2.2 2.3 2.4\\n\\n3. 4. 5. 6.\\n\\n6.1 6.2 6.3 6.4 6.5 6.6 6.7\\n\\n7.\\n\\n7.1 7.2 7.3 7.4 7.5 7.6\\n\\n8.\\n\\n8.1 8.2 8.3 8.4 8.5 8.6 8.7\\n\\n9.\\n\\n9.1 9.2 9.3 9.4\\n\\nGroße KI-Modelle für Deutschland\\n\\n9\\n\\nBetriebswirtschaftliche Aspekte ............................................................................... 184 Kosten ..................................................................................................................... 184 Einnahmen ............................................................................................................. 189 Finanzierungsmodelle von LEAM ............................................................................. 193 Öffentliche Finanzierung ....................................................................................... 194 Private Finanzierung .............................................................................................. 195 Public-Private-Partnership .................................................................................... 197 Rechtliche Rahmenbedingungen.......................................................................... 199 Auswirkungen der Rechtsmaterien auf die Finanzierungsmodelle ................... 205 Abschließende Übersicht ...................................................................................... 218 Gesellschaftsstruktur von LEAM............................................................................... 220 Öffentliche Finanzierung ....................................................................................... 220 Private Finanzierung .............................................................................................. 221 Public-Private-Partnership .................................................................................... 221 Szenario für ein LEAM KI-Servicezentrum ............................................................... 223 Fazit ............................................................................................................................. 227 Beurteilung der Machbarkeit ................................................................................ 227 Ausblick ................................................................................................................... 229 Quellenverzeichnis .................................................................................................... 231 I. Abbildungsverzeichnis .............................................................................................. 240 II. Tabellenverzeichnis ................................................................................................... 242 III. Abkürzungsverzeichnis ............................................................................................. 243 IV. V. Methodik der Machbarkeitsstudie ........................................................................... 245 Anhang ………………………………………………………………………………………………………………………251\\n\\n10.\\n\\n10.1 10.2\\n\\n11.\\n\\n11.1 11.2 11.3 11.4 11.5 11.6\\n\\n12.\\n\\n12.1 12.2 12.3\\n\\n13. 14.\\n\\n14.1 14.2\\n\\nGroße KI-Modelle für Deutschland\\n\\n10\\n\\nMACHBARKEITSSTUDIE\\n\\nzum Aufbau und Betrieb eines dedizierten KI-Hochleistungsrechenzentrums für das Trainieren großer KI-Modelle in Deutschland\\n\\nGroße KI-Modelle für Deutschland\\n\\nEinleitung\\n\\n11\\n\\nGroße KI-Modelle für Deutschland\\n\\n12\\n\\nWie leistungsstark große KI-Modelle bzw. KI-Foundation-Modelle 1 bereits sind, zeigen diese einleitenden Worte. Sie wurden nicht von einem der vielen Forscher:innen und Expert:innen geschrieben, die an dieser Studie mitgewirkt haben, sondern von dem auf GPT-3 basierenden Chatbot ChatGPT.\\n\\nDie Antworten von ChatGPT zeigen eindrucksvoll, wozu KI-Foundation-Modelle bereits heute fähig sind. Dabei steht die Entwicklung noch ganz Anfang und findet bisher vor allem in den USA und China statt. Um eine mittelfristige Abhängigkeit ausländischer Technologiekonzerne zu verhindern und dem Wettbewerb standzuhalten, müssen Deutschland und Europa in die Lage versetzt werden, diese nächste Generation innovativer KI-Technologien mitzugestalten.\\n\\nZu diesem Zweck hat der KI Bundesverband 2021 die Initiative LEAM - Large European Language Models - ins Leben gerufen, die von über 40 namhaften Institutionen aus Forschung und Wirtschaft sowie weiteren europäischen KI-Verbänden unterstützt wird. Eine zentrale Forderung der Initiative ist die Förderung einer europäischen KI- Recheninfrastruktur, die von Wissenschaft, Industrie und Start-ups gleichermaßen genutzt werden soll, sowie der Aufbau eines dedizierten KI-Hochleistungsrechenzentrums in Deutschland.\\n\\nUm die Umsetzbarkeit dieser Ziele zu überprüfen, hat das Bundesministerium für (BMWK) den KI Bundesverband beauftragt, eine Wirtschaft und Klimaschutz Machbarkeitsstudie zu LEAM durchzuführen. In dieser Studie werden die Herausforderungen und Potentiale von KI-Foundation-Modellen für die deutsche KI- Entwicklung kritisch untersucht, Strategien und Instrumente für die Umsetzung der LEAM- Ziele benannt und inhaltlich konkretisiert sowie Handlungsoptionen ausgearbeitet und miteinander verglichen. Die Erkenntnisse der Machbarkeitsstudie sind wegweisend für die Entwicklung der Künstlichen Intelligenz in Deutschland und entscheidend für die Innovationskraft des europäischen KI-Ökosystems und der digitalen Souveränität von Deutschland in der Zukunft.\\n\\n1 Die Begriffe “Große KI-Modelle” und “KI-Foundation-Modelle” sind synonym. Für diese Studie wird der Begriff “KI-Foundation-Modelle” benutzt, der international für diese Art der KI anerkannt ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n13\\n\\nZiele der Machbarkeitsstudie\\n\\nIn kürzester Zeit hat Künstliche Intelligenz gigantische Entwicklungssprünge gezeigt und damit selbst Technologieexpert:innen ins Staunen versetzt. Zurückzuführen ist dies insbesondere auf die großen Fortschritte im Bereich der großen KI-Modelle. Seit OpenAI GPT-3 eingeführt hat, wurden auf Basis der großen KI-Modelle, die auch Foundation- Modelle genannt werden, viele Anwendungen entwickelt.\\n\\nNeben den enormen Chancen, die sich daraus für die Arbeit und das Leben eröffnen, ergeben sich daraus auch einige Herausforderungen für die deutsche Wirtschaft, Wissenschaft und Gesellschaft, denn Europa ist im Wettbewerb um KI-Foundation- Modelle abgeschlagen. Für Deutschland heißt das konkret, Datenschutz und Datensicherheit verfolgen geringere Standards, Verzerrungen und mangelnde Datenqualität können aufgrund identifiziert und entgegengewirkt werden, und deutsche Unternehmen werden lediglich Nutznießer und keine Gestalter von Foundation-Modelle. Technologisch rutscht Deutschland damit in die Abhängigkeit.\\n\\nfehlender Transparenz nicht\\n\\nEine grundlegende Herausforderung dabei: Für die Erstellung von KI-Foundation- Modellen sind enorme Rechenkapazitäten und Ressourcen nötig, die im Vergleich zu den USA nicht ausreichend in Deutschland bereitstehen.\\n\\nZiel dieser Studie ist es ein Konzept zu erarbeiten, wie in Deutschland eine dezidierte KI- Recheninfrastruktur aufgebaut werden kann, die es ermöglicht KI-Foundation-Modelle zu trainieren und der Wirtschaft bereitzustellen. Dabei sollen vor allem die Bedürfnisse deutscher Unternehmen berücksichtigt werden.\\n\\nIndem die Studie Empfehlungen für die Ausgestaltung eines KI-Rechenzentrums und einer entsprechenden Servicegesellschaft gibt, möchte sie der privaten und öffentlichen Hand als eine Entscheidungsgrundlage für die Umsetzung von LEAM in Deutschland dienen.\\n\\nAutor:innen der Machbarkeitsstudie\\n\\nDie LEAM Machbarkeitsstudie wurde in Zusammenarbeit mit Alexander Thamm GmbH, Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), eco – Verband der Internetwirtschaft e. V., Fieldfisher LLP, Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS), Merantix Momentum GmbH, Simmons & Simmons und Ubermetrics Technologies GmbH durchgeführt. Die Gesamtprojektleitung oblag dem KI Bundesverband e.V..\\n\\nGroße KI-Modelle für Deutschland\\n\\n14\\n\\nGroße KI-Modelle für Deutschland\\n\\n15\\n\\nUnser Dank gilt auch dem gesamten Team des KI Bundesverbandes für die Umsetzung der LEAM-Konferenz und ihr Mitwirken im Rahmen der Studie: Daniel Abbou, Stefanie Baade, Dr. Sandra Bütow, Katharina Fischer, Franziska Fink, Phillip Handy, Benjamin Rodatz, Valentin Roth, Julia Sartisson und Esther Schragmann.\\n\\nGroße KI-Modelle für Deutschland\\n\\n16\\n\\nLeseanleitung und Dokumentenstruktur\\n\\nKapitel 1 bis 5 sind der Bestandsaufnahme gewidmet. Kapitel 1 beginnt mit einem Abschnitt über die wichtigsten Eigenschaften des neuen Paradigmas, gefolgt von einem Abriss des Standes der Technologie in der internationalen Forschung und Entwicklung (Kapitel 2). Dieser Abriss schildert auch vielfältige Anwendungen, die bereits realisiert und evaluiert wurden (Kapitel 2.2). Ein spezieller Abschnitt widmet sich den Technologien, die die Vertrauenswürdigkeit der KI-Foundation-Modelle sichern und kritische Fälle inadäquater Performanz verhindern sollen (Kapitel 2.3). Dem schließt sich eine Analyse des internationalen Wettbewerbs an, die auf einer Zusammenstellung aller bisher veröffentlichten KI-Foundation-Modelle beruht (Kapitel 3). In dieser Analyse werden auch die Ursachen des festgestellten Ungleichgewichts zwischen den USA, China und Deutschland bzw. Europa diskutiert.\\n\\ninternationalen KI-Entwicklung Nach dem Überblick zum aktuellen Stand der konzentrieren wir uns in den Kapiteln 4 und 5 auf die Situation in Deutschland. Dazu beleuchten wir insbesondere die Bedarfe der deutschen Wirtschaft und legen dar, wie die Forschung die Entwicklung der benötigten europäischen KI-Foundation-Modelle unterstützen kann. Zu diesem Zweck wurden zahlreiche Expert:innen in Wissenschaft und Wirtschaft befragt.\\n\\nDas sechste Kapitel zieht die Konsequenzen aus den Ergebnissen der vorangegangenen Kapitel. Die Untersuchung hat das große Potenzial der Technologie, gleichzeitig aber auch einen immensen Bedarf an Foundation-Modellen ermittelt. Dieses Kapitel beschriebt, welche Modelle vordringlich entwickelt werden sollten und könnten, ob und wie man die Daten für diese Modelle bekommen kann und welche Optionen es für künftige Modellentwicklungen gibt.\\n\\nDie folgenden zwei Kapitel untersuchen die Anforderungen, die an High-Performance- Computing-Systeme (HPC) und speziell an ein benötigtes KI-Hochleistungsrechenzentrum gestellt werden. In Kapitel 7 wird erklärt, wie die dafür notwendige Software-Architektur aussehen kann und wie daraus Anforderungen an Hardware und Personal abgeleitet werden können. Dabei zeigen wir auf, wie sich diese Architektur mittels Open-Source- Software (OSS) aufbauen lässt, um die Wichtigkeit von OSS für ein wettbewerbsfähiges KI- in Ökosystem zu betonen. Dabei wird die technische Machbarkeit unmittelbar Zusammenhang mit den wissenschaftlichen und infrastrukturellen Kapiteln dieser Machbarkeitsstudie gestellt, um so den gesellschaftlichen Mehrwert des Betriebs eines kompetitiven KI-Hochleistungsrechenzentrums zu erläutern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n17\\n\\nKapitel 8 betrachtet insbesondere die technischen Voraussetzungen, die von KI- Foundation-Modellen an die Infrastruktur eines entsprechenden Rechenzentrums gestellt werden. Außerdem wird eine Übersicht über die HPC-Kapazitäten im Bereich der Forschung und der kommerziellen Anbieter gegeben, die bereits für KI-Anwendungen geeignete HPC-Cloudlösungen anbieten. Es werden außerdem die nötigen Schritte für die Standortermittlung und den Aufbau eines KI-Hochleistungsrechenzentrums mit passender Gebäudeinfrastruktur beleuchtet. Weitere Möglichkeiten wie Collocation- Betrieb oder der mögliche Bezug von HPC-Rechenleistungen aus der Cloud werden anhand von Beispielen untersucht. Beim Betrieb eines HPC-Rechenzentrums sind mit Blick auf die Entwicklung der Strompreise und Anforderungen des Klimaschutzes insbesondere die Energieeffizienz der Systeme sowie der Nachhaltigkeitsaspekt beim Betrieb eines KI-Rechenzentrums wichtige zu untersuchende Faktoren.\\n\\nKapitel 9 bis 13 beleuchten die organisatorische, wirtschaftliche und juristische Machbarkeit eines KI-Hochleistungsrechenzentrums. Wir schlagen die Einrichtung eines LEAM-KI-Servicezentrums vor, das seine Dienste der Wirtschaft und Wissenschaft anbietet. Dafür werden verschiedene Organisationsmodelle, Gesellschaftsstrukturen und Finanzierungsmöglichkeiten näher beleuchtet und diskutiert. Darüber hinaus findet eine rechtliche Bewertung der Optionen statt. Das Kapitel schließt mit einem Szenario zur Realisierung eines LEAM-KI-Servicezentrums.\\n\\nDie Ergebnisse werden in Kapitel 14 zusammengefasst. Das Kapitel gibt außerdem die Empfehlung ab, eine Projektentwicklungsgesellschaft zu gründen, die das Thema weiter vorantreibt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n18\\n\\nDas Paradigma der KI-Foundation-Modelle\\n\\nGroße KI-Modelle für Deutschland\\n\\n19\\n\\n1. Das Paradigma der KI-Foundation-Modelle KI-Foundation-Modelle schreiben das neueste Kapitel in der Erfolgsgeschichte der Künstlichen Intelligenz.\\n\\nKI-Foundation-Modelle sind große neuronale KI-Modelle, die auf gigantischen Mengen generischer Daten vortrainiert wurden. Das Besondere an diesen Modellen ist, dass sich das im sogenannten Vortraining (pre-training) erworbene implizite Wissen als Grundlage für viele verschiedene Anwendungen eignet. Für manche Anwendungen ist sogar kein oder nur minimales zusätzliches Training erforderlich. Andere Anwendungen erfordern zwar weiterhin ein Training auf anwendungsspezifischen Lerndaten, wobei aber oft viel weniger Lerndaten benötigt werden oder aber eine Leistungsfähigkeit erreicht wird, die ohne das generische Vortrainieren nicht möglich war.\\n\\nDer breiten Öffentlichkeit bekannt geworden ist das neue Paradigma spätestens seit dem großen Medienecho auf GPT-3, einem großen Sprachmodell des kalifornischen KI-Labors OpenAI. GPT-3 wurde 2020 veröffentlicht und gilt als spektakulärer Durchbruch auf dem Gebiet der intelligenten Sprachtechnologie. Das KI-Modell kann anspruchsvolle Texte verfassen, Fragen beantworten, Sätze ergänzen und nebenbei auch noch ganz passabel übersetzen. Derzeit gilt die mediale Aufmerksamkeit dem neuen KI-System ChatGPT, Journalist:innen und Testbenutzer:innen einer Variante des GPT-3 Modells, das gleichermaßen fasziniert aber manchmal auch verschreckt, weil es so eloquent parliert und meist informative, bedachte und ausgewogene Antworten auch auf schwierige Fragen gibt und nach Benutzervorgaben mitunter sogar druckreife Texte für die verschiedensten Zwecke formuliert. Und all das in mehreren Sprachen.\\n\\nAbb. 1: Anzahl der Parameter großer KI-Sprachmodelle seit GPT-3 (Open Source Modelle rot markiert). Quelle: state of ai Report 2022 (Benaic & Hogarth, 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n20\\n\\nMöglich wurde diese erfolgreiche Innovation durch eine neue Architektur für neuronale Netze, Transformer genannt, die 2017 in der Google-Forschung entwickelt und zuerst erfolgreich in der automatischen Textübersetzung getestet wurde. 2018 folgten zwei KI- Foundation-Modelle, die jeweils nur Teile der Transformer-Architektur realisieren: GPT, der Vorläufer von GPT-3 von OpenAI und BERT, ein einflussreiches bidirektionales Sprachmodell aus der Google-Forschung.\\n\\nDie Grundidee des Transferlernens ist die Nutzung von bereits trainierten Netzen für neue Aufgabenstellungen. Anstatt für eine Anwendung ein neues Netzwerk zu trainieren, verwendet man ein Netzwerk, das bereits für eine andere Aufgabenstellung vortrainiert wurde. So kann durch den Einsatz des vortrainierten Netzes der Bedarf an annotierten (labeled) anwendungsspezifischen Lerndaten reduziert werden. Gelingt es nun, Aufgaben für das Vortraining so zu wählen, dass entweder hinreichend große Mengen an bereits annotierten Lerndaten vorhanden sind oder aber die annotierten Daten vollautomatisch hergestellt werden können, dann verringert sich der Aufwand für die Datenannotierung immens.\\n\\nDie ersten KI-Foundation-Modelle waren große Sprachmodelle. Sie unterscheiden sich von früheren KI-Modellen durch ihre vielseitige Verwendbarkeit. Möglich geworden ist diese Vielseitigkeit durch eine zentrale inhärente Eigenschaft der neuen KI- Kerntechnologie, die als Emergenz bezeichnet wird. Damit ist eine neue Stufe in der Evolution der Künstlichen Intelligenz erreicht. Bisherige neuronale KI-Modelle waren immer auf eine bestimmte Anwendung ausgerichtet. Sie beruhten in der Regel auf Training durch überwachtes (supervised) oder semi-überwachtes (semi-supervised) Lernen. Aber im Gegensatz zur menschlichen Intelligenz konnten die erworbenen Fähigkeiten nicht für weitere Aufgabenstellungen genutzt werden.\\n\\nDeshalb wurde diese Künstliche Intelligenz auch treffend als schmale KI (Narrow AI) bezeichnet und als solche der sogenannten Künstlichen Allgemeinen Intelligenz (Artificial General Intelligence - AGI) gegenübergestellt.\\n\\nNun gibt es aber viele Anwendungen, welche die Kombination von Aufgaben erfordern. So kann ein Voice-Chatbot aus der Kombination von Spracherkennung, semantischer Analyse, Fragenbeantwortung und akustischer Sprachausgabe bestehen, die jeweils durch eigene Modelle realisiert sind. Hier kann es zu Inkonsistenzen kommen, indem zum Beispiel eins dieser Modelle Wörter oder Begriffe kennt, die ein anderes nicht gelernt hat. Die Homogenisierung der Modelle hat diese Inkonsistenzen reduziert. Die KI-Foundation- Sprachmodelle können das vortrainierte implizite Sprachwissen bereits für mehrere Anwendungen einsetzen. Es kommt nicht mehr vor, dass eine Anwendung oder Komponente, Begriffe oder Satzstrukturen nicht kennt, die von einer anderen beherrscht werden. Das Paradigma der versatilen Foundation-Modelle stellt so den nächsten Schritt in Richtung einer breiteren KI dar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n21\\n\\nDie Bezeichnung Foundation-Modelle wurde erst im August 2021 durch das neue Center for Research on Foundation Models (CRFM) an der Stanford Universität vorgeschlagen (Bommasani et al., 2021), das sich als Teil des Stanford Institute for Human-Centered Artificial Intelligence\\'s (HAI) ganz dediziert dem neuen KI-Paradigma widmet. Der Terminus ist treffender als der Begriff große Sprachmodelle, denn Foundation-Modelle müssen nicht immer Sprachmodelle sein, sondern können z.B. auch auf Bildern, Videos oder DNA-Sequenzen trainiert und dann jeweils für eine Vielzahl von Anwendungen eingesetzt werden. Zudem gibt es auch große Sprachmodelle, die mit entsprechend ausgewählten und annotierten Daten nur für eine spezielle Anwendung, zum Beispiel maschinelle Übersetzung, trainiert wurden und sich somit nicht als Grundlage (Foundation) für viele verschiedene Anwendungen eignen.\\n\\nWeil die vortrainierten, vielseitigen Foundation-Modelle gegenwärtig die Diskussion um die nächsten Durchbrüche der KI dominieren, wurde der intuitive neue Begriff sehr schnell von der internationalen Forschungsgemeinschaft aufgegriffen. In ihrem initialen Positionspapier zu dem Forschungsthema schildern die Stanford-Wissenschaftler nicht nur das Anwendungspotenzial der bereits existierenden Modelle, sondern sie argumentieren auch überzeugend, dass das Paradigma der Foundation-Modelle die nächste Entwicklungsstufe der KI bestimmen wird, in der die Modelle Fähigkeiten aufweisen werden, die bis vor Kurzem noch undenkbar schienen und die den Menschen bei vielen Aufgaben übertreffen.\\n\\nIm gleichen Artikel schildern sie aber auch die Risiken, die entstehen können, wenn diese mächtige Technologie die Konzentration von technologischer und wirtschaftlicher Macht in der Hand einiger weniger IT-Konzerne verschärft. Die bloße Verfügbarmachung von fertigen Modellen genügt nicht, um die wirtschaftlichen und sozialen Interessen der Gesellschaft zu sichern und den Missbrauch der Technologie wirksam zu verhindern.\\n\\nGroße KI-Modelle für Deutschland\\n\\nKapitel2\\n\\n22\\n\\nTechnologische Grundlagen\\n\\nGroße KI-Modelle für Deutschland\\n\\n23\\n\\n2. Technologische Grundlagen Sprachmodelle gehören zu den Grundwerkzeugen der maschinellen Sprachverarbeitung (Natural Language Processing). Das sind mathematische Modelle, die bestimmen können, ob gewisse Sätze oder Äußerungen zur Sprache gehören oder nicht, beziehungsweise mit welcher Wahrscheinlichkeit sie das tun. Bereits die ersten Versionen von Siri, Alexa oder Google Translate nutzten stochastische Sprachmodelle, die für jede Abfolge von drei, vier oder fünf Wörtern die Wahrscheinlichkeit gelernt hatten, in genau dieser Reihenfolge in Texten oder gesprochenen Äußerungen vorzukommen. Mit solchen Modellen, die damals noch nicht als neuronale Netze realisiert wurden, konnte die Korrektheit oder Natürlichkeit verbessert werden. Man hat die Wahrscheinlichkeiten aber auch verwendet, um bei der Analyse gesprochener Eingaben Unsicherheiten in der akustischen Erkennung von Wörtern aufzulösen. Diese Wahrscheinlichkeiten waren durch syntaktische und semantische Faktoren bestimmt, insbesondere durch grammatische Regularitäten und Wortbedeutungen. Schon früh wurde in der Sprachverarbeitung deshalb die Idee entwickelt, die Bedeutung von Wörtern durch die Wörter in der Nachbarschaft zu erklären. Leider gibt die Schreibweise von Wörtern nur wenig Aufschluss über ihre Bedeutung. Daher entstand schon vor längerer Zeit die Idee, die Bedeutung eines jeden Wortes durch einen langen Vektor, eine Einbettung, zu repräsentieren. Allerdings stellte sich heraus, dass viele Wörter je nach Kontext unterschiedliche Bedeutungen haben. Beispielsweise kann „Bank“ ein Sitzmöbel oder ein Finanzinstitut sein. Vor fünf Jahren wurde von Google- Wissenschaftler:innen diese Bedeutungsunterschiede mit kontextsensitiven Einbettungen erfassen kann (Vaswani et al., 2017). Transformer sind eine Variante der tiefen Neuronalen Netze, die seit 2012 weite Teile der KI revolutioniert haben. Das wirklich Neue an diesen Transformern ist, dass sie die Einbettung des Kontextes in vergleichbar effizienter Weise berechnen können. Dies war zuvor nicht möglich, die Kontexte beschränkten sich vorher nur auf wenige Worte. Ähnlich wie wir Menschen, erkennen die Transformer dabei die relevanten Worte in weiteren Kontexten, auch über Satzgrenzen hinweg, und können dadurch den semantischen Bezug herstellen.\\n\\nder\\n\\nTransformer\\n\\nentwickelt,\\n\\nwelcher\\n\\nDa die Anzahl der unterschiedlichen Wörter der Sprache durch Verbindung von einfachen Wörtern zu zusammengesetzten Wörtern nahezu unbeschränkt ist, verwendet neuere Sprachmodelle statt der Wörter ein beschränktes Vokabular von Token (Teilwörtern und häufigen Wörtern), aus denen man jedes Wort zusammensetzen kann. Die Tokenisierung wird, genauso wie das KI-Modell, auf Trainingsdaten trainiert und ist somit Bestandteil des Modells.\\n\\nHerzstück des Transformers ist der Self-Attention Block, der in sehr flexibler Weise die Bedeutungsrelationen zwischen verschiedenen Token durch Korrelationen der jeweiligen Einbettungen ermittelt und damit neue Einbettungen konstruiert. Mit ihm wurden Sprachmodelle mit Milliarden von Parametern trainiert. Paradebeispiel ist GPT-3 (Brown et al., 2020), das syntaktisch und inhaltlich stimmige Texte von bisher unerreichter Qualität produzieren kann.\\n\\nGroße KI-Modelle für Deutschland\\n\\n24\\n\\nDie Ableitung von kontextsensitiven Einbettungen lässt sich am besten mit dem BERT- Modell (Devlin et al., 2019) erläutern: Jedem Token des Eingabetextes wird ein Einbettungsvektor zugeordnet, der die semantische Bedeutung des Tokens repräsentiert und ein weiterer Vektor, der die Position des Tokens im Text markiert. Diese Einbettungsvektoren sind Parameter und werden im Laufe des Trainings angepasst.\\n\\nDas Verfahren der Self-Attention kann nun in dem Satz „Die Bank verleiht Geld“ die Einbettung von „Bank“ durch die Berücksichtigung der Einbettung von „Geld“ so abändern, dass die Bedeutung „Finanzinstitut“ betont wird. Dazu berechnet es die „Korrelation\" (Skalarprodukt) der Einbettung von Bank mit denen sämtlicher anderen Token (Abb. 2). Dies geschieht für komplementäre „Aspekte\" der Einbettungen, die durch Parameter ausgedrückt werden (Attention-Heads). Schließlich werden die mit den Korrelationen gewichteten Einbettungen aller Token addiert, um eine neue Einbettung für das Token „Bank\" zu erzeugen, die der Bedeutung von „Geld\" Rechnung trägt. Jeweils eine Self-Attention Schicht mit mehreren Attention-Heads sowie eine anschließende nichtlineare voll verbundene Schicht von Neuronen bilden einen Encoderblock, welcher grundlegender Bestandteil fast aller Foundation-Modelle ist.\\n\\nAbb. 2: Verdeutlichung der Self-Attention am Satz \"Die Bank verleiht Geld\". Das Token Bank (unten) hat eine hohe Korrelation mit dem Token Geld (oben), wobei die Korrelation zu den anderen Token geringer ausfällt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n25\\n\\nPrimäres Ziel des BERT-Modells ist es, kontextsensitive Einbettungsvektoren zu bestimmen, die möglichst viele Informationen über die jeweiligen Token eines Textes enthalten. Zunächst werden die Parameter mit Zufallszahlen initialisiert. Weiter werden im Eingabetext ein Teil der Eingabetoken durch „MASK“ ersetzt. Für jeden Eingabetoken berechnet BERT als Autoencoder mit Hilfe einer Reihe von Encoderblöcken die kontextsensitiven Einbettungen, welche die semantische Bedeutung des Tokens erfassen und mit jeder Schicht besser werden. Trainingsziel ist es, aus den besonders aussagekräftigen Einbettungen in der obersten Schicht an der Position eines maskierten Tokens eine möglichst hohe Wahrscheinlichkeit für das maskierte Token zu prognostizieren. Dies geschieht durch Optimierung der Parameter. Da Teile der beobachteten Daten prognostiziert werden müssen und keine menschlichen Annotationen benötigt werden, spricht man hier von selbstüberwachtem Lernen.\\n\\n(Devlin et al., 2019)\\n\\nDas erworbene Wissen über Sprache kann man in einem zweiten Schritt auswerten, indem man BERT für eine weitere Klassifikationsaufgabe trainiert. Im einfachsten Fall wird für einen zusätzlichen „CLS“-Token eine kontextsensitive Einbettung berechnet, aus der mit Hilfe eines logistischen Regressionsmodells die Wahrscheinlichkeit der beobachteten Klasse prognostiziert werden kann. Ein Beispiel ist die Sentimentanalyse, bei der das Modell entscheiden muss, ob der Eingabetext eine negative oder positive Bewertung enthält. Diese zweite Trainingsaufgabe, auch Finetuning genannt, benötigt einen von Menschen annotierten Trainingsdatensatz. Obwohl das Finetuning alle Parameter des Modells anpasst, benötigt es nur einen kleinen Bruchteil des Trainingsaufwandes für das Basismodell, sodass die annotierten Trainingsdaten für das Finetuning meist relativ wenige Beispiele umfassen muss. Zur Unterscheidung wird die erste Trainingsaufgabe mit einem großen allgemeinen Textkorpus ohne Annotationen auch als Vortraining bezeichnet. BERT kann für viele semantische Klassifikationsaufgaben die Genauigkeit bisherige Modell wesentlich verbessern. Die Übertragung von erworbenem Wissen von einem Lernproblem auf ein anderes, aber verwandtes Problem nennt man Transferlernen.\\n\\nSprachmodelle verwenden ebenfalls Schichten von Self-Attention Modulen. Sie werden aber nicht darauf trainiert, maskierte Token innerhalb eines Textes zu prognostizieren, sondern sollen für einen bestehenden Anfangstext das nächste Token vorhersagen. Hierbei werden mehrere Schichten von Encoderblöcken auf die bisher bekannten Worte des Textes angewendet. Die kontextsensitive Einbettung des letzten bekannten Wortes in der obersten Schicht bildet dann die Eingabe für ein logistisches Regressionsmodell, das die Wahrscheinlichkeit der unterschiedlichen Token an der nächsten Position prognostiziert. Während des Trainings wird das Modell so angepasst, dass diese letzte Einbettung möglichst viel Informationen über das nächste Token enthält und die Token der Trainingsmenge eine hohe Wahrscheinlichkeit erhalten. Sprachmodelle wie GPT-3 (Brown et al., 2020) und PaLM (Chowdhery et al., 2022) sind in der Lage, Anfangstexte syntaktisch fehlerfrei und inhaltlich überwiegend stimmig fortzusetzen, indem sie einen Token nach dem anderen generieren. Die Auswahl des nächsten Tokens findet dabei zufällig gemäß den abgeschätzten Wahrscheinlichkeiten statt, so dass bei einer Wiederholung immer ein neuer Text entsteht.\\n\\nGroße KI-Modelle für Deutschland\\n\\n26\\n\\nDarüber hinaus kann ein Sprachmodell auch Anweisungen ausführen. Beispielsweise antwortet GPT-3 auf den Starttext “Create an outline for an essay about Walt Disney: I: Introduction” mit einem detaillierten Text über Walt Disney. Man kann also ohne Zusatztraining erreichen, dass GPT-3 eine bisher unbekannte Aufgabe löst. Häufig kann man die Qualität der Antworten noch durch zusätzliche Beispiele verbessern, z.B. durch die Instruktion „English: I do not speak French. French: Je ne parle pas français. English: Where is the restroom? French:“. GPT-3 erkennt die Eingabe als Aufforderung zur Übersetzung und liefert die französische Übersetzung. Dieses „k-shot Learning” eröffnet völlig neue Möglichkeiten zur Nutzung von Sprachmodellen, ohne zusätzliches Finetuning. Allerdings ist die Genauigkeit oft höher, wenn das Sprachmodell durch Finetuning für die neue Aufgabe trainiert wird.\\n\\nSequence-to-Sequence-Modelle (seq2seq) übersetzen eine Sequenz von Token in eine andere Sequenz. Wichtigstes Anwendungsgebiet ist die Übersetzung eines Textes in eine andere Sprache. Der Prototyp dieser Architektur wurde von (Vaswani et al., 2017) vorgestellt:\\n\\nDer Encoder ist ein BERT-Modell, welches kontextsensitive Einbettungen der Eingabetoken berechnet.\\n\\nDer Decoder arbeitet wie ein Sprachmodell und wird auf die bisher erzeugten Token der Übersetzung angewendet. Jeder Decoderblock enthält mehrere Self-Attentions, die die Korrelation mit den schon generierten Token der Übersetzung auswerten. Zum anderen werden über sogenannte Cross-Attentions die Informationen in den Einbettungsvektoren der Eingabetoken berücksichtigt. Aus den Einbettungen des obersten Decoderblocks wird die Wahrscheinlichkeit der Token für die nächste Wortposition der Übersetzung berechnet.\\n\\nDie Trainingsmenge enthält Paare von Eingabetexten und deren Übersetzungen. Beim Training werden die Parameter von Encoder und Decoder gleichzeitig so angepasst, dass die Wahrscheinlichkeiten der korrekten Ausgabetoken möglichst hoch werden.\\n\\nEs ist bemerkenswert, dass der ursprüngliche Encoderblock mit Multi-Head-Self-Attention immer noch von fast allen Foundation-Modellen verwendet wird. Abbildung 3 zeigt die Gemeinsamkeiten in der Struktur von BERT-Encoder, Sprachmodell und Transformer Encoder-Decoder.\\n\\nGroße KI-Modelle für Deutschland\\n\\n27\\n\\nAbb. 3: Zentraler Bestandteil der Foundation-Modelle sind Schichten mit Self-Attention Blöcken (blau), die kontextsensitive Einbettungsvektoren (violett) von Eingabetoken (grün) berechnen. Die logistische Schicht L prognostiziert die Wahrscheinlichkeit der Ausgabetoken. Beim Training werden die Parameter so optimiert, dass die Wahrscheinlichkeiten der korrekten fehlenden bzw. nächsten Token (gelb) möglichst hoch sind.\\n\\nIn den letzten Jahren wurde eine Reihe von Verbesserungen für die ursprüngliche Architektur gesucht. Bei der Self-Attention wächst der Rechenaufwand und der Speicherbedarf quadratisch mit der Länge der Eingabesequenz. Daher wurden Varianten entwickelt, bei denen der Aufwand nur noch linear ansteigt. Dies ist eine signifikante Verbesserung, die es ermöglicht, sehr viel längere Texte als Eingabesequenz zu geben. Dadurch können Sprachmodelle auf Textstellen weiter vorne im Text Bezug nehmen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n28\\n\\nFoundation-Sprachmodelle können gleichzeitig auf Dokumenten unterschiedlicher Sprachen trainiert werden, wenn ein gemeinsames Tokenvokabular verfügbar ist. Dies ermöglicht z.B. Frage-Antwort Systeme, die Informationen aus unterschiedlichen Sprachen in eine Antwort integrieren können. Dabei stellte sich heraus, dass die Modelle selbst dann die Bedeutung der Wörter in unterschiedlichen Sprachen in Beziehung setzen können, wenn jedes Trainingsdokument nur in einer Sprache verfasst ist (Liu et al., 2020).\\n\\nAuch große Foundation-Modelle können nur begrenzt Informationen in ihren Parametern speichern. Daher wurde mit unterschiedlichen Ansätzen versucht, zusätzliche Informationen in Foundation-Modelle einzubeziehen. Dabei werden hauptsächlich folgende Ansätze verfolgt:\\n\\nDie zusätzlichen Informationen werden sprachlich formuliert. Dies funktioniert sowohl für Tabelleninhalte (Yin et al., 2020) als auch für Wissensbasen. Beispielsweise verbalisiert TekGen (Agarwal et al., 2021) die komplette Wikidata Wissensbasis mit Hilfe des T5 Seq2seq-Modells. Dieser Datenbestand kann dann zum Beispiel als zusätzliche Trainingsdaten für ein Sprachmodell verwendet werden.\\n\\nDer vielversprechendste Ansatz ist die Verwendung von Retrievaltechniken. Viele aktuelle Sprachmodelle nutzen ein Retriever-Reader-Modul, um die gefundenen Dokumente zu berücksichtigen (Izacard and Grave, 2021).\\n\\nGroße KI-Modelle für Deutschland\\n\\n29\\n\\nAbb. 4: Ein Foundation-Modell kann Informationen aus verschiedenen Datenquellen verschiedener Modalitäten berücksichtigen. Dieses eine Modell kann dann eine Vielzahl von nachgelagerten Aufgaben lösen (Bommasani et al., 2021).\\n\\nEs ist nun möglich, auch andere Medieninhalte durch Token zu repräsentieren. Ein Bild kann zum Beispiel in kleine quadratische Pixelbereiche unterteilt und so durch Bildtoken dargestellt werden. Die Sprachmodelle lassen sich in nahezu unveränderter Form auf diese alternativen Tokensequenzen anwenden und sind in der Lage, die Leistung bestehender\\n\\nGroße KI-Modelle für Deutschland\\n\\n30\\n\\nModelle zu verbessern. Besonders beeindruckend ist die Mischung verschiedener Modalitäten mit denen beispielsweise aus einem Text ein Bild erzeugt werden kann. Abbildung 4 verdeutlicht das Vortraining dieser Modelle mit gleicher Architektur auf verschiedenen Medien und die Anwendung auf unterschiedlichste Aufgabenbereiche. Diese Modelle haben für eine extrem große Anzahl von Fragestellungen der Künstlichen Intelligenz den Stand der Kunst verbessert und werden daher als Foundation-Modelle bezeichnet (Bommasani et al., 2021). Sie bilden die Grundlage für eine Vielzahl weiterer KI-Anwendungen. Einen aktuellen Überblick über Foundation-Modelle gibt die Monographie von Paaß et al. (2023).\\n\\nDie hervorstechenden Merkmale des neuen Paradigmas der Foundation-Modelle sind Emergenz und Homogenisierung:\\n\\nEmergenz bezeichnet den Umstand, dass ein Foundation-Modell Fähigkeiten aufweist, die nicht explizit konstruiert, sondern implizit gelernt werden. Ein Beispiel ist das GPT-3-Modell, welches eine neue Aufgabe lösen kann, indem man es durch eine natürlichsprachliche Beschreibung der Aufgabe, den so genannten Prompt, instruiert. Das Modell kann also eine Aufgabe lösen, obwohl es nie dafür trainiert wurde.\\n\\nHomogenisierung rührt daher, dass nahezu alle aktuellen Modelle die Architektur von wenigen Foundation-Modellen (insb. BERT, RoBERTa, T5, GPT-3) implementieren. Dies bewirkt eine Vereinheitlichung von der Modellierung für ein breites Spektrum von Anwendungen. Folglich kann ein Modell durch Anpassungen für viele verschiedene Aufgaben verwendet werden. Ähnliche Ansätze zur Sequenzmodellierung können für Texte, Bilder, gesprochene Sprache, Video, DNA- Sequenzen u.ä. verwendet werden. Dies hat aber auch den möglichen Nachteil, dass diese KI-Systeme die gleichen problematischen Vorurteile oder Fehler einiger weniger Grundmodelle erben können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n31\\n\\n2.1 Die Bedeutung der Größe von KI-Foundation-Modellen\\n\\nDie Größe eines Modells, insbesondere die Anzahl der Parameter, hat einen entscheidenden Einfluss auf die Leistung des Modells, seinen Speicherbedarf und die Rechenressourcen für das Training. Kaplan et al. (2020) untersuchten empirisch die Abhängigkeit zwischen der Anzahl der Modellparameter, dem Umfang der Trainingsdaten und dem Rechenaufwand für das Training. Sie bewerteten eine große Anzahl von Modellen und zogen die folgenden Schlussfolgerungen:\\n\\nDie Leistung der Modelle hängt weitgehend von diesen drei Größen ab. Andere Architekturmerkmale wie Breite oder Tiefe haben nur einen schwachen Einfluss.\\n\\nWerden Modellgröße und Trainingsdaten in gleichem Maße erhöht, wächst die Modellgenauigkeit zuverlässig über einen großen Bereich des Rechenaufwandes. Wenn einer dieser Faktoren konstant gehalten wird, fällt die Verbesserung geringer aus und nähert sich einer Schranke.\\n\\nDies erklärt auch den Erfolg von großen Foundation-Modellen wie T5, GPT-3 oder PaLM. Allerdings erfordert das Training großer Modelle eine extrem leistungsfähige Infrastruktur.\\n\\nAbb. 5: Die Genauigkeit des „few-shot\"-Lernens von GPT-3 wird durch die Erweiterung der Modellgröße und der Anzahl der präsentierten Beispiele erhöht (Brown et al., 2020).\\n\\nAbbildung 5 zeigt, dass wichtige Eigenschaften von Foundation-Modellen erst für große Modelle beobachtet werden können (Emergenz). Während das GPT-3-Modell mit 13 Mrd. Parametern natürlichsprachige Anweisungen mit einer Genauigkeit von etwa 20 % beantworten kann, steigt dieser Anteil bei 175 Mrd. Parametern auf über 60 % (Brown et al., 2020). Offenbar benötigen die Modelle ein großes Geflecht von Beziehungen zwischen Begriffen, um korrekt auf natürlichsprachige Prompts zu reagieren. Eine mögliche Folge der Emergenz ist, dass es eine Reihe von Aufgaben gibt, die für die derzeitigen Foundation-Modelle unerreichbar sind, die aber bald erfolgreich bewältigt werden könnten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n32\\n\\nEine einfache Möglichkeit, die Anzahl der Parameter ohne höheren Trainingsaufwand zu erhöhen, ist eine Mixture-of-Experts-Architektur. Sie besteht aus einem einzigen Gating- Modul und einer Reihe von Expertenmodulen mit identischer Architektur, aber unterschiedlichen Parametern. Jedes Expertenmodul ist nur auf eine Teilmenge der Daten jede Eingabe den wenigen (z.B. 2) spezialisiert, und das Gating-Modul ordnet Expertenmodulen zu. Diese Zuordnung wird automatisch optimiert, so dass das gesamte Modell die optimale Leistung erbringt. Eine Verringerung des Rechenaufwands kann erreicht werden, da nur wenige Expertenmodule für eine Eingabe tatsächlich verwendet werden. Die Architektur ermöglicht massive Modelle und ist besonders effizient für verteilte Systeme, bei denen die Experten auf verschiedene Recheneinheiten verteilt sind. Beispiele sind Switch, GLaM und WuDao-2.0.\\n\\nDa die ersten Versionen erfolgreicher Modelle oft extrem groß sind, wurden verschiedene Techniken zur Komprimierung und Beschleunigung der Modelle entwickelt. Wissensdestillation (Hinton et al., 2015) überträgt das Wissen von einem großen Lehrermodell auf ein kleineres Schülermodell. Der Vorteil dieses Ansatzes ist, dass das Schülermodell beim Training die internen Aktivierungen des Lehrermodells nutzen kann. Für eine Reihe von Modellen ergab sich eine deutliche Reduzierung des Speicher- und Rechenaufwands bei nahezu identischer Leistung.\\n\\nFoundation-Modelle benötigen enorm viele Daten für das Training. Beispielsweise verwendet das PaLM-Modell 780 Milliarden Token (25-mal mehr als alle Texte in Wikipedia), die einen großen Bereich natürlicher Sprache abdecken (Chowdhery et al., 2022). Es ist wichtig, dass die Texte eine hohe syntaktische und inhaltliche Qualität besitzen, da sonst grammatikalische und fachliche Fehler sowie Vorurteile von dem Modell reproduziert werden. Texte mit niedriger Qualität findet man oft in sozialen Medien oder Nutzer:innenkommentaren im Internet. Sie sind nicht zum Training geeignet. Die Kuratierung der Datenqualität ist daher einer der aufwändigsten und kostspieligsten Aspekte des Trainings von Foundation-Modellen. Diese Aspekte werden im Folgenden noch ausführlich diskutiert.\\n\\nDie Leistung der ersten Foundation-Modelle wurde mit wenigen Benchmarks (z.B. GLUE) überprüft. Mittlerweile sind KI-Foundation-Modelle so leistungsfähig, dass sie Weltwissen aus vielen Bereichen abdecken und sogar Schlussfolgerungen durch die Kombination von Fakten ziehen können (Zhang et al., 2022). Es hat sich herausgestellt, dass viele Benchmarks nach einer gewissen Zeit saturiert sind und die Fortschritte aktueller Modelle nicht mehr erfassen können. Daher werden nun Batterien von mehreren hundert Benchmarks verwendet (z.B. BIG-Bench), die eine Vielzahl von Aufgaben abdecken, darunter logisches Denken, Übersetzung, Beantwortung von Fragen, Mathematik und andere. Mittlerweile übertrifft PaLM mit 5-shot Prompts die Leistung durchschnittlicher menschlicher Bearbeiter (Chowdhery, et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n33\\n\\n2.2 Anwendungsgebiete von KI-Foundation-Modellen\\n\\nIn diesem Abschnitt stellen wir die vielfältigen Anwendungsmöglichkeiten der Foundation-Modelle vor.\\n\\nDabei gehen wir erst auf Anwendungen im Sprachbereich ein, und beschreiben dann Anwendungen im Bereich Multimedia, Biowissenschaften und Robotersteuerung. Die hier aufgeführten Anwendungsgebiete stellen lediglich eine Übersicht dar und sind nicht vollständig. In vielen Bereichen lassen sie die möglichen Anwendungen aktuell noch gar nicht abschätzen.\\n\\nDie folgenden Abschnitte diskutieren exemplarisch relevante Anwendungsmöglichkeiten aus ganz unterschiedlichen Bereichen.\\n\\n2.2.1 Anwendungen im Sprachbereich\\n\\nAnwendung\\n\\nBeschreibung\\n\\nInformationsextraktion\\n\\nDokumentensuche\\n\\nBeantwortung von Fragen\\n\\nMaschinelle Übersetzung\\n\\nErzeugung von Computercode\\n\\nextrahiert Konzepte und Namen aus dem Text und gegebenenfalls deren Relationen findet passende Texte zu einer Anfrage. Dabei werden auch sinnverwandte Formulierungen berücksichtigt. erzeugt eine Antwort zu einer Frage. Grundlage sind die Informationen aus dem Vortraining und die Ergebnisse einer Dokumentensuche. Die Antwort kann ggf. erklärt werden. übersetzt einen Text aus einer Sprache in eine andere. Dies ist mit dem gleichen Modell für mehr als 100 Sprachen möglich. erzeugt aus einer natürlichsprachlichen Beschreibung lauffähigen Source-Code in ganz unterschiedlichen Programmiersprachen\\n\\nZusammenfassung und Vereinfachung von Dokumenten\\n\\nerfasst die wichtigsten Aussagen eines oder mehrerer Dokumente und verfasst eine vereinfachte Version\\n\\nGenerierung neuer Texte\\n\\nTextkorrektur\\n\\nDialogsysteme\\n\\nMeinungs- und Sentimentanalyse\\n\\nerzeugt eine inhaltliche kohärente Fortsetzung eines Textes. Dabei können inhaltliche Vorgaben berücksichtigt werden. verbessert und kontrolliert Texte in Bezug auf Rechtschreibung, Grammatik, Stil, Formatierung, Wirksamkeit oder Terminologie führen ein längeres Gespräch mit einem menschlichen Dialogpartner. Dabei werden Informationen über den Dialogverlauf gespeichert und wiederverwendet. erkennt und klassifiziert Meinungen und emotionale Einstellungen zu Produkten, Personen, Organisationen, Ereignissen usw.\\n\\nEntdeckung von Fake News und Bot Texten\\n\\nerkennt Falschaussagen und automatische hergestellte Nachrichten\\n\\nTabelle 1: Eine Auswahl möglicher Anwendungen auf Basis von Sprachmodellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n34\\n\\n[Sprachmodell GPT-3]\\n\\nDem privatwirtschaftlichen Forschungsunternehmen OpenAI gelang mit dem Sprachmodell GPT-3 ein wissenschaftlicher Durchbruch: Das Modell ist in der Lage, das nächste Wort in einem Satz vorherzusagen. Schnell bildete sich eine Community aus Entwickler:innen und Nutzer:innen um das Modell und Applikationen wurden entwickelt, die zeigen, dass GPT-3 überzeugende Aufsätze schreiben, Diagramme und Websites aus Textbeschreibungen erstellen, Computercode generieren und vieles mehr kann (Tamkin & Ganguli, 2021).\\n\\nSeit der Veröffentlichung von GPT-3 im Juni 2020 werden immer mehr konkurrierende, teilweise sogar als Open Source Lösung angebotene Modelle publiziert. Dabei bietet OpenAI GPT-3 als API an und ermöglicht Nutzer:innen so, das Modell über die OpenAI-Plattform zu nutzen oder GPT-3 in eigenen Anwendungen einzubauen. Daraus hat sich ein wachsender Markt an Tools und Anwendungen entwickelt, der heute viele Industrien und Geschäftsbereiche als auch Kreative beeinflusst.\\n\\nDas KI-Foundation-Modell macht es möglich, Text in Sekunden zusammenzufassen oder zu ergänzen. Mittels Chatbots können Kundeninteraktionen einfacher gesteuert werden. Weiterführend ermöglicht GPT-3 die Generierung von Programmcode durch die Eingabe von Sprachbefehlen. Realisiert werden Anwendungen dieser Art durch Start-ups, welche die API von OpenAI gegen ein Entgelt nutzen. Gerade im Bereich Gesundheitswesen, eCommerce sowie im Medien- und Communications-Bereich profitieren Unternehmen, die Chatbots sowie Anwendungen für Natural Language Understanding (NLU) entwickeln, massiv von der Weiterentwicklung großer Sprachmodelle.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n35\\n\\nInformationsextraktion Informationsextraktion ist die Aufgabe, automatisch für die Nutzer:innen relevante strukturierte Informationen aus unstrukturierten und/oder halbstrukturierten maschinenlesbaren Dokumenten zu extrahieren. Diese Funktionen sind von großer und Bedeutung Verwaltungsvorgängen. Hierbei sind insbesondere die Klassifikation von Dokumenten nach inhaltlichen Kriterien, die Eigennamenerkennung und die Relationsextraktion zu nennen, wobei letztere Relationen zwischen Eigennamen und Begriffen aus einem Text extrahiert und in einer Datenbank speichert. Foundation-Modelle haben bei diesen Aufgaben die Genauigkeit stark verbessert und übertreffen oft die Performanz von Menschen.\\n\\nfür\\n\\ndie\\n\\nautomatische\\n\\nErfassung\\n\\nvon\\n\\nTextdokumenten\\n\\nDokumentensuche Eine extrem wichtige Anwendung ist die Dokumentensuche. Dabei werden sowohl die Dokumente einer Textsammlung als auch die Anfrage durch eine Einbettung kodiert und nach Berechnung die zur Anfrage ähnlichsten Dokumente zurückgegeben. Vorteil ist, dass auch Synonyme und alternative Formulierungen des gleichen Sachverhaltes berücksichtigt werden. Diese einbettungsbasierten Retrievalverfahren übertreffen die klassische Stichwortsuche und werden mittlerweile bei allen Internetsuchmaschinen genutzt.\\n\\nAbb. 6: Von einem Foundation-Modell mit Hilfe von Retrieval gefundene Antwort auf eine Frage im Natural Question Benchmark. Aktuelle Modelle erreichen eine Genauigkeit (F1) von 80% (Zhanag et al., 2021).\\n\\nGroße KI-Modelle für Deutschland\\n\\n36\\n\\nBeantwortung von Fragen Bei der Beantwortung von Fragen (Question Answering, QA) erhält ein System eine natürlichsprachliche Anfrage und generiert automatisch eine Antwort in natürlicher Sprache. Fortschrittliche Systeme arbeiten in der Regel in zwei Stufen (Abb. 6): Für eine Frage findet ein einbettungsbasiertes Retriever-Modul eine Reihe von passenden Dokumenten aus einer Textsammlung. Hierbei werden auch Dokumente mit ähnlichen Inhalten gefunden, die unterschiedlich ausgedrückt wurden. Anschließend prozessiert ein Reader die Frage und die gefundenen Dokumente und generiert eine natürlichsprachliche Antwort. Retriever-Reader Module werden von vielen fortgeschrittenen Sprach- und Dialogmodellen eingesetzt und produzieren wesentlich bessere Antworten als sehr große Sprachmodelle ohne diese Erweiterung. Die Antwort kann durch die gefundenen Dokumente erklärt und begründet werden.\\n\\nMaschinelle Übersetzung Zur maschinellen Übersetzung gibt es mittlerweile Modelle, z.B. M2M von Facebook AI (Fan et al., 2022), die die Übersetzung zwischen beliebigen Paaren von über 100 Sprachen gestatten. Durch das gleichzeitige Training des Encoder-Decoders mit vielen Sprachen wird die Übersetzungsqualität für fast alle Sprachpaare wesentlich verbessert, insbesondere für regionale Sprachen mit wenigen Trainingsdaten.\\n\\nErzeugung von Computercode Die Erzeugung von Computercode aus einer textuellen Beschreibung ist eine spezielle Übersetzungsaufgabe (M. Chen et al., 2021), die mittlerweile relativ zuverlässig laufenden Code erzeugen kann und ein großes Anwendungspotential besitzt.\\n\\nDokumentenzusammenfassung Die automatische Zusammenfassung von Dokumenten kann helfen, die wichtigsten Informationen in Dokumenten zu erfassen. Modelle zur Zusammenfassung nutzen meist ein Seq2seq-Modell, welche als Eingabe ein Dokument erhalten und die Zusammenfassung ausgeben. Dabei werden insbesondere Modelle mit einer langen Eingabesequenz benötigt. Die Qualität der Zusammenfassung ließ sich mit Foundation- Sprachmodellen sehr stark erhöhen.\\n\\nAbb. 7: Zusammenfassung eines Textes von 800 Wörtern durch das Modell BRIO im Vergleich zu der von Experten erstellten Zusammenfassung (Liu et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n37\\n\\nTextgenerierung Die Generierung neuer Texte ist die zentrale Aufgabe von Sprachmodellen. Hierbei wird ein vorgegebener Starttext syntaktisch fehlerfrei und inhaltlich stimmig fortgesetzt. GPT- 3 ist hier das bekannteste Modell. Durch einen Starttext mit Anweisungen (Prompt) kann man GPT-3 beauftragen, eine Reihe von Punkten in den erzeugten Text aufzunehmen. Diese Anweisungen können auch Lösungsbeispiele enthalten, wodurch das Modell bei der Konstruktion der Ausgabe unterstützt wird (k-shot prompts). Da die Worte des neuen Textes entsprechend ihrer Wahrscheinlichkeit generiert werden, entsteht bei einer Wiederholung immer ein anderer Text.\\n\\nMittlerweile gibt es auch interaktive Verfahren zur Generierung von Texten (A. Yuan et al., 2022), in denen die Nutzer:innen die Ausgestaltung des Textes steuern können. PaLM ist ein mehrsprachiges, fortgeschrittenes Sprachmodell mit 540 Milliarden Parametern, welches auf mehr als 150 Benchmarks die Leistung durchschnittlicher menschlicher Bearbeiter:innen übertraf. Zudem konnte PaLM nach entsprechender Anleitung komplexe Aufgaben für seine Schlußfolgerungen liefern (Chowdhery, et al., 2022). Starttexte können Sprachmodelle im Prinzip dazu bringen, beleidigende Äußerungen und Fake News zu produzieren. Durch nachträgliche Filtertechniken, Finetuning und Retrieval können allerdings verletzende Äußerungen und Falschinformationen weitgehend vermieden werden.\\n\\nin einfache Schritte zerlegen und Erklärungen\\n\\nDialogsysteme Dialogsysteme (Chatbots) generieren automatisch adäquate Antworten auf die Äußerungen menschlicher Gesprächspartner:innen im Laufe eines längeren Gesprächs. Sie kombinieren Techniken zur Beantwortung von Fragen mit der Generierung von Geschichten und dem Retrieval zusätzlicher Informationen. Dabei wird auch der Stand der Diskussion und Informationen über die „Persönlichkeit“ des Chatbots in einer Datenbank gespeichert und durch Retrieval weiterverwendet. Eine Bewertung durch menschliche Prüfer:innen zeigt, dass z.B. das LaMDA System (Thoppilan et al., 2022) in Bezug auf Sensibilität, Sicherheit und Faktentreue der menschlichen Leistung nahekommt. Im Bereich des Kundenkontakts gibt es sehr viele Anwendungsfälle für Chatbots. Das Modell ChatGPT (OpenAI, 2022b) hat eine ähnliche Architektur, ist aber frei im Internet nutzbar. Es kann Fragen beantworten, Code generieren, Texte zusammenfassen, aber auch einen kohärenten Dialog führen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n38\\n\\n[ChatGPT]\\n\\nChatGPT ist ein am 30. November 2022 innerhalb einer Open-Beta veröffentlichtes Sprachmodell, welches ist, sinnvolle Konversationen mittels Text zu führen. Trainiert mit großen Datensätzen von Konversationen, ist es in der Lage, realistische Gespräche zu führen, Sachverhalte zu erklären oder Programmiercode zu erstellen. Die wichtigste Eigenschaft von ChatGPT ist dabei, dass es in der Lage ist, kontextbezogen zu antworten. Dabei können weiterführende Fragen innerhalb einer Konversation gedeutet und interpretiert werden.\\n\\nin der Lage\\n\\nChatGPT hat eine Vielzahl potenzieller Anwendungen, da es in der Lage ist, angemessene Antworten auf eine breite Palette von Aufforderungen zu generieren und Gespräche in verschiedenen Kontexten zu führen. Einige Beispiele für diese Anwendungen sind die Entwicklung von Chatbots für den Kundendienst oder die Bereitstellung von Informationen, die Entwicklung virtueller Assistenten, die den Nutzern bei Aufgaben und der interaktiver Beantwortung von Fragen helfen, und die Entwicklung Tutorensysteme. Dies sind nur einige der möglichen Einsatzgebiete von ChatGPT zur Entwicklung intelligenter und interaktiver Systeme, die mit Menschen auf natürliche und intuitive Weise kommunizieren können.\\n\\nDa mittels Fragestellungen direkt beantwortet werden können, stellt ein Modell wie ChatGPT sowie eventuelle zukünftige Entwicklungen dieser Art eine ernstzunehmende Herausforderung für konventionelle Suchanbieter wie Google dar. Mittels konversationeller KI-Modelle lassen sich Fragestellungen von Nutzern direkt beantworten sowie weiter spezifizieren, statt nur auf weiterführende Webseiten zu verweisen.\\n\\nkonversationeller KI-Modelle\\n\\nObwohl sich ChatGPT noch in der Beta-Version befindet, existieren bereits Suchmaschinen auf Basis des KI-Modells. Damit wird dem Nutzer ein viel spezifischeres Suchergebnis geboten. Gerade miteinander verknüpfte Inhalte lassen sich deutlich leichter finden und in Zusammenhang bringen. Auch als unternehmensinternes Tool, im Sinne einer “Single Source of Truth” sind KI-Modelle wie ChatGPT denkbar, da sie dem/der Nutzer:in ein sehr authentisches und simples Suchergebnis bieten - ähnlich einem Gespräch mit einem/einer Mitarbeitenden.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n39\\n\\n2.2.2 Anwendungen von multimodalen Foundation-Modellen\\n\\nKI-Foundation-Modelle sind in der Lage, nicht nur Token der natürlichen Sprache zu modellieren, sondern auch Token-Elemente beliebiger anderer Sequenzen. Im Folgenden werden Anwendungen des Paradigmas auf unterschiedlichen Datendomänen wie gesprochene Sprache, Bilder, Videos, DNA und Proteine vorgestellt, die teilweise völlig neue Problemlösungen ermöglichen.\\n\\nDabei erschließt insbesondere die Verknüpfung von Daten aus zwei oder mehr Domänen zusätzliche anspruchsvolle Anwendungsfelder. Hierbei gibt es Modelle, die nur auf eine Anwendung spezialisiert sind, aber auch Foundation-Modelle wie NÜWA, OFA und GATO, die viele Aufgaben gleichzeitig lösen können. Da bei dieser Art der Modelle verschiedene Datendomänen kombiniert werden, bezeichnet man sie als multimodale Modelle.\\n\\nAnwendung\\n\\nBeschreibung\\n\\nSprache zu Text\\n\\nübersetzt gesprochene Sprache in Text für viele unterschiedliche Sprecher:innen\\n\\nText zu Sprache\\n\\ntransformiert Text in gesprochene Sprache unter Berücksichtigung von Sprachmelodie und Sprecher:innenstimme\\n\\nErkennung von Objekten\\n\\nidentifiziert die wichtigsten Objekte eines Bildes und deren Positionen\\n\\nErkennung von Aktionen\\n\\nidentifiziert Aktionen in Videos und deren Positionen\\n\\nBild nach Text\\n\\ngeneriert zu einem Bild eine Bildbeschreibung\\n\\nText in Bild\\n\\nerzeugt zu einer Bildbeschreibung ein passendes Bild\\n\\nVideo nach Text\\n\\nbeschreibt die Objekte und Abläufe in einem Video durch Text\\n\\nText nach Video\\n\\nerzeugt zu einer Bildbeschreibung ein kompatibles Video\\n\\nGenomik\\n\\nDNA-Sequenzen werden analysiert und die daraus erzeugten Proteine prognostiziert\\n\\nProteomik\\n\\nDie 2D- und 3D-Struktur der Proteine wird vorhergesagt\\n\\nVerstärkungslern en\\n\\nSequenzen von Aktionen und Zuständen für Steuerungsprobleme werden prognostiziert\\n\\nLivesynchronisat ione\\n\\nübersetzen der Sprache eines Videos\\n\\nTabelle 2: Eine Auswahl möglicher Anwendungen auf Basis von multimodalen Modellen\\n\\nDie Anwendungsbereiche multimedialer Foundation-Modelle stellen wir in diesem Kapitel genauer vor.\\n\\nGroße KI-Modelle für Deutschland\\n\\n40\\n\\nGesprochene Sprache Ein Anwendungsbereich ist gesprochene Sprache. Das Audiosignal wird dabei oft durch sein Frequenzspektrum (MFCC) für jedes 10 msec Zeitintervall repräsentiert. Wav2vec 2.0 (Baevski et al., 2020) führt unüberwachtes Lernen auf Sprachdaten ohne Transkription mit Convolution- und Self-Attention-Schichten durch. Derartige Convolution-Schichten wurden ursprünglich für die Bilderkennung mit Convolutional-Neural-Networks (CNNs) entwickelt. Ähnlich wie das BERT-Modell für Text lernt es, maskierte „Sound-Tokens“ vorherzusagen. Die Transkription von Sprache in Text kann dann durch eine Kombination von Convolution- und Self-Attention-Schichten erfolgen (Zhang et al., 2020). Zur Transkription von Text in gesprochene Sprache erzeugt beispielsweise FastSpeech 2 (Ren et al., 2022) aus den eingegebenen Phonemen mit einem Seq2seq-Modell das Frequenzspektrum für kleine Zeitintervalle, aus dem direkt die Sprachausgabe produziert werden kann. Dabei werden verschiedene Informationen wie Dauer, Tonhöhe und Energie berücksichtigt. FastSpeech 2 wird von menschlichen Juroren besser beurteilt als konkurrierende Systeme.\\n\\nBilder Bilder können in eine Sequenz von Pixelbereichen zerlegt werden, die als Bild-Token genutzt werden können. Der Vision-Transformer (Dosovitskiy et al., 2020) verwendet Pixelbereiche der Größe 14x14 als Token und führt ein Vortraining mit einem sehr großen Datenbestand von 300 Mio. nicht-annotierten Bildern durch. Dabei sind maskierte Bild- Token zu prognostizieren. Anschließend erfolgt ein Finetuning des Modells auf den ImageNet-Daten zur Klassifikation von Bildern in 1000 Klassen. Der Vision-Transformer erzielte eine höhere Genauigkeit als alle CNNs bei gleichzeitig wesentlich geringerem Trainingsaufwand. Eines der ersten Modelle zur Kombination von Bildern und deren textueller Beschreibung ist das CLIP-Modell (Radford et al., 2021). Mit separaten Encodern erzeugt es aus einem Bild und dem zugehörigen Text je eine Einbettung. Die Differenz zwischen den beiden Einbettungen wird dann durch Training minimiert. Damit lässt sich zu einer Bildunterschrift das am besten passende Bild finden und umgekehrt.\\n\\nAbb. 8: Zu unterschiedlichen Texten von DALL-E 2 erzeugte Bilder (Ramesh et al., 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n41\\n\\nDie Erzeugung von Bildern aus Text verwendet oft CLIP, um zu einem Text passende Bild- Einbettung zu finden. Ein Diffusionsmodell kann den Prozess der Degradierung eines Bildes durch Zufallsänderungen modellieren. Hierbei wird sukzessive die Farbe einzelner Pixel zufällig geändert, so dass sich mit der Zeit eine graue Fläche ergibt. Dieser Prozess kann umgekehrt und zur Rekonstruktion von Bildern in hoher Auflösung aus den Einbettungen genutzt werden. Bekanntestes Modell ist DALL-E 2 (Ramesh et al., 2022), welches zusätzlich noch Bereiche eines Bildes modifizieren kann. Das OFA-Modell (Wang et al., 2022) kann gleichzeitig viele Aufgabe erledigen: die Position von Objekten in Bildern bestimmen, Bildunterschriften erzeugen, Fragen zu einem Bild beantworten, Objekte in einem Bild erkennen, fehlende Bereiche in einem Bild ausfüllen und Bilder zu einem Text erzeugen. Darunter lassen sich viele Anwendungen heute schon produktiv einsetzen, beispielsweise in der Bearbeitung von Fotos. Mittels des Modells Stable Diffusion (Rombach et al., 2022) ist es etwa möglich, unerwünschte Objekte aus Bildern zu entfernen, mehrere Bilder zu verbinden oder auch ausgehend von einer textuellen Bildbeschreibung ganz neue Bilder zu Werbezwecken bzw. als Ersatz für sogenannte „Stock Photos” zu generieren. Bei den so erzeugten Bildern handelt es sich um Unikate. Zwar spiegeln sie den Datensatz wider, mit dem das Modell trainiert wurde, sind aber keine Kopien. Dadurch, dass es sich bei den generierten Bildern nicht um Kopien handelt, können diese potentiell kommerziell genutzt werden. Allerdings sind die Copyright- Fragen noch nicht abschließend geklärt.\\n\\nVideo Um Videos mit Foundation-Modellen verarbeiten zu können, verwendet man meist Video- Token, die einen Pixelbereich in mehreren hintereinander folgenden Videobildern beschreiben. Flamingo (Alayrac et al., 2022) ist ein visuelles Sprachmodell, das Sequenzen von beliebig hintereinander folgenden Bildern, Videos und Texten verarbeiten kann. Es nutzt im Hintergrund ein großes Sprachmodell für Text. Das Modell kann einerseits Fragen zu Bildern beantworten oder Bilder beschreiben. Zum anderen kann es Aktionen in Videos beschreiben oder klassifizieren. Schließlich kann es durch gemischte Few-shot- Prompts eine neue Beschreibungsaufgabe zu erledigen.\\n\\naus\\n\\nTexten und Bildern/Videos\\n\\ninstruiert werden,\\n\\nAbb. 9: Zu unterschiedlichen Texten von CogVideo erzeugte Videos (Hong et al., 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n42\\n\\nNÜWA (Wu et al., 2021) ist ein Encoder-Decoder-Modell, das ein Video zu einem Text erzeugen kann. Es verwendet einen speziellen Attention-Mechanismus, um die Relation der Token sowohl für räumliche als auch für zeitliche Achsen zu erfassen. Das Modell kann einerseits eine Reihe von Bild-Aufgaben lösen, etwa Erzeugung eines Bildes zu einem gegebenen Text. Weiter kann es ein Video zu einem Text generieren, die Fortsetzung eines Videos prognostizieren oder Videos manipulieren.\\n\\nImagen-Video (Ho et al., 2022) wurde mit 60 Millionen Bild-Video-Paaren und 14 Millionen Text-Video-Paaren jeweils aus dem öffentlich zugänglichen LAION 400M-Datensatz (Schuhmann, 2021) trainiert. Mit Hilfe der Bilder können auch bestimmte Kunststile imitiert werden und z.B. ein Video im Stil von Monet erzeugt werden. Das Modell Make-a- Video (Singer et al., 2021) bietet die zusätzliche Möglichkeit zum unüberwachten Lernen auf Videodaten ohne textuelle Beschreibung, um realistische Bewegungen von Objekten und Szenen zu lernen. Zudem kann es zwischen einem Paar von Bildern einen dynamischen Übergang in Form eines Videos erzeugen. Beide Modelle sind nicht nur in der Lage, Videos mit hoher Wiedergabetreue zu generieren, sondern die Modelle umfassen ein hohes Maß an Weltwissen und sind im Detail kontrollierbar mit der Fähigkeit, verschiedene Videos und Textanimationen in verschiedenen künstlerischen Stilen und mit 3D-Objekt- verständnis zu erzeugen. Leider sind Make-a-Video und Imagen Video proprietär und der Modell-Code ist nicht frei verfügbar. Die beschränkte Länge von hochauflösenden Videos von derzeit fünf Sekunden ist offenbar der limitierende Faktor. Insgesamt sind die erzeugten Videos noch nicht perfekt.\\n\\nGenomik und Proteomik Die Entschlüsselung der Sprache der DNA ist eines der wichtigsten Ziele der biologischen Forschung. Der genetische Code ist universell und erklärt, wie die DNA in Proteine übersetzt wird. Im Gegensatz dazu variiert der regulatorische Code, der bestimmt, wann und wie die Gene exprimiert werden, zwischen verschiedenen Zelltypen und Organismen. ist ähnlich zur Polysemie und entfernten semantischen Beziehungen bei Dies natürlichsprachigen Texten. DNABERT wurde auf einer großen Menge von DNA- Sequenzen vortrainiert und kann durch Finetuning den Stand der Technik für viele von spezifische Prognoseaufgaben Sequenzmotiven (DNA-Abschnitten mit biologischer Relevanz) und die Prognose der Promotor-Regionen (Nukleotid-Sequenz, die die regulierte Expression eines Gens ermöglicht). MoDNA (An et al., 2022) und GeneBERT (Mo et al., 2021) haben eine ähnliche Funktionalität.\\n\\nverbessern. Darunter\\n\\nsind die Analyse\\n\\nProteine sind lineare Ketten von Aminosäuren, die durch kovalente Bindungen verbunden sind. Aminosäuren lassen sich durch ein Alphabet mit 25 Zeichen repräsentieren. Die Zeichenketten eignen sich hervorragend für viele Natural-Language-Processing (NLP) Methoden (Ofer et al., 2021). AminoBERT ist ein Sprachmodell (Chowdhury et al., 2022), welches aus einer Proteinsequenz als Eingabe die 3D-Proteinstruktur prognostiziert. Dabei wird auch eine natürliche Methode zur Beschreibung der Polypeptidgeometrie verwendet, die auf der Ebene des Polypeptids als Ganzes rotations- und translationsinvariant ist. Im Durchschnitt übertrifft das Modell AlphaFold2 (Jumper et al., 2021) und RoseTTAFold (Baek et al., 2021) bei verwaisten Proteinen und Klassen von konstruierten Proteinen und erreicht dabei eine bis zu 106-fache Reduzierung der Rechenzeit. Es gibt eine Reihe weiterer Modelle mit ähnlichen Ergebnissen, z.B. das\\n\\nGroße KI-Modelle für Deutschland\\n\\n43\\n\\nProtein-Sprachmodell ESMFold (Lin et al., 2022). Es erzeugt Einbettungen, die in nachgelagerten Aufgaben eingesetzt werden können, zum Beispiel zur Erfassung der strukturellen Eigenschaften von Proteinen.\\n\\n[AlphaFold]\\n\\nMit der Software AlphaFold gelang dem privaten Forschungsunternehmen DeepMind 2021 ein großer Durchbruch in der Forschung an der Faltung von Proteinen. Mithilfe eines KI-Modells erreichte das Team, die Faltung von Proteinen vorherzusagen und dadurch deren Form und Funktion innerhalb des Organismus genauer zu erforschen. Über 200 Millionen Strukturen von Proteinen konnten so gefunden und in einer Protein-Datenbank gespeichert werden - im Gegensatz zu den vorher nur circa 1 Millionen verfügbaren Proteinstrukturen.\\n\\nAlphaFold hat bereits heute einen bedeutenden, direkten Einfluss auf die menschliche Gesundheit. Bei einem Treffen mit Forscher:innen der European Society of Human Genetics wurde deutlich, wie wichtig die AlphaFold-Strukturen für Biolog:innen und Kliniker:innen sind, die versuchen, die Ursachen zu entschlüsseln. Darüber hinaus beschleunigt AlphaFold die Entdeckung von Medikamenten, indem es ein besseres Verständnis neu identifizierter Proteine ermöglicht, die als Angriffspunkte für Medikamente in Frage kommen. Es hilft Wissenschaftler:innen, schneller potenzielle Medikamente zu finden, die sich an diese Proteine binden.\\n\\nseltener genetischer Krankheiten\\n\\nEnde Infobox Für das Wirkstoffdesign ist die Vorhersage der Interaktion zwischen einem Arzneimittel und dem Zielorgan wichtig. Sie ist für die Entdeckung neuer Medikamente und die Umwidmung bestehender Medikamente von entscheidender Bedeutung. Yazdani-Jahromi et al. (2021) beschreiben ein Sprachmodell für derartige Anwendungen.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n44\\n\\nSteuerung dynamischer Systeme Verstärkungslernen (auch als Reinforcement Learning bekannt) betrachtet ein System mit Zuständen, Aktionen und Belohnungen zu bestimmten Zeitpunkten. Der Agent wählt zu den gegebenen Zuständen eine neue Aktion, während die nächsten Zustände und die Belohnung durch die Umgebung bestimmt wird. Ziel ist, eine Strategie zu erlernen, die jedem Zustand eine Aktion zuordnet und die Summe der Belohnungen maximiert. Mit derartigen Systemen lassen sich Brett- und Videospiele beschreiben, aber auch Robotersteuerungen und selbstfahrende Autos. Der Decision-Transformer (L. Chen et al., 2021) ist ein Sprachmodell, welches die Aktionen nacheinander prognostiziert. Dabei schätzt er die optimale Summe aller zukünftigen Belohnungen. Das Modell wird auf einer großen Menge von beobachteten Zeitreihen trainiert. Anschließend kann der Agent zu einem gegebenen Zustand die Aktion auswählen, welche zur höchsten prognostizierten Belohnungssumme führt. GATO (Reed et al., 2022) ist ein multimodales Modell, welches Text, Bilder und Sequenzen von Werten verarbeiten und daraus Steuerungsstrategien ableiten kann. Es erzielte auf mehr als 600 Benchmarks gute Resultate.\\n\\nAbb. 10: Das Gato-Modell generiert aus den aktuellen Zuständen (hellgelb) neue Aktionen (dunkelgelb). Die Umgebung produziert daraus neue Zustände, usw. Das Modell kann Texte, Messwerte, Bilder, usw. verarbeiten (Reed et al. 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n45\\n\\n2.3 Vertrauenswürdige KI-Foundation-Modelle\\n\\nDer großflächige Einsatz von KI-Systemen und das damit verbundene technische Innovationspotential wird erwartungsgemäß Auswirkungen von breiter wirtschaftlicher, aber auch gesellschaftlicher Relevanz haben. Bei ChatGPT haben sich innerhalb von fünf Tagen über eine Millionen Nutzer:innen angemeldet, um das System selbst auszuprobieren. Dieses große Interesse der breiten Öffentlichkeit zeigt, wie schnell und stark große KI-Systeme die öffentliche Diskussion prägen können. Aus dem Black-Box ähnlichen Charakter der eingesetzten trainierten Modelle ergeben sich Risiken, die besondere Maßnahmen im Hinblick auf den vertrauenswürdigen Einsatz von KI notwendig machen. Die vielfältigen Anwendungsmöglichkeiten von KI-Foundation- Modellen erfordern einen systematischen Ansatz zur Bewertung bzw. Abschwächung der entstehenden Risiken. Sie müssen parallel zur Modellkonstruktion im LEAM-Projekt angegangen werden.\\n\\nDie folgende Darstellung orientiert sich an dem im Kontext des KI.NRW-Flagship-Projekts „Zertifizierte KI (Poretschkin, 2022) erarbeiteten risikobasierten Ansatz zur vertrauenswürdigen KI und den vorhergehenden Arbeiten (Cremers et al., 2019; Poretschkin et al., 2021). Dieser Ansatz zielt darauf ab, KI-spezifische Risiken systematisch zu erfassen, messbar zu machen und unter Berücksichtigung von verschiedenen Dimensionen der Vertrauenswürdigkeit hinreichend zu reduzieren.\\n\\n”\\n\\nWir stellen im Folgenden sechs Dimensionen der Vertrauenswürdigkeit dar und skizzieren aktuelle Maßnahmen zu deren Identifikation und Mitigation.\\n\\nFairness Die Dimension Fairness soll sicherstellen, dass die KI-Anwendung nicht zu ungerechtfertigter Diskriminierung führt. Typische Ursachen hierfür sind unausgewogene (mit Bias behaftete) Trainingsdaten oder auch die statistische Unterrepräsentation von Personengruppen, welche zu einer verringerten Qualität der KI-Anwendung in Bezug auf diese Gruppen führen kann. Da die Foundation-Modelle mit Texten trainiert weprrden, die von Menschen verfasst wurden, spiegeln diese Texte häufig die in der Gesellschaft vorhandenen Stereotype wider. Hierbei kann es insbesondere zu einer Bevorzugung oder Benachteiligung von geschlechtsspezifischen oder ethnischen Gruppen kommen. Die Entwickler:innen großer KI-Foundation-Modelle sind sich vieler der immanenten Risiken bewusst und implementieren korrektive Maßnahmen, die darauf ausgerichtet sind, unerwünschte Effekte zu vermeiden. So berichten die Entwickler:innen von DALL-E (OpenAI, 2022a), dass spezielle Techniken eingesetzt werden, um einem in genutzten Trainingsdaten tatsächlich vorhandenen Bias (z.B. „heroic firefighter” wird zumeist als männliche Person dargestellt) entgegenzuwirken. Große Sprachmodelle wie PaLM und LaMDA verwenden mit gutem Erfolg filter-basierte Techniken, um unerwünschte Ausgaben („toxic language”) zu vermeiden. Diese Methoden sind aber sehr spezifischer Natur und mit großer Wahrscheinlichkeit im breiten Einsatz nicht ausreichend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n46\\n\\nAutonomie und Kontrolle Diese Dimension zielt auf zwei Dinge ab: zum einen die Autonomie der KI-Anwendung und zum anderen die Autonomie des Menschen. Einerseits ist hier zu beurteilen, welcher Grad an Autonomie (Gehman et al., 2020) für die Anwendung angemessen ist. Andererseits wird untersucht, ob der Mensch durch die KI-Anwendung angemessen unterstützt wird und ausreichend Handlungsspielraum in der Interaktion mit der KI- Anwendung erhält. Die spezifische Herausforderung liegt darin, dass die Möglichkeiten der Interaktion mit dem Menschen meist erst im Design der konkreten nachgelagerten KI-Anwendung festgelegt werden und nicht durch das Foundation-Modell an sich kontrolliert werden kann, das in dem Prozess aber eine zentrale Rolle spielt. Hier ist noch substantielle Forschungsarbeit notwendig, um die Querbeziehungen zwischen den Anwendungen und den Foundation-Modellen kontrolliert abdecken zu können.\\n\\nÄhnlich zur zwischenmenschlichen Kommunikation, können „toxische“ Modellausgaben (Gehman et al., 2020) (etwa Beleidigungen oder Mobbing) zu psychologisch-emotionalen Beeinträchtigungen der Nutzer:innen führen. Jenseits von verletzender Sprache können (manipulative) Kommunikationsstrategien oder die Vorspiegelung falscher Tatsachen emotionale Abhängigkeiten schaffen und damit die menschliche Autonomie potentiell einschränken. Einen frühen Ansatz, diesem Risiko zu begegnen, schlagen Glaese et al. (2022) vor. Sie optimieren ihre Conversational AI dergestalt, dass jene ihre maschinelle Natur, wenn immer nötig, offen kommuniziert. Auch hier stehen wir erst ganz am Anfang und die deutsche KI-Forschung könnte wesentliche Beiträge liefern.\\n\\nTransparenz Unter diesem Oberbegriff sind Aspekte der Nachvollziehbarkeit, Reproduzierbarkeit und Erklärbarkeit subsumiert. Die Dimension Transparenz untersucht insbesondere, ob die grundlegende Funktionsweise der KI-Anwendung für Nutzer:innen und Experten:innen angemessen nachvollziehbar ist und ob Ergebnisse der KI-Anwendung reproduziert und ggf. begründet werden können. Die Transparenz-Dimension wird in Foundation-Modellen zwar bereits auf der Ebene der Dokumentation und Beschreibung der Daten/Modelle (z.B. durch Modelcards (Gehman et al., 2020) adressiert, bedarf aber noch einer systematischen Herangehensweise in Bezug auf den tatsächlichen Einsatz der KI- Foundation-Modelle in konkreten Anwendungen.\\n\\nWie andere soziale Medien, kann ein Chatbot durch Fine-tuning oder Prompts dazu gebracht werden, dem/der Nutzer:in nur bestimmte Aspekte zu kommunizieren. Sie befinden sich dann in einer „Filterblase\", in der Nachrichten, die nicht der geäußerten Meinung entsprechen, ausgeblendet werden. Für diese Problematik gibt es mittlerweile Audit-Verfahren (Cen & Shah, 2021), mit denen überprüft werden kann, ob die Plattform unerwünschte inhaltliche Filter verwendet, wobei nur ein Black-Box-Zugriff auf den Filteralgorithmus erforderlich ist. In allen diesen Bereichen haben wir bisher wenig Kontrolle speziell bei Foundation-Modellen und es sind weitere Anstrengungen im Bereich der Forschung und dann wahrscheinlich auch der Regulierung erforderlich.\\n\\nGroße KI-Modelle für Deutschland\\n\\n47\\n\\nVerlässlichkeit Diese Dimension bezieht sich vornehmlich auf die Qualität der KI-Komponente und beurteilt u.a. deren Robustheit, das heißt die Konsistenz ihrer Ausgaben. Ein erschwerender Umstand dabei ist, dass die Erzeugung von Text meist zufallsgesteuert ist. Ein wichtiger Aspekt ist auch die Faktentreue der Ausgaben, da Foundation-Modelle nicht zwischen plausiblen und korrekten Zusammenhängen unterscheiden können (Lin et al., 2021). Beispielsweise hat sich der Wahrheitsgehalt von „Angela Merkel ist Bundeskanzlerin“ im Laufe der Zeit geändert. Im Bereich von Foundation-Modellen wird hier bereits mit verschiedenen Ansätzen (z.B. „safety fine-tuning”) untersucht, wie unerwünschte („unsafe”) Ausgaben möglichst vermieden werden können.\\n\\nDie „factfulness“ von Sprachmodellen zu erhöhen, ist zurzeit eine stark bearbeitete Forschungsrichtung im Bereich NLU (Natural Language Understanding, siehe z.B. Glaese et al. (2022), Nakano et al. (2021)). Retrieval-basierte Foundation-Modelle wie WebGPT, Retro, und LaMDA können auf eine große Sammlung von aktuellen Textdokumenten zugreifen, um den zu erstellenden Text durch relevante abgerufenen Informationen zu verbessern. Shuster et al. (2021) haben gezeigt, dass der Einsatz von Retrieval die Rate der ,,Halluzinationen” reduziert. Insgesamt erlaubt Retrieval die gezielte Verwendung aktuellen Wissens, um die Antwortqualität zu verbessern.\\n\\nEin weiteres Problem sind (mutwillige) Zweckentfremdungen, etwa zur Generierung von „Fake News“ oder zur Erstellung gewaltverherrlichender Texte, für die Brundage et al. (2022) einen Maßnahmenkatalog vorschlagen. Allgemeiner ist es schwierig, die Fähigkeiten und damit das Missbrauchspotential von großen Modellen einzuschätzen, da deren „abilities“ (Fähigkeiten) oftmals hochgradig sensitiv auf die Details des jeweiligen Modellinputs (Prompting) sind - z.B. Chain-of-Thought Prompting (Suzgun et al., 2022) und Prompt Injection Attacks (Branch et al., 2022) - und sich ex-ante nicht vorhersagen lassen. Die HELM-Initiative (Liang et al., 2022) nähert sich diesem Problem, indem sie eine Vielzahl an Sprachmodellen standardisiert evaluiert und vergleicht und dabei neben Performanz auch Aspekte wie Kalibrierung, Robustheit und Fairness berücksichtigt.\\n\\nEin wichtiger Aspekt in diesem Kontext ist “Trusted-AI”: Die Möglichkeiten, Garantien über die Korrektheit der Ergebnisse der Foundation-Modelle geben zu können, wie sie insbesondere in sicherheitskritischen Kontexten wichtig sind. Hier gibt es vor allem zwei Ansätze: Deduktive Verifikation, die auf Basis von grundlegenden Theorien über die KI- Algorithmen formale Beweise über die Korrektheit bestimmter Aspekte führt, und induktive Validierung, die durch systematisches Testen Aussagen über die Eigenschaften von KI-Systemen ableiten. Beide Bereiche werden in der KI teilweise schon erfolgreich angewendet, müssen aber insbesondere für die hier betrachteten, sehr komplexen Modelle noch deutlich weiterentwickelt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n48\\n\\nSicherheit Diese Dimension adressiert sowohl Eigenschaften der funktionalen Sicherheit als auch die Absicherung gegenüber Angriffen und Manipulationen der KI-Anwendung. Da sich die Maßnahmen in dieser Dimension primär auf die Einbettung der KI-Komponente beziehen, können Foundation-Modelle hier unter anderem durch klassische Methoden der IT- Sicherheit geschützt werden. Dennoch bleiben auch Fragen offen, die sich insbesondere aus dem Zielkonflikt ergeben, einerseits immer mehr auch öffentlich zugängliche Trainingsdaten zu nutzen, diese aber gleichzeitig vor Angriffen, die sich auf das Modellverhalten auswirken könnten, zu schützen.\\n\\nDialogsysteme wie BlenderBot 3 verwenden fine-tuning auf „korrekten” Dialogen, um z.B. das System darin zu verbessern, auf kritische Anmerkungen auch angemessen zu reagieren (Ung et al., 2022). Dennoch erfordern viele der bekannten Herausforderungen der vertrauenswürdigen KI für Foundation-Modellen bessere Lösungen. Als ein Beispiel sei hier nur das Problem der „visuellen Synonyme” genannt (Mishkin et al., 2022), mit denen sprachbasierte Filtertechniken umgangen werden können (z.B. „rote Flüssigkeit” statt „Blut”).\\n\\nDatenschutz Diese Dimension bezieht sich auf den Schutz sensibler Daten im Kontext von Entwicklung und Betrieb einer KI-Anwendung. Dabei wird sowohl der Schutz personenbezogener Daten als auch von Geschäftsgeheimnissen adressiert. In Zusammenhang mit sind hier insbesondere Risiken zu beachten, die sich im Umfeld der Techniken um „Model- Inversion” ergeben. So konnten bereits durch gezielte und systematische Abfragen von Modellen sensible Daten wie Sozialversicherungsnummern oder auch realistische Abbildungen (vorher unbekannter) Personen erzeugt werden.\\n\\nFerner sind für die oftmals generativ verwendeten Sprachmodelle sogenannte „training data extractions“ relevant, bei denen die erzeugten Modellausgaben Fragmente der Trainingsdaten enthalten. Sofern letztere nicht vollständig von personenbeziehbaren oder urheberrechtlich geschützten Informationen bereinigt wurden, können solche „Extraktionen“ Datenschutz- oder Eigentumsrechte von Dritten verletzen (Carlini et al., 2021). Nichol (2022) schlägt hierzu eine Ad-hoc-Mitigationsstrategie vor.\\n\\nEs gibt mehrere Möglichkeiten, Datenschutzprobleme bei Foundation-Modellen zu entschärfen. Ein Ansatz wäre, Sequenzen aus den generierten Daten herauszufiltern, die bereits in den Trainingsdaten vorkommen, etwa durch einen Bloom Filter. Ein anderer Ansatz ist das Training mit differential privacy Ansätzen. Hier gibt es einen hohen Forschungsbedarf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n49\\n\\nForschungsrichtungen und -ansätze Es bedarf weiterer intensiver Forschung und Entwicklung, um die Anforderungen aus allen beschriebenen sechs Dimensionen der vertrauenswürdigen KI für Foundation- Modelle systematisch abzudecken. Die bestehenden Ansätze zur Adressierung spezieller Risiken von Foundation-Modellen müssen erweitert und systematisch zusammengeführt werden. Dabei stellt neben der Komplexität und der schieren Größe der Foundation- Modelle auch die Vielfalt möglicher Anwendungen eine besondere Herausforderung dar. Ohne diese Ansätze ist es meist nicht möglich, die entwickelten Foundation-Modelle in der Praxis verantwortungsvoll zu nutzen.\\n\\nZusätzlich zu den technischen Maßnahmen während der Entwicklungs- und Testphase benötigt man einen kontinuierlichen Prozess und Regeln zur Governance, um den Einsatz eines Foundation-Modells zu begleiten. Diese sind bei den bisherigen Modellen aus den USA und China nicht gegeben bzw. wenig kontrollierbar. Während des Modellbetriebs ist sicherzustellen, dass die Prinzipien einer weiterhin vertrauenswürdigen KI erfüllt bleiben. Grundsätzlich sind geeignete organisatorische Maßnahmen zu ergreifen, um in Situationen, in denen z.B. ein mögliches Fehlverhalten eines Modells auftritt, reagieren zu können. Hierbei ist auch das Wechselspiel zwischen möglicherweise verschiedenen beteiligten Organisationen, wie dem Entwickler des Foundation-Modells und dem Anbieter einer darauf aufbauenden Anwendung, zu berücksichtigen. Dabei ist insbesondere darauf zu achten, dass zur Behebung gefundene Fehler auch erneute technische Maßnahmen, wie z.B. Modellverbesserungen oder Einführung weiterer Filter, angestoßen werden können.\\n\\nzu überwachen und\\n\\nMögliche und nötige Schritte umfassen:\\n\\nTieferes Verständnis der Strukturen und Funktionsweise von Foundation-Modelle\\n\\nVerfahren zur Risikobewertung und Tests entlang spezifischer Proxy-Aufgaben\\n\\nEtablierung von Benchmarks zur Vertrauenswürdigkeit von Foundation-Modellen\\n\\nUntersuchung und Test von semantischen Eigenschaften des latenten Raums\\n\\nValidierung von Modellen zur Input- oder Output-Überwachung\\n\\nDefinition geeigneter organisatorischer Maßnahmen zur Überwachung des\\n\\nlaufenden Betriebs\\n\\nGroße KI-Modelle für Deutschland\\n\\n50\\n\\nSPOTLIGHT 2txt NLG GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\n2txt ist Anbieter einer Software zur automatischen Textgenerierung auf Basis von KI-Sprachmodellen. Die Software ist für Anwendungsfälle im Travel-Bereich, in der Finanzbranche und im E-Commerce optimiert. 2txt zeichnet sich durch besonders einfaches und schnelles Setup, leichte Integrierbarkeit in Enterprise- Anwendungen und vor allem durch eine konstant zuverlässige und sehr hohe Textqualität aus.\\n\\nJohannes Bubenzer, Founder und CEO von 2txt\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir setzen Foundation-Modelle mit linguistischen Modellen ein, um beschreibende Texte für diverse\\n\\nThemengebiete im B2B zu generieren. So erzeugen wir z.B. Produktbeschreibungen, Ortsbeschreibungen oder Finanzberichte komplett automatisiert. Wir kombinieren Foundation-Modelle mit klassischen Sprachmodellen, um das beste aus beiden Welten zu erhalten: Foundation-Modelle macht unser Produkt skalierbar und kreativ, während linguistischen Modelle unsere Produkte kontrollierbar und zuverlässig machen.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle ermöglichen es uns, unser Produkt in kürzester Zeit zu skalieren. Arbeitsschritte, die früher Monate gedauert haben, können mit Hilfe von Foundation-Modellen in Sekunden erledigt werden. Das spart Ressourcen, Kosten und Zeit und ermöglicht uns, einen wesentlich größeren Markt zu adressieren.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Heutzutage: Die Anbieter von KI-Modellen betreiben diese als digitale Services. Das bedeutet, dass sie Zugriff auf alle eingehenden Daten und ausgehende Texte haben. Das ist ein Albtraum für Datenschutz und Geschäftsgeheimnisse der nutzenden Unternehmen. Für die nahe Zukunft: Es ist abzusehen, dass die Entwicklungen im Bereich der KI- Foundation-Modelle eine der zentralen technologischen Revolutionen der Menschheit auslösen wird. Es werden intelligente Maschinen entstehen und es wäre ein unermesslicher sozialer und wirtschaftlicher Fehler, den Wettlauf um diese Technologien privatwirtschaftlichen Firmen in den USA oder China zu überlassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n51\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Unternehmen können nur dann nachhaltige KI-Geschäftsmodelle aufbauen, wenn sie einen technologischen Vorsprung aufbauen und verteidigen können. Deswegen ist es essentiell, dass wir Downstream Modelle auf freien und offenen Foundation-Modellen mit eigenen Daten trainieren können. Ansonsten wäre jeglicher technologische Fortschritt in diesem Feld zu leicht replizierbar.\\n\\nZusätzlich sind einige der interessantesten Anwendungen für KI-Foundation-Modelle solche, involviert sind, z.B., wenn es um Kundenkommunikation oder Business Intelligence geht. Solche Services sind völlig undenkbar, wenn die Daten unkontrollierbar in die USA oder nach China transferiert werden müssen.\\n\\nin denen persönliche Daten\\n\\n2.4 Offene Forschungsfragen, neueste Entwicklungen und\\n\\nErwartungen\\n\\nDie Entwicklung von Foundation-Modellen befindet sich noch in ihren Anfängen und es bestehen viele offene Fragen und potenzielle Risiken. Gleichzeitig sind die Chancen insbesondere für die Wirtschaft sehr vielversprechend und Foundation-Modelle prägen bereits heute die Geschäftsmodelle und zugrundeliegenden KI-Lösungen von Unternehmen mit steigender Tendenz.\\n\\nDie folgenden Entwicklungen sind abzusehen:\\n\\nAusweitung auf weitere Domänen Die starke Entwicklung und Nutzung von Foundation-Modellen hat im NLP-Bereich seinen Anfang genommen und wird sich voraussichtlich auf alle anderen KI-Bereiche ausbreiten und weitere Datentypen abdecken: Bilder, Ton, Videos, Genom- und Proteinsequenzen, Sensordaten und so weiter.\\n\\nMultimodalität und das Limit von Textdaten Multimodalität ist ein wichtiger Faktor. Es ist bereits absehbar, dass multimodale Foundation-Modelle höhere Genauigkeiten als unimodale Modelle erreichen werden und eine noch größere Vielfalt von Anwendungsfällen abdecken können. Darüber hinaus ist festzustellen, dass die ausschließliche Verwendung von Textdaten zum Training von Foundation-Modellen bereits heute an ihre Grenzen stößt, da sehr große Teile der verwendbaren digitalen Texte des Internets schon genutzt werden. Hier könnten multimodale Datensammlungen eine Lösung darstellen, bspw. die enorme Menge von Videos auf YouTube.\\n\\nGroße KI-Modelle für Deutschland\\n\\n52\\n\\nHürde zum Training von Foundation-Modellen Die existierende Kluft zwischen einer überschaubaren Anzahl von Technologiekonzernen mit den notwendigen Ressourcen, um Foundation-Modelle zu trainieren, und all jenen, denen es an den finanziellen Mitteln und entsprechenden Rechenressourcen mangelt, könnte in Zukunft weiter wachsen. Diesem Trend wirken Initiativen wie LEAM entgegen. Weiteren Einfluss nehmen Bestrebungen, die eine KI-Demokratisierung durch verteiltes Lernen vorantreiben oder Open Source Kollektive wie Hugging Face, die mit der Big Science Initiative das Modell BLOOM entwickelt und öffentlich gemacht haben.\\n\\nKeine Monopolbildung, aber verzögerte Entwicklung Trotz des eingeschränkten Zugangs zu Rechenressourcen ist eine Monopolbildung zur jedoch zum aktuellen Zeitpunkt eher Entwicklung von Foundation-Modellen unwahrscheinlich. Im Fall von GPT-3 wurden vergleichbare Modelle veröffentlicht: Jurassic-1-Modelle von A21 Labs, OPT von Meta, die Modelle GPT-Neo und GPT-J von Eleuther AI, bis hin zu nicht-englischen Modellen wie dem russischen ruGPT-3 von Sber, dem koreanischen HyperCLOVA von Naver, den chinesischen CPM-1/CPM-2-Modellen der Tsinghua-Universität, PanGu-α von Huawei und Wu Dao 2.0 von der Beijing Academy of Artificial Intelligence. Dennoch ist darauf hinzuweisen, dass die Entwicklung der Wissenschaft dem Stand der amerikanischen Wirtschaft um bis zu zwei Jahre hinterher ist. Ein solcher Umstand ist höchst ungewöhnlich.\\n\\nEnge Zusammenarbeit von Entwicklung und Anwendung Die Grenze zwischen KI-Entwickler:innen und Anwender:innen wird in den kommenden Jahren vermutlich unschärfer. Grund dafür ist, dass immer mehr Menschen ohne KI- Expertise in der Lage sein werden, Foundation-Modelle erfolgreich für ihre eigenen Fälle anzupassen. Gleichzeitig ist somit ein exponentieller Anstieg neuer KI-basierter Produkte zu erwarten.\\n\\nGrounding Foundation-Sprachmodelle lernen alleine Korrelationen zwischen Begriffen und sprachlichen Konzepten. Dabei ist z.B. ein Hund mit den Begriffen Leine, Ohren, Katze, Säugetier, Bein, Fell, Schwanz, Spielzeug, Bellen usw. verbunden. Was fehlt sind Aspekte wie z.B. die dreidimensionale Gestalt des Hundes, seine Art, sich zu bewegen, der Klang seines Bellens, seine dynamische Reaktion auf Katzen oder Menschen. Damit zusammen hängen Gesetzmäßigkeiten der Physik, wie z.B. die Permanenz und Verformbarkeit von Objekten, die Wirkung der Schwerkraft. Daher lässt sich das Konzept des Hundes am besten lernen, wenn es in mehreren Medien auftaucht, zum Beispiel als Bild, in Worten oder in einem Film, in dem er eine Katze jagt. Die Verwendung von multimodalen Foundation-Modellen bietet die Möglichkeit für ein solches integriertes Lernen von Konzepten in der Welt. Yann LeCun sagt: „Anstelle von Sprache oder Bildern wird die nächste KI-Generation jedoch direkt aus Videos lernen. Meta unternimmt derzeit große Anstrengungen, um Videodaten aus der Ich-Perspektive für diese neue KI-Generation zu sammeln, aber auch YouTube-Videos sind als Trainingsmaterial geeignet\\'\\' (Schreiner, 2022; Jawahar, 2021). Das kürzlich vorgeschlagene Foundation-Modell PLATO ist ein erster Versuch, um intuitive Physik aus Videos zu lernen (Piloto et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n53\\n\\nPlanung und Simulation Daniel Kahneman hat auf der Grundlage langjähriger Studien des menschlichen Verhaltens eine Hypothese über zwei verschiedene Denksysteme entwickelt (Kahneman, 2011). Fast Thinking ist schnell, instinktiv und emotional. Beispiele hierfür sind das Verstehen eines einfachen gesprochenen Satzes oder das Erkennen eines Objekts auf einem Bild. Fast Thinking läuft ständig und erzeugt Eindrücke, Intuitionen und schnelle Urteile auf der Grundlage unserer unmittelbaren Wahrnehmungen. Fast Thinking kann zum großen Teil schon heute mit den existierenden Foundation-Modellen geleistet werden. Slow Thinking ist langsamer, überlegter und logischer. Es ist z.B. dafür verantwortlich in einer engen Parklücke zu parken oder die Rechenaufgabe 16*34 zu lösen. Slow Thinking kommt nur dann zum Einsatz, wenn Probleme mit Fast Thinking auftreten, das heißt, wenn es die Wahrnehmungen nicht gut erklären kann. Slow Thinking ist in der Lage, Probleme mental durchzuspielen und Ergebnisse für verschiedene Randbedingungen zu simulieren. Es entspricht daher weitgehend dem Konzept der Steuerung durch Prognosemodelle (model predictive control). Diese Fähigkeiten können ansatzweise von Foundation-Modellen wie GATO (Reed et al., 2022) realisiert werden. Notwendig Integration dieser Modelle mit den Perzeptionsmodellen für unterschiedliche Medien und die flexible Anwendbarkeit auf neue Planungsprobleme. Yann LeCun zufolge ist die „Fähigkeit, Modelle der Welt zu konstruieren, im Grunde das Wesen der Intelligenz”. Diese Modelle werden nicht nur benötigt, um physische Bewegungen vorherzusagen, sondern auch das Verhalten von Menschen, wirtschaftliche Aktivitäten usw. Die große Herausforderung der Künstlichen Intelligenz im nächsten Jahrzehnt besteht darin, prädiktive Modelle der Welt zu erlernen, um mit Unsicherheiten umzugehen (Fridman, 2022).\\n\\nist eine\\n\\nGroße KI-Modelle für Deutschland\\n\\n54\\n\\nKI-Foundation-Modelle im internationalen Vergleich\\n\\nGroße KI-Modelle für Deutschland\\n\\n55\\n\\n3. KI-Foundation-Modelle im internationalen Vergleich\\n\\nDieses Kapitel beleuchtet die Entwicklung von KI-Foundation-Modellen im internationalen Vergleich. Es zeigt, dass die USA und China aktuell führend sind und erläutert die Gründe, warum Europa bislang keine Vorreiterrolle in der Entwicklung großer KI-Modelle einnimmt.\\n\\nAbb. 11: Um 2016 tauchte ein neuer Trend zu sehr großen Modellen auf, die von großen Internetfirmen trainiert wurden (rot). Diese waren in der Lage, die notwendigen Investitionen zu finanzieren. Die untere blaue Linie veranschaulicht den Berechnungsaufwand der anderen Modelle, z.B. von Universitäten (Sevilla et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n56\\n\\nIm internationalen Vergleich zeichnen sich folgende Trends deutlich ab: 2\\n\\nVon der Wissenschaft oder KI-Community entwickelte Modelle zur öffentlichen Verwendung im Text- und Bildbereich liegen etwa 14 bzw. 15 Monate nach der Erstentwicklung aus dem wirtschaftlichen Sektor (Benaic & Hogarth, 2022).\\n\\nDer Großteil der veröffentlichten Foundation-Modelle stammt aus den USA und China: Seit 2017 stammen 73 % aus den USA, 15 % aus China.\\n\\nDie meisten der entwickelten Modelle stammen aus dem wirtschaftlichen, nicht wissenschaftlichen Umfeld: 86 % der Modelle wurden seit 2017 aus dem wirtschaftlichen Sektor heraus entwickelt, 13 % im wissenschaftlichen Sektor.\\n\\nAbb. 12: Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) 3 unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach Ländern\\n\\n2 Für den internationalen Vergleich wurden 125 Foundation-Modelle ab der initialen Entwicklung des Transformers in 2017 betrachtet. Dazu wurde der Datensatz von Sevilla et al. (2022) nach Foundation-Modellen gefiltert und um aktuelle Veröffentlichungen erweitert. 3 Gleitkommaoperationen, Plural von FLOP (floating-point operation) - hier eine Maßeinheit zum Vergleich von Trainingsaufwänden gemessen in den kleinsten Operationen, nicht zu verwechseln mit FLOPS (floating-point operations per second).\\n\\nGroße KI-Modelle für Deutschland\\n\\n57\\n\\nAbb. 13: Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach vier Kategorien von Organisationen\\n\\nDie Übersichtsgrafiken (Abb. 12 und 13) zeigen ganz eindeutig, dass das Technologiegebiet von zwei Ländern, den USA und China, dominiert wird, und dass Europa einschließlich Deutschland im Verhältnis zu seiner Größe und Rolle in der globalen Gesellschaft stark unterrepräsentiert ist. 73 % der Modelle stammen aus den USA und 15 % aus China.\\n\\nAus Deutschland und anderen EU-Ländern wurden tendenziell kleinere Foundation- Modelle mit niedrigerem Trainingsaufwand veröffentlicht. Das BLOOM Modell stellt eine Ausnahme dar und wurde von einem Wissenschaftskollektiv bestehend aus über 250 Institutionen auf dem Jean Zay Supercomputer in Frankreich trainiert. Als einziges Modell aus Deutschland taucht in den Abbildungen 12 und 13 das Modell Luminous des deutschen KI-Unternehmens Aleph Alpha auf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n58\\n\\n[Aleph Alpha]\\n\\nAleph Alpha aus Heidelberg ist ein unabhängiges, deutsches KI- Forschungsunternehmen, gegründet von Jonas Andrulis und Samuel Weinbach. Jonas ist erfolgreicher KI-Serienunternehmer und ehemaliger Senior Manager aus Apples KI-Forschungsabteilung für geheime Innovationsprojekte in Kalifornien. Samuel hat 10 Jahren Erfahrung für KI-Innovation und ist aktuell einer der führenden Köpfe für das Engineering von großen Sprachmodellen.\\n\\nAleph Alpha hat auf europäischen Daten und in fünf Sprachen ein GPT3-Äquivalent entwickelt, das in der größten Ausbaustufe die doppelte Anzahl von Parametern verglichen zu OpenAIs bestem Angebot bietet. Zusätzlich entwickelte das Team um Jonas und Samuel eine multimodale Erweiterung, die nicht nur Text, sondern auch Bilder im Kontext versteht. Damit hat Aleph Alpha nach eigenen Aussagen Anfang 2023 das weltweit einzige multimodale Angebot für große Sprachmodelle. Diese und weitere Innovationen werden in zahlreichen akademischen Publikationen und Open-Source Veröffentlichungen mit der Community geteilt. Mit vielen der wissenschaftlichen Spitzenforschern besteht eine enge Zusammenarbeit.\\n\\nAleph Alpha gelang der Aufbau eines Teams aus 50 internationalen Experten von den besten Unternehmen aus den USA und dem Rest der Welt. In zwei Finanzierungsrunden konnte das Team mit der Unterstützung von einigen der Spitzeninvestoren unabhängige Rekordfinanzierung in Höhe von 28 Mio. EUR sichern.\\n\\naus Deutschland\\n\\nund\\n\\nEuropa\\n\\neine\\n\\nZum souveränen Betrieb auch für sicherheitskritische Anwendungsszenarien hat Aleph Alpha ein spezialisiertes Rechenzentrum aufgebaut und betriebt damit leistungsstärkste kommerzielle KI-Rechenzentrum Europas. Die aktuell das Technologie von Aleph Alpha wird aktuell in Unternehmen verschiedenster Größe und Branche im Finanzsektor, bei Gesundheit, Recht und in Verwaltung und Sicherheit eingesetzt.\\n\\nDurch die angedachte LEAM-Infrastruktur könnte Aleph Alpha in den Forschungs- und Open Source-Projekten auch für moderne Foundation-Modelle mit hohen Anforderungen Unterstützung anbieten. Auch für Aleph Alphas KMU-Kunden ohne eigene Rechenzentren sind diese Möglichkeiten entscheidende Zutaten für den Eintritt in ein neues Technologiezeitalter.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n59\\n\\nAlle erheblichen Entwicklungssprünge im Bereich der Foundation-Modelle kommen aus den USA und zu einem geringeren Grad auch aus China. Das betrifft sowohl die Arten der Modelle und neue Funktionalitäten, als auch Sprünge in der Modellgröße und die Erschließung neuer Datendomänen.\\n\\nIn den USA sind die wichtigen Durchbrüche in wenigen Industrielaboren erzielt worden oder Investitionen über gigantische Infrastrukturen und großzügige Personalmittel für Spitzenforscher:innen verfügen. Dazu gehören die Labs von Google, Meta, Microsoft, Amazon sowie OpenAI und zu einem geringeren Anteil auch AllenAI.\\n\\nin Labs, die durch privatwirtschaftliche\\n\\nIn China wurde mit dem Modell Wu Dao 2 an der Beijing Academy of AI der erste große Durchbruch aus mehreren Spitzenuniversitäten und arbeitet eng mit industriellen Partnern (u.a. Xiaomi, Meituan und Kuaishou) zusammen. Ein großer Teil der Investition stammt vom chinesischen Staat. Weitere große Modelle kommen aus den Forschungslaboren von Alibaba, Baidu, Tencent, Huawei, Inspur und anderen chinesischen Hightech-Konzernen.\\n\\nerreicht. Dieses\\n\\nLabor\\n\\nvereint\\n\\nExpert:innen\\n\\nUnter dem Aspekt der Innovationskraft können wir grob drei Klassen von Modellen unterscheiden. Als diese gelten:\\n\\ni. Die bekanntesten Modelle, die jeweils bahnbrechende Durchbrüche\\n\\nrepräsentieren,\\n\\nii. Modelle, die bestehende Modelltypen in der Größe, den Daten oder ihren\\n\\nFunktionalitäten verbessern oder erweitern sowie\\n\\niii. Modelle, die bestehende Modelle mehr oder weniger genau kopieren und sich technologisch nur unwesentlich von ihren Vorbildern unterscheiden. Dazu gehören auch verkleinerte oder vereinfachte Nachahmungen.\\n\\nWährend Modelle der Klassen (i) und (ii) wesentlich zur Evolution der Technologie beitragen, sorgen Modelle der Klasse (iii) für eine Verfügbarkeit der Modelle in Organisationen und Geschäftsmodellen, in denen die Nutzung zur aus wirtschaftlichen, technischen oder regulatorischen Gründen nicht möglich war.\\n\\nDie Entwicklung von Modellen der Klassen (ii) und (iii) hat einen wichtigen Seiteneffekt: Sie bewirkt den Aufbau oder die Verstärkung der technologischen Kompetenz für zukünftige Technologieentwicklungen\\n\\nAlle Modelle der Klasse (i) und die meisten Modelle der Klasse (ii) wurden in den USA und China geschaffen. Die meisten Modelle aus anderen Ländern kopieren die GPT- Architektur, manchmal mit geringen Modifikationen, und erweitern die Lerndaten, um gewisse Sprachen (mitunter auch Anwendungen) besser abzudecken. Dazu gehören die Modelle aus Israel, Russland, Schweden, Frankreich und Deutschland. Es gibt derzeit einige Modelle der Klasse (ii), u.a. das Modell GPT-SW3 von Sweden AI, das Modell Luminous der deutschen Firma Aleph Alpha und das Modell BLOOM, das von der französischen Initiative OpenScience gemeinsam mit Hugging Face geschaffen wurde. Alle\\n\\nGroße KI-Modelle für Deutschland\\n\\n60\\n\\ndiese Modelle wurden allerdings erst 15-24 Monate nach der Veröffentlichung von GPT-3 fertiggestellt.\\n\\nUm Foundation-Sprachmodelle entwickeln zu können, müssen drei essentielle Voraussetzungen erfüllt sein: Kompetenz, Infrastruktur und Daten.\\n\\nVerfügbarkeit von intellektueller Kompetenz: Gibt es genügend Expert:innen, die das notwendige Wissen und u.U. auch Erfahrung für die Modellentwicklung mitbringen, und sind diese Personen verfügbar?\\n\\nVerfügbarkeit von Infrastruktur: Reicht die Recheninfrastruktur für die Modellentwicklung, das heißt minimal für Datenaufbereitung, Training und Evaluation?\\n\\nVerfügbarkeit von Daten: Gibt es hinreichende Mengen von Daten in einer oder mehreren Sprachen, um damit die emergenten Fähigkeiten zu erzeugen? Das ist nicht alleine eine Frage der Menge an Daten, denn durch hohe Diversität und Qualität kann ein Mangel an Masse bis zu einem gewissen Grad kompensiert werden.\\n\\nDie Auswertung der Expert:inneninterviews und Umfragen (s. Kapitel 4 und 5) hat gezeigt, dass sowohl Deutschland als auch Europa in allen drei Voraussetzungen vor erheblichen Herausforderungen stehen. Im Rahmen der Studie wird ein Fokus auf die verfügbare Infrastruktur gelegt. Dennoch muss betont werden, dass für erfolgreiche europäische KI- Foundation-Modelle alle drei Voraussetzungen erfüllt werden müssen.\\n\\nDie Verfügbarkeit von hinreichenden Infrastrukturen ist derzeit zum Flaschenhals für die erfolgreiche Beteiligung bei der Technologieentwicklung und -kommerzialisierung geworden. Infrastrukturen, die neue Modelle der Klasse (i) ermöglichen, sind weitaus größer als die Mindestvoraussetzungen für die Entwicklung von Modellen der Klasse (ii). Die Forschung an der Spitze der Technologieentwicklung erfordert das kreative Experimentieren mit vielen Kandidaten für neue Architekturen und Lernmethoden und deren zahlreichen Varianten. Das erschwert die Teilnahme von Universitäten an dieser Forschung. 86 % der Modelle wurden aus dem wirtschaftlichen Sektor heraus entwickelt und nur 13 % im wissenschaftlichen Sektor. Von der Wissenschaft oder KI-Community entwickelte Modelle zur öffentlichen Verwendung im Text- und Bildbereich liegen etwa 14 bzw. 15 Monate nach der Erstentwicklung in den Industrielaboren.\\n\\nSelbst die großen US-Universitäten können sich ohne Kooperationen mit den Forschungslaboren der Industrie nicht mehr am Wettbewerb beteiligen. Die US-Regierung hat daher die National Artificial Intelligence Research Resource Task Force (NAIRRTF) eingerichtet, um die Infrastrukturen für die KI-Forschung deutlich zu verbessern (THE NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH RESOURCE TASK FORCE (NAIRRTF), o.D.).\\n\\nGroße KI-Modelle für Deutschland\\n\\n61\\n\\nVersuche, die durchaus beachtlichen Hochleistungscomputer der wissenschaftlichen Hochleistungsrechenzentren für das Trainieren von Foundation-Modellen zu nutzen, haben nur begrenzten Erfolg. Es lassen sich zwar mit guter Planung neue GPT-Modelle trainieren und das auch mit zusätzlichen Daten und kleinen Modifikationen. Für die für das systematische Optimierung und Weiterentwicklung der Modelle und Experimentieren mit neuen Modelltypen fehlt jedoch die kontinuierliche Verfügbarkeit von hinreichend großen Computerressourcen. Auch für die Entwicklung und Evaluation neuer Anwendungsklassen können Hochleistungscomputer der Wissenschaft nicht genutzt werden.\\n\\nUm die Entwicklung von Modellen der Klasse (iii) muss man sich weniger Sorgen machen. Durch die kontinuierliche Zunahme von Rechenleistung in Industrie und Wissenschaft und durch die ständige Verbesserung der Lernverfahren wird das Kopieren von Modelltypen leichter werden.\\n\\nWährend die Anforderungen an Rechenkapazität für das Trainieren von Modellen, selbst mit Erweiterungen der Lerndaten, eher abnehmen, nimmt die erforderliche Rechenleistung für die Entwicklung von Modellen der nächsten Generationen noch stark zu. Der Grund dafür ist die bevorstehende Fusion von Modalitäten zum Erwerb von Weltwissen und zusätzlichen Funktionalitäten, die die Nutzung von großen Volumina an Filmdaten und analogen Daten aus der realen Welt erfordern. Das Kapitel 8 beleuchtet das Thema der Infrastruktur näher.\\n\\nGroße KI-Modelle für Deutschland\\n\\n62\\n\\n[OpenGPT-X]\\n\\nOpenGPT-X ist ein Kooperationsprojekt mit Partnern aus Wissenschaft, Wirtschaft und Technologie. Das Ziel des Projektes ist die Schaffung großer KI-Sprachmodelle, um innovative Sprachanwendungen für Europa und nach europäischen Werten voranzutreiben. Mittels Gaia-X, der sicheren Dateninfrastruktur zur Förderung der Innovation in Europa, wird OpenGPT-X KI-Sprachmodelle und Sprachdienste europaweit offen und in mehreren Sprachen zur Verfügung stellen.\\n\\nAls Anwendungsbeispiele zukünftige Produktentwicklungen werden beispielsweise im Bereich Medien KI-Sprachmodelle zur Fragenbeantwortung bei interaktiven Medienformaten entwickelt. In der Domäne Finanzwesen werden die der Modelle Schadensabwicklung durch Versicherungen eingesetzt und in der Mobilität sollen sie als persönliche Assistenten beim Autofahren zu mehr Sicherheit und Fahrkomfort beitragen.\\n\\nfür\\n\\nfür\\n\\neine\\n\\neffizientere\\n\\nDokumentenverarbeitung\\n\\nbei\\n\\nAktuell trainieren die Projektpartner:innen ein erstes Modell mit zwölf Milliarden Parametern. Dabei liegt der Fokus darauf, möglichst inhaltlich korrekte Antworten von dem Sprachmodell zu erhalten.\\n\\nDas Vorhaben wird vom Bundesministerium für Wirtschaft und Klimaschutz von Januar 2022 bis Dezember 2024 im Rahmen des Förderprogramms Innovative und im digitalen Ökosystem Gaia-X praxisnahe Anwendungen und Datenräume gefördert. Beteiligt sind das Fraunhofer-Institut für intelligente Analyse- und Informationssysteme (IAIS), das Fraunhofer-Institut für Intelligente Schaltungen (IS), 1&1 IONOS SE, das Forschungszentrum Jülich, die Technische Universität Dresden, die Alexander Thamm GmbH, das Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Aleph Alpha, Control Expert, Westdeutscher Rundfunk (WDR) und der KI Bundesverband.\\n\\nZwischen OpenGPT-X und LEAM gibt es einen engen Austausch. Die Ergebnisse und Erfahrungen des Projektes sind in die Ausarbeitung der Machbarkeitsstudie eingeflossen und werden auch weiterhin in die Planung von LEAM einfließen.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n63\\n\\nBedeutung für die technologische Souveränität Deutschlands Im internationalen Vergleich wird deutlich: Deutsche Unternehmen sind aus Mangel an leistungsfähigen europäischen Alternativen maßgeblich auf die Nutzung amerikanischer und chinesischer Foundation-Modelle angewiesen. Daraus ergeben sich eine Reihe an Herausforderungen, denn die bestehenden Modelle erfüllen in vielerlei Hinsicht nicht die europäischen Qualitätsstandards.\\n\\nDiese Defizite fassen wir in fünf Punkten zusammen:\\n\\n(1) Inhaltliche Funktionalität: Deutsche und europäische Inhalte sind unterrepräsentiert. Das gilt insbesondere für wirtschaftliche, gesellschaftspolitische, rechtliche, regionale und kulturelle Themen. Die großen Modelle sind zudem eher auf den Massenmarkt hin ausgerichtet als auf die Anforderungen der Anwendungen in Wirtschaft, Politik, Bildung.\\n\\n(2) Sprachliche Funktionalität: Die bestehenden Modelle sind durch die verwendeten Lerndaten, aber auch durch die Evaluationsdaten und Benchmarks viel stärker auf das Englische, im Falle der chinesischen Modelle auch auf das Chinesische, hin ausgerichtet als auf andere Sprachen. Sprachen, die im Internet stark vertreten sind, wie das Deutsche, Französische und Spanische, sind zwar auch gut repräsentiert, aber in der messbaren Funktionalität längst nicht so wirksam abgedeckt. Die meisten europäischen Sprachen sind nur sehr unzureichend repräsentiert. Ein wichtiges Ergebnis der linguistischen Forschung ist, dass Sprache das Denken beeinflusst und sogar die grundlegenden Aspekte menschlicher Erfahrung verändert: Raum, Zeit, Kausalität und die Beziehung zu anderen (Boroditsky, 2012). Um die deutsche Kultur adäquat zu erfassen, sind daher Foundation-Modelle für die deutsche Sprache erforderlich.\\n\\n(3) Verfügbarkeit: Die\\n\\nihrer Eigentumsverhältnisse und Lizenzmodelle nur eingeschränkt für kommerzielle Anwendungen einsetzbar. Das gilt für die Anpassung durch modifizierte (erweiterte, korrigierte, gefilterte) zusätzliches Pretraining durch andere Trainingsaufgaben, Inferenz (also praktischen Einsatz), few-shot prompting, large- scale fine-tuning und Integration in umfangreichere Anwendungen.\\n\\ngroßen\\n\\ninternationalen Modelle\\n\\nsind wegen\\n\\nLerndaten,\\n\\n(4) Sicherheit, Verlässlichkeit: Für viele europäische Anwendungen wären zusätzliche Maßnahmen zur Gewährleistung besserer Performanz in Hinblick auf Sicherheit und Verlässlichkeit erforderlich, in Korrektheit, Konsistenz und Datenschutz. Zu den Sicherheitsanforderungen gehört aber auch die Vertraulichkeit der Eingabedaten im Test- und Inferenzgebrauch. Eine weitere Anforderung ist die Verlässlichkeit im Hinblick auf Persistenz, das heißt die langfristige ständige Verfügbarkeit der eingesetzten vortrainierten und insbesondere der durch aufwendiges Nachtraining angepassten Modelle. Dies gilt vor allem dann, wenn diese oder in Anwendungen Sicherheitsinteressen ausfallsicher und unterbrechungsfrei betrieben werden müssen.\\n\\ninsbesondere\\n\\neingesetzt werden,\\n\\ndie\\n\\naus wirtschaftlichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n64\\n\\n(5) Ethische Akzeptabilität: Die Ausgaben der Modelle verletzen mitunter durch Bias (Aussagen/Entscheidungen basierend auf falschen Vorurteilen) und Toxizität (Verwendung von ethisch oder stilistisch-ästhetisch nicht akzeptablen sprachlichen Ausdrucksweisen) die de-facto Standards für den Einsatz in Wirtschaft, Politik und Bildung. Daher muss es für die Anwendung möglich sein, Korrektur- und Filtermaßnahmen eigenständig zu definieren, in die Modelle zu integrieren und anzupassen.\\n\\nDie europäische Forschung kann mit Recht stolz sein auf ihre ersten europäischen Foundation-Sprachmodelle wie Aleph Alpha, BLOOM oder GPT-SW3. Weitere Modelle wie Open GPT-X sind in der Vorbereitung. Diese europäischen Modelle sind ermutigende Beispiele dafür, dass Europa, wenn auch mit etwas Verspätung, Foundation- Sprachmodelle entwickeln kann. Sie sind aber noch kein Indiz für ein Aufschließen der europäischen Forschung in die Avant-Garde der internationalen Forschung zu diesem Thema. Zudem decken diese Modelle trotz großer Fortschritte bisher weder die deutsche Sprache noch die Bandbreite der anderen europäischen Sprachen in dem Maße ab, wie es heute bereits für das Englische erreicht ist.\\n\\nEin Grund dafür sind die materiellen Forschungsbedingungen. In der Künstlichen Intelligenz, wie auch in der gesamten Informatik gilt, dass nur Forschungsgruppen, die für eine neue Technologie spielerisch viele Möglichkeiten der Realisierung, Adaption und Optimierung ausprobieren können, längerfristig erfolgreich sein können. Wenn im Falle der Foundation-Modelle die benötigten materiellen Infrastrukturen für Computation und Speicherung so groß sind, dass die universitäre Forschung von der Spitzenforschung ausgeschlossen ist, werden die neuen Durchbrüche aus den Spitzenlaboren der Industrieunternehmen kommen, die sich zusätzlich zur Infrastruktur auch personell und kulturell einen echten Forschungsbiotop aufbauen und leisten können. Ein Kennzeichen solch erfolgreicher Strukturen ist der scheinbare Überfluss, gekennzeichnet durch Redundanz in Infrastruktur und personeller Kompetenz.\\n\\nDer Begriff AI CERN für die von CLAIRE 4 und anderen Akteuren geforderte und dringend benötigte Infrastruktur für die europäische KI-Forschung ist eine wirksame Metapher, aber im gewissen Sinne auch irreführend. Der infrastrukturelle Bedarf der Teilchenphysik unterscheidet vom Bedarf der KI und der gesamten von Softwaretechnologieforschung: Während Grundlagenforschungsprojekten gebucht werden kann, um in wenigen sehr großen Experimenten grundlegende Forschungsfragen zu beantworten, nutzen die KI- Forschungsgruppen der großen industriellen Labore die Computerressourcen fast durchgängig für große Zahlen von Experimenten für das schrittweise Trainieren großer Modelle und für deren Evaluation und Modifikation. Die Evolutionszyklen bestehend aus Modifikation der Technologie und Selektion sind sehr kurz, weil sie nicht wie in der Physik oder in den klassischen Ingenieurwissenschaften in der physikalischen Welt realisiert und\\n\\nsich\\n\\nsich\\n\\nstark\\n\\ndie\\n\\nInfrastruktur\\n\\ndes CERN\\n\\n4 Die Confederation of Laboratories for Artificial Intelligence Research in Europe (CLAIRE) ist ein Verein mit dem Ziel, Forschung, Innovation und Zusammenarbeit im Bereich der KI zu verstärken. Mehr als 1000 KI-Expert:innen aus ganz Europa unterstützen die Bestrebungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n65\\n\\ngetestet werden müssen. Auch die Wege vom Labor in die Anwendungen können in der KI sehr kurz sein. So konnte Google die großen Sprachmodelle zeitnah nach ihrer Fertigstellung bereits im Produktionsbetrieb für die Suche und für die Platzierung von Werbung testen.\\n\\nWegen der hohen infrastrukturellen Anforderungen gibt es bisher weltweit nur eine kleine Zahl von Forschungslaboren und Entwicklungs\\xadzentren, die den heutigen Stand der KI-Technologie in ihrer vollen Komplexität beherrschen. Das heißt, es gibt nur ganz wenige Organisationen, die Training und Evaluation von großen multilingualen, multimodalen und multimedialen Foundation-Modellen selbst ausführen können. Mit der multimodalen und multimedialen Verknüpfung der Daten und der Kombination der assoziativen Modelle mit expliziten Daten- und Wissensrepositorien nimmt diese Komplexität noch weiter zu.\\n\\nMit dieser Komplexität wachsen natürlich auch die Einstiegsbarrieren für neue Akteure, denn zusätzlich zu gut ausgebildeten Expert:innen wird deren extensive individuelle und kollektive Erfahrung im Trainieren, Evaluieren und Einsatz großer Foundation-Modelle benötigt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n66\\n\\nBedarf der Wirtschaft an KI- Foundation-Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n67\\n\\n4. Bedarf der Wirtschaft an KI-Foundation- Modellen\\n\\nEine grundlegende Frage, die diese Studie klären möchte, ist, welche Bedeutung Foundation-Modelle für die Wirtschaft haben und welche Anforderungen Unternehmen an die Modelle stellen.\\n\\nUm dieser Frage auf den Grund zu gehen wurden zwei Methoden angewandt 5. Diese sind:\\n\\neine Umfrage an kleine und mittlere Unternehmen, die sich auf die Entwicklung von KI spezialisiert haben sowie\\n\\n\\n\\nleitfadengestützte Interviews mit den Leiter:innen KI, Data, o. ä. deutscher Großunternehmen.\\n\\nDie an der Umfrage teilnehmenden Unternehmen lassen sich wie folgt zusammenfassen:\\n\\nAnzahl Empfänger\\n\\n373 Mitglieder des KI Bundesverbandes 25 Kontakte aus dem Netzwerk der Merantix Momentum GmbH\\n\\nAnzahl Antworten\\n\\n71 Teilnehmer (18 % Rückmeldequote)\\n\\nGröße der Unternehmen 90 % weniger als 100 Mitarbeiter\\n\\nBranche der Unternehmen (Mehrfachnennung möglich)\\n\\n80 % Informationstechnologie 25 % Dienstleistungen je 11 % Industrie & Medien\\n\\nEinsatz von KI\\n\\n90 % setzen KI ein\\n\\nTabelle 3: Übersicht der wichtigsten Information zu der Umfrage für die Wirtschaft\\n\\n5 Eine genauere Betrachtung der Methodik findet sich im Kapitel V.\\n\\nGroße KI-Modelle für Deutschland\\n\\n68\\n\\nInterviews wurden mit folgenden Personen durchgeführt. Ergebnisprotokolle der Interviews befinden sich im Anhang B.3:\\n\\nTitel\\n\\nVorname\\n\\nName\\n\\nUnternehmen\\n\\nDr. Dr. Dr. Dr.\\n\\nDr. Dr.\\n\\nDr. Dr.\\n\\nDr.\\n\\nDr.\\n\\nDr. Dr. Dr. Dr. Dr. Dr. Dr.\\n\\nDr.\\n\\nWolfgang Maik Marion Hans-Jörg Michael Jean-Paul Mario Corina Matthias Stephan Frank Sebastian Thomas Sabine Nico Rainer Christian Stephan Jochen Michael Armin Lorenz Feiyu Andreas Michael Dirk\\n\\nSebastian\\n\\nHauner Friedel Legler Vögel Fausten Schmetz Deng Apachiţe Dorner Kaulbach Säuberlich Kaiser Wolf Donauer Kelling Sträter Spannbauer Meyer Kaiser Müller-Wünsch Kurrle Determann Xu Wierse May Schlesinger\\n\\nHallensleben\\n\\nAllianz SE BASF SE Bayer AG BMW Group Robert Bosch GmbH Burda Media BWI GmbH Continental AG DATEV eG Deutsche Bahn AG EnBW Energie Baden-Württemberg AG Ergo Group AG Hugging Face, Inc. Infineon Technologies AG Infineon Technologies AG Ionos SE Lufthansa Group Munich RE Mercedes-Benz Group AG Otto GmbH & Co KG Porsche AG Rewe Group SAP SE sicos BW GmbH Siemens AG TÜV Süd AG VDE Elektronik Informationstechnik e. V. Volkswagen AG Zalando SE\\n\\nVerband\\n\\nder\\n\\nElektrotechnik\\n\\nDr. Dr.\\n\\nPatrick Alexander\\n\\nvan der Smagt Borek\\n\\nTabelle 4: Befragte Expert:innen aus der Wirtschaft\\n\\nin die LEAM- Alle Erkenntnisse aus der Umfrage und den Machbarkeitsstudie eingeflossen. Im Folgenden werden wir zusammenfassend einige Aspekte und Übereinstimmungen illustrieren.\\n\\nInterviews sind\\n\\nGroße KI-Modelle für Deutschland\\n\\n69\\n\\nAuswertung der Interviews und Umfrage Ungefähr 66 % der befragten KI-Unternehmen setzen bereits Foundation-Modelle ein oder beabsichtigen, diese in der Zukunft einzusetzen. Dies ist ein beachtlicher Anteil, wenn man bedenkt, dass Foundation-Modelle eine relativ neue Entwicklung der interviewten Intelligenz sind. Daneben bestätigen auch viele der Künstlichen Großunternehmen, bereits KI-Foundation-Modelle im produktiven Einsatz zu haben oder aktuell an Anwendungen zu arbeiten. Dadurch wird eindeutig, wie essentiell Foundation- Modelle für die gesamte Wirtschaft bereits sind. Die Interviewten geben außerdem an, dass die Bedeutung in den nächsten Jahren weiter steigen wird. Es sei aktuell noch nicht abzusehen, welche Disruption und neuen Geschäftsmodelle KI-Foundation-Modelle in den nächsten Jahren ermöglichen werden.\\n\\nAbb. 14: Ergebnisse der Umfrage mit KMUs zu deren Einsatz von Foundation-Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n70\\n\\nKI-Unternehmen, die aktuell noch nicht mit KI-Foundation-Modellen arbeiten, geben verschiedene Gründe an, die erfüllt werden müssten, damit sie Foundation-Modelle nutzen würden. Genannt wurden hier insbesondere niedrigere Kosten, die Bereitstellung von Open Source-Modellen sowie die Verfügbarkeit von Daten, die jeweils rund 58 % der Befragten angaben. Weitere Hürden, die Unternehmen als Gründe äußerten, Foundation- Modelle nicht zu nutzen, sind ein Mangel an Recheninfrastruktur (38 %), datenschutzrechtliche Hürden (33 %), ein Mangel an qualifizierten Mitarbeiter:innen (25 %) sowie verfügbaren europäischen Modellen (25 %).\\n\\nAbb. 15: Ergebnisse der Umfrage mit KMUs zu Hindernissen beim Einsatz von Foundation- Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n71\\n\\nAuch Großunternehmen teilten diese Bedenken in den geführten Interviews. Dabei wurde deutlich: Großunternehmen verfolgen nicht das Ziel, eigene Foundation-Modelle zu entwickeln. Stattdessen wollen sie bestehende Modelle für ausgewählte Anwendungen anpassen. In der aktuellen Lage ist dies häufig aber aus Compliance-Gründen nicht möglich, da die Modelle nicht frei, sondern nur über Programmierschnittstellen zur Verfügung stehen und Daten für das Tuning aus dem europäischen Wirtschaftsraum heraus gesendet werden müssen. Vor allem mit sensiblen Datensätzen ist daher ein Anpassen der Modelle für Großunternehmen nicht möglich. Die befragten Großunternehmen sehen darin einen klaren Nachteil im internationalen Wettbewerb. Selbst im stark reglementierten und national ausgeprägten Versicherungsumfeld berichten Unternehmen, dass neue, digitale Geschäftsmodelle, die außerhalb Europas entstehen, eine Gefahr darstellen.\\n\\nDarüber hinaus zeigten die Interviews mit Großunternehmen, dass generelle Foundation- Modelle häufig nicht ausreichen, um den hohen Qualitätsstandards der Unternehmen zu entsprechen. Stattdessen brauche es Foundation-Modelle, die auf die Bedürfnisse einzelner Branchen abgestimmt sind. Das Ziel sollte es also sein, bspw. ein Gesundheitsmodell, ein Industriemodell und ein Versicherungsmodell zu entwickeln, auf deren Basis die Unternehmen dann einzelne Anwendungen entwickeln können. Trotz bestehender Datensätze findet diese Entwicklung aktuell nicht statt, da die deutschen Unternehmen ihre Patienten-, Maschinen- und Versicherungsdaten nicht in die USA übertragen möchten. Dieser Wunsch nach speziellen Modellen spiele auch eine Rolle, da auf dem Datensatz einer bestimmten Population trainierte Modelle nicht direkt auf andere Populationen übertragbar seien. So berichtet beispielsweise das Chemie- und Pharmaunternehmen Bayer davon, dass sich amerikanische Patient:innendaten von europäischen, asiatischen oder afrikanischen unterscheiden.\\n\\nDaneben fehle es in vielen Unternehmen an gut ausgebildeten Mitarbeiter:innen und es sei schwierig, KI-Modelle in den laufenden Betrieb zu integrieren.\\n\\nInsgesamt wird die Bedeutung von Foundation-Modellen für die gesamtwirtschaftliche Entwicklung Deutschlands mit 73 % als sehr hoch eingeschätzt. Besonders der Aufbau eines europäischen KI-Ökosystems wird von 82 % der Befragten, die Berücksichtigung von Werten wie Transparenz, Reduktion von Bias und Nachhaltigkeit von 85 % der Befragten als relevant erachtet. Dabei wird die direkte Zusammenarbeit mit der Forschung etwa von der Hälfte (54 %) der Befragten als bedeutend eingeschätzt. Die Zusammenarbeit mit KMUs und Start-ups hingegen von 67 %. Neben der bereits hohen Nutzung von Foundation-Modellen zeigen diese Ergebnisse die gesamtwirtschaftliche Relevanz, die ihnen in der Industrie zugesprochen wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n72\\n\\nAbb. 16: Ergebnisse der Umfrage mit KMUs zur Bedeutung von unterschiedlichen Aspekten der Foundation-Modell-Entwicklung\\n\\nDie Interviewten sehen in KI-Foundation-Modellen eine strategische Bedeutung für den Wirtschaftsstandort Europa. So betonen sie die Notwendigkeit eigener europäischer Modelle, um die Wettbewerbsvorteile der Technologie vollständig zu nutzen, die Qualität der Modelle umfänglich zu kontrollieren und Sicherheitsrisiken zu minimieren. Der letzte Punkt sei mit Hinblick auf die geostrategische Situation Europas und existierende Diskussionen rund um 5G und Huawei besonders wichtig. Aktuell haben für die befragten Unternehmen besonders Sprachmodelle eine hohe Relevanz bei der Nutzung und Entwicklung von Foundation-Modellen (genannt von 71 % der Unternehmen). Dies reflektiert den hohen Erfolg und die Prominenz von Sprachmodellen, wie GTP-3. Ebenfalls als wichtig eingeschätzt werden multilinguale Sprachmodelle (52 %) und multimodale Modelle (38 %). Beide Arten von Modellen stellen logische nächste Schritte von Sprachmodellen dar und werden von Unternehmen wie OpenAI mit Dall-E 2 und ChatGPT bereits erfolgreich entwickelt. Wie oben angedeutet, gehen die Einschätzungen der Großunternehmen noch einen Schritt weiter. In den Interviews wurden zwar auch vor allem Sprach- und Multimodale Modelle genannt, mit denen viele Unternehmen bereits experimentieren, daneben brauche es aber speziellere Modelle für einzelne Anwendungsbereiche. In der aktuellen Situation seien nur inkrementelle Fortschritte und keine Disruption möglich.\\n\\nGroße KI-Modelle für Deutschland\\n\\n73\\n\\nMit jeweils circa 34 % der Befragten finden auch Geschäfts- und Fertigungsprozesse sowie Robotik als bedeutsame Bereiche Beachtung. Diese werden in der aktuellen Foundation- Modell Entwicklung wenig fokussiert behandelt. Hier kann für LEAM eine Chance liegen, diese Nischen in der Modellentwicklung zu besetzen.\\n\\nAbb. 17: Ergebnisse der Umfrage mit KMUs zur Relevanz von unterschiedlichen Arten von KI- Modellen bei der Foundation-Modell-Entwicklung (Antworten mit einer Antwortrate von weniger als 20 % wurden ausgelassen. Die vollständigen Antworten befinden sich in Anhang A.2)\\n\\nInsgesamt zeigt die Auswertung den Bedarf an europäischen Foundation-Modellen. Die befragten Unternehmen hatten am Ende der Umfrage die Möglichkeit, weitere Kommentare zu hinterlassen. Ähnlich wie in den Expert:inneninterviews mit der Wissenschaft (s. Kapitel 5) wurde vermehrt angemerkt, dass „alle außer [den] große[n] Internet-Konzerne[n] darauf beschränkt sind, existierende Foundation-Modelle zu benutzen” und somit in massive Abhängigkeit geraten. Biases können so bei down-stream Anwendungen nur schwer vermieden werden. Ein weiteres häufiges Thema in den Antworten war, dass die europäische Wirtschaft bereits jetzt im internationalen Vergleich zurückliegt. Diese Aussagen bestätigen auch die Interviews mit den Großunternehmen. Es brauche jetzt eine gemeinsame Aktion im Bereich Infrastruktur und Daten und es sei unbestritten, dass kompetitive europäische Modelle, amerikanischen vorzuziehen seien. Besonders im Bereich Datenschutz und -sicherheit würden europäische Modelle die Implementierung enorm vereinfachen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n74\\n\\nErfreulicherweise geben viele Interviewte an, die Initiative LEAM unterstützen zu wollen und einem Engagement im Rahmen eines Joint Ventures bzw. einer PPP generell offen gegenüberzustehen.\\n\\nDie Auswertung der Umfrage sowie der Interviews lässt sich in drei Kernaussagen zusammenfassen:\\n\\n(1) KI-Foundation-Modelle werden elementarer Bestandteil der IT-Anwendungs- Architektur. Branchenübergreifend implementieren bzw. planen Unternehmen die Implementierung von Anwendungen auf Basis der Modelle.\\n\\n(2) Aktuelle Modelle haben häufig den Nachteil, dass sie nicht Open Source verfügbar und damit frei anpassbar sind. Es gibt erhebliche datenschutzrechtliche Bedenken bzgl. der Nutzung außereuropäischer Modelle. Europäische Open-Source-Modelle werden daher als Alternative gebraucht.\\n\\n(3) Die Aufgabe kann nicht von einem Akteur alleine bewältigt werden. Es braucht eine gemeinsame Aktion der Unternehmen in Deutschland. Die Wirtschaft erkennt diesen Bedarf und steht einem Engagement offen gegenüber.\\n\\nGroße KI-Modelle für Deutschland\\n\\n75\\n\\nUnterstützung bei der Entwicklung durch Forschung und Wissenschaft\\n\\nGroße KI-Modelle für Deutschland\\n\\n76\\n\\n5. Unterstützung bei der Entwicklung durch Forschung und Wissenschaft\\n\\nAus den Interviews mit führenden Industrie- und KI-Unternehmen wird deutlich: Die Nachfrage nach europäischen Foundation-Modellen ist groß. Eines der primären Ziele, die sich LEAM daher gesetzt hat, ist es Foundation-Modelle für die Wirtschaft bereitzustellen.\\n\\nDie Entwicklung von Foundation-Modellen erfordert jedoch umfangreiche Maßnahmen im Bereich der Forschung und Entwicklung. Aus diesem Grund ist es wichtig, die spezifischen Anforderungen von Forschung und Wissenschaft zu kennen und mitzudenken, damit die Entwicklung von leistungsfähigen Foundation-Modellen gelingt.\\n\\nHierfür wurden im Rahmen dieser Studie Interviews mit führenden Wissenschaftler:innen aus Deutschland durchgeführt. Darin wird herausgestellt, wie der aktuelle Forschungsstand von KI-Foundation-Modellen in der Wissenschaft ist und welche Rahmenbedingungen nötig sind, um Wirtschaft und Wissenschaft bei der Entwicklung dieser entsprechend zu unterstützen.\\n\\nMethodik der Interviews Die Interviews wurden durch fünf Leitfragen strukturiert:\\n\\nLeitfrage 1 „Relevanz von Foundation-Modellen in der Wissenschaft“\\n\\nLeitfrage 2 „International führende Arbeitsgruppen“\\n\\nLeitfrage 3 „Wissenschaftliche und wirtschaftliche Defizite der Foundation- Modelle“\\n\\nLeitfrage 4 „Maßnahmen zur Förderung der Forschung und wirtschaftlichen Nutzung von Foundation-Modellen in Deutschland“\\n\\nLeitfrage 5 „Sonderstellung der Foundation-Modelle und zukünftige Entwicklungen“\\n\\nDie Interviews wurden mit 21 Expert:innen zwischen Anfang Oktober und Mitte Dezember 2022 geführt (s. Tabelle im Anhang B.1).\\n\\nDie Erkenntnisse aus den Interviews sind in die Planung von LEAM und an den passenden Stellen in den gesamten Text dieses Kapitels eingeflossen. Im Folgenden werden wir lediglich einige ausgewählte Aspekte der Interviews entlang der fünf Leitfragen illustrieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n77\\n\\nAuswertung der Interviews In der Leitfrage 1 „Relevanz von Foundation-Modellen in der Wissenschaft“ beschreiben sich die Expert:innen als Nutzer:innen der Modelle und erkennen die große Bedeutung der Modelle – auch für die Wirtschaft – als Forschungsparadigma der kommenden Jahre an. Für viele Expert:innen bestehen Hürden, um auf einer internationalen Bühne wissenschaftlich mitwirken zu können, da der Zugriff zu den notwendigen Technologien fehlt und sie somit in ihrer Forschung eingeschränkt sind. Foundation-Modelle können klimaverträglich gestaltet werden, wenn zentral verwaltete Modelle der KI-Community zur Verfügung gestellt werden können. Die Reproduktion der Modelle würde damit hinfällig.\\n\\nAls Antwort auf Leitfrage 2 „international führende Arbeitsgruppen“ wurden US- amerikanische Technologie-Konzerne wie Microsoft, Open AI, Google, Deepmind, und Meta genannt. In einzelnen Fällen wurden Universitäten wie Stanford oder die Carnegie Mellon University erwähnt, die bei den kleineren Sprachmodellen kompetitiv sind. Die chinesischen Modelle wurden hingegen selten erwähnt. Als Erfolgsfaktoren wurden der Zugang zu Infrastruktur, Daten und Talenten hervorgehoben.\\n\\nManche Interviewten betonten Deutschlands Stärke in anderen Gebieten wie dem Vorhandensein von gut ausgebildeten Wissenschaftler:innen, die allerdings aufgrund der internationalen Angebote oftmals nicht In diesem Zusammenhang wurde auch von der Abhängigkeit von den überwiegend kommerziellen Anbietern der Modelle gewarnt.\\n\\nin Deutschland bleiben.\\n\\nDie Leitfrage 3 „Wissenschaftliche und wirtschaftliche Defizite der Foundation-Modelle“ wurde mit der mangelnden Anpassbarkeit der Foundation-Modelle beantwortet. Es fehlt an deutschem und auch mehrsprachigem Vokabular, Fachwissen, Robustheit/Invarianz, sowie Erklärungen der Ergebnisse. Diese Hürden können nicht durch ein Nachtraining abgebaut werden, bzw. nur zu finanziellen Bedingungen der Hyperscaler. Probleme wie Intransparenz bezüglich der Trainingsdaten und -Prozeduren Bias, Privacy und erschweren die Nutzung. Die Vertrauenswürdigkeit der existierenden Modelle ist damit infrage gestellt und rechtliche Fragen bleiben unbeantwortet.\\n\\nDas kontinuierliche Lernen und auch die Verbindung mit Domänenwissen, Unternehmenswissen oder Applikationswissen, wie etwa Faktenwissen aus Wissensgraphen ist ausbaufähig. Die Frage nach der Kontrolle und Validierung der fehlende ist eine offene Forschungsfrage. Weitere Defizite sind Ergebnisse Geschäftsmodelle und die geringe Effizienz der Modelle. An letzterer wird aktiv geforscht, aber der Vergleich mit sehr großen Foundation-Modellen (z.B. GPT-3) kann nicht gezogen werden, was im Gegensatz zur wissenschaftlichen Praxis steht.\\n\\nDie Generalisierungsfähigkeit der Modelle wurde als unzureichend eingestuft. Derzeit werden zum größten Teil (Sprach-)Daten als einzige Wissensquellen genutzt, welche in zukünftiger Entwicklung ein limitierender Faktor sein können. Somit ist eine Verbesserung der Kuratierung von Trainingsdaten, die Anreicherung mit Wissen sowie mehr komplementäre Daten für die Modellentwicklung wie Ontologien, Sequenzdaten oder Bilder nötig.\\n\\nGroße KI-Modelle für Deutschland\\n\\n78\\n\\nFoundation-Modelle können als Kulturgut, bzw. ein öffentliches Gut für die Grundlagenforschung verstanden werden, die einem zentralem und transparenten Entwicklungsprozess unterliegen sollten, um Vertrauen zu schaffen. Andernfalls können Machtkonzentration und sinkende digitale Souveränität die Folge sein.\\n\\nAls Antwort zur Leitfrage 4 „Maßnahmen zur Förderung der Forschung und wirtschaftlichen Nutzung von Foundation-Modellen in Deutschland“ wurde hauptsächlich die Förderung einer Infrastruktur und der leichte Zugang zu jener für Wirtschaft und Wissenschaft genannt. Komplizierte Antragsverfahren und zu lange Wartezeit auf Rechenkapazität stellen eine erhebliche Hürde für beide Sektoren dar. Darüber hinaus ist die Kuratierung von geeigneten Trainingsdaten essentiell, wobei die europäische Sprachenvielfalt und Multimodalität fokussiert betrachtet werden sollten. Zusätzlich spielen Zeitreihendaten und Ontologien eine wichtige Rolle. In allen Fällen ist die Rechtssicherheit zu berücksichtigen und sicherzustellen, dass die Datenstrategie ethischen Prinzipien genügt.\\n\\nAttraktive Forschungsbedingungen als starkes Ökosystem sind nötig, in dem Forscher:innen kollaborieren können. Es sollen Anreize geschaffen werden, noch nicht stark digitalisierte, alte Industrien interessanter für junge Forscher:innen zu machen.\\n\\nIm Themenkomplex der Leitfrage 5 „Sonderstellung der Foundation-Modelle und implizite Wissen der Foundation-Modelle zukünftige Entwicklungen“ wurde das hervorgehoben. Die Modelle sind damit u.a. in der Lage, Programmiersprachen zu erlernen, was bis vor einigen Jahren technologisch nicht möglich war.\\n\\nPotential wird in Deutschland und Europa darin gesehen, Sprache als wichtigstes menschliches Kommunikationsmittel in vielen verschiedenen wirtschaftlichen B2B- Anwendungen durch Assistenzsysteme zu unterstützen (z.B. beim Einkauf, bei der Ansprache der Kund:innen, bei der Kommunikation mit den Kund:innen, bei der schnelleren Abarbeitung von Dokumenten, Rechnungen, Verkauf, Service, Ermittlung von Stimmungen etc.).\\n\\nWissenschaftliche Durchbrüche werden in der Effizienzsteigerung (mobile Anwendungen und Edge-Computing) und bspw. in der Verarbeitung längerer Texte gesehen, wozu auch das Erkennen dokumentübergreifender Beziehungen gehört. Weiteres Potential wird auch in einer kontrollierteren Generierung von Texten gesehen.\\n\\nEinige Expert:innen betonten das Potential durch die Einbindung von Wissen bis hin zu (symbolischen) Subsystemen, die vom neuronalen System (assoziativ) angesprochen werden. Hier wird Modularität zur Anpassung führen, ohne Gelerntes „vergessen\" zu müssen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n79\\n\\nSPOTLIGHT Alexander Thamm GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie Alexander Thamm GmbH [at] ist ein führendes deutsches Beratungsunternehmen für Data & AI. Mit 350 Mitarbeiten unterstützt [at] seit mehr als 10 Jahren DAX-Konzerne sowie mittelständische Unternehmen und setzt innovative KI-Projekte um.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Der Einsatz von Künstlicher Intelligenz ist bei unseren Kunden ein wichtiges Thema. Wir entwickeln AI- Strategien, Konzepte und implementieren Projekte auf Basis der neuesten wissenschaftlichen Erkenntnisse. Dabei spielen Sprachverarbeitung und Foundation-Modelle eine immer wichtigere Rolle und wir investieren in die Nutzung der Technologie in Deutschland, unter anderem in der aktiven Mitwirkung am Projekt OpenGPT-X Unsere Teams sind auf vielfältige Bereiche der KI-Entwicklung spezialisiert und setzen unter anderem Projekte um in den Bereichen Bildverarbeitung, Natural Language Processing, Forecasting, Anomalie-Detection. Beispiele hierfür sind ein KI- gesteuertes System zur Unterstützung der Zug-Disposition bei der DB, Robotersystem zur Unterstützung der Altenpflege und neuartige Verfahren für das autonome Fahren.\\n\\nAlexander Thamm, Founder und CEO der Alexander Thamm GmbH.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle werden zentraler Bestandteil der KI-Anwendungen und der Infrastruktur in vielen Bereichen. Derzeit entwickeln wir individuelle KI-Applikationen oft von Grund auf unter Nutzung spezifischer Kundendaten. In der Zukunft wird es hier eine Verlagerung hin zum Transfer-Learning bzw. Tuning von existierenden, leistungsfähigen Foundation_Modellen geben.\\n\\nGleichzeitig werden durch die Nutzung von Foundation-Modellen neue Anwendungsgebiete erschlossen und wir werden für unsere Kunden Applikationen entwickeln, die derzeit noch schwer umsetzbar sind – vor allem im Bereich NLP. Der Markt wird wachsen und wir sehen hier eine große Chance für uns, aber vor allem auch für die Wettbewerbsfähigkeit der deutschen Wirtschaft.\\n\\nDas hat intensive Auswirkungen auf unser Geschäftsmodell, vor allem, wenn wir auf die Nutzung und Lizenzierung von Foundation-Modellen angewiesen wären, auf die wir nur über APIs zugreifen können und auf die wir keinen direkten Einfluss haben. Wenn wir diese Modelle dann nur von nicht-europäischen Anbietern beziehen können, müssen wir uns zusätzlich noch intensiv mit Datenschutz- und Datensicherheitsaspekten auseinandersetzen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n80\\n\\nDamit unsere Kunden und unser Unternehmen nicht in eine einseitige Abhängigkeit geraten, ist es enorm wichtig, dass wir auch auf Foundation-Modelle zugreifen können, die in Deutschland oder Europa entwickelt wurden, und wir diese nicht nur über APIs nutzen können. Gleichzeitig hat die Berücksichtigung von europäischen Werten, z.B. beim Thema Bias, für uns und unsere Kunden eine enorme Bedeutung.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? US-amerikanische Internet-Unternehmen investieren derzeit intensiv in die Entwicklung und Verbreitung von Foundation-Modellen. Durch die Bereitstellung über APIs können die ersten am Markt befindlichen Services gleichzeitig eine Menge Daten z.B. über die Nutzungsschwerpunkte sammeln. Damit besteht die Gefahr, dass sich hier wieder – wie bei den Suchmaschinen – Monopole bilden und eine zunehmende technologische Abhängigkeit entsteht. Wenn die zentralen KI-Anwendungen nur aus Übersee kommen, werden sich langfristig unsere Aktivitäten auf die Gestaltung von Frontends- und Workflows beschränken. Wir haben keinen oder nur noch geringen Einfluss auf die Modelle, was vor allem hinsichtlich Qualität und Bias problematisch ist. Damit könnte diese Entwicklung auch zu einer potenziellen Bedrohung unseres derzeitigen Geschäftsmodells werden – und zu unserer Unternehmens-Mission, die Wettbewerbsfähigkeit der europäischen Wirtschaft in diesem Bereich sicherzustellen.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Europäische Foundation-Modelle, die wir als Basis für unsere KI-Entwicklungen nutzen könnten, würden uns befähigen, auch in Zukunft innovative Anwendungen zu entwickeln. Da wir nicht nur über APIs zugreifen, sondern die Modelle als Open Source zur Verfügung hätten, könnten wir in vielen Bereichen auch unsere eigenen Forschungsaktivitäten intensivieren und für unsere Kunden State-of-the Art KI- Systeme bauen. Damit wäre sichergestellt, dass wir auch in Zukunft bei unseren Kunden Prozesse optimieren und neue Produkte und Geschäftsmodelle ermöglichen können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n81\\n\\nMethodik der Umfrage Zusätzlich zu den 20 interviewten Expert:innen aus der industriellen und universitären Forschung wurden aus Kapazitätsgründen, und um das Meinungsbild noch detaillierter darzustellen, weitere vertiefende Stellungnahmen von 21 führenden KI-Forscher:innen in Form einer Umfrage eingeholt. Die Umfrage wurde zwischen Anfang Oktober und Mitte Dezember 2022 durchgeführt und umfasste 23 Fragen (siehe Tabelle im Anhang A.1). Hinzugekommen sind einige detailliertere Fragen, die aufgrund der begrenzten Interviewzeit in den Expert:inneninterviews nicht gestellt werden konnten.\\n\\nAlle Erkenntnisse aus der Umfrage sind in die LEAM-Machbarkeitsstudie und in den Text des gesamten Kapitels eingeflossen. Im Folgenden werden wir in Anlehnung an die geführten Expert:inneninterviews einige Aspekte und Übereinstimmungen entlang ausgewählter Leitfragen illustrieren.\\n\\nAuswertung der Umfrage In der Umfrage haben sich überwiegend die Aussagen der Interviewten aus den Expert:inneninterviews bestätigt.\\n\\nDie große Mehrheit aller befragten KI-Forscher:innen arbeiten bereits mit KI-Foundation- Modellen oder beabsichtigen, die Modelle zukünftig einzusetzen. Alle schätzen ihre Bedeutung für die Wissenschaft und Wirtschaft als hoch ein.\\n\\nDie Art der verwendeten Daten ist durchaus unterschiedlich. So wurden u.a. öffentlich verfügbare Texte und wissenschaftliche Publikationen sowie medizinische und technische Daten aufgeführt. Das Thema fehlender Daten ist allerdings auch einer der Gründe, warum Foundation-Modelle noch nicht überall eingesetzt werden. Weitere Gründe waren eine mangelnde Transparenz im Datenschutz bei existierenden Modellen, fehlende Infrastruktur, technische Limitierungen, Fachkräftemangel, hohe Investitionskosten und Unklarheiten bei der Regulation. Dennoch besteht der Wille, Foundation-Modelle künftig einzusetzen.\\n\\nDie Diversität der genannten möglichen Einsatzgebiete spiegelt dies ebenso wider, wobei auch die Art der als bedeutsam eingeschätzten Foundation-Modelle eine Rolle spielt – Bilder, Audio und Mobilitätsdaten wurden hier genannt. Keine größere Rolle spielen die vermeintlichen Gefahren, die von den Modellen ausgehen könnten. Lediglich potentieller militärischer Missbrauch, sowie die derzeit noch fehlende Erklärbarkeit wurden hier genannt. Außerdem wurden die Dominanz einzelner Big-Tech-Unternehmen und die fehlenden Antworten seitens der akademischen Forschung bemängelt. Auf dieses Ungleichgewicht wurde zudem häufiger in den Expert:inneninterviews hingewiesen.\\n\\nIm internationalen Vergleich wird Deutschland lediglich in den Bereichen Kompetenz und Kreativität sowie Ausbildungsmöglichkeiten als wettbewerbsfähig angesehen. In anderen Punkten wie Datenlage, Hardware und Förderung fällt Deutschland hinter den USA und China zurück. Die schwache Digitalisierung und Kommerzialisierung werden zudem als fehlende Voraussetzungen gesehen. Daraus folgt auch die zunehmende Abwanderung von Talenten, die in Deutschland einfach nicht die gleichen Voraussetzungen finden, wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n82\\n\\nanderswo. Dies betrifft nicht nur fehlende Investitionen, sondern auch die starke Regulierung neuer Technologien, ausgeprägte gesellschaftliche Ängste und die generelle Abneigung gegenüber dem Fortschritt, der aus der KI-Forschung entstehen kann.\\n\\nsowie wissenschaftlicher Die Fragestellungen bzgl. Daten und Datenverarbeitung wurde von den Befragten durchschnittlich als „eher hoch” eingestuft. Gleiches gilt für die domänenagnostischen Pre-Trainings und das nachfolgende domänenspezifische Fine-Tuning. Auch hier sahen die Befragten die Relevanz als „eher hoch” an. Für diese grundlegenden Themen besteht demnach größerer Forschungsbedarf innerhalb Deutschlands.\\n\\nRelevanz\\n\\nverschiedener Modellarchitekturen\\n\\nBzgl. der inhaltlichen Einsatzgebiete besteht wie erwartet keine einheitliche Meinung, da die Befragten aus unterschiedlichen wissenschaftlichen Richtungen kommen. Die Notwendigkeit von Foundation-Modellen in den Anwendungen der verschiedenen Disziplinen wurde generell als „mittel” bis „eher hoch” eingestuft. Erklärbarkeit, Aufbau von Common Sense Wissen, Einbezug weiterer Datenquellen (außer Text), hybride Verarbeitung (symbolisch und subsymbolisch), Kausalität, uvm. wurden ebenso genannt.\\n\\nDie erwarteten Kosten der Umfrageteilnehmer:innen stehen im Einklang mit den tatsächlich bekannten Kosten für große Foundation-Modelle. Die Befragten waren in dieser Hinsicht bereits gut informiert. Bzgl. der Regulation von Foundation-Modellen fühlte sich jedoch der überwiegende Teil der Befragten als nicht ausreichend informiert, wenngleich sie dies als Voraussetzung für ein erfolgreiches Einsetzen der Technologie sahen.\\n\\nBezüglich möglicher Mängel der Daten wie Bias, Diskriminierung und Misrepresentation fielen die Antworten dichotom aus. Nur eine kleine Mehrheit der Befragten gab an, sich mit diesen zu beschäftigen und sah sich in der Lage, diese angemessen zu adressieren. Diese Ergebnisse stehen im Einklang mit den Ergebnissen der Interviews, in denen die Hälfte aller Befragten auf diese Problematik hinwies. Die Mehrzahl der Befragten empfand ihr Wissen über die Regulation der Entwicklung von Foundation-Modellen als Voraussetzung für einen möglichen Einsatz der Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n83\\n\\nZusammenfassend zeigten sich die Erkenntnisse der Interviews großteils kongruent mit den Ergebnissen der Umfrage. Demnach sind die momentan führenden Nationen auf dem Gebiet der Foundation-Modelle die USA und China. Faktoren, die diese Entwicklung begünstigten, sind die Datenlage, Hardware, Ressourcen und Förderungen. Im internationalen Vergleich wird Deutschland lediglich in den Bereichen Kompetenz und Kreativität sowie Ausbildungsmöglichkeiten als vergleichbar angesehen. Faktoren, welche in Deutschland den Einsatz von Foundation-Modellen noch hindern, seien fehlende Infrastruktur und Zugang, technische Limitierungen, Fachkräftemangel, Bedenken bzgl. Datenschutzes und mangelnde Information über die Regulation der Entwicklung von Foundation-Modellen.\\n\\nZudem wurden auch mögliche Gefahren der Foundation-Modelle genannt, diese bezogen sich hauptsächlich auf die Monopolisierung der Technologie sowie mögliche Mängel der Modelle wie Bias, Diskriminierung, Toxizität, Misrepresentation und Erklärbarkeit. Die meistgenannten Interessen für Foundation-Modelle bezogen sich auf Multimodalität, die europäische Sprachenvielfalt, Erklärbarkeit und den Aufbau von allgemeinem Wissen.\\n\\nAbschließend lässt sich festhalten, dass die Mehrheit aller befragten KI-Forscher:innen bereits mit Foundation-Modellen arbeitet oder beabsichtigt, die Modelle zukünftig einzusetzen und ihre Bedeutung für die Wissenschaft und Wirtschaft als hoch eingeschätzt werden.\\n\\nDie deutsche Wissenschaft und Forschung sind also bestens in der Lage leistungsfähige Foundation-Modelle umzusetzen, um der Nachfrage der Wirtschaft gerecht zu werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n84\\n\\nChancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle\\n\\nGroße KI-Modelle für Deutschland\\n\\n85\\n\\n6. Chancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle Bei der Frage welche Modelle am dringendsten von der Wirtschaft gebraucht werden, zeichnet sich ein deutliches Bild ab. Multilinguale Sprachmodelle wurden in den Befragungen der Expert:innen am häufigsten genannt. An zweiter Stelle folgten multimodale Modelle, die ebenfalls Sprache beinhalten.\\n\\nObwohl die wirtschaftliche Bedeutung von Foundation-Modellen in der Biomedizin, in der autonomen Steuerung von Fahrzeugen und Robotern und nicht zuletzt auch im Management von Produktions- und Geschäftsprozessen als sehr hoch eingeschätzt wurde, wurden Sprachmodelle durchweg als wichtigster Anwendungsbereich genannt.\\n\\nEs ist wenig überraschend, dass großen multilingualen Sprachmodellen die höchste Priorität gegeben wurde. Die meisten der bereits erfolgreichen Foundation-Modelle sind entweder Sprachmodelle oder multimodale Modelle mit einem hohen Anteil an Sprachdaten und sprachlicher Funktionalität. Durch die zentrale Rolle der Sprache in der in nahezu allen Teilbereichen der menschlichen menschlichen Gesellschaft lassen sich die Sprachmodelle für eine Vielzahl von Aufgaben in fast allen Sektoren der Wirtschaft und Gesellschaft einsetzen.\\n\\nIntelligenz und\\n\\nBei der Entwicklung von eigenen europäischen Foundation-Modellen empfehlen wir daher, mit multilingualen Sprachmodellen zu beginnen. Dafür gibt es mehrere Gründe, die wir in diesem Kapitel näher erläutern möchten.\\n\\nUnmittelbarer Bedarf Sprachtechnologien werden bereits heute in unzähligen Anwendungen genutzt: Dazu gehören Chatbots und Voice Assistants, automatische Übersetzung, Texterzeugung, Textkorrektur, Text\\xadzusammen\\xadfassung, Textvereinfachung, semantische Suche, Verschlagwortung, Tutorensysteme, Informationsextraktion, Entdeckung von Fake News, automatische Klassifikation und Beantwortung von Emails, Sprachlernsoftware, und forensische Textanalyse. ist die heutige Sprachtechnologie noch stark verbesserungsfähig. Die bestehenden Defizite in der Verlässlichkeit schränken die Märkte für die Sprachtechnologien noch stark ein. Mit den immensen Leistungssteigerungen durch Foundation-Modelle können in allernächster Zukunft größere Märkte erschlossen werden, denn Basisanwendungen mit Produkten, Dienstleistungen und Vertriebskanälen existieren bereits. Deutschland hat Hunderte von Unternehmen, die von dem Technologiefortschritt profitieren würden. Andererseits wären die Produkte und Dienstleistungen dieser Firmen bedroht, wenn stattdessen nur ausländische, insbesondere US-amerikanische Anbieter mithilfe der neuen Technologie leistungsfähigere Produkte auf den Markt bringen. Die betroffenen deutschen Firmen sind als KMUs nicht in der Lage, selbst große Foundation-Modelle zu trainieren.\\n\\nIn allen diesen Anwendungsbereichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n86\\n\\nWirtschaftliches Potential Das wirtschaftliche Potential liegt nicht nur in der Verbesserung der Qualität, der Funktionalität und der Marktchancen von bestehenden Anwendungen. Durch die zusätzliche Leistungsfähigkeit werden viele neue Anwendungen möglich. Dazu gehören z.B. leicht zu bedienende Zugänge für Bürger:innen, Patientin:innen, Mitarbeiter:innen zu praktischer Information, zu Wissen und zu Vorgängen, die sie betreffen. Das gilt für öffentliche Verwaltungen, Gesundheitssysteme, Unternehmen und jedes andere Teilsystem der Gesellschaft. Als Folge der Digitalisierung wird der direkte passive und aktive Zugriff auf all diese Daten und Prozesse zwar möglich, ist jedoch oft zu schwer zu bedienen. Erst wenn der digitale Zugang so einfach ist, wie zu der Zeit, als man mit kooperativen menschlichen Ansprechpartner:innen kommunizierte, wird die Digitalisierung ihr ganzes Potential entfalten können und allseits akzeptiert werden. Neueste Chatbots wie Googles LaMDA und OpenAIs ChatGPT demonstrieren, dass Foundation-Sprachmodelle mächtig genug sind, um solche Schnittstellen zu digitalen Diensten zu realisieren. Schon sehr bald werden wir alle in der Lage sein, in unserer Muttersprache, gesprochen oder geschrieben, ohne Behördentermine, Warteschleifen oder komplexe Eingabemasken mit allen Diensten zu kommunizieren, die unseren Alltag bestimmen. In der Wirtschaft werden solche Systeme die Kommunikation mit anderen Unternehmen und mit Endkunden:innen revolutionieren. Multilinguale Modelle gestatten es Organisationen mit wenig Aufwand, Akquise und Kundenkommunikation auf andere Länder auszudehnen. Im Bildungswesen werden natürlichsprachliche Lern- oder Tutorsysteme den Wissensstand der Lernenden ermitteln, Defizite erkennen und diese gezielt durch geeignetes Lernmaterial oder personalisierte Wissenselemente und Erklärungen überwinden.\\n\\nGesellschaftliche Relevanz Unsere Sprache ist ein so wichtiges Element der menschlichen Kultur, dass wir die Technologien, die den Gebrauch der Sprache erleichtern, beeinflussen und für verschiedenste Zwecke einsetzen, selbst beherrschen, und sie für unsere Ziele anpassen müssen. Die Sprachtechnologie wird schließlich in der Zukunft eine noch wichtigere Rolle in der Kommunikation zwischen Menschen und zwischen Mensch und Technik einnehmen. Alle gesellschaftlichen Entwicklungen spiegeln sich auch in der Sprache wider, das zeigt sich z.B. in den aktuellen Bemühungen um eine Sprache, die unseren ethischen Werten, unserem Geschichtsverständnis und unserem wissenschaftlichen Weltbild entspricht.\\n\\nVon hoher gesellschaftlicher Relevanz ist auch die Multilingualität. Deutschland sieht seine eigene Zukunft als Teil einer multikulturellen und multilingualen europäischen Gesellschaft. Nur so können wir im geopolitischen Kräftespiel unsere Werte bewahren und wirtschaftlich eine Rolle in der Welt einnehmen, die es uns ermöglicht, unseren Lebensstandard zu erhalten. Eine zentrale Komponente der europäischen Integration ist das Prinzip der Gleichheit unter den beteiligten Sprachen. Selbst wenn das Englische in der wissenschaftlichen, technischen, wirtschaftlichen Kommunikation weltweit eine besondere Rolle als Lingua Franca einnimmt, haben wir allen Mitgliedern der Europäischen Union garantiert, dass ihre Sprachen erhalten und geschützt werden, unabhängig von der Zahl ihrer muttersprachlichen Sprecher:innen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n87\\n\\nEine große Barriere für den gemeinsamen digitalen Binnenmarkt der europäischen Gemeinschaft ist die Mehrsprachigkeit dieses Marktes. Im Unterschied zu unseren größten globalen Wettbewerbern, den USA und China, müssen die Endkunden der Produkte und Dienstleistungen in vielen Sprachen erreicht werden. Während US- amerikanische und chinesische Anbieter kleinere europäische Sprachen gefahrlos vernachlässigen können, dürfen europäische Anbieter das nicht. Das gilt natürlich nicht nur für digitale Produkte, ein Beispiel sind die Beipackzettel der Arzneimittelhersteller.\\n\\n2018 verabschiedete das europäische Parlament eine Resolution mit dem Titel „Gleichstellung von Sprachen im digitalen Zeitalter” (Parliament, 2018), die nicht nur die Gleichheit der Sprachen betont, sondern darüber hinaus auch Forderungen an die Sprachtechnologie richtet, um diese Gleichstellung auch in der Praxis zu erreichen.\\n\\nWissenserwerb anderen sich Foundation-Sprachmodelle Datendomänen, wie Proteinen oder DNA-Sequenzen, dadurch aus, dass über die probabilistische Modellierung sprachlicher Texte Wissen zu vielen, wenn nicht sogar zu allen Sachgebieten gelernt wird. Ein Sachgebiet könnte nur dann völlig unberücksichtigt bleiben, wenn es überhaupt nicht in digitalen Texten repräsentiert ist. Durch diese Form des Wissenserwerbs wird auch ein ungelöstes Problem der Wissensmodellierung über Knowledge Engineering in der symbolischen KI angegangen, nämlich die Schwierigkeit, die Verbindungen zwischen den Sachgebieten herzustellen, z.B. zwischen Werkstoffen und Verfahren, zwischen Werkstoffen und Marktpreisen oder zwischen synthetisierbaren Proteinen und deren Beschaffungsquellen. Durch die Vielseitigkeit des erworbenen ist das in klassischen oder auch neu definierten Wissensgebieten Wissens Anwendungspotential der Foundation-Sprachmodelle besonders hoch.\\n\\nzeichnen\\n\\ngegenüber Modellen\\n\\nin\\n\\nEntwicklungsstand und Vergleichbarkeit Für die Wahl von Foundation-Sprachmodellen spricht auch der Stand der Entwicklung. Durch das Vorhandensein von mehrjähriger gut dokumentierter Erfahrung im Training der Sprachmodelle und umfangreichen und vielseitigen Benchmarks ist es leichter, zur Spitzenforschung aufzuschließen und den eigenen Fortschritt zu kontrollieren als bei neueren Arten von Foundation-Modellen.\\n\\nVorhandene Kompetenz Auf den Gebieten der neuronalen NLP und der multilingualen Sprachtechnologie gibt es in Deutschland eine anerkannte Forschungstradition und eine starke wissenschaftliche Community mit weltweit anerkannten Spitzenforscher:innen und exzellent ausgebildeten Nachwuchskräften. Es gibt auch eine Vielzahl von älteren Sprachtechnologie Unternehmen und neuen KI-Start-ups, die die NLP-Komponenten in marktfähige Produkte integrieren können und die Anforderungen der bestehenden Märkte kennen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n88\\n\\nVerfügbare Datenbestände Durch mehrere erfolgreiche EU-Projekte zur Sammlung von Sprachdaten für europäische Sprachen haben wir Zugriff auf große Mengen von geeigneten Daten für europäische Foundation-Sprachmodelle. Auch sogenannte kleinere Sprachen sind in den Datensammlungen vertreten. Viele der Daten haben eine hohe Qualität, weil sie von öffentlichen Verwaltungen und gemeinnützigen Organisationen der EU-Mitgliedstaaten eingebracht wurden. Unter den Daten sind auch viele parallele bilinguale und multilinguale Textkorpora. Die dadurch verfügbaren Sprachdaten erhöhen die Erfolgswahrscheinlichkeit für Sprachmodelle, die den Anspruch haben, den speziellen europäischen Anforderungen zu genügen, und sie lassen hoffen, dass diese Foundation- Modelle zumindest nach diesem Kriterium deutliche Vorteile gegenüber den großen bestehenden Modellen bieten.\\n\\nPotential für künftige Entwicklungen Neuere Entwicklungen von multimodalen und multimedialen Sprachmodellen verbinden die Daten der repräsentierten Datendomänen untereinander. Das bewirkt, dass man sich mit Hilfe von textueller Eingabe auch thematisch passende Bilder oder Videos generieren lassen kann. Diese Funktionalität wird als cross-modal (transmodal) bezeichnet. Der Sprache kommt hierbei jeweils eine besondere Bedeutung zu, denn sie ist geeignet um die Konzepte und Kriterien der Benutzer:innen auszudrücken. In der Zukunft wird die Zahl der Datendomänen zunehmen, wobei die Sprache voraussichtlich immer die semantische Basis für die Kombinationen darstellen wird. Daher können neue Kombinationen durch die Erweiterung von Sprachmodellen hergestellt werden.\\n\\nAus diesen Gründen empfehlen wir, die Entwicklung von Foundation-Modellen mit großen europäischen Sprachmodellen zu beginnen, denn hier liegt das größte Potential für die deutsche Wirtschaft und die Nachfrage ist besonders hoch. Schrittweise sollen die Foundation-Modelle dann auch um weitere Datenarten erweitert werden.\\n\\n6.1 Erste europäische multilinguale Foundation-Sprachmodelle\\n\\nMit Blick auf Sprachmodelle ergeben sich eine Vielzahl an Fragen für die konkrete Umsetzung und Weiterentwicklung. Im Folgenden werden wir ausführen, welche Architekturen und Daten den Sprachmodellen zugrunde gelegt werden sollen und welche Prioritäten die Entwicklung und die Evaluation der Modelle bestimmen sollen.\\n\\nArchitektur der Basis Foundation-Modelle Für die Basisarchitektur gibt es derzeit keine ernsthafte Alternative zum Transformer- Ansatz. Fast alle großen Foundation-Sprachmodelle folgen diesem Ansatz, der sich auch für Bild-, Video- und Proteindaten als tragfähig erwiesen hat und sich somit auch für die geplanten Erweiterungen um multimodale und multimediale Daten eignet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n89\\n\\nNun gibt es allerdings verschiedene Ausformungen des ursprünglichen Transformer Modells, die grob in drei Klassen fallen: (i) Encoder-Decoder Modelle, die mehr oder weniger dem Ausgangsmodell entsprechen, (ii) Encoder Modelle und (iii) Decoder Modelle.\\n\\nAm erfolgreichsten sind bisher Modelle der Klassen (i) und (iii), wobei die allergrößten Modelle häufiger in die Klasse (iii) fallen, was aber sicher daran liegt, dass der Trainingsaufwand für große Decoder Modelle sehr viel geringer ist.\\n\\nDie Fachwelt geht aber mehrheitlich davon aus, dass Modelle der Klasse (i), also volle Transformer Modelle, Vorteile für die bestimmte anspruchsvolle Aufgabenstellungen bieten, die die tiefere Analyse der Eingabe und die dadurch erzeugte abstrakte Repräsentation der semantischen Beziehungen nutzen können. Dazu gehören die Beantwortung von Fragen und andere Aufgaben, die Schlussfolgerungen erfordern, sowie die automatische Übersetzung.\\n\\nBisher konnten Decoder-Modelle die Nachteile der einfacheren Architektur durch Größe, eine größere Menge von Lerndaten und eine höhere Zahl von Parametern ausgleichen. Besonders die Performanz von ChatGPT, des neuesten Modells der GPT-3 Klasse, demonstriert auf beeindruckende Weise, dass mit dem Decoder-Modell (zusammen mit geeigneten Verfahren des Nachtrainierens) Funktionalitäten möglich sind, die einen hohen Grad an semantischer Abstraktion erfordern.\\n\\nFür die erste Generation an Modellen wird daher empfohlen, die Decoder Architektur der GPT-Modelle zu verwenden. Das bietet die folgenden Vorteile:\\n\\nschneller Kompetenzaufbau durch die Verwendung der einfacheren Architektur ● bessere Vergleichbarkeit mit den neuesten GPT-Modellen und mit europäischen Modellen (BLOOM, Luminous, GPT-SW3)\\n\\ndurch die Vergleichbarkeit eine leichtere Evaluation der Beiträge der zusätzlichen Daten\\n\\nkürzere Zeit bis zu ersten verwendbaren Ergebnissen\\n\\nVersionen der ersten Modelle sollen sich von den bestehenden Vorbildern aber auch durch die Auswahl der Lerndaten unterscheiden. Zusätzlich zu den bisher eingesetzten gecrawlten Webdaten werden spezielle europäische Korpusdaten verwendet, die sowohl die europäischen Sprachen als auch die relevanten Gegenstandsbereiche besser abdecken. Einen besonderen Effekt für die angestrebte Multilingualität erwarten wir vom Einsatz paralleler bilingualer und multilingualer Korpora.\\n\\nfür die Wirtschaft\\n\\nDaneben werden die Trainingsdaten auch um Wissensdaten erweitert, die aus großen Wissensgraphen stammen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n90\\n\\n6.1.1 Lerndaten\\n\\nLerndaten für Foundation-Sprachmodelle sind große Mengen von digitalen Textdaten, in denen die europäischen Sprachen bestmöglich repräsentiert sind.\\n\\nDie zentrale Komponente der Trainingsdaten für die größten Sprachmodelle sind sehr große Textvolumina, die durch Webcrawling kuratiert wurden. Die gemeinnützige Organisation Common Crawl sammelt und archiviert seit 2011 große Teile des World Wide Webs. Diese werden kostenlos für Forschung, Entwicklung und andere Nutzungen zur Verfügung gestellt, seit 2013 im Web-Archivformat WARC.\\n\\nAufbauend auf den Datenbeständen von Common Crawl wurden die Web-Crawl Corpora zusammengestellt und gereinigt, die den größten Foundation-Sprachmodellen zugrunde liegen. Eine Auswahl dieser Datensätze soll hier vorgestellt werden.\\n\\nC4 - Colossal Clean Crawled Corpus Das ist insbesondere der Korpus C4 (Colossal Clean Crawled Corpus), der sowohl exklusiv für das Englische, aber auch in multilingualen Varianten existiert (Raffel et al., 2020) Das Attribute „Clean“ bezieht sich auf mehrere Verfahren zur Datensäuberung, die eingesetzt wurden, um echte monolinguale Texte von Mischdaten zu trennen.\\n\\nZu den Säuberungsverfahren gehörte auch der Einsatz von Blockierlisten (Blocklists), Listen von Wörtern, an denen man obszöne, rassistische und anderweitig anstößige Texte zu erkennen hoffte. Beispiele sind die Lists of Dirty, Naughty, Obscene, and Otherwise Bad Words (LDNOOBW), die auf GitHub für das Englische und ca. 25 weitere Sprachen angeboten werden. Im C4 Korpus wurden nun alle Webseiten ausgefiltert, auf denen sich mindestens eines der anstößigen Wörter in der jeweiligen Sprache fand. Man nahm an, dass es bei der großen Menge an Daten weniger Probleme geben würde, wenn nach diesem groben Kriterium mitunter auch ungerechtfertigt gefiltert würde, als durch die Aufnahme anstößiger Inhalte in die KI-Modelle.\\n\\nEs konnte dann aber von Kritiker:innen gezeigt werden, dass die Löschung von Texten mit sexuellen Bezügen oder mit Slangausdrücken dazu führte, dass wichtige Bereiche der Gesellschaft in den so „gereinigten“ Texten unterrepräsentiert waren, unter anderem Teile der LGBTQ-Gemeinschaft oder ethnische Minderheiten (Dodge et al., 2021). Zum Glück gibt es aber auch „noblocklist“ Versionen des C4 Korpus, auf die diese lexikalischen Filter nicht angewandt wurden.\\n\\nmC4 Für multilinguale Foundation-Sprachmodelle wurde das Webkorpus mC4 geschaffen, eine spezielle multilinguale Fassung des C4 Korpus, dessen Vorteile durch die Performanz des mit mC4 trainierten Modells mT5 demonstriert werden konnten (Xue et al., 2021).\\n\\nDas Korpus mC4 enthält 27 TB Textdaten für 101 Sprachen. Die best repräsentierte Sprache ist natürlich Englisch mit 10401 GB, während das westafrikanische Yoruba mit nur mehr 0,158 GB in diesem Korpus das Schlusslicht bildet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n91\\n\\nDas Englische macht mit 2733 Mrd. Token 5,67 % der Daten aus. Deutsch liegt mit 1404 GB oder 347 Mrd. Token nach Russisch und Spanisch an vierter Stelle, was 3,05 % entspricht. Kleinere europäische Sprachen wie Litauisch, Estnisch und Lettisch sind noch mit jeweils 11, 6,9 und 7,9 Mrd. Tokens vertreten. Die kleinsten im Korpus präsenten europäischen Sprachen sind Luxemburgisch mit 1 Mrd. und Irisch mit 0,5 Mrd. Tokens. Im Gegensatz zum Luxemburgischen ist das Irische seit dem 1. Januar 2022 sogar eine der Arbeitssprachen der Europäischen Union.\\n\\n6.1.2 Europäische Projekte\\n\\nGlücklicherweise gab es in Europa seit 2010 eine Reihe von Projekten, die europäische Sprachdaten für die Forschung und Wirtschaft kuratieren und verfügbar machen. Diese von der EU in verschiedenen Programmen geförderte Vorhaben sollten die europäische KI, in die Lage versetzen, die europäische Sprachenvielfalt technologisch beherrschbar zu machen und somit für die europäische Wirtschaft und den gemeinsamen digitalen Binnenmarkt Wettbewerbsschranken abzubauen.\\n\\ninsbesondere die Sprachtechnologie,\\n\\nDazu gehören insbesondere die Vorhaben ParaCrawl und European Language Resource Coordination (ELRC), gefördert im Programm Connecting Europe Facility (CEF), und das European Language Grid (ELG), gefördert im Programm Horizon 2020.\\n\\nParaCrawl ParaCrawl hat von 2017 bis 2021 in drei Phasen parallele Texte in den europäischen Sprachen durch Web-Crawling kuratiert. Das Ergebnis ist ein paralleler Datensatz mit 41 Sprachpaaren, die hauptsächlich Englisch mit einer anderen Sprache verbinden (39 von 41). Neben den europäischen Sprachen enthält ParaCrawl auch Daten für neun ressourcenarme, außereuropäische Sprachen.\\n\\nELRC - European Language Resource Coordination Der Schwerpunkt des Infrastrukturvorhabens ELRC, das im Januar 2023 abgeschlossen wird, lag auf der Stärkung der europäischen Übersetzungstechnologie. Die europäische Wirtschaft und Gesellschaft soll auch unabhängig von Google Translate und den anderen großen Übersetzungsdiensten der amerikanischen Hyperscaler in der Lage sein, Sprachbarrieren zu überwinden, ohne dass dabei die Stellung von europäischen Sprachen, auch nicht der kleineren und kleinsten Sprachen, leidet. Daher lag der Schwerpunkt von ELRC auf der Sammlung oder Erzeugung von bilingualen und multilingualen parallelen Korpora, die dann zum überwachten Trainieren von maschinellen Übersetzungssystemen eingesetzt werden können. Eine Herausforderung bestand darin, hochqualitative und wirtschaftlich bzw. gesellschaftlich relevante Korpora in den Staaten der EU zu kuratieren, und das von Organisationen in allen EU- Mitgliedsländern.\\n\\nELRC hat insgesamt über 200 Milliarden Wörter an hochqualitativen Sprachdaten in Europa gesammelt und aufgearbeitet und ein europaweites ELRC Netzwerk in allen EU- Mitgliedsstaaten etabliert. Die Daten enthalten 5600 Ressourcen: bilinguale und\\n\\nGroße KI-Modelle für Deutschland\\n\\n92\\n\\nmultilinguale parallele Korpora sowie monolinguale Textkorpora. Es gibt mehr als 880 parallele Korpora mit mehr als 1 Mio. Wörtern, darunter mehr als 230 mit mehr als 10 Mio. Wörtern und mehr als 50 mit über 100 Mio. Wörtern. Die europäischen Nationalsprachen sowie die Sprachen der größten Minderheiten sind in den Daten vertreten. Über 950 Ressourcen enthalten deutsche Daten und selbst kleine Sprachen wie das Irische und das Maltesische sind noch mit über 300 bzw. 200 Ressourcen recht gut vertreten.\\n\\nELG - European Language Grid Das Projekt ELG European Language Grid hat unter seinem Namen eine Plattform geschaffen, auf der zehntausende sprachtechnologische Ressourcen für die Nutzung bereitgestellt werden, die meisten davon unter nichtkommerziellen Lizenzbedingungen. Unter diesen Ressourcen finden sich auch tausende von Textkorpora, darunter auch sehr viele für europäische Sprachen, die in den großen Webkorpora zu wenig repräsentiert sind.\\n\\nEFNIL - European Federation of National Institutions for Language Im Dachverband EFNIL (European Federation of National Institutions for Language) haben sich nationalen Sprachinstitutionen der europäischen Staaten zusammengeschlossen. Zu diesen Institutionen gehören Sprachinstitute wie das Institut für Deutsche Sprache in Mannheim oder das Institut für die Tschechische Sprache in Prag, aber auch Einrichtungen der Regierungen wie der Dänische Sprachrat oder die Generaldelegation für die Französische Sprache. Viele dieser Einrichtungen verwalten und pflegen nationale Korpora, große, gut gepflegte und mehr oder weniger repräsentative Textsammlungen für ihre jeweilige Sprache oder Sprachen. In der Vergangenheit waren diese Daten für die Sprachtechnologie meist nicht zugänglich, weil die jeweils geltenden Eigentums- oder Urheberrechte dieser Nutzung im Wege standen.\\n\\nOPUS Neben diesen EU-Projekten gibt es bereits seit 2004 die Initiative OPUS des Nordic Natural Language Processing Lab, die jetzt über drei Millionen parallele Texte mit über 100 Millionen Sätzen frei zur Verfügung stellt.\\n\\neTranslation Die meisten europäischen Organisationen in Wirtschaft, Politik und Zivilgesellschaft schrecken aus Sicherheitsgründen davor zurück, die Übersetzungsdienste großer multinationaler Unternehmen für Übersetzung, Suche und Dolmetschen zu verwenden. Für die reine Textübersetzung hat die Europäische Kommission bereits 2017 den Übersetzungsdienst eTranslation geschaffen, der europäischen Organisationen die kostenlose Übersetzung zwischen allen Arbeitssprachen der EU anbietet.\\n\\nDieser Dienst der Generaldirektion Übersetzung der Europäischen Kommission setzt bereits neuronale Übersetzungssysteme für viele Sprachpaare ein, wobei in der Regel immer zwischen Englisch und einer weiteren europäischen Sprache übersetzt wird und indirekt über das Englische als Zwischensprache alle anderen Übersetzungen vorgenommen werden. Die Europäische Kommission verfügt über keine Foundation- Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n93\\n\\nObwohl eTranslation in der Qualität nicht an Google Translate heranreicht, wird der Dienst für Informationsübersetzungen verwendet, was nicht nur an den Sicherheitsanforderungen der Nutzerorganisationen liegt, sondern auch an den Stärken des europäischen Systems im Hinblick auf europaspezifische Sprache und Inhalte. Pro Jahr werden ca. 300 Mio Seiten übersetzt.\\n\\nvon\\n\\nvielen Organisationen\\n\\nfür\\n\\neine\\n\\nRohübersetzung\\n\\nbzw.\\n\\n6.1.3 Die Bedeutung paralleler Sprachdaten\\n\\nEs ist eine offene Forschungsfrage, welche Rolle parallele Sprachdaten längerfristig in der Welt der Foundation-Modelle spielen werden.\\n\\nWenn parallele Sprachdaten bereits vorhanden sind, kann man diese als Übersetzungspaare direkt in die Daten für das Pretraining übernehmen, wobei das Modell dann ganz alleine lernt, dass es sich um Paare von bedeutungsgleichen Sätzen in den jeweiligen zwei Sprachen handelt. Man kann aber auch das überwachte Trainieren der Übersetzungsfähigkeit in das Pretraining integrieren, ohne dass zusätzlich Kosten für die Datenproduktion oder -annotation anfallen. Die parallelen Daten können dann natürlich auch noch als monolinguale Daten für die Ergänzungs- oder Ersetzungsaufgaben des selbstüberwachten Pretrainings verwendet werden.\\n\\nEs wurde aber beobachtet, dass sich die Übersetzungsfähigkeit als eine emergente Funktionalität einstellt, sobald nur hinreichend große Textvolumina für die Einzelsprachen in den Pretrainingsdaten vorhanden sind. Textübersetzung wird somit zu einer Zero-Shot oder höchstens zu einer Few-Shot Anwendung des multilingualen Foundation-Modells. Für Sprachen, die nicht hinreichend in den Lerndaten repräsentiert sind, müsste das Modell dann weiterhin durch überwachtes Lernen nachtrainiert werden. Somit verringert sich der Bedarf an parallelen Korpora, die in ihrer bestehenden Menge begrenzt und teuer zu produzieren sind.\\n\\nIn ihrem Forschungsbericht nmT5 - Is parallel data still relevant for pre-training massively multilingual language models? zeigen Kale et al. (2021) jedoch, dass parallele Korpora wegen der Knappheit der Daten für viele Sprachen immer noch eine wesentliche Bedeutung für die Anwendbarkeit der Modelle in diesen Sprachen und für die Qualität der Übersetzungen haben.\\n\\nDieses Alleinstellungsmerkmal hat großes wirtschaftliches Potential, das über Anwendungen für Textübersetzung und multilinguale parallele Texterzeugung weit hinausgeht. Ein weiteres Anwendungsgebiet ist die translinguale Suche (crosslingual search), und zwar sowohl die Suche nach Dokumenten als auch die Suche nach Informationen und Wissensinhalten. Wenn die Bürger:innen oder Kund:innen, insbesondere Menschen, die nicht Englisch als Muttersprache haben, in ihrer eigenen Sprache in vielsprachigen Inhalten suchen können, erleichtert das ihr Leben. Eine andere Anwendung ist die Unterstützung von bilingualer und multilingualer Konversation, wie z.B. durch eine simultane Dolmetscherfunktion für Beratungen, Besprechungen und Verhandlungen. Zoom bietet, aufbauend auf deutscher Übersetzungstechnologie, bereits\\n\\nGroße KI-Modelle für Deutschland\\n\\n94\\n\\ndie fast-simultane Übersetzung zwischen neun großen Sprachen an, darunter fünf EU- Sprachen, aber eine Erweiterung auf die Breite der europäischen Sprachen ist schon alleine wegen des Mangels an geeigneten Trainingsdaten nicht in Sicht.\\n\\n6.1.4 Empfehlung\\n\\nTrotz der Tatsache, dass die europäischen Sprachen, insbesondere die sogenannten kleineren Sprachen, in den Korpora noch nicht hinreichend repräsentiert sind, sollten zuerst die bestehenden Korpora C4 bzw. mC4 für das Training der multilingualen europäischen Foundation-Modelle eingesetzt werden. Dies erlaubt zum einen eine Vergleichbarkeit mit anderen großen Sprachmodellen und verhindert, dass Daten fehlen, die essentiell zur Performanz der bekannten Modelle beigetragen haben. Zum anderen erlaubt es, die Beiträge der speziellen europäischen Daten für relevante Anwendungen besser beurteilen zu können.\\n\\nEs gibt bisher keine Untersuchungen darüber, zu welchen Anteilen sich die Daten in den durch die europäischen Projekte bereitgestellten Korpora mit den Webdaten der großen amerikanischen Foundation-Modelle überlappen. Dadurch, dass Common Crawl aber keine Daten einsammelt, die nur nach Registrierung zugänglich sind und zudem die Crawling-Bestimmungen (nofollow, robots.txt) der originären Websites respektiert, ist anzunehmen, dass über die europäischen Projekte große Volumina an zusätzlichen Daten für das Training verwendet werden können.\\n\\nELRC und ELG werden vom DFKI koordiniert. Die Datenhaltung und -bereitstellung für beide Projekte wird vom Projektpartner in Athen verantwortet. Die Projektkoordinatoren und der Direktor des ILSP haben ihre Bereitschaft bekundet, die Entwicklung europäischer Modelle bei der Datenkuratierung aus ihren Beständen zu unterstützen. Jahrestagung den nationalen Im September 2021 wurden auf der EFNIL Sprachinstitutionen die Möglichkeiten und Absichten bezüglich europäischer Foundation- Modelle vorgestellt. Die erste Kommunikation mit EFNIL Mitgliedsorganisationen ergab deren grundsätzliche Bereitwilligkeit, ihre Korpusdaten unter kontrollierten Bedingungen für das Training von Foundation-Modellen zur Verfügung zu stellen.\\n\\nILSP\\n\\nDas Institut für Deutsche Sprache hat seine Bereitschaft erklärt, die hochqualitativen Textkorpora des Instituts von rund 40 Milliarden Wörtern für das Training europäischer Modelle verfügbar zu machen und das Vorhaben zu Aspekten der Repräsentanz der deutschen Sprache in den europäischen Foundation-Modellen bei Bedarf auch wissenschaftlich zu unterstützen.\\n\\nDarüber hinaus sammelt das Projekt OpenGPT-X Daten für das Training eines europäischen Sprachmodells. Die dort gemachten Erfahrungen und verwendeten Daten können auch über das Projekt hinaus Anwendung finden.\\n\\nDiese Daten, zusammen mit dem Vorhandensein der parallelen Sprachdaten für europäische Sprachen, ermöglichen das Training erster europäischer Foundation- Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n95\\n\\nDurch die Verwendung der parallelen Daten im Pretraining gibt es die Möglichkeit, mehrere Ziele zu erreichen:\\n\\n(1) die Erfüllung der besonderen europäischen Anforderungen auf Überwindung der\\n\\nSprachgrenzen\\n\\n(2) eine bessere Berücksichtigung der kleineren europäischen Sprachen\\n\\n(3) die Schaffung eines besonderen Alleinstellungsmerkmals der Modelle gegenüber\\n\\nden bestehenden Foundation-Modellen\\n\\nSPOTLIGHT Bayer AG An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nBayer ist ein weltweit tätiges Unternehmen mit Kernkompetenzen auf den Life-Science-Gebieten Gesundheit und Ernährung. Mit seinen Produkten und Dienstleistungen will das Unternehmen Menschen nützen und die Umwelt schonen, indem es zur Lösung grundlegender Herausforderungen einer stetig wachsenden und alternden Weltbevölkerung beiträgt.\\n\\nDr. Marion Legler, Head of Decision Science & Advanced Analytics, Bayer Pharma.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir verwenden KI-Foundation-Modelle, um große Textmengen automatisch zu verarbeiten und unseren Experten zu helfen, schnell relevante Informationen zu finden. Zum Beispiel, die Modelle:\\n\\nSie konvertieren die von den Ärzten während der klinischen Studien verfassten Texte in standardisierte medizinische Diagnosecodes, was die anschließende manuelle Überprüfung durch unsere Experten vereinfacht. ● Lesen Sie die gesamte medizinische Literatur und die Berichte über klinische Studien, um diejenigen zu identifizieren, die für die Therapiegebiete und Behandlungen von Bayer besonders relevant sind.\\n\\nTausende von Dokumenten der Zulassungsbehörden (EMA, FDA usw.) werden durchgesehen und die Themen in jedem Teil der Dokumente automatisch klassifiziert, so dass die Informationen leicht auffindbar sind. Scannen Sie die von Patienten erhaltenen Mitteilungen auf unerwünschte Ereignisse. ● Gruppierung der von Bayer-Vertretern nach Besprechungen mit Ärzten verfassten Erkenntnisse, um aufkommende Diskussionsthemen zu entdecken.\\n\\nWir verwenden auch vortrainierte Computer-Vision-Modelle für Anwendungsfälle, in denen nur begrenzt kommentierte Bilder zur Verfügung stehen, wie z. B. im Zusammenhang mit bestimmten Krebstumoren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n96\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Die meisten Anwendungsfälle, die durch diese Modelle ermöglicht werden, waren vorher nicht möglich, insbesondere solche, bei denen es darum geht, große Mengen von Dokumenten zu erkennen. In anderen Fällen, wie der Kodierung in klinischen Studien oder der Erkennung von unerwünschten Ereignissen in Texten, unterstützen die KI-Modelle die manuelle Arbeit der menschlichen Experten und sparen etwa 50 % der Zeit, die diese für die sich wiederholenden Aufgaben aufwenden.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Da die meisten großen Sprachmodelle nur in englischer Sprache verfügbar sind, müssen wir oft erst aus anderen Sprachen ins Englische übersetzen und dann die Modelle verwenden. Der Inhalt und die Feinheiten des Textes gehen bei der Übersetzung sicherlich verloren. Open-Source-Modelle, die alle europäischen Sprachen abdecken, könnten dazu beitragen, dass Patient:innen, Ärzt:innen oder Behörden, die sich in verschiedenen Sprachen äußern, gleichermaßen berücksichtigt werden. Zwar gibt es solche Modelle bereits (BLOOM), doch werden sie derzeit kaum genutzt, da sie auf allgemeine, nicht- medizinische Texte trainiert sind. Die Herausforderung, jemals ein brauchbares mehrsprachiges Modell für Gesundheitsanwendungen zu haben, besteht darin, dass die wissenschaftliche Literatur und die Dokumente, die zum Trainieren medizinischer Sprachmodelle verwendet werden, ursprünglich nur auf Englisch geschrieben sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n97\\n\\n6.2 Vermeidung von Falschaussagen, Bias und Toxizität\\n\\nWie in Kapitel 2.3 beschrieben, wurden bei neuronalen Modellen, die auf großen Mengen von Webdaten trainiert wurden, mehrere Formen von inadäquatem Verhalten beobachtet. Um die Foundation-Modelle so zu gestalten, dass sie den Kriterien für in Wirtschaft und Gesellschaft genügen, muss solches Verhalten Anwendungen verhindert oder minimiert werden. Deshalb sollte die Verhinderung des Fehlverhaltens eine zentrale Priorität bei Entwurf, Training und Evaluation der Modelle sein.\\n\\nFalschaussagen Die deutlichste Form von Falschverhalten sind fehlerhafte Aussagen. Das Modell wird solche Aussagen tätigen, wenn trotz des Pretrainings Wissenslücken verbleiben oder Wissensinkonsistenzen durch widersprüchliche Trainingsdaten erst entstehen. Zudem können Foundation-Modelle nicht zwischen faktisch korrekten Aussagen und plausiblen inkorrekten Aussagen unterscheiden, die ja auch im Bereich der Literatur oft zu finden sind. Die Methoden zur Behebung sind eine bessere Datenauswahl und die Hinzunahme von Wissen aus nichtsprachlichen Wissensbeständen (s. Kapitel 2.3).\\n\\nBias Schwieriger zu entdecken und auch schwerer zu verhindern sind Unausgewogenheiten bis hin zu Voreingenommenheiten, falschen Verallgemeinerungen und Fehlurteilen mit ethisch verwerflichen Konsequenzen. Diese werden oft unter dem Sammelbegriff Bias subsumiert. Häufig kommentierte Formen von Bias sind Voreingenommenheiten in Bezug auf Geschlecht und ethnische oder soziale Herkunft, die wir auch bei Menschen finden. (ref)\\n\\nAllerdings muss nicht jeder Bias negativ sein, es kann z.B. vorkommen, dass im Zusammenhang mit einem Produkt die häufigere Erwähnung von seltenen Gefahren im Vergleich zur Erwähnung der Vorteile des Produkts zu Aussagen führt, die einen vorsichtigen Gebrauch nahelegen. Es ist auch nicht möglich, alle Formen von negativen Bias vorherzusehen, weil diese von von Problembewusstsein und Entwicklungen bestimmten zeitabhängigen Sensibilisierungen bestimmt werden.\\n\\ngesellschaftlichen\\n\\nVersuche, präventiv alle Quellen von negativem Bias aus den Trainingsdaten zu tilgen, sind daher unrealistisch. Stattdessen versuchen Entwickler:innen, durch gezieltes Nachtrainieren bereits bekannte Formen von Bias durch adäquates Antwortverhalten zu überschreiben. Einen ähnlichen Ansatz haben die Entwickler von ChatGPT gewählt, die damit wirksam auf Kritiken bezüglich der früheren GPT-Modelle reagiert haben.\\n\\nZum adäquaten Antwortverhalten gehört auch eine Pluralität und Ausgewogenheit in der Nennung von alternativen Antworten. Die Ergänzungsaufgaben des Pretrainings reichen nicht aus, um ein solches Verhalten zu erreichen. Wenn ein Modell bei mehreren möglichen Antworten nicht einfach die wählen soll, die statistisch durch die Lerndaten präferiert ist, muss ein Sprachmodell das angemessene Antwortverhalten durch dediziertes Training erlernen. Auch hier hat ChatGPT vorgemacht, wie dieses Ziel durch Nachtrainieren erreicht werden kann. OpenAI hat diese Verbesserungen im Wesentlichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n98\\n\\ndurch Bestärkungslernen (Reinforcement Learning) erreicht, das heißt in diesem Fall die systematische Korrektur der Gewichte durch die Reaktion von Testbenutzern.\\n\\nAus den Testeingaben zusammen mit den Antworten und Reaktionen der Testbenutzer:innen lassen sich annotierte Lerndaten generieren, so dass man spätere Modelle dann überwacht nachtrainieren kann und so die Kosten für die manuelle Leistung spart.\\n\\nZusätzlich zum Nachtrainieren bietet sich auch eine bewährte einfachere Methode für die Verbesserung von Antwortverhalten an: die Einbettung der Benutzer:inneneingaben in Prompts, die in einem oder mehreren Sätzen spezifizieren, welche Form der Antwort erwartet wird. Diese Methode wird auch vielfach eingesetzt, um Inhalt und Form der erwarteten Antworten auf die Anforderungen spezifischer Anwendungen anzupassen.\\n\\nToxizität Das Phänomen der Toxizität reicht von der Verwendung obszöner oder ethisch anstößiger Ausdrücke bis hin zu Äußerungen, die von Menschen als Ausdruck von Hass oder Verachtung interpretiert werden können oder auf andere Weise als beleidigend oder verletzend empfunden werden.\\n\\nVersuche, solche Ausgaben durch Zensur der Lerndaten zu erreichen, also durch das automatische Ausfiltern von Texten, die bestimmte anstößige Wörter oder Ausdrücke enthalten, sind nicht das geeignete Mittel, um Toxizität zu verhindern. Viele Wörter, die auf die Listen der anstößigen Wörter (Blocklists) gelangt sind, haben auch Verwendungen, die durchaus akzeptabel sind. Das Ausfiltern der harmlosen Verwendungen würde auf der Datenseite ganze Themenbereiche schwächen. Zum anderen können nicht alle Beleidigungen oder Verächtlichmachungen an den verwendeten Wörtern alleine erkannt werden. Es zeigt sich aber, dass die neueren Sprachmodelle selbst eine abstrakte Zuordnung von sprachlichen Ausdrücken zu sprachlichen Stilen und Register erlernen. Viele offensichtliche Formen der Toxizität kann das Modell nach Bestärkungslernen eigenständig vermeiden. Der Sprachgebrauch von ChatGPT erscheint im Vergleich zu früheren Foundation-Modellen sehr vorsichtig, ja fast schon konservativ.\\n\\nOb sich aber verlässlich alle Äußerungen vermeiden lassen, deren Aussagen oder Präsuppositionen von sensibilisierten Benutzer:innengruppen als beleidigend oder verletzend empfunden werden können, ist eine offene Frage. Dieses Thema ist ja auch eine Herausforderung für menschliche Textproduzent:innen. Das Problem wird sich aber ohnehin auch nur bei ganz speziellen Anwendungen stellen.\\n\\nEmpfehlung Für alle Formen von inadäquatem Antwortverhalten gilt: Selbst wenn es nicht sofort möglich ist, alle Formen dieses Fehlverhaltens für alle Anwendungen zu 100 % auszuschließen, so ist es doch für den Erfolg der Foundation-Modelle und deren Akzeptanz in Wirtschaft und Gesellschaft essentiell, Art, Grad und Häufigkeit von potentiellen Fehlverhalten empirisch zu bestimmen bzw. vorherzusagen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n99\\n\\nEs sollten deshalb schon während der Entwicklungszeit der ersten Modelle eine Testbatterie von Eingabeprompts für alle bekannten Arten von Bias und Toxizität zusammenstellen, die geeignet sind, Ausdrücke des Fehlverhaltens hervorzurufen und in ihrer Häufigkeit zu messen. Die ständige Evaluation und Verbesserung der Modelle in Bezug auf diese Probleme sollte ein zentrales Forschungsthema sein.\\n\\n6.3 Verbindung\\n\\nvon\\n\\nFoundation-Modellen\\n\\nmit\\n\\ngroßen\\n\\nWissensbeständen\\n\\nNeben dem aus Texten gewonnenen Wissen können auch bereits explizit formalisiertes Wissen für Foundation-Modelle verfügbar gemacht werden. Explizites Wissen kann in Datenbanken, Ontologien oder Wissensgraphen kodiert sein. Heutzutage werden für die Repräsentation großer Wissensbestände hauptsächlich Wissensgraphen (engl. Knowledge-Graphen, kurz KG) eingesetzt, weil sie die Vorteile von Ontologien und Datenbanken verbinden und sich im großvolumigen Einsatz bewährt haben.\\n\\nDie Nutzung von KGs soll mehrere Probleme der Foundation-Modelle lösen oder reduzieren:\\n\\nFalsche Antworten bei unzureichendem Wissen oder unzureichender Konfidenz: Neuronale Modelle geben in solchen Situationen manchmal ganz falsche Antworten. Auch ohne direkt nach ihnen gefragt zu werden, können Modelle mitunter in Ausgaben auch Fakten behaupten, die nicht der Wahrheit entsprechen. Das gehört in den Bereich der sogenannten Halluzinationen.\\n\\nDynamik des Wissens: Änderungen im Wissen oder gänzlich neue Fakten sind oft in den Trainingsdaten noch nicht repräsentiert oder im Vergleich zum überholten Wissen statistisch unterrepräsentiert.\\n\\nLücken im Detailwissen: KGs enthalten auch Details, die wegen mangelnder allgemeiner Relevanz im frei zugreifbaren Internet nicht zu finden sind, z.B. gewisse Teile von Produktspezifikationen, Mitgliederlisten, Messwerte usw.\\n\\nZur Nutzung der Wissensrepositorien gibt es drei vielversprechende Ansätze:\\n\\n(1) Die Aufnahme der Wissensbestände in die Trainingsdaten (z.B. KELM) (2) Der Zugriff des Modells auf die Wissensbestände als Teil der Inferenz (3) Die Berücksichtigung von Wissen in großen Textkorpora durch Retrieval\\n\\nBeim ersten Ansatz kann man die Wissenselemente des KG, sogenannte RDF Triple, automatisch in Sätze einer natürlichen Sprache umwandeln, wobei sich wegen der Benamung in den KGs das Englische anbietet. Alternativ kann man die RDF-Triples aber auch in der RDF-Syntax wie Sätze einer eigenen Sprache zu den multilingualen Trainingsdaten hinzufügen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n100\\n\\nDie Hinzunahme des Wissens zu den Trainingsdaten hat den Vorteil, dass sie bereits beim Pretraining das Weltwissen des Models verbessern. Außerdem besteht die Hoffnung, dass die Projekte erlernen können, zwischen einfachen sprachlichen Aussagen und den entsprechenden Wissensgraph-Fragmenten zu übersetzen ähnlich der Übersetzung zwischen natürlichen Sprachen.\\n\\nBeim zweiten Ansatz erlernt das Modell, durch SPARQL Queries auf benötigtes Wissen zuzugreifen und dieses für die Berechnung der endgültigen Beantwortung zu berücksichtigen. Dieser Ansatz hat den Vorteil, dass alles neue Wissen im KG unmittelbar ohne Nachtraining verfügbar wird.\\n\\nBeide Ansätze werden gegenwärtig bereits im Projekt Open GPT-X getestet. Folgeprojekte werden in der Realisierung dieses Ziels auf Erkenntnissen und praktischen Resultaten dieses Vorläufer-Vorhabens aufbauen können.\\n\\nGeeignete Kandidaten für Wissensrepositorien sind dabei DBPedia und WikiData zwei große generische Wissensgraphen, die von großen Wissensgemeinschaften (Knowledge Communities) gepflegt werden und kostenfrei nutzbar sind. In diesen Ressourcen ist allerdings das Wissen über die europäischen Wirtschaftsunternehmen nicht vollständig und auch nicht immer für alle Firmen aktuell. Ins Auge gefasst sollte daher auch die Einbeziehung von Spezialwissensquellen wie OpenCorporates oder alternative kommerzielle Angebote.\\n\\nEin dritter Ansatz sind Retrievalverfahren zur Nutzung zusätzlicher großer Textdatenbestände. Hierbei (z.B. es Suchmaschinenergebnisse) handeln, die noch nicht in die Trainingsdaten eingeflossen sind. Zum anderen können das auch interne Daten (z.B. Servicereports) sein, die als Zusatzinformation für das Modell wertvoll sein können. Über ein embedding-basiertes Retriever-Reader-Modell können sie in die Antworterzeugung einfließen und so Fehler in den erzeugten Texten reduzieren und aktuelle Informationen verwenden. Multilinguale Retriever-Reader-Modelle können dabei die Information in unterschiedlichen Sprachen nutzen.\\n\\nkann\\n\\nsich\\n\\num\\n\\naktuelle\\n\\nTexte\\n\\n6.4 Kombination von Sprache mit anderen Modi und Medien\\n\\nAuf die Frage nach dem Faszinierenden an Foundation-Modelle bemerkte Prof. Andreas Dengel: „Von Forschungsseite ist natürlich Multimodalität sehr spannend.\" Als zusätzliche Medien kommen insbesondere Bilder, Videos, gesprochene Sprache, Audio, und 3D- Modelle in Betracht, die mit Text kombiniert werden können. Yann LeCun, Forschungschef von Meta, geht noch einen Schritt weiter: „Anstelle von Sprache oder Bildern wird die nächste KI-Generation jedoch direkt aus Videos lernen. Meta unternimmt derzeit große Anstrengungen, um Videodaten aus der Ich-Perspektive für diese neue KI-Generation zu sammeln, aber auch YouTube-Videos sind als Trainingsmaterial geeignet\\'\\' (Schreiner, 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n101\\n\\nDie europäische Wirtschaft und Wissenschaft sollte diese Entwicklung ernst nehmen. Eine Implementierung und Training dieser Modelle für europäische Sprachen und Bildinhalte sowie ihre Erweiterung auf größere Dauer sind ein lohnendes Ziel für europäische Modelle.\\n\\nDie erfolgreiche Nutzung der Modelle erfordert die Lösung einer Reihe von Problemen:\\n\\nVerlängerung der zeitlichen Dauer von Videos um etwa eine Größenordnung. • Mögliche Verwendung einer autoregressiven Architektur. • Effiziente Nutzung von bestehenden Modellen zur Einzelbilderzeugung (z.B. Stable Diffusion).\\n\\nIn der Regel reicht ein einfacher Satz nicht mehr zur Spezifikation der Inhalte aus, sondern es muss eine Storyline mit mehreren Punkten angegeben und berücksichtigt werden.\\n\\nNeue Strategien um längere Eingabesequenzen verarbeiten zu können, z.B. nach dem Muster von S4.\\n\\nErweiterte Ansätze zur temporalen und räumlichen Disaggregation bei der Videogenerierung durch Diffusions-Modelle.\\n\\nEinbeziehung weiterer Modalitäten, wie etwa gesprochene Sprache und Geräusche.\\n\\nEinbindung von existierenden Bildern und 3D-Modellen, welche mit den Video- Techniken animiert werden können.\\n\\nGleichzeitige Behandlung verschiedener Objekte der gleichen Art. • Berücksichtigung von Kamerabewegungen, Morphing und Szenenwechsel.\\n\\nFür synthetisch generierte Videos gibt es einen riesigen Anwendungsbereich:\\n\\nBei Ausbildung und Lehre können Zusammenhänge direkt visualisiert werden. Ein Beispiel ist: „Zeige wie der indische Subkontinent das Himalayagebirge auffaltete.”\\n\\nIm Sprachunterricht können Szenen und Abläufe visualisiert werden, die dann die/der Schüler:in beschreiben muss.\\n\\nBei der personalisierten Werbung für ein neues Produkt: „Zeige wie Karl Müller mit seinem neuen Elektromobil über den Gotthardpass fährt”.\\n\\nAnleitungen zum Gebrauch eines Produktes können „on the fly” für eine neue Umgebung erstellt werden.\\n\\nAnimationsfilme lassen sich auf kostengünstige Art und Weise produzieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n102\\n\\nDas Erlernen von Text-Video-Modellen erfordert in der Regel einen Datensatz von Videoclips mit manuell eingegebenen Untertiteln. Die Erstellung solcher Datensätze ist jedoch teuer und zeitaufwendig und daher nur schwer in großem Umfang möglich. Stattdessen lassen sich diese Modelle mit Hilfe von Videos mit natürlichsprachlichen Annotationen in Form von automatisch transkribierten Sprachdaten trainieren. Ein Beispiel ist der HowTo100M Datenbestand mit 136 Millionen Videoclips aus 1,22 Millionen kommentierten Lehrvideos im Internet (Miech et al., 2019), die Menschen bei der Ausführung und Beschreibung von über 23.000 verschiedenen visuellen Aufgaben zeigen. Allerdings hat dieses Vorgehen auch einige potentielle Nachteile. Einerseits sind die durch Spracherkennung erzeugten Texte nicht fehlerfrei und die zeitliche Zuordnung ist nicht perfekt. Zudem handelt es sich um die eingeschränkte Domäne von Lehrvideos. Die sind umfangreicher und erfassen einen größeren neueren Datenbestände Themenbereich. HD-VILA-100M enthält 100 Millionen Videos in 720p-Auflösung, welche gleichzeitig Audiodaten, Untertitel und Video-Frames enthalten und durchschnittlich 13.4 Sekunden dauern (Zellers et al., 2022). Die Autor:innen zeigen, dass Audiodaten den Trainingserfolg signifikant verbessern. (Nagrani et al., 2022) übertragen Untertitel aus Bild-Text-Daten auf Videoclips ohne zusätzlichen manuellen Aufwand. Mit dieser Pipeline erstellen sie unscharf annotierte Audio-Video-Daten mit Millionen von gepaarten Clips und Beschriftungen. Sie zeigen, dass mit diesen Daten sehr leistungsfähige Modelle zur Videosuche und Videountertitelung trainiert werden können.\\n\\nInsgesamt gibt es auf Youtube, Shutterstock, Dreamstime und Reddit viele Millionen Videos, die mit Audio und teilweise mit Untertiteln verfügbar sind. Eine weitere wertvolle Ressource sind die Archive der Rundfunkanstalten, die für den barrierefreien Zugang zu ihren Sendungen routinemäßig Transkripte und Untertitel produzieren müssen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n103\\n\\nSPOTLIGHT Continental Automotive Technologies An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nContinental entwickelt wegweisende Technologien und Dienste für die nachhaltige und vernetzte Mobilität der Menschen und ihrer Güter. Das 1871 gegründete Technologieunternehmen bietet sichere, effiziente, intelligente und erschwingliche Lösungen für Fahrzeuge, Maschinen, Verkehr und Transport. Continental erzielte 2021 einen Umsatz von 33,8 Milliarden Euro und beschäftigt aktuell mehr als 190.000 Mitarbeiterinnen und Mitarbeiter in 58 Ländern und Märkten.\\n\\nDr. Corina Apachiţe, Head of AI, Continental Automotive Technologies\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case?\\n\\nWir setzen Foundation-Modelle insbesondere im Bereich „Natural Language Processing\", also der Analyse von Texten ein. Dort sind diese Modelle momentan das Maß aller Dinge. Unsere Anwendungen sind dabei die Analyse von „Requirements- Dokumenten zur Unterstützung unserer Entwickler, oder im Bereich \"Conversational AI\", also generell bei Mensch-Maschine-Schnittstellen. Eine weitere Anwendung, die zukünftig eine Rolle im automatisierten Fahren spielen kann, ist die Analyse und Formalisierung von Verkehrsregeln mit Hilfe von Sprachmodellen. Im Bereich Bildverstehen oder Bildgenerierung können Foundation-Modelle auch für das Erstellen und Analysieren von Bild-Datensätzen verwendet werden, um beispielsweise Trainingsdaten für AI-Modelle automatisch zu annotieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Für qualitativ hochwertige Anwendungen im Bereich Sprachverstehen sind Foundation-Modelle unerlässlich. Somit ermöglichen uns erst die KI-Foundation- Modelle unsere Anwendungen. Im Bereich Textanalyse von Requirements- Dokumenten spart uns die Technologie erheblich Zeit, macht die Analyse einfacher und effizienter und entlastet damit unsere Mitarbeiter:innen. Die Nutzbarkeit von Chat-Bots hängt primär von der Qualität der KI-Modelle ab. Zukünftige Foundation- Modelle, wie beispielsweise multi-modale Varianten, die sowohl Text als auch Bilddaten verstehen, werden auch für eine Vielzahl weiterer Aufgaben interessant sein, die bisher nur schwer umzusetzen sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n104\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Generell haben KI-Modelle eine direkte Abhängigkeit von den Daten, mit denen sie trainiert wurden. Ihre Qualität und Funktion der Modelle hängen primär von diesen Daten ab. Somit müssen Modelle primär auf Daten der entsprechenden Regionen trainiert werden. Europäische Besonderheiten, wie beispielsweise die Sprachdiversität sowie regulatorische oder kulturelle Unterschiede im Straßenverkehr, werden von nicht-europäischen KI-Modellen möglicherweise vernachlässigt. Qualität und Funktionen verlangen also nach regionalen Lösungen.\\n\\nZudem sorgen die hohen datenschutzrechtlichen Standards in Europa dafür, dass sensible Daten kaum an amerikanische oder chinesische KI-Modelle gesendet werden können. Des Weiteren bleiben die Modelle geschlossene Systeme („Black Box\"). Detailliertere Analysen oder Weiterentwicklungen der Modelle werden nicht ermöglicht.\\n\\nEine weitere Schwierigkeit ist die Abhängigkeit von den Besitzern der KI-Modelle in kommerzieller Hinsicht. Zugangshürden könnten aufgebaut oder spezielle Eigenschaften der Modelle designt werden, die nicht im Interesse europäischer Nutzer:innen sind. Und nicht zuletzt geht es auch um Standortnachteile durch Abwanderung von Talenten dorthin, wo die „besten\" Foundation-Modelle erstellt werden.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Jeder dieser Aspekte wäre ein großer Vorteil für die Verwendung von KI-Foundation- Modellen. Die Veröffentlichung als Open Source würde die Untersuchung der Inhalte von Foundation-Modellen vollumfänglich ermöglichen. Es gäbe mehr Gestaltungsmöglichkeiten, gerade auch im Hinblick auf die hohe Diversität in Europa. Foundation-Modelle können angepasst und beliebig modifiziert werden, wie z.B. das Zuschneiden von Modellen auf regionale Besonderheiten, auf die Bedürfnisse verschiedener Industrien oder Anwendungsdomänen.\\n\\nIn hohen Datenschutzstandards sehen wir einen kompetitiven Vorteil, der zusammen mit Transparenz die Akzeptanz von Foundation-Modellen erheblich erhöhen wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n105\\n\\n6.5 Fragestellungen und Weiterentwicklungen\\n\\nGerade auch für multimodaler Modelle gilt die Interviewaussage von Prof. Dr. Wrobel: „Das größte Defizit ist das mangelnde Verständnis der Fähigkeiten und Grenzen.” Dies wird von Prof. Schütze in seinem Interview unterstützt und konkretisiert: „Es wäre nochmal ein Paradigmenwechsel, wenn man andere Modalitäten vollumfänglich in die Modelle integrieren könnte und wenn man auch tatsächlich echtes Grounding berücksichtigen könnte.”\\n\\nGegenüber reinen Sprachmodellen weisen Foundation-Modelle, die Texte mit Bildern bzw. Videos kombinieren, noch eine zusätzliche Komplexitätsdimension auf. Daher sollten Ansätze entwickelt werden, um die Modellergebnisse zu erklären und die Zuverlässigkeit der Resultate für bestimmte Eingaben abzuschätzen.\\n\\nVideos enthalten sehr viele unterschiedliche Informationstypen, die bei der Analyse und Generierung integriert werden können:\\n\\nOCR kann verwendet werden, um im Video sichtbare Schriftzüge zu erkennen. Dazu kann man z.B. ein Texterkennungsmodell integrieren, welches Texte oder Tokeneinbettungen liefert.\\n\\nGesprochene Sprache lässt sich über eine Spracherkennung gewinnen, die Texte oder Tokeneinbettungen produziert.\\n\\nGesichter und die zugehörigen Personen sind besonders wichtige Merkmale zum Verständnis eines Videos. Hier lassen sich Modelle zur Gesichtserkennung in die Video-Pipeline einbeziehen.\\n\\nAudio, zum Beispiel Motorengeräusche, können bei der Interpretation von Videos verwendet werden. Hier lässt sich beispielsweise ein Audio-Erkennungssystem nutzen, welches auf den YouTube-8M Daten trainiert wurde, bei denen Objekte in den Videos annotiert wurden.\\n\\nSzenen (z.B. Bäume, Berge, Friedhof) können über ein Szenen-Modell erkannt werden, welches mit dem PLACE365 trainiert wurde.\\n\\nObjekte, wie sie etwa im ImageNet annotiert sind.\\n\\nBewegungen, die durch Modelle zur Aktionenerkennung erkannt werden können, welche mit den Kinetics-Daten trainiert werden können.\\n\\nEin erstes Modell in diese Richtung ist Merlot Reserve, das Audio, Untertitel und Video einbezieht (Zellers et al., 2022).\\n\\nDie gleiche Szene kann auch durch mehrere Videos erfasst werden, z.B. binokulare Kameras oder mehrere Kameras mit unterschiedlichem Gesichtsfeld. Hier besteht die Aufgabe, diese Abläufe zu integrieren und daraus eine einheitliche Interpretation zu gewinnen. Eine derartige Anordnung erleichtert die Rekonstruktion der 3D-Szene.\\n\\nModelle zur Erzeugung von Videos können jedoch auch missbraucht werden, um beispielsweise gefälschte, hetzende, herabsetzende oder bösartige Inhalte zu erzeugen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n106\\n\\nDiese Gefahren können z.B. durch interne Filter für die Eingabetexte und für die Prüfung der erzeugten Videoinhalte reduziert werden. Hier gibt es aber einen großen Forschungsbedarf, um die Modelle abzusichern und schwer erkennbare soziale Vorurteile und Stereotypen herauszufiltern. Google und Meta nennen diese Gefahren als Grund, warum sie ihre Modelle noch nicht freigeben.\\n\\nAuch in diesem Bereich der KI-Foundation-Modelle zeichnet sich bereits eine amerikanische Dominanz ab. Europäische Entwickler:innen muss die Möglichkeit gegeben werden, Videodaten zu nutzen und auf deren Basis fortschrittliche KI- Foundation-Modelle zu entwickeln. Nur so kann sichergestellt werden, dass europäische Werte und Standards auch in diesen Modellen Beachtung finden. Dafür müssen Entwickler:innen und Forscher:innen bestmögliche Voraussetzungen haben.\\n\\n6.6 Foundation-Modelle in anderen Datendomänen\\n\\nDie vorgeschlagene Strategie der Modellentwicklung, die multilingualen Foundation- Sprachmodellen schrittweise um Wissensbestände und Multimodalität zu erweitern, bietet auch eine gute Ausgangsbasis für die Entwicklung von Foundation-Modellen in ganz anderen Daten- oder Wissenschaftsdomänen. Erfolgreiche Anwendungen von Modellen in Genetik und Proteomik zum Beispiel beruhen auf der Einsicht, dass auch in diesen Bereichen Muster in Symbolsequenzen gelernt werden können, die neue Erkenntnisse zu Eigenschaften, Wirkung oder Veränderung der Moleküle ermöglichen. Es ist sehr wahrscheinlich, dass sich in Chemie, Werkstoffwissenschaften, Biowissenschaften oder anderen Natur- und Ingenieurwissenschaften weitere solche Einsatzgebiete finden werden.\\n\\nGroße zukünftige Anwendungsbereiche der Technologie liegen in der Betriebswirtschaft und in allen anderen Bereichen der Gesellschaft, in denen Prozesse gestaltet, gesteuert, optimiert, automatisiert und überprüft werden müssen. Weil Prozesse sequentielle Abfolgen von Teilprozessen und Einzelhandlungen sind, liegt der Einsatz von Transformermodellen zum Entwurf, zur Verbesserung und zum Monitoring solcher Prozesse nahe. Durch die Digitalisierung der Unternehmen und Verwaltungen wird es hinreichend Daten zu den tatsächlichen Abläufen in Geschäfts-, Produktions- und Verwaltungsprozessen geben, aus denen Foundation-Modelle ein generelles Wissen über die Eigenschaften und Elemente dieser Prozesse erwerben können. Solch ein Ansatz zur Gewinnung von neuronalen Prozessmodellen wird wahrscheinlich sehr schnell multimodal werden, indem er die sprachlichen Benamungen und Beschreibungen von Prozesselementen sowie die Informationsobjekte der Prozesse wie Formulare, Datentransaktionen, Geolokationen und Transportwege in die Lerndaten einbezieht. GATO (Reed et al., 2022) ist ein erstes multimodales Modell, welches Sequenzen von Texten, Bildern und Messwerten verarbeiten und daraus Steuerungsstrategien ableiten kann. Es erzielte auf mehr als 600 Benchmarks gute Steuerungsergebnisse.\\n\\nDas in dieser Studie diskutierte KI-Rechenzentrum würde den Einstieg der deutschen Wirtschaft und Forschung zu solchen neuen Modelltypen sehr erleichtern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n107\\n\\n6.7 Zusammenfassung\\n\\nBis hierhin hat die Studie gezeigt, dass das wirtschaftliche Potential und die gesellschaftliche Relevanz von KI-Foundation-Modellen immens sind. Um aber das volle wirtschaftliche Potential auszuschöpfen, müssen europäische Entwickler:innen dazu befähigt werden, eigene Foundation-Modelle zu entwickeln. Ansonsten besteht die Gefahr, von amerikanischen Modellen abhängig zu werden oder die Modelle gar nicht zu nutzen. Beide Fälle bedeuten einen erheblichen Wettbewerbsnachteil für die Wirtschaft.\\n\\nUm qualitativ hochwertige Foundation-Modelle zu trainieren, müssen Bias, Toxizität und Falschaussagen reduziert oder bestenfalls komplett ausgeschlossen werden. Auch wenn es hier bereits große Fortschritte gibt, hilft nur die Arbeit mit und die Forschung an KI- Foundation-Modellen, diesen Themenkomplex anzugehen. Die deutsche und die europäische Gesellschaft sollten alles daransetzen, diese Entwicklungen nicht den Amerikanern oder Chinesen zu überlassen.\\n\\nDabei sollte der Fokus zunächst auf Sprachmodellen liegen. Die Multilingualität Europas ist eine Herausforderung, aber zugleich auch eine große Chance für europäische Sprachmodelle. Eine adäquate Abbildung dieser Multilingualität in der Funktionalität der Foundation-Modelle erhöht deren Akzeptanz und den wirtschaftlichen Nutzen. Sie ist aber auch von hoher Relevanz für die gesamte Technologieentwicklung, denn der größte Teil der Welt ähnelt in Bezug auf Sprachenvielfalt eher Europa als den USA oder China.\\n\\nVerschiedene Projekte bereiten bereits vielsprachige Datensätze auf und stellen diese zur Verfügung. Dahinter steht bereits die nächste Generation der Foundation-Modelle in den Startlöchern, welche z.B. Videos erzeugen könnten. Hier ist der Entwicklungsbedarf noch größer, das Potential immens und viele Fragestellungen weiter ungelöst.\\n\\nDeutsche und europäische Entwickler:innen und Forscher:innen müssen jetzt befähigt werden, nach besten Standards an KI-Foundation-Modellen zu arbeiten und zu forschen. Der erste Schritt dafür sollte der Aufbau einer kompetitiven Infrastruktur für das Training an KI-Foundation-Modellen sein.\\n\\nWie das technisch gelingen kann, darin gibt nun das nächste Kapitel einen Einblick.\\n\\nGroße KI-Modelle für Deutschland\\n\\n108\\n\\nVoraussetzungen bei Software und Personal\\n\\nGroße KI-Modelle für Deutschland\\n\\n109\\n\\n7. Voraussetzungen bei Software und Personal Die in LEAM angedachte Entwicklung und das Training von KI-Foundation-Modellen (Bommasani et al., 2021) setzt eine integrierte und leistungsfähige Hard- und Software- Infrastruktur voraus, wie sie es bisher und in öffentlichen Investitionsplänen in Deutschland und der EU noch nicht gibt. Während sich diese Infrastruktur in ihrer Grundstruktur von etablierten Strukturen in High-Performance-Computing-Systemen lässt, setzt die Arbeit mit KI-Foundation-Modellen besondere (HPC) ableiten Voraussetzungen an deren Organisation und der dabei eingesetzten Software. Dieses Kapitel erklärt sowohl diese besonderen KI-Elemente und deren Zusammenwirken mit der HPC-Grundstruktur, als auch die dafür notwendigen Voraussetzungen und Investitionen, um LEAM möglich zu machen.\\n\\nHierfür ist es notwendig, gleichzeitig die technischen Voraussetzungen und Möglichkeiten der LEAM-Initiative wie auch ihrer Chance für das KI-Ökosystem und die deutsche und europäische Gesellschaft zu betrachten. Während in diesem Kapitel gezeigt werden kann, dass die Herausforderungen an LEAM auf der Software-Seite lösbar sind, werden auch ihre sozialen und ökonomischen Mehrwerte aufgezeigt. Ein Ziel von LEAM sollte es sein, das monolithisch simplifizierte Berufsbild der „IT-Fachkraft” aufzubrechen, um die vielfältigen, spezialisierten und voneinander abhängigen Rollen, die für ein wettbewerbsfähiges KI-Ökosystem notwendig sind, zu differenzieren und zu stärken. Als technologisches, soziales und ökonomisches Leuchtturmprojekt kann LEAM dafür den richtigen Impuls setzen, damit Deutschland und Europa zu einem globalen KI- Wettbewerber avanciert, wenn die Chancen des KI-Ökosystems richtig eingeschätzt und genutzt werden.\\n\\nIn diesem Kapitel wird eine Variante für den Software-Stack eines KI-HPC-Systems und die dafür benötigten hochspezialisierten Berufe und Fähigkeiten skizziert. Hierbei ist es möglich, strukturell auf existierenden und praxisbewährten HPC-Systemen aufzubauen und um KI-spezifische Komponenten zu ergänzen. 6 Besonderes Augenmaß wird darauf gelegt, den Stack mit Hilfe von Open-Source-Software (OSS) zu konstruieren, da das globale KI-Ökosystem einerseits auf nicht-proprietärer Software aufgebaut ist und andererseits so eine Abhängigkeit von Software-Konzernen vermieden werden kann. Dieser Aspekt der Unabhängigkeit fördert auch die Souveränität und Resilienz des KI- Ökosystems, da auf Zulieferer im Software-Bereich weitestgehend verzichtet werden kann, wie es z.B. auch die Digitalstrategie der Bundesregierung vorsieht (Bundesministerium tatsächlichen Implementierung können die Details abweichen, für diese Machbarkeitsstudie ist jedoch\\n\\nfür Digitales und Verkehr, 2022).\\n\\nIn der\\n\\n6 Wir betrachten in diesem Kapitel bevorzugt zentralisierte und homogen organisierte Infrastrukturen, obwohl B. Yuan et al., (2022) prototypisch gezeigt haben, dass KI-Modelle auch dezentralisiert und heterogen auf vernetzten Rechnern trainiert werden können. Zentralisierte Infrastrukturen haben den Vorteil, dass der eingesetzte Software-Stack einfacher und weniger fehleranfällig gehalten werden kann, die Datenquellen zuverlässiger und schneller verfügbar sind sowie Nutzerrechte und Sicherheitsvorkehrungen (DMZ, VPN) schlichter zu handhaben sind. Darüber hinaus können mit einem zentralisierten KI-Hochleistungszentrum zuverlässigere Statistiken über Energieverbrauch und Effizienz erhoben werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n110\\n\\nentscheidend, dass LEAM auf der Software-Ebene mit bereits existierenden Technologien umgesetzt werden kann.\\n\\nBei der Betrachtung dieser Infrastruktur lassen sich vier aufeinander aufbauende Ebenen (Layer) unterscheiden, die mit einzelnen Modulen interagieren, um die Entwicklung und das Training von KI-Modellen möglich zu machen (Abb. 18). Diese Layer setzen jeweils verschiedene spezialisierte Berufe voraus, um deren komplexe Funktionalität bereitstellen zu können. Diese einzelnen Hard- und Software-Layer sind dabei jeweils für bestimmte Aufgaben optimiert, um dedizierte, rechenintensive Anwendungen in einem effizienten Rechenprozess zu organisieren. In der Grafik sind die für LEAM notwendigen und für die Arbeit mit KI-Foundation-Modellen besonderen KI-Elemente im Trainings- und Applikations-Layer gebündelt, welches auf den System- und Framework-Layern der High- Performance-Computing-Infrastruktur aufsitzt und von den Nutzer:innen durch ein Service-Layer angesteuert werden kann.\\n\\nAbb. 18: Simplifizierte Darstellung der Hard- und Software-Infrastruktur von HPCs\\n\\nDas System-Layer bildet dabei mit seinen Recheneinheiten ein Hardware-Fundament, welches durch das Framework-Layer in betriebsfähige Software-Systeme gebündelt wird. Das Trainings- und Applikations-Layer nutzt diese Systeme, um KI-Anwendungen durchzuführen, während es vom Data-Storage & -Loading-Modul mit Daten beliefert wird. Der zentrale Unterschied zum Aufbau von traditionellen HPC-Systemen liegt hier darin, dass für die Entwicklung von KI-Modellen anwendungsspezifische KI-Beschleuniger wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n111\\n\\nz.B. GPUs, FPGAs, Cerebras-Chips und andere (siehe System-Layer) statt CPUs eingesetzt werden. Diese sind notwendig, damit die auf den System- und Framework-Layern aufbauenden, rechenintensiven Trainings- und innerhalb akzeptabler Zeiträume abgeschlossen werden können. 7 Während CPUs für die Datenaufbereitung über eine Cloud-Lösung gemietet werden könnten, sollten die mehreren tausend GPUs und unterstützenden CPUs, die für den Betrieb von LEAM notwendig wären, lokal verfügbar sein. In Kapitel 8 wird für diese Machbarkeitsstudie mit einer Anzahl von 4480 GPUs kalkuliert.\\n\\nInferenzanwendungen\\n\\nTabelle 5: Im Betrieb von LEAM werden für Training, Tuning und Inference Tausende GPUs benötigt.\\n\\nLEAM ist für den Auf- und Ausbau des Innovationsstandorts Deutschland und Europa daher unerlässlich, um dieses komplexe und aufeinander abgestimmte Geflecht an Hard- und Software aufzubauen und der Wissenschaft und Wirtschaft zur Verfügung zu stellen. Diese massive Recheninfrastruktur ist für die Entwicklung von KI-Foundation-Modellen notwendig. So erfordert das Training großer KI-Modelle zum Beispiel eine Vielzahl leistungsfähiger und optimierter KI-Beschleuniger, auf denen sowohl Daten als auch Modelle parallel geschaltet werden, was eine deutliche Leistungssteigerung hiesiger Rechenzentren erfordert. Dabei ist die Wiederverwendbarkeit dieser rechenintensiven Modelle von zentraler Bedeutung, um die dabei verbrauchten Ressourcen und investierten Kosten amortisieren zu können.\\n\\nDiese Leistungssteigerung ist durch die Erfüllung technologischer Voraussetzungen erreichbar, wie z.B. durch das effiziente und schnelle Laden und Speichern von Daten. Gleichzeitig ist sie aber auch von sozialen Veränderungen abhängig, wie z.B. die gezielte Anziehung von Fachkräften, um das Wachstum des KI-Ökosystems aufrechtzuerhalten. Hier sollte LEAM sowohl ein Anstoß und eine Inspiration sein, in Technologien zu investieren, die den Innovations- und den Wirtschaftsstandort Deutschland und Europa stärken sowie Wege zu finden, um Fachkräfte differenzierter und erfolgreicher zu umwerben und auszubilden.\\n\\n7 Mit einer einzelnen NVIDIA V100 GPU braucht man 355 Jahre, um GPT-3 zu trainieren (Li, 2020). CPU-basierte Frameworks sind mindestens 5-10 mal langsamer als GPU-basierte Frameworks. Ohne den Einsatz von GPUs braucht es also mehrere tausend Jahre Rechenzeit, um GPT-3 zu trainieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n112\\n\\nAus einer für diese Machbarkeitsstudie durchgeführten Umfrage mit 71 Teilnehmer:innen aus dem KI-Ökosystem Deutschlands ging hervor, dass die Berufe DevOps, MLOps, Machine Learning Engineer und Machine Learning Researcher für die Entwicklung und das Training von KI-Foundation-Modellen besonders wichtig sind. Des Weiteren braucht es für den Betrieb von LEAM Software Engineers und System-Administrator:innen, während Site Reliability Engineers unter den Befragten als weniger wichtig erachtet wurden. 56 % der Befragten gaben an, dass die Entwicklung eines KI-Foundation-Modells mehr als 25 Mio. Euro kosten würde, während 37 % davon ausgehen, dass sich diese Kosten innerhalb von fünf bis zehn Jahren amortisieren.\\n\\nDieses Kapitel der Machbarkeitsstudien zeigt also auf, dass die Herausforderungen an Software für die Entwicklung von KI-Foundation-Modellen in LEAM mit den heute zur Verfügung stehenden Technologien und Mitteln bereits lösbar sind, wenn sich Deutschland und Europa bereit zeigen, in die Integration von LEAM zu investieren. Durch den Einsatz von Open-Source-Software in Verbindung mit wenigen, ausgewählten proprietären Applikationen lassen sich auch hierzulande große KI-Modelle entwickeln und trainieren, obgleich die dafür notwendige Hardware zum größten Teil aus den USA und dem nicht-europäischen Ausland stammt.\\n\\nEine der größten Herausforderungen und Chancen für LEAM stellen die dafür notwendigen spezialisierten und umworbenen Fachkräfte dar. Für das Training der in LEAM vorgesehenen KI-Foundation-Modelle wird ein Team von etwa 20 dedizierten Expert:innen entlang der oben beschriebenen Rollen vorausgesetzt. Während das KI- Ökosystem in Deutschland und Europa bereits heute von hochqualifizierten Expert:innen betrieben wird, übersteigt die Nachfrage das Angebot noch deutlich (Streim, 2022). LEAM kann dazu beitragen, die Ausbildungsqualität und die Attraktivität des hiesigen KI- Ökosystems zu verbessern, und vor allem bewirken, dass KI-Fachkräfte in Deutschland und Europa bleiben wollen, weil sie hier mit LEAM Voraussetzungen wiederfinden, wie sie es derzeit nur außerhalb Europas gibt.\\n\\n7.1 Applikations-Layer: Trainings- & Inference-Technologien\\n\\nDas Training und die Entwicklung von großen KI-Foundation-Modellen bergen im Vergleich zu klassischen, verteilten Systemen ohne naiven Parallelismus einige Besonderheiten: Deep-Learning-basierte KI-Entwicklungen sind für die Konzeption und das Training von KI-Foundation-Modellen einzigartig befähigt, weil deren Algorithmen parallel geschaltet werden können. Das bedeutet, dass eine Vielzahl von Datenpunkten gleichzeitig verarbeitet werden, anstatt sie nacheinander, also sequentiell, zu bearbeiten. Diese Eigenschaft bildet einen wesentlichen Vorteil gegenüber klassischen HPC- Anwendungen und erlaubt es, spezifische algorithmische Vorteile auszunutzen, um das Training großer KI-Foundation-Modelle überhaupt erst in endlicher Zeit zu ermöglichen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n113\\n\\nAbb. 19: Die Architektur des Trainings- & Applikations-Layers im Detail\\n\\nDaten- & Modell-Parallelismus Diese Eigenschaften sind im „Daten-” bzw. „Modell-Parallelismus” zusammengefasst und erklären, warum dafür eine massive Recheninfrastruktur notwendig ist (Hugging Face, o.D.). Während im Daten-Parallelismus Trainingsdaten zu gleichen Anteilen auf die KI- Beschleuniger zwar aufgeteilt werden kann, ist die Anzahl der zu berechnenden Parameter in KI-Foundation-Modellen so groß, dass die Modelle selbst im Modell- Parallelismus 8 auf mehrere Beschleuniger aufgeteilt werden müssen. Das bedeutet, dass nicht jeder Beschleuniger eine identische Kopie des Modells parallel berechnet und deren Ergebnisse iterativ an alle parallelen Beschleuniger übermittelt, sondern die zu berechnenden Daten sequentiell von einem Beschleuniger zum nächsten weitergegeben werden müssen. Damit man dabei die Modellparameter aktualisieren kann, überträgt der letzte Beschleuniger in einer Reihe das Fehlersignal (Loss) wieder an den ersten Beschleuniger zurück. Daten- und Modell-Parallelismus 9 können gleichzeitig verwendet werden, was die Komplexität der Entwicklung weiter erhöht.\\n\\nBei solch einer zirkulären Sequenzierung bleiben allerdings die meisten KI-Beschleuniger ungenutzt, wenn diese gerade keine Operation in der Reihenschaltung durchführen. Diese Stillzeit kann im Modell-Parallelismus über raffiniertes Programmieren (Kosson et al., 2021) und durch Optimierungssysteme minimiert werden, die eine höchstmögliche Auslastung aller Beschleuniger gewährleisten (s. Kapitel 8.3). Diese Voraussetzungen machen das Programmieren und das Betreuen des Trainings von KI-Foundation-Modellen kompliziert und herausfordernd (Bommasani et al., 2021).\\n\\n8 Narayanan et al. (2021) beschreiben Modell- bzw. Pipeline-Parallelismus im Detail. 9 Mudigere et al. (2022) beschreiben Best Practices im Training großer Recommender-Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n114\\n\\nTrainingsmanagement, Evaluation & Benchmarking Im Applikations-Layer wird auch das zuverlässige Management von einzelnen Trainingsjobs gesteuert, um bei einem Hardware-Versagen keine Resultate zu verlieren. Anders als bei dem Training von kleineren KI-Modellen kommt es beim Training von KI- Foundation-Modellen häufig zu einem Ausfall einzelner KI-Beschleuniger. 10 Ein zuverlässiges Launch- und Relaunch-System beugt dabei durch fortlaufendes Monitoring Ergebnis-Verluste vor, um von derselben Stelle aus weiter zu trainieren, bei der der Fehler aufgetreten ist.\\n\\nDes Weiteren werden auch die Evaluation und das Benchmarking von KI-Modellen in diesem Layer implementiert. Dazu gehören sowohl das Monitoring während des Trainingsprozesses, als auch das Testen der Modelle an anwendungsrelevanten Datensätzen, um deren Performanz festzustellen und diese später in Data & Model Card- Dokumentationen überführen zu können (Pushkarna et al., 2022). Für das Training und die Evaluation ist auch eine Anbindung an die ETL-Infrastruktur 11 notwendig, die hier implementiert, getestet und versioniert wird.\\n\\nWiederverwendbarkeit in Deployment-Infrastruktur Aufgrund des rechenintensiven Trainings von KI-Foundation-Modellen steht die Wiederverwendbarkeit von kostspielig trainierten KI-Modellen im Mittelpunkt, um die dafür notwendigen Ressourcen und Kosten zu amortisieren. Dabei werden u.a. Destillierungsmechanismen, Adaptions- oder Finetuning-Verfahren eingesetzt (s. Kapitel 2), welche wiederum anwendungsrelevante Benchmarks voraussetzen.\\n\\nDabei spielt die Entwicklung und Betreuung einer optimalen Deployment-Infrastruktur eine zentrale Rolle, da so große Effizienzsteigerungen erreicht werden können. Hier muss auf eine Implementierung Wert gelegt werden, um die KI-Foundation-Modelle bestmöglich mit der darunterliegenden Hardware betreiben zu können und externe Anfragen per API schnell und zeitnah zu beantworten. 12 Es muss hierbei auf komplexe Produktionsaspekte geachtet werden, um einen reibungslosen Prozess auch unter Stress zu gewährleisten. So sollte die Deployment-Infrastruktur z.B. resilient gegenüber einer Häufung von Anfragen sein, mit einer stabilen API ausgestattet sein sowie robuste Zugangsbeschränkungen und Sicherheitschecks beinhalten.\\n\\n10 Dies ist ein bekanntes Phänomen aus der Datacenter-Branche: Je mehr Festplatten betrieben werden, desto häufiger fallen diese aus. Gleiches gilt dementsprechend auch für HPC- Beschleuniger. 11 ETL steht für Extract-Transform-Load und beschreibt die Aggregation von Rohdaten aus einer Produktionsdatenbank in ein Format, das zur Analyse der Daten verwendet werden kann. 12 Beispielhaft können hier Freeware-Bibliotheken wie TensorRT und Triton genannt werden, welche von NVIDIA zur Verfügung gestellt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n115\\n\\nRessourceneffizienz des Trainings Das ressourcenintensive Training von KI-Foundation-Modellen kann zu einem Problem werden, wenn der Trainingsvorgang nicht kontinuierlich kontrolliert wird und dabei der Energieverbrauch mit geeigneten Maßnahmen verringert wird. Die Nachhaltigkeit muss also bei der Konzeption eines LEAM-KI-Hochleistungsrechenzentrum und beim Entwickeln und Trainieren von KI-Modellen von Beginn an mitgedacht werden. 13\\n\\nDie in der KI-Entwicklung verbrauchten Ressourcen können auf dem Software-Level durch die regelmäßige und fortlaufende Messung von CO2-Äquivalenten festgehalten werden. Dabei werden die gemessenen CO2-Äquivalenten an drei verschiedenen Komponenten kontrolliert, um sie der richtigen Quelle zuzuordnen (Hintemann, 2020).\\n\\nWährend des Trainings und der Inferenz benötigen die KI-Beschleuniger Energie zum Ausführen der Rechenoperationen, sogenannter MACs. 14 Diese lassen sich auf dem Applikations- und Framework-Level feststellen (Bannour et al., 2021; CodeCarbon, 2020). Aufgrund der hohen Datenmenge verbraucht der Data-Storage & -Loading-Layer für das Bereitstellen, die Verarbeitung und den Transport von Daten ebenfalls einen signifikanten Anteil des Energieverbrauchs. Die zuverlässige Kühlung des Gesamtsystems und andere, kleinere Komponenten benötigen darüber hinaus eine ununterbrochene Stromzufuhr. Wie das Hochleistungszentrum an sich ressourcenarm/klimaneutral konzipiert werden kann wird im Kapitel 8.6 erläutert.\\n\\nDie verschiedenen Messungen können direkt an den Service-Layer übergeben werden, um den Nutzer:innen die Einsicht und Kontrolle über den Energieverbrauchs des Gesamtsystems und des Trainingsprozesses zu gewährleisten. Über Warnsignale kann ein unkontrollierter Mehrverbrauch verhindert werden. Solche Kontroll-Angaben werden teilweise bereits heute von Hyperscalern zur Verfügung gestellt.\\n\\nVoraussetzungen Die Lingua Franca für die Entwicklung moderner KI-Systeme ist Python (van Rossum, 1995), wobei KI-Anwendungen zunehmend auch in anderen Programmiersprachen entwickelt werden. Für die Entwicklung des Service-Layers kommen viele Sprachen in Frage, die hier nicht aufgelistet werden sollen. Die Softwarebibliotheken, die beim Trainieren und der Inferenz zum Einsatz kommen, sind großteils, wenn nicht gänzlich, durch Open-Source-Software abdeckbar. Beispielhaft sind hier auf dem Framework-Level PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2016), Keras (Keras, 2015/2022), und Jax (Frostig et al., 2018) zu nennen.\\n\\nBei der Orchestrierung nutzt der Applikations-Layer teilweise dieselbe Software wie der Framework-Layer, bspw. Ray (Moritz et al., 2018), Slurm (Yoo et al., 2003), kubeflow (Kubeflow, 2017/2022), hydra (Hydra, 2019/2022), Abseil (Abseil Python Common Libraries, 2017/2022) und andere (s. Kapitel 7.4). Weiterhin werden auch numerische Bibliotheken\\n\\n13 Vgl. bspw. (CSTB Releases Report Fostering Responsible Computing Research, 2022; Patterson et al., 2022) und die darin enthaltenen Referenzen. 14 MAC steht für Multiply-Add-Compute, der zentralen Rechenoperation von Computerprozessoren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n116\\n\\nwie BLAS („An Updated Set of Basic Linear Algebra Subprograms (BLAS)”, 2002), NumPy (Harris et al., 2020) und andere Verwendung finden. Beim ML-spezifischen Monitoring kann auf Bibliotheken wie AimStack (Arakelyan et al., 2020) oder MLFlow (MLflow, 2018/2022) zurückgegriffen werden. Diese Liste ist ausschließlich exemplarisch und dient lediglich der Darstellung der Wichtigkeit von Open-Source-Software für die Entwicklung moderner KI-Applikationen.\\n\\nDie Aufgaben im Applikations-Layer erfordern eine hohe Spezialisierung, welche üblicherweise von Informatiker:innen und Software-Entwickler:innen in geteilten Rollen übernommen wird. Diese kann man in vier Rollenprofile unterscheiden: Machine Learning Researcher konzipieren und entwickeln neue Algorithmen, während sich Machine Learning Engineers mit der Optimierung des Trainings- und Inferenzcodes und der Implementierung des verteilten Lernens beschäftigen. Zusammen bilden sie ein Team, um KI-Modelle zu skalieren.\\n\\nFull-Stack und Backend-Ingenieur:innen entwickeln den Service-Layer und arbeiten u.A. mit Systemadministrator:innen und Dev-Ops-Expert:innen zusammen, um die Benutzeroberfläche mit den verschiedenen Funktionalitäten des Gesamtsystems zu koppeln. Data Engineers und Data Scientists arbeiten an dem Data-Storage & Data- Loading-Layer und seiner Infrastruktur. Das beinhaltet das Design der Datenbanken oder anderer Speichereinheiten, die Entwicklung der ETL-Pipelines sowie das Kontrollieren der Daten-Lade-Pipelines und das korrekte Design der Trainings-Datensätze.\\n\\nGroße KI-Modelle für Deutschland\\n\\n117\\n\\nSPOTLIGHT Fyrfeed GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nFyrfeed erstellt Content für B2B-Unternehmen durch eine Kombination aus Fachautoren und Künstlicher Intelligenz. Kunden müssen dadurch bloß 5 Minuten pro Monat investieren und sparen 80% der Kosten gegenüber Agenturen.\\n\\nFyrfeed-Gründerteam: Ehud Alexander Avner, Dr. Thomas Lindemann, Benjamin Zengler\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir setzen, neben anderen Technologien und Tools, auch große, allgemeine Sprachmodelle ein, um Fachautoren bei der Erstellung von hochwertigem Content – z. B. Beiträgen für soziale Medien, Blogartikeln oder Whitepapers – zu unterstützen.\\n\\nDieser Ansatz, bei dem Mensch und KI zusammenarbeiten, nennt sich Human-in-the- Loop.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? KI-Foundation-Modelle ermöglichen uns, qualitativ hochwertige Texte schnell und kostengünstig anzubieten. Statt Modelle von Grund auf selbst neu trainieren zu müssen, was mit erheblichem Kosten- und Zeitaufwand verbunden ist, können wir allgemeine, vortrainierte Modelle sofort einsetzen. Dadurch werden Weiterentwicklungen des Produkts sowie das Testen neuer Anwendungsmöglichkeiten um ein Vielfaches einfacher.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Da wir Kunden aus der ganzen Welt bedienen, ist es denkbar, dass wir für verschiedene Sprachen verschiedene Modelle einsetzen. Obwohl bestehende State- of-the-Art-Sprachmodelle multilingual einsetzbar sind (und dies tun wir auch sehr erfolgreich), kann die Qualität zwischen Sprachen (und Fachbereichen) variieren. Modelle, die auf europäische (und außereuropäische) Sprachen spezialisiert sind – und gleichzeitig auch alle Vorteile von Open Source anbieten – wären ein großer Gewinn.\\n\\nGroße KI-Modelle für Deutschland\\n\\n118\\n\\n7.2 Data-Storage & -Loading-Layer\\n\\nDer Erfolg vieler KI-Foundation-Modelle hängt wesentlich von einem effizienten und schnellen Aufbereiten der zu ladenden Daten ab. Sowohl in der ETL als auch während des Trainings der Modelle ist Effizienz ausschlaggebend. Damit kommt dem Data-Storage & Loading-Layer eine besondere Bedeutung zu, wie sie es so in anderen HPC-Systemen oft nicht gibt.\\n\\nBeim Training von KI-Foundation-Modellen wird das Laden der Daten mithilfe eines mehrschichtigen Prozesses beschleunigt: Der ETL-Vorgang sowie das Filtern und Aufbereiten der Rohdaten werden sequentiell ausgeführt, sodass diese möglichst latenzarm geladen und im Daten-Parallelismus verteilt werden können. Für ETL werden dabei häufig große, klassische CPU-Server eingesetzt. Diese müssen in der Lage sein, Peta- und Exabyte an Daten zu verarbeiten, z.B. beim Scrapen von Webseiten und bei Map- Reduce-Jobs (Dean & Ghemawat, 2004) und ähnlichen, massiven Vorgängen. Dabei kommen spezialisierte Tools 15 in der Daten-Infrastruktur zum Einsatz, wie z.B. Spark (Apache Spark, 2014/2022), Flink (Apache Flink, 2014/2022), oder Dask (Dask, 2015/2022).\\n\\nDie per ETL bereitgestellten Rohdaten werden dann entweder in einer Datenbank oder einem Cloud-basierten Storage-Layer abgelegt. Dabei ist es möglich, aus diesen Rohdaten einen vorgefilterten Datensatz zu generieren und ebenfalls lokal oder in der Cloud abzulegen. Für das Vorfiltern werden ähnliche Tools und Ressourcen wie auch für ETL benötigt, diese sind also bereits vorhanden. Diese Art der Datenverarbeitung beschleunigt später den Trainingsprozess, setzt aber einen erhöhten Speicheraufwand voraus.\\n\\nWährend des Trainings von KI-Foundation-Modellen ist es wichtig, die Daten effizient und möglichst ohne Redundanzen zum KI-Beschleuniger zu transportieren. Dazu ist nicht nur eine entsprechende Netzwerk-Architektur notwendig (s. Kapitel 8), sondern auch ein dedizierter Software-Stack, welcher für das Laden von Daten in verteilten Systemen optimiert wurde. Dabei ist essentiell, dass Daten aufgrund ihrer Größe nicht auf einzelne Rechner geladen und gespeichert werden können, sondern durch ein effizientes Streaming-System bereitgestellt werden müssen, was die Entwicklung deutlich erschwert.\\n\\nDabei werden die Daten periodisch immer wieder in zufälliger Reihenfolge gestreamt (Nguyen et al., 2022), wenn sie von variierenden KI-Beschleunigern angefragt werden. Der Software-Stack muss in der Lage sein, sowohl von lokalen Datenbanken oder Festplatten, als auch von unterschiedlichen Cloud-Storage-Systemen lesen zu können, um etwaigen Benutzeranforderungen entsprechen zu können. 16 Solche Systeme werden z.B. von Amazon, Google und Microsoft vertrieben oder werden von Konzernen privat betrieben. - Ebenso müssen beim Laden Filterfunktionen, Datenaugmentierungen und\\n\\n15 Diese und folgende Listen haben keinen Anspruch auf Vollständigkeit und es werden nur exemplarische Elemente jeder Kategorie erwähnt. Die genaue Wahl der Tools hängt am Ende von den Implementierungsdetails ab und soll zu diesem Zeitpunkt nicht festgelegt werden 16 Erwartungen an die Datenhoheit sind erfahrungsgemäß sehr heterogen. Um eine breite Akzeptanz und Benutzung zu erreichen, muss das System also mit möglichst vielen Szenarien kompatibel sein.\\n\\nGroße KI-Modelle für Deutschland\\n\\n119\\n\\ntransformationen unterstützt werden, wie es beim Training von Deep Neural Networks üblich ist. Dies ist erforderlich, um Modelle robust und generalisierbar zu trainieren. Der Software-Stack muss auch dazu fähig sein, mehrere Datensätze miteinander zu kombinieren, um neue ETL-Jobs zu vermeiden und somit Ressourcen zu sparen.\\n\\nDer Data-Storage- & Loading-Layer muss auch mit lokalen Benutzerrechten sowie mit geltendem Datenrecht kompatibel sein. Als Bibliothek muss die Daten-Infrastruktur Access Control Levels (ACL 17) berücksichtigen, um den Zugang zu Daten abzusichern und so (un-)beabsichtigte Zugriffe auf Daten von Dritten zu vermeiden. Dies wird über eine Schnittstelle zum Service-Layer gesteuert, in dem die Benutzerverwaltung organisiert ist.\\n\\nInternationales Datenrecht sieht vor, dass das Speichern und Verarbeiten von Daten streng regulierten Praktiken zugrunde liegen muss, wenn es die Verarbeitung von personenbezogenen Daten betrifft, wie es z.B. in der Datenschutzgrundverordnung geregelt ist (Data Protection in the EU, o.D.). Darüber hinaus sind in verschiedenen geographischen Räumen auch bestimmte Zertifikate verpflichtend oder werden vom Markt erwartet, wie etwa SOC-2/SOC-3 (System and Organization Controls, o.D.) in den USA und Nordamerika oder ISO 27001 (ISO - ISO/IEC 27001 and Related Standards — Information Security Management, o.D.) bzw. ISO 27017/27018 (ISO 27017 and ISO 27018 Certification | DEKRA, o.D.) im europäischen Raum. Daten mit bestimmten Eigenschaften, bspw. Informationen über Gesundheit oder Kreditwürdigkeit unterliegen weiteren, lokal regulierten Bestimmungen, wie etwa die in den USA geltenden Regulierungen HIPAA oder FCRA.\\n\\nDarüber hinaus kann es auch notwendig sein, dass der Daten-Layer innerhalb einer Demilitarized Zone (DMZ) oder in einem Virtual Private Network (VPN) auf die Daten zugreifen kann, falls die Daten nicht innerhalb des Hochleistungszentrums temporär abgelegt werden können. Dies sollte allerdings in der Praxis vermieden werden, da die damit verbundene Latenzzeit die Geschwindigkeit beim Laden stark beeinträchtigen kann.\\n\\nVoraussetzungen Für die Entwicklung und Betreuung der Daten-Systeme sind besondere Fähigkeiten von Data Engineers und Machine Learning Engineers notwendig, um das ETL und das Stream- basierte, verteilte Laden der Daten auf die Beschleunigerknoten zu gewährleisten. Eingesetzte Software sind z.B. Spark, Flink, oder Dask für ETL oder torchdatasets (Maszke, 2019/2022), Squirrel (Sohofi et al., 2022/2022), Deep Lake (Deep Lake, 2019/2022), ffcv (FFCV, 2021/2022) und andere (Ofeidis et al., 2022) für das verteilte Laden der Daten. Es handelt sich hier in der Regel um frei verfügbare Open-Source-Software, deren Beschaffung keine Herausforderung darstellt. Zum Speichern der Daten können klassische verteilte Datenbanken wie HDFS (Apache Hadoop, 2014/2022) oder GlusterFS (GlusterFS, 2011/2022), Cloud-Strorage Systeme, ähnlich Google Cloud Storage, Azure Blob Storage oder auch NAS-Systeme verwendet werden.\\n\\n17 Man unterscheidet hierbei zwischen Read, Write, Execute und Discoverable. Also die Erlaubnis, Daten zu lesen, zu schreiben, Anwendungen auszuführen oder - weniger bekannt - die Fähigkeit, Informationen über das Vorhandensein von Daten zu finden, ohne diese jedoch lesen zu dürfen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n120\\n\\n7.3 System-Layer\\n\\nIm System-Layer werden die tatsächlichen KI-Beschleuniger und die für deren Einsatz notwendige Hardware-Struktur organisiert, um hochleistungsfähige und verlustarme Rechenleistung zur Verfügung zu stellen. KI-Beschleuniger („AI Accelerator”, 2022) sind typischerweise spezialisierte Prozessoren, welche für die Entwicklung und das Training von KI-Modellen optimiert wurden (Reuther et al., 2022). Der Einsatz dieser Prozessoren ist unerlässlich, um den zeitlichen Rechenaufwand für KI-Foundation-Modelle zu beherrschen sowie wettbewerbsfähig zu sein (Khan & Mann, 2020). Weltweit führend in der Herstellung von High-Performance-Prozessoren sind die US-amerikanischen Firmen NVIDIA (Campa et al., 2020), AMD (AMD, 2021) und Intel (Intel, 2022), aber auch Start-ups wie z.B. Tachyum (Tachyum, 2022). Edge- und Embedded-Systems, wie z.B. NVIDIA Jetsons oder FPGAs, kommen hier nicht in Betracht, da sie nicht für das Training von KI- Foundation-Modellen geeignet sind.\\n\\nDiese Prozessoren werden mithilfe spezialisierter Bibliotheken gesteuert, die oft von Hardware-Herstellern selbst entwickelt werden. Beispielhaft zu nennen sind hier CUDA (NVIDIA Developer, 2013) und ROCm (AMD, o.D.), welche in der Regel als Freeware 18 mit liegende Server- der Hardware zur Verfügung gestellt werden. Das darüber Betriebssystem steuert die physikalische Hardware mittels dieser Bibliotheken an, wobei fast immer Linux-Betriebssysteme, wie Distributionen der Debian- (Ubuntu), Redhat- (Fedora, CentOS) oder Arch-Familien, eingesetzt werden (Joseph et al., 2022). Die einzelnen Server-Einheiten, welche mehrere KI-Beschleuniger bündeln, werden dann mittels entsprechender Middleware in das Kommunikations-Substrat eingebunden, wie es in Abbildung 20 näher beschrieben ist.\\n\\nAbb. 20: Die Architektur des System- und Data-Storage & Loading-Layers im Detail\\n\\n18 Im Vergleich zu OSS stehen bei Freeware-Bibliotheken nur die kompilierten Binärdateien zur Verfügung und nicht der gesamte, menschenlesbare Quellcode.\\n\\nGroße KI-Modelle für Deutschland\\n\\n121\\n\\nKommunikations-Substrat Das Kommunikations-Substrat dient einerseits der nahtlosen Verbindung von System- und Framework-Layer, aber auch dazu den KI-Beschleunigern Datenbanken und Datenspeicher zur Verfügung zu stellen. Dabei kann das Kommunikations-Substrat mehrere Server-Einheiten miteinander verknüpfen, um große Datenmengen effizient verarbeiten zu können. Die dabei für KI-Anwendungen typischerweise eingesetzten Kommunikationsbibliotheken basieren auf spezialisierten Technologien, wie MPI („Message Passing Interface”, 2022), OpenMPI („Open MPI”, 2022) oder NCCL (NVIDIA, o.D.).\\n\\nDie genaue Topologie des Substrates ist durch die tatsächliche, physikalische Verkabelung der Hardware gegeben und muss beim Bau eines Hochleistungszentrums mitgedacht werden. Dabei muss bereits zu Anfang klargestellt werden, welche Anwendungen die KI- Beschleuniger berechnen werden.\\n\\nVoraussetzungen Für die Konzeption, den Aufbau und die Betreuung des System-Layers und des Kommunikations-Substrats bedarf es als Systemadministrator:innen ausgebildete Fachkräfte. Für die Betreuung wird spezialisierte Kontroll-Software eingesetzt, welche aus proprietärer und Open-Source-Software (OSS) bestehen kann. Generell besteht die Hardware- und Software-Infrastruktur für KI-Hochleistungsrechenanwendungen überwiegend aus OSS, was deren Bedeutung für das KI-Ökosystem unterstreicht (Sonnenburg et al., 2007).\\n\\nDarüber hinaus übernehmen die Systemadministrator:innen auch die Rechtevergabe und Kontrolle der Nutzer:innen, bspw. über entsprechende Access Control Levels mittels LDAP- oder AD-Systeme, aber auch die Serveradministration und das -monitoring, um die dauerhafte Gesundheit des Systems zu gewährleisten. Des Weiteren fällt auch die Administration von DMZs und VPNs sowie Cyber-Sicherheits-Vorkehrungen in den Aufgabenbereich der Systemadministrator:innen.\\n\\n7.4 Framework- & Service-Layer\\n\\nFramework-Layer In dem über dem Kommunikations-Substrat liegenden Framework-Layer werden die spezialisierten KI-Technologien eingesetzt (Abb. 21). Dazu werden die im System-Layer als Server-Einheiten durch Containerlösungen wie Docker (Merkel, 2014) abstrahiert, um die Umgebung auf die entsprechenden KI-Systeme zu normalisieren (Carpintero, 2021) und zu homogenisieren. Damit lassen sich unterschiedlichste KI-Systeme entwickeln, obwohl die darunterliegende Hardware dieselbe bleibt.\\n\\nzusammengefassten\\n\\nKI-Beschleuniger\\n\\nnochmals\\n\\nIn der Container-Umgebung wird ein weiteres, eigenes Linux-basiertes Betriebssystem eingesetzt, welches mit Hochleistungsbibliotheken wie BLAS oder cuBLAS („Basic Linear Algebra Subprograms”, 2022) sowie für die Entwicklung von KI-Modellen notwendigen Programmiersprachen wie Python, C und C++ samt ihrer Compiler ausgestattet sind. Weiterhin befinden sich hier auch die für Deep Learning-Anwendungen spezifischen\\n\\nGroße KI-Modelle für Deutschland\\n\\n122\\n\\nFrameworks, mit welchen sich KI-Anwendungen programmieren lassen. Darunter zählen z.B. PyTorch, TensorFlow, Keras oder Jax (Gopani, 2021).\\n\\nAbb. 21: Die Architektur des Framework- & Service-Layers im Detail\\n\\nService-Layer Über die für die operative Entwicklung und das Training von KI-Foundation-Modellen notwendige Hard- und Software-Infrastruktur werden manche der darin enthaltenen Services im Service-Layer über Benutzeroberflächen abgebildet. Zum Beispiel werden hier unveränderte KI-Modelle für eigene Use-Cases eingesetzt oder KI-Modelle automatisch an bereitgestellte Datensätze adaptiert. Im Service-Layer können aber auch administrative Prozesse wie die Kontrolle von Nutzern oder die Anfragen von weiteren Ressourcen abgewickelt werden.\\n\\nVoraussetzungen Im Framework-Layer wird die Cluster-Orchestrierung gesteuert, um gezielt Aufträge im System starten zu können und verfügbare System-Ressourcen effizient einsetzen zu können. Hierfür wird ein Job-Management-System eingesetzt, welches typischerweise auf SLURM, Kubernetes (Kubernetes (K8s), 2014/2022), Terraform (Terraform, 2014/2022), DockerHub oder anderen Komponenten basiert (Mujkanovic et al., 2020). Damit lassen sich Aufträge aneinanderreihen und individuell priorisieren. Darüber hinaus gehört zum Framework-Layer auch eine für Deep Learning-Anwendungen spezifische Komponente, um Daten passgenau und effektiv aus den Datenspeicher-Systemen in die Rechnerumgebung zu laden (s. Kapitel 7.2).\\n\\nFür diese Fülle an spezialisierten Aufgaben werden Fachkräfte benötigt, die in der teilweise automatisierten Operationalisierung der Entwicklungs- (DevOps) und Machine- Learning-Umgebung (MLOps) ausgebildet sind. Dabei ist es notwendig, sowohl die dauerhafte Gesundheit der Umgebungen zu gewährleisten, als auch über Weiter- und Neuentwicklungen von Software-Lösungen für das Framework-Layer informiert zu bleiben, um bei eingehender Prüfung Verbesserungen am System vorzunehmen. Für die Orchestrierung und Priorisierung der Cluster werden vor allem Software Engineers und DevOps-Expert:innen gebraucht.\\n\\nGroße KI-Modelle für Deutschland\\n\\n123\\n\\n7.5 LEAM als Leuchtturmprojekt für die Zukunft des KI-Ökosystems\\n\\nEine für die Konzeption, das Training und die Unterhaltung von KI-Foundation-Modellen notwendige und leistungsfähige KI-Software-Umgebung setzt aber nicht nur technische und infrastrukturelle Bedürfnisse voraus, sondern stellt auch vielfältige und zahlreiche Voraussetzungen an gut ausgebildetes Personal, welche derzeit in Deutschland und der EU noch kaum oder gar nicht erfüllt werden. So belegen Studien zum Fachkräftemangel in der IT-Dienstleistungsbranche einen Mangel an spezialisiert ausgebildeten Fachkräften, aber auch eine fehlende Differenzierung zwischen einzelnen hochspezialisierten Rollen, die für die Entwicklung und das Training von KI-Foundation-Modellen notwendigerweise zusammenwirken müssen (Hickmann & Koneberg, 2022). Dies erschwert eine passgenaue Ansprache und Werbung für den Nachwuchs.\\n\\nIn der Praxis wird die Arbeit an KI im Allgemeinen und insbesondere an KI-Foundation- Modellen in Teams organisiert, die verschiedene Fähigkeiten und Ausbildungen einbringen. Spezialisierte Teams müssen verschiedene Programmiersprachen sowie Mathematik, Datenwissenschaft und Datentechnik beherrschen, aber auch vertiefte Kenntnisse in Informatik, Statistik und Wissen über Software-, Hardwarekomponenten und -architekturen vereinen: „Machine Learning-Spezialisten arbeiten in Teams – mit ML- Spezialisten an der Spitze, Softwareentwicklern in großer Zahl an der Basis und Datenwissenschaftlern und typische Personalstruktur fortschrittlicher Technologiecluster wie dem Silicon Valley” (Philippe Lorenz & Kate Saslow, 2019, eigene Übersetzung).\\n\\ningenieuren dazwischen – das\\n\\nist die\\n\\nHeute sind diese vielfältigen Teams in Deutschland allerdings unterbesetzt, sodass es „für neun von zehn offenen Stellen [in der Informatik] zuletzt keine passend qualifizierten Arbeitslosen” gab (Hickmann & Koneberg, 2022). Bitkom-Präsident Achim Berg sagte dazu: „Der sich verschärfende Mangel an IT-Spezialistinnen und -Spezialisten wächst sich zu einer ganz realen Bedrohung für Deutschlands große Transformationsaufgaben aus.” (Bitkom e.V., 2022a).\\n\\nIn einer dreiteiligen Studie hat das Center for Security and Emerging Technology (CSET) an der Georgetown University festgestellt, wie schnell der Bedarf an KI-Expert:innen wächst: Zwischen 2015 und 2019 ist der Anteil der gesamten Arbeitnehmer:innen, die in den USA direkt am KI-Ökosystem beteiligt sind, von sechs auf neun Prozent gewachsen – zu 14 Millionen Arbeitnehmer:innen (Gehlhaus et al., 2021). Innerhalb der nächsten zehn Jahre soll diese Berufsgruppe dabei sogar doppelt so schnell wachsen wie der Bundesdurchschnitt (Gehlhaus et al., 2021). Doch anders als in den USA, können bürokratische Hürden, ausländische Fachkräfte einzustellen und an den Arbeitsmarkt zu binden, hierzulande deutlich höher sein.\\n\\nIn der schulischen und beruflichen Bildung ist es dabei notwendig, stärker zu differenzieren und die unterschiedlichen Rollen im KI-Ökosystem gezielter auszubilden. Hier können neben Universitäten insbesondere Fachhochschulen und Hochschulen für für spezialisierte Angewandte Wissenschaften mehr Aufgabenfelder investieren, während Universitäten mehr Mittel für die Ausbildung von ML-Spezialisten zur Verfügung haben sollten (Wannemacher & Bodmann, 2021).\\n\\nin attraktive Angebote\\n\\nGroße KI-Modelle für Deutschland\\n\\n124\\n\\nDas Bildungsangebot und der Arbeitsmarkt der USA sind uns dabei mehrere Schritte voraus. Dazu stellte CSET weiterhin fest, dass viele Berufswege im KI-Ökosystem keine mehrjährige Ausbildung an einer Universität oder gar einen Doktortitel benötigen. In der Studie, „Training Tomorrow’s AI Workforce,” plädiert das CSET für eine neue Strategie, zu befähigen, KI-Expert:innen nicht-universitäre Bildungseinrichtungen besser auszubilden, in dem diese auf föderaler und regionaler Ebene gefördert werden sowie besser mit den lokal verorteten Unternehmen verknüpft werden (Gehlhaus & Koslosky, 2022). So sollen Student:innen und Arbeitgeber:innen bessere Voraussetzungen vorfinden, regional integrierte KI-Ökosysteme zu betreiben.\\n\\nDarüber hinaus plädiert CSET für eine Abkehr von weitreichenden, langjährigen Studiengängen in der Informatik hin zu einer größeren Vielfalt an modular aufgebauten Weiterbildungskursen, um sich schneller zu spezialisieren und so für den Arbeitsmarkt verfügbar zu sein. Dieser Strategiewechsel muss laut CSET aus der US-Regierung heraus gesteuert werden, indem eine Stabstelle für die Ausbildung von KI-Expert:innen im Bundeskabinett geschaffen wird (Gehlhaus & Koslosky, 2022). Eine ebenso zentralisierte, befähigte Stabsstelle wäre auch in Deutschland und der EU ein wichtiges Signal, um den Fachkräftemangel im Detail richtig beurteilen zu können und das Angebot an KI- Weiterbildungen in der Masse und in der Tiefe zu verbessern.\\n\\nGerade diese wirtschaftliche Entwicklung des KI-Ökosystems hin zu einer zentralen, alle anderen Wirtschaftsbereiche durchdringenden Querschnittsbranche untermauert die Wichtigkeit von LEAM als Befähiger der deutschen und europäischen Wissenschaft und Wirtschaft. LEAM kann neben seinen wirtschaftlichen Möglichkeiten auch ein wirtschaftspolitischer Leuchtturm sein, um Expert:innen auszubilden, anzuwerben und langfristig zu binden, wie auch junge Menschen dazu zu inspirieren, IT-Berufe zu erlernen. LEAM kann also nicht nur ein starkes Zeichen für die Wettbewerbsfähigkeit des Innovationsstandorts Deutschland und Europa setzen, sondern auch über die konkrete Anwendung hinaus einen für die Zukunftsfähigkeit unserer Wirtschaft notwendigen Bildungsauftrag in der Gesellschaft erfüllen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n125\\n\\n7.6 Zusammenfassung\\n\\nDie Herausforderungen des von LEAM vorgeschlagenen KI-Hochleistungsrechenzentrum zur Entwicklung und des Trainings von KI-Foundation-Modellen sind auf der Software- Ebene technisch lösbar. Die dafür notwendigen Technologien wurden bereits entwickelt und stehen größtenteils als Open-Source-Software zur Verfügung. Die wesentliche Herausforderung liegt also nicht in der Entwicklung der Software, sondern im Betrieb eines solchen KI-Hochleistungsrechenzentrum, wofür eine Vielzahl an hochspezialisierten Expert:innen in Teams zusammenarbeiten muss.\\n\\nFalls eine Umsetzung gelingt, kann LEAM wissenschaftliche Durchbrüche aus Deutschland und Europa lancieren, indem mit komplexer, heute bereits verfügbarer Software auf moderner, massiver Hardware KI-Modelle robust skaliert werden. So könnte LEAM die deutsche und europäische Wissenschaft und Wirtschaft nicht nur zukunfts- und wettbewerbsfähig machen, sondern auch für unsere Gesellschaft über die tatsächliche, technische Umsetzung Mehrwerte in der Bindung von Fachkräften und der Lösung von gesellschaftlichen Aufgaben bieten\\n\\n.\\n\\nGroße KI-Modelle für Deutschland\\n\\n126\\n\\nAufbau eines KI-Hochleistungsrechenzentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n127\\n\\n8. Aufbau eines KI-Hochleistungsrechenzentrums Als einer der Hauptgründe, warum in Deutschland bzw. Europa kein regelmäßiges Training großer KI-Foundation-Modelle à la GPT-3 möglich ist, nennen die Expert:innen, die für diese Studie interviewt wurden, die fehlende High Performance Computing (HPC) Infrastruktur für die Berechnung solcher Modelle. Denn KI-Foundation-Modelle stellen insbesondere an die Rechenkapazitäten besondere Anforderungen. Anders als bei herkömmlichen HPC-Systemen, die Central Processing Unit (CPU)-basiert arbeiten, werden im Bereich Künstlicher Intelligenz und speziell bei der Berechnung großer KI- Foundation-Modelle außergewöhnlich hohe Rechenkapazitäten von bis zu 4500 Graphics Processing Unit (GPU) benötigt, die trotz dieser hohen Kapazität sehr lange Laufzeiten benötigen, um die nötigen Rechenaufgaben zu bewältigen. Diese Art von konzentrierten Rechenkapazitäten sucht in Deutschland und EU bisher noch ihresgleichen. Indem Industrie und Wissenschaft die Zugänge zu diesen Rechenkapazitäten fehlen, werden Sprunginnovationen im Bereich KI-Foundation-Modelle stark erschwert. Ein Rechenzentrum (RZ) ist eine Infrastruktureinrichtung, in der Computer, Server, Speichersysteme und andere Technologiekomponenten zusammengefasst sind, um eine große Menge an Daten und Anwendungen zu verarbeiten und zu speichern (Hintemann & Clausen, 2018). Rechenzentren dienen in der Regel als zentrale Ressource für die Verarbeitung und Speicherung von Daten und Anwendungen in Unternehmen, Organisationen und Institutionen. Sie können auch für verschiedene Zwecke eingesetzt werden, von der Verarbeitung von Transaktionen und der Bereitstellung von IT-Diensten bis hin zur Ausführung von KI-Anwendungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n128\\n\\nEs gibt verschiedene Arten von Rechenzentren, die sich in ihrer Größe, ihrem Zweck und ihrer technischen Ausstattung unterscheiden (vgl. Tabelle 6).\\n\\nRechenzentrum\\n\\nBeschreibung\\n\\nEnterprise Rechenzentren\\n\\nEnterprise Rechenzentren sind Rechenzentren, die von Unternehmen und Organisationen betrieben werden, um die Verarbeitung und Speicherung von Daten und Anwendungen für ihre internen Zwecke zu ermöglichen. Enterprise Rechenzentren können in verschiedenen Größen und Formen auftreten, von kleinen Räumen mit wenigen Servern bis hin zu großen Anlagen mit tausenden Computern und Servern.\\n\\nCollocation Rechenzentren\\n\\nCollocation Rechenzentren sind Rechenzentren, die von Dritten betrieben werden und an Unternehmen und Organisationen vermietet werden. Collocation Rechenzentren bieten den Mietern Platz für ihre eigenen Server und andere Computerkomponenten und stellen die erforderliche Infrastruktur wie Strom, Kühlung und Netzwerkverbindungen zur Verfügung.\\n\\nHyperscaler Rechenzentren\\n\\nHyperscaler Rechenzentren sind Rechenzentren, die von Hyperscaler-Unternehmen betrieben werden, um die Verarbeitung und Speicherung von Daten und Anwendungen für ihre Kunden zu ermöglichen. Hyperscaler Rechenzentren sind oft sehr groß und bieten eine hohe Rechenleistung und Speicherkapazität, um große Mengen an Daten schnell und effizient zu verarbeiten und zu speichern.\\n\\nTabelle 6: Beispiele für Rechenzentren\\n\\nGroße KI-Modelle für Deutschland\\n\\n129\\n\\nNeben dem Geschäftsmodell können Rechenzentren auch nach ihrer Größe unterschieden werden. Folgende Größenangaben können hier als Referenz dienen (The role of data centers in an interconnected world, o.D.):\\n\\nArt\\n\\nGrößen\\n\\nLeistungs- aufnahme\\n\\nSchwerpunkt\\n\\nMicro Data Center\\n\\nAb 1 Server-Rack aufwärts; passende Konfigurationen für einen Container\\n\\n100 kW\\n\\nEDGE-Anwendungen, die von der Nähe zur IoT- Quelle profitieren\\n\\nKleine Rechenzentren (auch EDGE- Rechenzentren)\\n\\nca. 500 m2\\n\\n1 MW\\n\\nOftmals ein Unternehmens-eigenes Rechenzentrum für kritischen Datenbestand\\n\\nMittlere Collocation/Hostin g Rechenzentren\\n\\nca. 10.000 m2\\n\\n10 MW\\n\\nMulti Tenant Collocation RZs sowie Ausrichtung auf Hosting und Managed Services\\n\\nGroße Collocation Rechenzentren\\n\\nca. 50.000 m2\\n\\n50 MW\\n\\nMulti und Single Tenant Collocation sowie große Hosting/Managed Service Anbieter mit internationaler Ausrichtung\\n\\nHyperscaler\\n\\n100.000 m2 und mehr\\n\\n100 MW und mehr\\n\\nGroße, global operierende Cloud-Anbieter, die oftmals an mehreren Standorten (10-200) weltweit tätig sind\\n\\nTabelle 7: Größen von Rechenzentren\\n\\nAbgrenzung von KI-Hochleistungsrechenzentren LEAM plant die Berechnung von großen KI-Foundation-Modellen, hierzu ist spezielle Hardware notwendig. Die benötigten KI-Hochleistungsrechenzentren unterscheiden sich in erster Linie durch ihren Fokus auf die Verarbeitung von KI-Anwendungen. Andere Rechenzentren sind meist auf die Verarbeitung allgemeiner Daten und Anwendungen ausgerichtet und können für eine Vielzahl von Zwecken verwendet werden. Ein KI- Hochleistungsrechenzentrum hingegen ist speziell für die Verarbeitung von KI- Anwendungen entwickelt und ausgestattet und bietet die erforderliche Rechenleistung und -umgebung, um KI-Modelle und -Algorithmen schnell und effizient zu trainieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n130\\n\\nKI-Hochleistungsrechenzentren grenzen sich von anderen Rechenzentren auch in ihrer technischen Ausstattung und Funktionsweise ab. KI-Hochleistungsrechenzentren können zum Beispiel über eine höhere Rechenleistung und spezielle Hardware wie Grafikprozessoren (GPUs) verfügen, die für die Verarbeitung von KI-Anwendungen besonders geeignet sind. Sie können auch über spezielle Software-Tools und - Umgebungen verfügen, die es ermöglichen, KI-Modelle und -Algorithmen zu entwickeln und zu trainieren.\\n\\n8.2 Anforderungen an ein KI-Hochleistungsrechenzentrum\\n\\nInsgesamt sind KI-Hochleistungsrechenzentren also auf die speziellen Anforderungen von KI-Anwendungen ausgerichtet und bieten die erforderliche Infrastruktur und Ressourcen, um KI-Anwendungen effektiv zu verarbeiten. Im Folgenden werden die konkreten Anforderungen an ein KI-Hochleistungsrechenzentrum zur Berechnung großer KI- Foundation-Modelle beschrieben.\\n\\nProzessoren Für die Berechnung von KI-Anwendungen sind Grafikprozessoren (Graphic Processing Units/GPUs) geeignet. GPUs sind speziell für die Bearbeitung von Graphikaufgaben konzipiert. Sie verfügen über eine Vielzahl von Rechenkernen, wodurch sie große Datenmengen schnell und parallel verarbeiten können. Dadurch können sie Aufgaben wie das Rendern von 3D-Grafiken oder das Trainieren von Modellen für maschinelles Lernen und Künstliche Intelligenz schnell bewältigen. In vielen Rechenzentren werden jedoch anstatt von GPUs, CPUs (Central Processing Units) verwendet. CPUs haben eine geringere Anzahl an Rechenkernen (6-12) und können ein breites Spektrum an Aufgaben bewältigen, sind aber nicht besonders gut für Aufgaben geeignet, die ein großes Maß an paralleler Verarbeitung erfordern.\\n\\nNetzwerkanforderungen Neben der Leistung der Prozessoren ist es für die Berechnung von großen KI-Foundation- Modellen von entscheidender Bedeutung, welche Verbindungstechnologie und welche Bandbreite zwischen den GPUs eingesetzt wird und mit welcher Bandbreite die einzelnen Knoten verbunden sind. Es ist zwingend notwendig, die Arbeitslast auf sehr viele GPUs zu verteilen. Dafür sind laut der befragten Expert:innen eine InfiniBand oder schnelle Ethernet Verschaltung und eine starke Bandbreite innerhalb des Clusters, aber auch zwischen Cluster und Storage notwendig. InfiniBand ist besonders gut für den Einsatz im HPC-Bereich geeignet und bis zu 10-mal leistungsfähiger als das gängige Interconnect PCIe. Außerdem sollten hier Storage-Umgebungen mit geringer Latenz eingebunden werden (Hensel & Ostler, 2020).\\n\\nGroße KI-Modelle für Deutschland\\n\\n131\\n\\nLeistungsdichten Als Leistungsdichte wird die elektrische Aufnahme der IT-Komponenten pro Server-Rack bzw. pro m² Whitespace (Flächenbedarf pro Rack) verstanden. Laut der befragten Expert:innen steigen die Leistungsdichten pro Rack im High Performance Computing Bereich stetig und liegen aktuell zwischen 20-30 kW pro Rack und vereinzelt auch höher. Die für die Berechnung von Foundation-Models benötigte Leistungsdichte liegt zwischen 30 und 45 kW (z.B. NVIDIA Superpod) pro Rack und somit im High Performance Computing-Bereich. Die Leistungsdichte hat ebenfalls Einfluss auf die Wahl der eingesetzten Kühlung der Systeme.\\n\\nKühlung Die von der Server-Hardware aufgenommene elektrische Leistung wird zu fast 100 % in Wärme umgewandelt. Je höher die Leistungsdichte eines Server-Racks, desto höher die abgegebene Wärmemenge. Computer dürfen die zugelassenen Betriebstemperaturen jedoch nicht überschreiten, um einen sicheren Betrieb zu gewährleisten. Hersteller von in der Regel die einzuhaltenden Temperaturbereiche und Luftfeuchtebedingungen vor, an die wiederum die Garantieleistungen geknüpft sind. Viele klimatechnische Vorgaben für den Betrieb von IT-Equipment in Serverräumen finden sich in den „Data Center Power Equipment Thermal Guidelines and Best Practices” des amerikanischen Verbandes ASHRAE wieder, an denen sich die IT-Hersteller ausrichten (ASHRAE, 2016). Um die Temperaturen im zulässigen Bereich zu halten, gibt es verschiedene Möglichkeiten der Kühlung, die in Rechenzentren zum Einsatz kommen. Generell wird zwischen Methoden der Luftkühlung sowie der Flüssigkeitskühlung (Wasserkühlung) unterschieden.\\n\\nfür\\n\\nsie\\n\\nIT-Equipment schreiben daher\\n\\nDa Kühlflüssigkeiten physikalisch dichtere Medien sind als Luft, können diese in der Regel auch deutlich höhere Wärmelasten abtransportieren. Aufgrund der stetig steigenden Leistungsdichten in den Racks und der damit steigenden Abwärmemenge wird es herausfordernder, Systeme besonders im HPC-Bereich effizient mit einer Luftkühlung zu kühlen. Laut der befragten Expert:innen ist die Luftkühlung eines Racks mit einer Leistungsdichte bis maximal 20 kW pro Rack umsetzbar. So entspricht 300 m³/h Luft einem Wasserdurchsatz von 90 l/h (Dürr, 2018).\\n\\nBei der von LEAM angestrebten Leistungsdichte von 36 kW pro Rack sind nur eine direct- to-chip Flüssigkeitskühlung oder flüssigkeitsgekühlte Racks geeignet. Flüssigkeiten sind ein effizienteres Wärmeübertragungsmedium als Luft und eignen sich somit besser bei hohen Leistungsdichten. Expert:innen geben jedoch an, dass bisher nur sehr wenige Rechenzentren mit einer Flüssigkeitskühlung oder hybriden Lösung aus Luft- und Flüssigkeitskühlung ausgestattet sind. Die Gebäudeinfrastruktur wird heute bei neuen Rechenzentren von Rechenzentrumsbetreibern und Collocation-Anbietern so geplant, dass die Installation einer Flüssigkeits-Kühlung möglich ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n132\\n\\nKühlmethode\\n\\nBeschreibung\\n\\nLuftkühlung\\n\\nDie gängigste Methode zur Luftkühlung ist die raumbasierte Kühlung mit Doppelboden in Kalt- und Warmgangkonfiguration. Hierbei wird die kühle Luft durch Auslassöffnungen im Doppelboden in den sog. Kaltgang vor die Racks geleitet und durch Ventilatoren der Server in den Serverschrank eingesaugt. Die erwärmte Luft wird auf der Rückseite des Racks über den Warmgang abgeleitet und einer erneuten Kühlung zugeführt. Des Weiteren finden sich auch Konzepte, bei denen auf einen Doppelboden verzichtet wird. In diesem Fall wird die kalte Luft von der Seite in den Serverraum eingeblasen und die erwärmte Abluft über eine spezielle „Doppelbodendecke” aus dem Raum abgeführt.\\n\\nDirect-to-Chip/Direct-to- Plate Flüssigkeitskühlung\\n\\nBei der Direct-to-Chip Kühlung wird eine Kühlflüssigkeit in kleinen Schläuchen oder Kupferrohren zu Kühlplatten geleitet, die sich direkt neben den zu kühlenden Komponenten (z.B. CPUs und GPUs) befinden. Die so erwärmte Flüssigkeit wird über einen Wärmetauscher und weitere nachgelagerte Kühlkreisläufe abgeführt.\\n\\nTauchkühlung/Immersion Cooling Flüssigkeitskühlung\\n\\nBeim sog. Immersion Cooling werden die elektronischen Bauteile, in der Regel die komplette Platine eines Servers, in einem Behälter mit einer speziellen elektrisch nichtleitenden Flüssigkeit eingetaucht, die das Kühlmittel darstellt. Die Flüssigkeit nimmt die Wärme auf und wird dann über einen Kühler abgeleitet, um die Wärme abzuführen.\\n\\nKühlung über Rücktüren Flüssigkeitskühlung\\n\\nWassergekühlte Rücktüren von Serverschränken bieten den Vorteil, dass konventionelle Servertechnik in den Schränken verbaut werden kann. Die von den Ventilatoren abtransportierte, erwärmte Abluft wird dabei durch Wärmetauscher in den rückseitigen Rack- Tür aufgenommen. Durch die angeschlossene Verrohrung wird die Wärme aus dem Serverraum abgeführt. Da bei dieser Technologie sowohl ein Luftstrom zum Abtransport der Wärme in die Wärmetauscher der Rücktüren als auch ein Wasserkreislauf zum weiteren Transport aus dem Serverraum zum Einsatz kommt, stellen diese Systeme quasi einen hybriden Lösungsansatz dar.\\n\\nTabelle 8: Übersicht über die Kühlmöglichkeiten in Rechenzentren\\n\\nGroße KI-Modelle für Deutschland\\n\\n133\\n\\nGebäudeinfrastruktur Die Anforderungen an die Gebäudeinfrastruktur eines KI-Rechenzentrums unterscheiden sich nicht grundlegend von den Anforderungen anderer Rechenzentren. Zu beachten ist jedoch, dass vor dem Bau eines neuen Rechenzentrums die Kühltechnologien geplant und die Bauweise des Gebäudes entsprechend angepasst wird. Die für das LEAM- Vorhaben benötigten Compute-Ressourcen und verwendeten Leistungsdichten von ca. 36 kW/Rack können nur durch eine Flüssigkeitskühlung gekühlt werden. Deshalb sollte die Gebäudeinfrastruktur für die Nutzung einer Flüssigkeitskühlung ausgestattet sein. Dies ist beim Bau eines Rechenzentrums sowie bei der Auswahl eines geeigneten Collocation/Housing Anbieters zu beachten.\\n\\nFlächenbedarf Entscheidenden Einfluss auf den benötigten Flächenbedarf hat die Leistungsdichte pro Rack. Wird bei einer vorgegebener Gesamtrechenkapazität die Leistungsdichte pro Rack erhöht, nimmt die Anzahl der benötigten Racks ab und der Flächenbedarf sinkt. Wird dagegen die Leistungsdichte pro Rack verringert, werden mehr Racks benötigt, um die angestrebte Gesamtleistung zu erreichen und somit steigt auch der Flächenbedarf. Laut der interviewten Expert:innen liegt der Flächenbedarf pro Rack (sog. Whitespace) in der Regel bei brutto 3,0-3,5 m² (die reine Standfläche eines Racks beträgt dabei ca. 1 m²). Für die von der LEAM-Initiative kalkulierten flüssigkeitsgekühlten 140 Racks (z.B. 4 NVIDIA DGX H 100 Knoten/Rack) mit einer Gesamtleistung von ca. 4,0 MW zur Berechnung und Training des KI-Foundation-Modells ergibt sich demnach ein Flächenbedarf von 345-525 m². Beim Einsatz von älteren Knoten (z.B. NVIDIA DGX A 100) in luftgekühlten Racks in ergibt sich eine höhere Anzahl von Racks und eine entsprechend größere Fläche, da weniger Knoten pro Rack verbaut werden können.\\n\\nStromversorgung Die grundsätzlichen Anforderungen von KI-Hochleistungsrechenzentren sind laut der interviewten Expert:innen mit den Anforderungen an die Stromversorgung anderer Rechenzentren gleichzusetzen. Gleichwohl zeichnen sich Höchstleistungsrechenzentren durch eine höhere Leistungsdichte (kW/Rack) und damit verbunden eine deutlich höhere Abwärmelast pro Serverraum aus. Die Stromversorgung eines Rechenzentrums besteht in der Regel aus mehreren Komponenten, die zusammenarbeiten, um sicherzustellen, dass das Rechenzentrum mit ausreichend Strom versorgt wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n134\\n\\nZu diesen Komponenten gehören in der Regel:\\n\\n1. Netzeinspeisung: Hier liegt der Übergabepunkt für den Strom, der für den Betrieb des Rechenzentrums benötigt wird. In der Regel wird der Strom aus dem bezogen. öffentlichen Kleinst- und Kleinrechenzentren werden meist über den normalen Hausanschluss mit 400 V Drehstrom versorgt. Bei größeren Abnahmemengen erfolgt die Einspeisung in der Regel mit 10 kV bzw. mit 20 kV (Mittelspannung). von MSHV\\n\\n2. Stromverteilung: Die Hauptstromversorgung wird über\\n\\nkaskadierende Abstufungen (Mittelspannungshauptverteilung) und NSHV (Niederspannungshauptverteilung) bis zu den jeweiligen Anschlusspunkten auf Netzteilebene im Rechenzentrum verteilt. Die Stromverteiler verteilen den Strom an die verschiedenen Bereiche des Rechenzentrums und sorgen dafür, dass der Strom zu den benötigten Stellen gelangt.\\n\\n3. Unterbrechungsfreie Stromversorgung (USV): Um sicherzustellen, dass das Rechenzentrum auch bei kurz- und langfristigen Stromausfällen weiter betrieben werden kann, muss nach INF.2 des BSI eine USV installiert werden. Die USV stellt über Batteriepuffer oder Schwungmassen-Systeme eine kontinuierliche Stromversorgung des Rechenzentrums sicher, bis die Hauptstromversorgung wiederhergestellt wird. Weitere Aufgaben einer USV sind das sog. Glätten von Spannungsstößen (Surge; <4ms), Abfedern von Oberschwingungen oder die galvanische Trennung des internen vom externen Stromkreislauf. Der Einsatz einer USV ist in Deutschland Pflicht.\\n\\nAuf das Thema Nachhaltigkeit in Bezug auf die Stromversorgung wird im Kapitel 8.6.3 eingegangen.\\n\\nEnergieverbrauch & Effizienzparameter Der PUE-Wert (Power Usage Effectiveness) ist eine vom Industriekonsortium The Green Grid eingeführte technische Kennzahl, welche die von der IT im Rechenzentrum verbrauchte Energie ins Verhältnis zum Gesamtenergieverbrauch setzt. In der Theorie beträgt der optimale PUE-Wert 1,0 (Gesamtstromverbrauch entspricht dem reinen IT- Verbrauch ohne sonstige energetische Aufwände wie z.B. für Kühlung oder für die USV- Verlustleistung). Der durchschnittliche PUE-Wert neu gebauter Rechenzentren betrug im Jahr 2015 unter 1,5, während der durchschnittliche PUE-Wert der luftgekühlten Bestandsrechenzentren in Deutschland im selben Jahr bei 1,8 lag. Im Jahr 2010 lag der durchschnittliche PUE-Wert der deutschen Rechenzentren noch bei 1,98 (Stobbe et al., 2015).\\n\\nEinfluss auf den PUE-Wert eines Rechenzentrums hat in erster Linie die Klimazone, in der das Rechenzentrum betrieben wird, sowie die eingesetzte Kühltechnologie, gefolgt von den energetischen Aufwänden zum Betrieb einer USV. In warmen Klimazonen (z.B. Mittelmeerraum) sind die energetischen Aufwände für die Kühlung naturgemäß höher als in kälteren Klimazonen (z.B. Skandinavien). Im Hinblick auf die Strompreisentwicklung ist die Senkung des PUE die vorrangige Möglichkeit, um die Betriebskosten zu senken (Lamonica, 2014). Rechenzentren mit einer Luftkühlung liegen durchschnittlich bei einem PUE-Wert zwischen 1,5 und 1,2 währenddessen flüssigkeitsgekühlte Rechenzentren einen\\n\\nGroße KI-Modelle für Deutschland\\n\\n135\\n\\nPUE-Wert von bis zu 1,06 erreichen können (PUE-Werte im Google-Rechenzentrum). Deutsche Rechenzentrumsbetreiber:innen bestätigten dies in der Befragung und gaben an, dass der PUE-Wert bei neuen, wassergekühlten Systemen Werte bereits bis zu 1,1 beträgt. Aktuell werde hier konzentriert die Nutzung der Abwärme optimiert, um insgesamt klimaneutral zu werden.\\n\\nDie vom Umweltbundesamt entwickelte Berechnungsmethode KPI4DCE (Key Performance Indicators for Data Center Efficiency) ist ein ganzheitlicher Ansatz für die Berechnung der Energieeffizienz von Rechenzentren, welcher auch den Lebenszyklus des IT-Equipment und der einbezieht. Diese Berechnungsmethode ist in der Theorie aussagekräftiger als der PUE-Wert allein, jedoch ist sie auch aufwendiger. Nicht allen Rechenzentren stellen die geforderten Messwerte zur Verfügung. Das Ziel ist eine automatisierte Messwertaufnahme (Schödwell et al., 2018). Auf EU-Ebene gibt es jedoch im Rahmen der Initiative Climate Neutral Data Centre Überlegungen, Rechenzentren mit diesen Kennzahlen zukünftig zu überwachen (Climate Neutral Data Centre Pact – The Green Deal Need Green Infrastructure, o.D.).\\n\\ntechnischen Versorgungsstruktur\\n\\nSkalierbarkeit / Modularer Aufbau Für die Skalierbarkeit eines KI-Hochleistungsrechenzentrums gelten die gleichen Gesetzmäßigkeiten wie für die Skalierbarkeit anderer Rechenzentren. Die Skalierbarkeit wird maßgeblich von drei Faktoren beeinflusst: ausbaubare Leerflächen, die maximale Leistungsdichte pro Rack und die maximal zugesicherte Stromleistung (im Gegensatz zum aktuellen Verbrauch). Hat ein Rechenzentrum beispielsweise die vorhandene Fläche bereits vollständig ausgeschöpft, kann die Leistungsdichte in den Racks unter Prüfung der vorhandenen Kühlung und Stromversorgung erhöht werden. Sind die zur Verfügung stehenden Serverräume bisher nicht vollständig genutzt, kann durch die Inbetriebnahme weiterer Räume die Gesamtleistung des Rechenzentrums erhöht werden. Seit 2021 ist eine leichte Abnahme der Gesamt-IT Flächen von 2,1 Mio. m² in Deutschland durch eine Erhöhung der Leistungsdichten festzustellen. Diese Konsolidierungsbestrebungen machen sich am stärksten durch die Flächenabnahmen im Bereich der traditionellen, häufig unternehmenseigenen, Rechenzentren erkennbar. Hier ist eine Migration der installierten IT-Leistung hin zu Cloud- und Edge-Betriebsmodellen erkennbar (Hintemann et al., 2022). Die IT-Fläche als Maß zur Beschreibung der Entwicklung der Rechenzentrumskapazitäten ist somit nur noch sehr bedingt aussagekräftig.\\n\\nBei entsprechender Grundstücksfläche ist laut der befragten Expert:innen auch ein modularer Rechenzentrumsaufbau denkbar. Modularer Aufbau bedeutet, dass zunächst nicht die gesamte Grundstücksfläche bebaut wird, sondern zunächst nur die Flächen, die aktuell gebraucht werden. Flächen für absehbares zukünftiges Wachstum des Rechenzentrums werden freigehalten, sofern dies in entsprechenden Bauanträgen vorgesehen ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n136\\n\\nLatenzen Der Begriff beschreibt das Zeitintervall zwischen dem Moment, in dem eine Anfrage an ein System gestellt wird, und dem Zeitpunkt, an dem die Antwort des Systems empfangen wird. Latenzen werden in Millisekunden oder Mikrosekunden gemessen. Den befragten Expert:innen zufolge sind die Latenzen für das Training eines KI- Foundation-Modells nicht von großer Bedeutung, da dies lokal auf einem geeigneten Rechencluster erfolgt. Für den Betrieb und die Entwicklung von Inference Anwendungen, die später auf Basis des Foundation-Modells entstehen, sollte ein Rechenzentrum je nach Use Case über eine Bandbreite von mindestens 100 Gbit/s verfügen und Latenzanforderungen von unter 10 ms RTD (Round Trip Delay) erfüllen.\\n\\nAnforderungen an Zertifizierungen, Datenschutz und Compliance KI-Hochleistungsrechenzentren sind in Bezug auf ihre Anforderungen im Bereich (DSGVO) mit anderen Zertifizierungen und der Datenschutzgrundverordnung laut Rechenzentren gleichzustellen. Die gängigsten Zertifizierungen sind u.a. Rechtsexpert:innen ISO 27001 (Zertifizierung auf der Basis von IT-Grundschutz), ISO 9001 (Qualitätsmanagementsystem) und (Bau und Betrieb sicherer Rechenzentren). Der Kriterienkatalog C5 des BSI beschreibt die Mindestanforderungen für sicheres Cloud Computing und muss berücksichtigt werden.\\n\\nISO EN 50600\\n\\n8.3 Nachhaltigkeitsaspekte\\n\\nAbwärme zu den wesentlichsten Die Nutzung der entstehenden Abwärme gehört in Rechenzentren und wird aufgrund verschiedener Nachhaltigkeitsaspekten Herausforderungen rege diskutiert. Bislang weitgehend ungenutztes Potenzial liegt in der Einspeisung CO2-freier Abwärme von Rechenzentren in Nah- und Fernwärmenetze. Die vorhandene Kühlungstechnologie ist dabei der ausschlaggebende Faktor, wie energieeffizient die entstandene Abwärme eines Rechenzentrums weiter genutzt werden kann. Ist eine Luftkühlung im Rechenzentrum installiert, erreicht die Abwärme laut Expert:innen Temperaturen von 30-35°C, in Spezialfällen auch bis zu 50°C. Diese Temperaturen sind jedoch zu niedrig, um die Abwärme direkt in ein Wärmenetz einspeisen zu können. Dies bedeutet, dass vor der Weiterleitung der Abwärme eine Wärmepumpe eingesetzt werden muss, um die Temperatur den Anforderungen des Wärmenetzes anzupassen. Ist eine Wasserkühlung im Rechenzentrum verbaut, kann die Abwärme mit Temperaturen von 60-70° C direkt einem Wärmenetz der vierten Generation zur Verfügung gestellt werden. Technologisch ältere Wärmenetze erfordern allerdings höhere Einspeisetemperaturen. Durch das Hochverdichten der zu niedrigen Abwärme entstehen hier zusätzlich energetische Aufwände durch den Betrieb der Wärmepumpen. Moderne Methoden der Wasserkühlung werden aufgrund ihrer Neuheit noch selten genutzt (vgl. Kühlung).\\n\\nGroße KI-Modelle für Deutschland\\n\\n137\\n\\nEine weitere Herausforderung lokale Abnahme der Abwärme über Nahwärmenetze. Auch wenn es möglich wäre, die Abwärme für die Beheizung umliegender Gebäude zu nutzen, fehlt es häufig vor Ort an Abnehmern (z.B. Wohn- und Büro- und Industriegebäuden), die genügend Abwärme aufnehmen können, sowie den politischen und regulatorischen Rahmenbedingungen (Bitkom e.V., 2022b). Es können deshalb bislang nur kleine Teile der Abwärme der Rechenzentren genutzt werden.\\n\\nist die\\n\\nStrom Ein weiterer Nachhaltigkeitsaspekt ist die Stromversorgung eines Rechenzentrums mit grünem Strom. Entscheidend ist hierbei eine konsequente und erfolgreiche Umsetzung der Energiewende, die den Ausbau und insbesondere die Verfügbarkeit von Strom aus erneuerbaren Energien in Deutschland beschleunigt (Bitkom e.V., 2022b). Grüner Strom wird aus erneuerbaren Energiequellen wie Sonne, Wind, Wasser oder Biomasse gewonnen.\\n\\nZukünftig wird das Forschungsprojekt ESCADE des Bundesministeriums für Wirtschaft und Klimaschutz (BMWK) durch weltweit führende Hard- und Software-Technologien prüfen, wie die Nachhaltigkeitsbilanz von KI-Anwendungen verbessert werden kann.\\n\\n8.4 Infrastrukturanforderungen im Detail\\n\\nDie kalkulierten Infrastrukturanforderungen zur Berechnung des KI-Foundation-Models erfolgten auf Basis der öffentlichen Informationen zu dem GPT-3 Sprachmodell von OpenAI.\\n\\nTabelle 9: Compute Anforderungen für die Berechnung eines Foundationmodells\\n\\nGroße KI-Modelle für Deutschland\\n\\n138\\n\\nNach Einschätzung der Expert:innen werden für das Training des LEAM Foundation- Modells folgende Compute-Ressourcen benötigt. Dabei wird exemplarisch für die Berechnung von einem Einsatz von 560 der leistungsfähigsten GPU-Knoten (z.B. NVIDIA DGX H 100, AMD Instinct MI200 oder Intel Ponte Vecchio Data Center GPU) ausgegangen. Für einen Trainingsdurchlauf benötigt das System 694 Stunden, insgesamt werden für das Training des Foundation-Modells vier Durchläufe und 2777 Stunden bzw. 115 Tage benötigt. Für die Installation des Systems würden 140 Racks benötigt, in denen jeweils 4 DGX-Knoten à 8 GPU verbaut sind. Pro DGX entsteht Abwärme von bis zu 9 kW pro Rack, sodass insgesamt Werte von 36 kW pro Rack erreicht werden können. Die benötigte Kühlung ist bei diesen Leistungsdichten nur durch Flüssigkeitskühlung bzw. direct-to-Chip Kühlung erreichbar. Bei der neuesten Generation von GPU-Systemen (z.B. NVIDIA DGX H100) ist die Möglichkeit zur direct-to-Chip Kühlung gegeben.\\n\\nAbb. 22: MLPerf hardware: accelarators (Zhang et al., 2022, S.18)\\n\\nDer Artificial Intelligence Index Report 2022 der Stanford University beschreibt die Entwicklung der Anzahl der genutzten GPU in den Top HPC-Systemen für das Training von Machine Learning Algorithmen und zeigt auf, dass für das Training die schnellsten KI- Algorithmen Rechencluster mit einer sehr hohen Anzahl von GPU eingesetzt werden (Zhang et al., 2022). Die maximale Zahl der genutzten GPU-Beschleuniger ist vermutlich seit der Erhebung im Januar 2021 nochmal gestiegen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n139\\n\\n8.5 Standortauswahl\\n\\nWas sind die Parameter für eine qualifizierte Auswahl eines Standortes für ein KI- Rechenzentrum? Neben der regionalen Verortung wird untersucht, was in Bezug auf die Energieversorgung sowie die Anbindung an vorhandene Infrastrukturen zu beachten ist.\\n\\n8.5.1 Vorhandene HPC-Kapazitäten in Deutschland\\n\\nForschungsbereich Das Angebot an High-Performance-Computing für die Wissenschaft wird in Deutschland entsprechend der Leistungsfähigkeit der HPC-Zentren auf den Ebenen 0-3 strukturiert. In Deutschland gibt es drei Hochleistungsrechenzentren der Ebene 1. Das Gauss Centre for Supercomputing (GCS) vereint die drei bedeutendsten Höchleistungsrechenzentren unter einem Namen. Die Gesamtleistung des Jülich Supercomputing Centre (JSC) in Nordrhein-Westfalen, dem Leibniz-Rechenzentrum (LRZ) in Garching bei München und dem Höchstleistungsrechenzentrum Stuttgart (HLRS) beträgt 130 Peta-FLOPS (Stand November 2021). Die HPC-Zentren haben unterschiedliche Ausrichtungen und können so unterschiedlichen Nutzeranforderungen gerecht werden. Außerdem können sie zusammen oder arbeitsteilig agieren.\\n\\nan Ebene Forschungseinrichtungen Nationales Hochschulen. Hochleistungsrechnen (NHR) haben sich acht der 12 universitären HPC-Zentren der Ebene 2 zusammengeschlossen. Dazu gehören:\\n\\n2\\n\\numfasst\\n\\n12\\n\\nüberregionale\\n\\nHochleistungsrechenzentren\\n\\nund\\n\\nZum\\n\\nVerbund\\n\\n\\n\\nIT-Center - RWTH Aachen\\n\\nZuse-Institut Berlin - Berlin University Alliance\\n\\nHochschulrechenzentrum (HRZ) - Technische Universität Darmstadt\\n\\nZentrum für\\n\\nInformationsdienste und Hochleistungsrechnen - Technische\\n\\nUniversität Dresden\\n\\nRegionales Rechenzentrum Erlangen - Universität Erlangen-Nürnberg\\n\\nGesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen - Universität Göttingen\\n\\nSteinbuch Centre for Computing (SCC) - Karlsruher Institut für Technologie\\n\\nPaderborn Center for Parallel Computing - Universität Paderborn\\n\\nZur Ebene 3 gehören regionale HPC-Zentren und Institutionen mit eigenen Clustern für Anwendungen, die eine geringere Leistungsfähigkeit benötigen. Exemplarisch werden hier zwei der leistungsfähigsten HPC-Zentren vorgestellt:\\n\\nGroße KI-Modelle für Deutschland\\n\\n140\\n\\nJülich Supercomputing Centre (JSC) Das Jülich Supercomputing Centre am Forschungszentrum Jülich gGmbH (JSC) ist ein Institut der Helmholtz-Gemeinschaft Deutscher Forschungszentren und ist durch den Bund (90 %) sowie das Land Nordrhein-Westfalen (NRW) (10 %) grundfinanziert. Das JSC verfügt u.a. über den Supercomputer JULES mit 86 Peta-FLOPS Leistung, einen der derzeit leistungsstärksten Rechner Europas. Der JUWELS Multi-Petaflop Supercomputer verfügt über ein Booster Modul, das mit NVIDIA Ampere GPUs ausgestattet ist und somit für die Berechnung von großen KI-Foundation-Modellen geeignet ist. Der Zugang zu HPC- Rechenressourcen erfolgt über halbjährliche Projektaufrufe, die anhand eines objektiven Peer-Review-Verfahrens ausgewählt werden. Das FZ Jülich ist eine gemeinnützige GmbH des öffentlichen Rechts und die Finanzierung erfolgt hier zu 50 % über die EU über die Organisation PRACE - Partnership for Advanced Computing in Europe und zu 50 % über das Bundesministerium für Bildung und Forschung (BMBF) und das Land NRW über den Verein Gauss Centre for Supercomputing (GCS). Für Projekte (üblicherweise Simulationen im Bereich Klimaforschung und Quantenphysik) gilt eine maximale Berechnungszeit von 24 Stunden. Nur in Ausnahmefällen und für größere Projekte sind Jobketten und Reservierung möglich. Ab 2023 wir hier mit der Installation des ersten europäischen Exascale-Rechners JUPITER begonnen, der unter anderem genutzt werden soll, um rechenintensive Machine-Learning-Algorithmen der neuesten Generation zu trainieren (Jülich Forschungszentrum, 2022). Das JSC richtet sich vornehmlich an die Wissenschaft und vergibt die Rechenzeit in einem kompetitiven Verfahren zweimal jährlich nach dem Peer-Review-Verfahren.\\n\\nDas FZ Jülich ist auch Partner des durch das Bundeswirtschaftsministerium geförderten Projekts OpenGPT-X, in dessen Rahmen ein Sprachmodell auf dem Supercomputer JUWELS trainiert wird. Aktuell nutzt das Projekt allerdings nur rund 320 GPUs. Dieser Wert wird sicherlich noch steigen, ist aber nicht mit den hier vorgeschlagenen 4480 GPUs vergleichbar.\\n\\nHöchstleistungsrechenzentrum Stuttgart (HLRS) Das Höchstleistungsrechenzentrum Stuttgart (HLRS) ist ein zentrales Institut der Universität Stuttgart, das u.a. den Supercomputer Hawk mit 26 Peta-FLOPS betreibt und seit 25 Jahren für Wissenschaft und Industrie zur Verfügung steht. Das HLRS ist Mitglied des deutschen Gauss Centre for Supercomputing (GCS), wodurch es eine teilweise Grundfinanzierung durch das Bundesministerium für Bildung und Forschung (BMBF) erhält. Der andere Teil der Grundfinanzierung wird durch das Land Baden-Württemberg bereitgestellt. Darüber hinaus finanziert sich das HLRS durch Forschungsmittel (Projektförderung) und Einnahmen aus der Nutzung der HLRS HPC-Rechenkapazitäten durch Unternehmen und die Industrie. Die Ressourcennutzung durch die Privatwirtschaft ist auf ca. 10 % der Rechenkapazität beschränkt und machte im Jahr 2021 rund 2 % der Drittmitteleinnahmen aus. Kennzeichnend für das HLRS sind die sogenannten Solution Center, die als externe Gesellschaften den Transfer in die Wissenschaft und Wirtschaft organisieren und den Zugang zu Höchstleistungsrechnern fördern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n141\\n\\nNutzung von HPC-Rechenkapazitäten aus dem Bereich der Forschung am Beispiel HLRS *:\\n\\nDas HLRS kann zurzeit maximal 192 GPUs (24 GPU-Knoten) gleichzeitig für die Berechnung eines Foundation-Modells anbieten. Hier würden dann entsprechend 16.192 Knotenstunden für einen Trainingsdurchlauf anfallen. Das entspricht in etwa 675 Tagen Dauerbetrieb von 24 KI-Knoten mit jeweils 8 GPUs.\\n\\nEine exklusive Nutzung sämtlicher GPU-Knoten in dieser Form wäre am HLRS aktuell nicht realisierbar und nicht mit den zeitlichen Anforderungen an die Innovationszyklen bei der Entwicklung eines Foundation-Modells vereinbar (vgl. 1.3 Anforderungen an ein KI-Hochleistungsrechenzentrum).\\n\\nFür die GPU/CPU Rechenleistung fallen die folgenden Kosten für die Berechnung eines Foundation-Modells an:\\n\\n1. GPU-Nutzung Die 24 KI-Knoten des HLRS benötigen ca. 23 * 694 Knotenstunden für die Berechnung (560/24= ca. 23) --> 16.193 Knotenstunden = 4 Durchläufe ergeben dann 64.772 Knotenstunden.\\n\\n64.772 KI-Knotenstunden x 19,50 EUR pro KI-Knotenstunde entsprechen einem Preis von 1.263.054 EUR\\n\\nFür das Preprocessing der Daten, das bis zu 20.000 CPU-Cores und eine geschätzte maximale Laufzeit von 1.000 Stunden in Anspruch nehmen wird, würden folgende Zielkosten für die Nutzung der CPU-Cluster am HLRS entstehen:\\n\\n2. CPU-Nutzung 20.000 CPU-Cores für 840 Stunden Laufzeit --> 168.000.000 Core-Stunden Die aktuellen HAWK-Knoten am HLRS besitzen 128 CPU-Cores. Da hier pro Knotenstunde abrechnet wird, ergeben sich:\\n\\n131.250 Knotenstunden x Forschungspreis (Stand: 2022) in Höhe von 1,13 EUR/Knotenstunde = ca. 80.000 EUR\\n\\nBei der Nutzung durch die Industrie wird am HLRS zudem ein Zuschlag in Höhe von 10% bis 30% veranschlagt.\\n\\nDie Kostenabschätzung erfolgte auf Basis einer öffentlich zugänglichen Entgeltordnung mit Stand 2022.\\n\\nAn allen untersuchten Standorten mit HPC-Rechenkapazitäten stehen die benötigten GPU-Hardwareressourcen für LEAM nicht in ausreichendem Umfang zur Verfügung, sodass diese nicht als Bereitsteller von Infrastruktur in Frage kommen. Diese kommen eher als Nutzer von zukünftig verfügbaren spezialisierten KI-Recheninfrastrukturen in Frage. Kooperationen sind hier ebenfalls denkbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n142\\n\\nKommerzielle Anbieter Neben den Forschungseinrichtungen gibt es auch kommerzielle Anbieter, die HPC- Kapazitäten am Markt bereitstellen. Von diesen sind insbesondere die sogenannten Hyperscaler relevant. Als Hyperscaler werden Unternehmen bezeichnet, die sehr große Rechenzentren betreiben und ihren Kunden Clouddienste auf Basis von hochskalierbaren Infrastrukturen zur Verfügung stellen. Rechenleistung, Speicherkapazität und andere Ressourcen können hierbei nahezu verzögerungsfrei auf Anfrage bereitgestellt werden und die Abrechnung der verwendeten Ressourcen erfolgt in der Regel nach tatsächlicher Nutzung. Hyperscaler unterscheiden sich von anderen Anbietern durch die schiere Größe der jeweiligen Rechenzentren (teilweise deutlich über 100 MW Leistung) und ihre Fähigkeit, große Mengen an Daten und Rechenleistung schnell und effizient zu verarbeiten und die Dienste weltweit, hoch skalierend anbieten zu können. Um die gestiegenen Kundenanforderungen an niedrige Latenzen bedienen zu können, betreiben sie in der Regel mindestens ein Rechenzentrum auf den relevanten Kontinenten und bieten eine garantierte Datenspeicherung und -verarbeitung über verschiedene geografische Verfügbarkeitszonen an.\\n\\nDie größten und bekanntesten Hyperscaler sind Amazon Web Services (AWS), Microsoft Azure und Google Cloud Platform (GCP) aus dem amerikanischen Raum sowie zunehmend die chinesischen Unternehmen Tencent Cloud und Alibaba. Daneben gibt es auch noch weitere große Unternehmen wie IBM, Oracle oder HPE, die HPC-Kapazitäten bereitstellen und einige kleinere, hochspezialisierte KMUs und Start-ups wie z.B. Lambda. Als deutsche Unternehmen bieten bspw. IONOS, Northern Data und auch Aleph Alpha HPC-Lösungen an.\\n\\nGenerell stellen alle diese Unternehmen spezielle Systeme zur Verfügung, die für den gedachten Einsatzzweck der Verarbeitung von KI-Anwendungen konzipiert wurden und Zugriff auf leistungsfähige GPUs ermöglichen. Unterschiede gibt es jedoch bei der verwendeten Hardware und der Bereitstellung. Bei den meisten Anbietern sind z.B. NVIDIAs A100 GPUs verfügbar. Der Einsatz der neuesten Generation der H100 GPUs wird aktuell evaluiert und soll in Kürze z.B. bei Microsoft Azure zur Verfügung stehen. Ein weiterer Unterschied ergibt sich in der Art der Bereitstellung der gewünschten Ressourcen. Als Cloudspezialisten bieten naturgemäß alle Unternehmen die Möglichkeit von virtualisierten Systemen an. Hierbei laufen auf einem Hostsystem ein oder mehrere virtualisierte Systeme. Manche Anbieter wie IBM und Microsoft Azure bieten darüber hinaus auch den Zugriff auf dedizierte Systeme an, die sich noch flexibler konfigurieren lassen. Im Fall von Microsoft Azure kann sogar ein dedizierter Supercomputer des Herstellers Cray integriert werden.\\n\\nAuch wenn somit prinzipiell der Aufbau eines geeigneten Clusters möglich wäre, stehen zumindest in Europa nicht ausreichend Ressourcen in Form von GPUs zur Verfügung. Laut Aussage der befragten Expert:innen, gibt es keinen Anbieter, der die geforderte Anzahl von ca. 4.500 GPUs der neuesten Generation aus einem Rechenzentrum heraus zur Verfügung stellen kann. Die verteilte Nutzung von Ressourcen aus mehreren Rechenzentren scheitert aktuell an der notwendigen Bandbreite des internen Netzwerks. In diesem Zusammenhang muss jedoch erwähnt werden, dass die Anbieter vermutlich langfristigen Nutzungsverträgen bereit wären, entsprechende Kapazitäten bei\\n\\nGroße KI-Modelle für Deutschland\\n\\n143\\n\\naufzubauen. Die Kosten hierfür dürften jedoch höher ausfallen als bei den anderen hier betriebswirtschaftliche aufgezeigten Vergleichsrechnung für verschiedene HPC-Angebote für den Bezug von Rechenleistung aus der Cloud erfolgt in Kapitel 10.1.\\n\\nBetriebsmodellen.\\n\\nEine\\n\\nbeispielhafte\\n\\nDie Nutzung von Kapazitäten außerhalb Europas wäre zwar denkbar, steht aber dem Ziel des Aufbaus eines deutschen bzw. europäischen KI-Ökosystems zur Entwicklung, Bereitstellung, Betriebs sowie der Integration und Validierung besonders leistungsfähiger KI-Modelle entgegen.\\n\\nEin weiterer Punkt, der gegen den Aufbau eines cloudbasierten Systems unter Verwendung eines Hyperscalers spricht, ergibt sich aus der Zielgruppe der KMU. Auch wenn das Niveau und die Standards in Bezug auf Datensicherheit bei den Hyperscalern eher über dem Durchschnitt liegen, gibt es nach wie vor Bedenken bezüglich des Schutzes vor dem Zugriff Unbefugter, insbesondere staatlicher Stellen, auf die eigenen Daten. Je sensibler die eigenen Daten eingeschätzt werden, desto größer ist die Skepsis. Dies könnte unter Umständen dazu führen, dass notwendige Trainingsdaten nicht zur Verfügung gestellt werden. Hinzu kommt, dass viele Hyperscaler so genannte Lock-in Effekte zur Kundenbindung nutzen, die einen leichten Einstieg ermöglichen und einen späteren Wechsel zu einem anderen Anbieter erschweren.\\n\\nSPOTLIGHT Merantix Momentum GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDas Leadership-Team: Dr. Johannes Otterbach, Nicole Büttner-Thiel, Dr. Thomas Wollmann.\\n\\nMerantix Momentum ist ein in Berlin ansässiges KI- Startup, das auf die Einführung und Skalierung von KI- basierten Lösungen in verschiedenen Branchen spezialisiert ist. Mit einem erfahrenen Team lösen wir als KI-Service-Anbieter die Herausforderungen unserer Kunden durch maßgeschneiderte Machine-Learning- Lösungen und sichern so deren zukünftige Wettbewerbsfähigkeit in digitalen und datengetriebenen Märkten. Mit einer eigenen Forschungsabteilung unterstützen wir gleichzeitig aktiv den Transfer von Machine-Learning-Methoden in die produktive Anwendung bei Firmen und Organisationen in Deutschland und Europa.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Merantix Momentum spezialisiert sich auf die Implementierung und Umsetzung riskanter KI-Innovationsprojekte. Dabei begleiten wir unsere Kunden und Partner von der initialen Use Case Entwicklung, über die Datenstrategie bis hin zur Entwicklung und auch dem Produktionsbetrieb der KI-Lösungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n144\\n\\nFoundation-Modelle bieten dabei den Vorteil, schnell und effizient auf limitierten Datensätzen unserer Kunden, neue Deep-Learning Lösungen zu entwickeln. Hierbei stellt es sich als vorteilhaft heraus, dass Foundation-Modelle auf einer breiten Datenbasis trainiert wurden, die mittelbar auf die Kundendaten übertragbar sind. Mit einer breiteren Verfügbarkeit verschiedener Foundation-Modelle, das heißt trainiert auf diversen Daten, lässt sich somit die Anwendung moderner KI-Methoden auf bisher unerschlossene Anwendungen realisieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? KI-Foundation-Modelle unterstützen uns bei der Projektentwicklung an mehreren Stellen. Zum einen helfen sie bei der Use Case Entwicklung, da sie ein schnelles Prototypisieren ermöglichen und damit die Kreativität unserer Kunden und Partner, aber auch unserer Entwickler:innen entfesseln. Zum anderen bieten sie aber auch die Möglichkeit, durch Distillation kleine und effiziente Modelle zu entwickeln, die durch das Trainieren eines neuen Modells von Grund auf gar nicht erst möglich gewesen wären, da die Datenlage oftmals nicht ausreichend ist. Damit schlagen KI- Foundation-Modelle gleich zweifach durch: In der Innovationsphase durch Unterstützung im Kreativprozess und in der anschließenden Entwicklung, die sonst nicht möglich wäre.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Zunächst einmal sollte klargestellt werden, dass bestimmte Biases gewollt sind. Hierbei handelt es sich um ethische und moralische Grundeinstellungen der Modelle, eben basierend auf europäischen Werten. Dies wird bedingt durch die verschiedenen Kulturkreise, in welchen die Modelle entwickelt werden. Abgesehen davon sind Europa-basierte Modelle hilfreich, um Anwendungen schneller in die Praxis bringen zu können. Der europäische Bias ermöglicht es uns, voraussichtlich schneller die Modelle anpassen zu können mit weniger Daten. Zum anderen müssen wir uns weniger mit dem Output der Modelle und deren Untersuchung auf ungewollten Output oder mögliche Schwachstellen beschäftigen, da eine gewisse minimale Operationslinie angenommen werden kann, die man bei nicht-europäischen Modellen so nicht annehmen darf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n145\\n\\n8.5.2 Erforderliche Standortparameter\\n\\nGeopolitische und datenschutzrechtliche Verortung Im europäischen Kontext stellen die sogenannten FLAP-Märkte (Frankfurt, London, Amsterdam und Paris) – in den vergangenen Jahren häufig durch Dublin als FLAP-D ergänzt – die historisch gewachsenen Gravitätszentren für die großen Internet Hubs in Europa dar. Durch diese digitalen Internetzentren verlaufen, ähnlich der Entwicklung der historischen Seidenstraße, die weltweiten Datenübertragungsleitungen für den globalen Internetverkehr. Mit Ausnahme von London liegen diese Zentren im Geltungsbereich der Europäischen Union und stellen damit einen datenschutzrechtlich sicheren EU- konformen Rechtsrahmen für ihre Nutzer:innen dar.\\n\\nHistorisch betrachtet folgt diese Entwicklung dem Aufbau der internationalen Telekommunikationshubs in der ersten Hälfte des 20. Jahrhunderts bzw. schon den Entwicklungen der ersten industriellen Revolution ab der zweiten Hälfte des 18. Jahrhunderts. Diese verdichtete Ansiedlung digitaler Infrastrukturelemente ist aus volkswirtschaftlicher Sicht im Bereich der sogenannten „Blauen Banane”, einer dicht besiedelten Kette von Ballungsräumen angefangen von Manchester, dem Großraum London über die Amsterdamer „Randstad”, das Ruhrgebiet, die Rhein-Main-Region hin zu den Industriestandorten in Mannheim, Ludwigshafen und Basel bis schlussendlich in die Industriezentren Norditaliens mit Mailand und Turin zu verorten.\\n\\nMit einer Übertragungskapazität von mehr als 50 Tbit/s zählt allein die West-Ost- Datentrasse von Dublin über Amsterdam Richtung Frankfurt zu den größten transeuropäischen Trassenführungen für das IP-Routing, entlang derer sich die großen Volumina an Datenverkehr im Internet bewegen. Dies entspricht mehr als dem fünffachen Durchsatz des heute weltweit größten Internetknotens in Frankfurt (Simons & Frese, 2021). Ein weiterer Datenkorridor (Nord-Süd) verbindet die skandinavischen Rechenzentrumsansiedlungen mit den europäischen und amerikanischen Content- Anbietern via Stockholm, Kopenhagen, Düsseldorf, Frankfurt und Paris.\\n\\nEntlang dieser Trassen hat sich in den vergangenen 20 Jahren eine digitale Ökonomie mit allen Ausprägungen der Wertschöpfungskette herausgebildet. Um diese großen Rechenzentrumsansiedlungen haben sich häufig digitale Ökosysteme aus den Bereichen Software-Entwicklung, Content, KI oder IT-Dienste angesiedelt. Den Gravitätsanker für diese Ansiedlungen bilden häufig kurze Latenzzeiten, breitbandige Anbindung an die transkontinentalen Backbone-Trassen, verdichtete Metropolstrukturen mit der entsprechenden Anzahl potenzieller Nutzer:innen sowie ein ausreichend zur Verfügung stehender Markt an gut ausgebildeten Fachkräften.\\n\\nSowohl auf europäischer als auch auf nationaler Ebene ist der Wachstumstrend im Rechenzentrumsmarkt weiterhin ungebrochen. Zwischen 2016 und 2021 wuchsen die Kapazitäten gemessen in IT-Anschlussleistung um 30 % (Hintemann et al., 2022). Die Profiteure dieses Trends sind allen voran die Rechenzentrumsanbieter mit installierten Leistungen ab 5 MW und deutlich darüber hinaus. Kleinere Installationen unter 5 MW befinden sich aktuell in einem stagnierenden oder absteigenden Trend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n146\\n\\nGemessen an der Bedeutung nimmt die Rhein-Main-Region als Rechenzentrums-Standort in Deutschland unverändert eine Spitzenposition ein. Neben Frankfurt wird künftig Berlin für Rechenzentrums-Entwicklungen eine immer stärkere Rolle spielen. Gemessen an der IT-Anschlussleistung pro Einwohner kommt Hessen auf einen mehr als dreimal höheren Wert als die Stadtstaaten Hamburg oder Berlin. Neben Berlin werden künftig auch Standorte wie München, Hamburg sowie die Regionen Köln/Düsseldorf und Leipzig/Dresden für Rechenzentrumsentwicklungen immer wichtiger (Hintemann et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n147\\n\\nIm Bereich High Performance Computing (HPC) sind in Deutschland folgende bedeutende Standorte zu nennen:\\n\\nStadt\\n\\nEinrichtung\\n\\nPeak Performance\\n\\nMemory\\n\\nEggenstein- Leopoldshafen\\n\\nFrankfurt\\n\\nOffenbach\\n\\nHamburg\\n\\nHamburg\\n\\nKaiserslautern\\n\\nKaiserslautern\\n\\nKöln\\n\\nGöttingen\\n\\nDarmstadt\\n\\nStuttgart\\n\\nAachen\\n\\nJülich\\n\\nBerlin\\n\\nGarching\\n\\nGarching\\n\\nPaderborn\\n\\nMainz\\n\\nDresden\\n\\nErlangen\\n\\nSteinbuch Centre for Computing - ForHLR - bwUniCluster Center for Scientific Computing - LOEWE CSC - FUCHS Deutscher Wetterdienst - Cray XC40 - Cray CX40 II Deutsches Elektronen Synchrotron - Standort Zeuthen - Maxwell German Climate Computing Center Mistral (HLRE-3) Competence Center HPC - Beehive - Seislab - Ocean 1 Reg. Hochschulrechenzentrum - Elwetritsch - Elwetritsch II Reg. Rechenzentrum Uni Köln - CHEOPS Ges. f. wissen. Datenverarbeitung - Scientific Compute Cluster - Emmy Hochschulrechenzentrum - Lichtenberg II Höchstleistungsrechenzentrum - Hawk - Cray CS-Storm - vulcan IT Center RWTH - CLAIX-2016 - CLAIX-2018 Jülich Supercomputing Centre - JUWELS Konrad-Zuse Zentrum - Lise Leibniz Rechenzentrum - Super MUC - Super MUC NG Max Planck Computing & Data Facility - COBRA Paderborn Center for Parallel Comp. - XCL FPGA Cluster - Noctua 1 - Noctua 2 Zentrum für Datenverarbeitung - Mogon - Clover - Mogon 2 Center for Information Services / HPC - Taurus - Alpha Centauri Erlangen National Center for HPC - Emmy - Meggie - TinyGPU - TinyVec - TinyARM - TinyFAT - Alex - Fritz\\n\\n1171 TFlop/s 444 TFlop/s\\n\\n823 TFlop/s 41 TFlop/s\\n\\n1073 TFlop/s 1073 TFlop/s\\n\\n16 TFlop/s -\\n\\n3590 TFlop/s\\n\\n67 TFlop/s 35 TFlop/s -\\n\\n134 TFlop/s\\n\\n100 TFlop/s\\n\\n2883 TFlop/s 8261 TFlop/s\\n\\n3148 TFlop/s\\n\\n26000 TFlop/s - -\\n\\n678 TFlop/s 4965 TFlop/s\\n\\n12000 TFlop/s\\n\\n7907 TFlop/s\\n\\n3580 TFlop/s\\n\\n12720 TFlop/s\\n\\n835 TFlop/s 7100 TFlop/s\\n\\n379 TFlop/s 106 TFlop/s 3125 TFlop/s\\n\\n2621 TFlop/s -\\n\\n232 TFlop/s 511 TFlop/s - - - - - -\\n\\n136 TB 86 TB\\n\\n70 TB 18 TB\\n\\n125 TB 125 TB\\n\\n402 TB\\n\\n266 TB\\n\\n14 TB 6 TB 23 TB\\n\\n17 TB 53 TB\\n\\n36 TB\\n\\n92 TB 498 TB\\n\\n251 TB\\n\\n1 TB 9 TB 102 TB\\n\\n88 TB 251 TB\\n\\n286 TB\\n\\n455 TB\\n\\n197 TB\\n\\n530 TB\\n\\n512 GB 53 TB 355 TB\\n\\n90 TB 10 TB 194 TB\\n\\n279 TB 35 TB\\n\\n36 TB 47 TB 5 TB 96 TB 128 GB 22 TB 65 TB 242 TB\\n\\nTabelle 10: HPC-Standorte in Deutschland\\n\\nGroße KI-Modelle für Deutschland\\n\\nCPU-Cores\\n\\n34800 Cores 18304 Cores\\n\\n18960 Cores 6456 Cores\\n\\n29552 Cores 29952 Cores\\n\\n2288 Cores 26732 Cores\\n\\n101196 Cores\\n\\n3224 Cores 1584 Cores 11600 Cores\\n\\n5624 Cores 10520 Cores\\n\\n9712 Cores\\n\\n16640 Cores 116152 Cores\\n\\n61824 Cores\\n\\n702896 Cores 608 Cores 13856 Cores\\n\\n16152 Cores 62736 Cores\\n\\n123088 Cores\\n\\n110016 Cores\\n\\n86016 Cores\\n\\n136960 Cores\\n\\n32 Cores 10960 Cores 143488 Cores\\n\\n35760 Cores 5120 Cores 52248 Cores\\n\\n64536 Cores 1632 Cores\\n\\n11088 Cores 14560 Cores 1392 Cores 12 Cores 64 Cores 2484 Cores 8960 Cores 67968 Cores\\n\\n148\\n\\nDie oben aufgeführten Rechenzentrumsstandorte für HPC-Anwendungen lassen sich in unmittelbarer Nähe zu Universitäten und Forschungseinrichtungen verorten und folgen nicht zwingend der vorab beschriebenen Entwicklungslogik gewerblicher Collocation und Hyperscale-Rechenzentren Im wissenschaftlichen und universitären Bereich haben sich diese Rechenzentren in der Regel über eigene Netze (z.B. das Wissenschaftsnetz X-WiN, welches vom Deutschen Forschungsnetz DFN betrieben wird) untereinander verbunden. Die Anbindung des X-WiN an externe Netze erfolgt an dedizierten Standorten über lokale Internet-Knotenpunkte, wie z.B. am DE-CIX in Frankfurt am Main und Hamburg, am ECIX in Düsseldorf und am BCIX in Berlin.\\n\\nentlang historisch\\n\\nentwickelter Datentrassen.\\n\\nRegionale Verortung: Flächenbedarf, Kubatur, Gebäudeinfrastruktur Bei der regionalen und lokalen Verortung von Rechenzentren rücken andere Kriterien in den Vordergrund als bei einer geopolitischen oder nationalen Betrachtung. Als generelle Ansiedlungskriterien von Rechenzentren wären hier zu nennen:\\n\\nDie Grundstücke liegen nicht in direkter Nachbarschaft zu oder in Wohngebieten. • Eine einfache Erreichbarkeit, u.a. durch öffentlichen Personennahverkehr oder Straßenanbindung ist gegeben.\\n\\nAusschluss von ansiedlungsbehindernden Bedrohungs-/Gefahrenlagen (siehe auch Seveso II/III-Gebiete): Nähe zu Flughäfen, chemischer Industrie, Güterverkehrsstrecken, Elektromagnetische Exposition, Schwingungsquellen etc. Aber auch mögliche Gefahrenlagen durch Naturereignisse (Hochwasser, aktive seismische Zonen, Nähe zu Küstenlinien, etc.) sind zu vermeiden.\\n\\nEs existieren Ansätze für lokale Nutzungen des Rechenzentrums, u.a. ein vielfältiges Nutzerspektrum an datenzentrierten Unternehmen und Forschungsinstitutionen; idealerweise im Umkreis von 50 km.\\n\\nEine zuverlässige und redundant ausgebaute sowie skalierbare Stromversorgung wird bereitgestellt. Eine räumliche Nähe zum Umspannwerk wird hierbei angestrebt. Bei redundanter Versorgung über zwei Umspannwerke wird häufig die geographische Mitte zwischen zwei Umspannwerken bevorzugt.\\n\\nEine gute Anbindung an überregionale Glasfasertrassen ist gegeben. Idealerweise sind mindestens zwei überregionale (sog. Longhaul) Glasfasertrassen-Anbieter vorhanden, welche in Summe drei schleifen- und kreuzungsfreie Wegeführungen mit mindestens 20 nutzbaren Glasfaserpaaren je Weg zur Trassenanbindung realisieren können; die maximale Distanz zur Trasse beträgt 5 km, bezogen auf eine potenzielle Ansiedlungsfläche.\\n\\nEs ist ein Zugang zu einem Internetknoten-Anbieter vorhanden, der mit einer diskriminierungsfreien, verteilten Plattformkonzeption eine Vielzahl von Interconnection-Diensten auf Enterprise Niveau realisieren kann.\\n\\nEin zukünftig wichtiger werdender Faktor wird die Nähe zu möglichen Abnehmern von Abwärme (z.B. Quartiers-Konzepte, Schwimmbad, Vertical Farming etc.) oder die direkte Einspeisemöglichkeit in ein Nah- oder Fernwärmenetz bilden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n149\\n\\nFlächenbedarf Für Großrechenzentren und Hyperscaler spielt eine ausbaufähige Flächen-Skalierbarkeit eine entscheidende Rolle für das jeweilige Ansiedlungskonzept. Ansiedlungen von mittelständischen Betreibern gehen mit entsprechend kleineren Grundstücksgrößen ins Rennen. Grundsätzlich zu unterscheiden sind:\\n\\n\\n\\n„Solitär”-Rechenzentrum: Bau eines einzelnen in sich geschlossenen Rechenzentrums (häufig für unternehmensinterne Verwendung). Größenordnung: 0,5 bis ca. 3 ha Grundstücksfläche, Leistungsaufnahmen in der Regel bis 10 MW\\n\\nCampus Rechenzentrum: Es stehen mehrere jeweils für sich autark betriebene Rechenzentrums-Betriebseinheiten auf einem größeren Rechenzentrumscampus. Das Betriebsmodell ist häufig auf Collocation, Shell & Core oder Hyperscale ausgerichtet. Größenordnung: 10 ha Grundstücksfläche und mehr, Leistungsklasse: 50-120 MW\\n\\nGroßer Hyperscale Campus: Ab ca. 15 ha aufwärts. Das aktuell größte Ansiedlungsprojekt in Europa befindet sich in der Nähe von Esbjerg, Dänemark und umfasst ca. 200 ha Arealfläche (die Größe Monacos); Leistungsklassen: >100 MW bis hin zu 300 MW und darüber hinaus.\\n\\nKubatur / Baukennzahlen Das Maß der baulichen Nutzung Intensität einer Grundstücksausnutzung Bestandteil des öffentlichen Baurechts und stellt somit ein wichtiges städtebauliches Steuerungsinstrument des BauGB dar. Bei der Errichtung von Rechenzentren sind aus baurechtlicher Sicht verschiedene Vorgaben einzuhalten. Unter anderem gibt die Grundflächenzahl (GRZ) vor, wie groß der Flächenanteil eines Grundstücks sein darf, der überbaut werden darf. Die Kubatur beschreibt den umbauten Raum eines Gebäudes. Analog zur Erhöhung der Leistungsdichte in einem Serverrack lässt sich die Gesamtanzahl betreibbarer Server mit ansteigender Zahl der Geschosse (Geschossflächenzahl GFZ) eines Gebäudes erhöhen. Die Baumassenzahl (BMZ) schließlich gibt an, wie viele Kubikmeter Baumasse je Quadratmeter Fläche eines Grundstücks zulässig sind.\\n\\nist als Angabe über die\\n\\nDiese Vorgaben sind im lokalen Baurecht entsprechend verankert und können vom sind mehrgeschossige Errichter nicht Rechenzentrumsbauten jedoch sind Beispiele von Industriegebieten üblich, Rechenzentren in Hochhäusern, wie die 60 Hudson Street in Manhattan oder das Royal Bank of Canada Data Centre in Toronto in Deutschland bisher nicht anzutreffen.\\n\\nverändert werden.\\n\\nIn Deutschland\\n\\nin\\n\\nEnergieversorgung: Anbindung und Energieversorgungssicherheit Rechenzentren erfordern für einen (ausfall-)sicheren Betrieb ein redundantes und durchdachtes Stromversorgungskonzept. Um die erforderliche Redundanz zu erreichen, kann, sofern möglich, auf die getrennte Einspeisung über zwei Umspannwerke zurückgegriffen werden. Ist dies nicht der Fall, empfiehlt sich eine Ringeinspeisung über zwei getrennte Zuführungen eines Umspannwerkes.\\n\\nGroße KI-Modelle für Deutschland\\n\\n150\\n\\nWird das Rechenzentrum über eine Mittelspannung (10 kV oder 20 kV) versorgt, ist in der Regel auf dem Grundstück eine kundeneigene oder energieversorgereigene Mittelspannungsstation errichtet. Transformatoreneinspeisung und Messeinrichtungen sind in der Regel in diese Anlagen integriert. Um die Mittelspannung entsprechend in Rechenzentren heruntertransformieren Gießharztransformatoren herkömmlichen Trockentransformatoren wird hierbei eine höhere Teilentladungs-, Kurzschluss- und Spannungsfestigkeit erreicht und die Empfindlichkeit gegenüber Umweltbelastungen wie Feuchtigkeit und Staub ist ebenfalls geringer (Dürr, 2018). Vereinzelt kommen auch Öltransformatoren zum Einsatz. Diese sind bauartbedingt verlustarmer und weisen eine höhere andere Brandschutzerfordernisse auf, die ggf. einen größeren Raumbedarf mit sich bringen. Eine weitere notwendige Komponente zur Sicherstellung der Energieversorgung stellen die Unterbrechungsfreien Stromversorgungen (USV) sowie die Netzersatzanlagen (NEA; z.B. Dieselgeneratoren) dar.\\n\\nzu können, werden üblicherweise\\n\\neingesetzt.\\n\\nGegenüber\\n\\nEnergieeffizienz\\n\\nauf. Öltransformatoren weisen\\n\\njedoch\\n\\nDie Aufgabe einer USV besteht vorrangig in zwei Punkten: a) der kurzfristigen Überbrückung bei Stromausfällen durch Umschaltung auf Batteriespeicher oder Schwungmassenspeicher sowie b) der Eliminierung von Spannungsschwankungen und - spitzen sowie Frequenzabweichungen. Ausgehend vom Wirkungsprinzip lassen sich dabei drei grundsätzliche Typen von USV-Anlagen unterscheiden:\\n\\nVFD (Voltage and Frequency Dependent): Der Strom wird bei diesem USV-Typ direkt vom Eingang zum Ausgang durchgeleitet. Hierbei erfolgt keine „galvanische Entkopplung” mit der vorgeschalteten Last. Über den Eingang der USV wird kontinuierlich der Batteriespeicher mit Energie versorgt. Die Umschaltung im Falle eines Stromausfalls auf Batteriebetrieb ist jedoch unterbrechungsbehaftet und kann bis zu 10 ms benötigen. Dies ist ein Wert, der sich bei empfindlicher IT- Hardware u.U. bereits bemerkbar machen kann.\\n\\nVI (Voltage Independent): USV-Geräte vom Typ VI arbeiten mit einem AC/DC- Wandler als zentrale Komponente, der sowohl als Stromrichter als auch für die Aufladung der Batterien zuständig ist. Die USV läuft im Gegensatz zum vorgenannten Typ auch dann „aktiv\\'\\' mit, wenn der Strom über die Netzspannung zur Verfügung steht. Es werden jedoch Spannungsspitzen herausgefiltert, welche die IT-Hardware schädigen könnten. Die Umschaltzeit liegt hierbei zwischen 2,5 und 10 ms.\\n\\nVFI (Voltage and Frequency Independent): Im Gegensatz zu den beiden oben aufgeführten USV-Typen stellt eine USV vom Typ VFI zusätzlich noch sicher, dass es neben Spannungsspitzen und -schwankungen nicht auch zu unerwünschten Frequenzabweichungen kommt. Der Strom an der Ausgangsseite ist hier komplett entkoppelt vom Strom an der Eingangsseite. Da die USV dieses Typs im Dauerbetrieb arbeitet, fallen keine zusätzlichen Umschaltzeiten an.\\n\\nGroße KI-Modelle für Deutschland\\n\\n151\\n\\nBei der Auslastung der USV-Systeme ist darauf zu achten, dass mit sinkendem Auslastungsgrad der Wirkungsgrad der Anlage kontinuierlich abnimmt. Die erhöhte Verlustleistung macht sich so bei der Energieeffizienz negativ bemerkbar. Die Aufgabe eines Systems aus USV und Batteriespeicher besteht jedoch immer nur darin, eine relativ kurze Zeit (wenige Minuten) zu überbrücken, bis eine Netzersatzanlage (NEA; = Notstromdiesel) ist. längerfristigen Notstromersatzanlagen sind nach ISO 8528 genormt.\\n\\nfür\\n\\nden\\n\\nNotbetrieb\\n\\nangelaufen\\n\\nDieselbetriebene Notstromersatzanlagen müssen regelmäßigen Tests zur Sicherstellung des Betriebs unterzogen werden. Hierbei werden verschiedene Testbetriebsarten unterschieden:\\n\\nNetzparalleler Lastprobebetrieb: Die NEA wird gestartet und mit der Sinuswelle der Netzversorgung synchronisiert. Im Anschluss wird der Generatorschalter eingekuppelt und die NEA parallel zum Netz betrieben. Nach vorhergehender Rücksprache mit dem Energieversorger kann sogar Last in das Netz zurückgespeist werden.\\n\\nLastprobebetrieb im Inselbetrieb: Die NEA wird im Netzparallelbetrieb wie oben beschrieben gestartet. Nach Hochlaufen der Last wird jedoch der Netzschalter ausgekoppelt, sodass die NEA nun die volle Last für den Betrieb der Server erbringen kann. Diese Methode ist der vorangestellten vorzuziehen, da sie die reellen Bedingungen im Falle eines Netzausfalles besser abbilden kann.\\n\\nNetzausfalltest („Back Building Test”): Hierbei handelt es sich um einen „echten” Netzersatztest. Der zentrale Netzschalter wird vor dem Anlaufen der NEA ausgekoppelt, so dass USV und NEA spontan einspringen müssen. Viele Betreiber schrecken vor dieser Art des Netztests zurück, da sie das Risiko zu hoch einschätzen, dass Anlagen nicht anlaufen und es zu einem „echten” Ausfall im Rechenzentrum kommen kann.\\n\\nDie Kunden im Rechenzentrum werden in der Regel über anstehende Tests von USV und NEA im Vorfeld durch den Betreiber informiert. Testläufe von Dieselaggregaten müssen (u.a. auch aus emissionsrechtlichen Erfordernissen) bei den zuständigen Behörden genehmigt werden. In der Regel wird ein Stundenkontingent pro Jahr (z.B. 30 h/a) für den Test-Betrieb genehmigt. Die Dieselgeneratoren werden im Standby-Betrieb elektrisch vorgewärmt, um im Einsatzfall möglichst kurze Anlaufzeiten bis zum Erreichen der vollen Last gewährleisten zu können. Somit verbrauchen NEAs auch offline einen gewissen Betrag elektrischer (Heiz-)Energie.\\n\\nGroße KI-Modelle für Deutschland\\n\\n152\\n\\nVerfügbarkeitsklassen Rechenzentren werden nach dem Grad vorhandener redundanter Komponenten in sogenannte Verfügbarkeitsklassen (VK1 - VK4) unterteilt. Dabei unterscheidet man:\\n\\nVerfügbarkeitsklasse 1\\n\\n(N): Bezeichnet man einzelne Komponenten eines Rechenzentrums (z.B. eine NEA, eine USV, ein Klimaschrank) mit der Variablen „N”, so liegt bei diesem Konzept keine zusätzliche Redundanz vor. Bei Ausfall einer Komponente muss diese zuerst gewartet/repariert werden, bevor diese wieder in Betrieb geht.\\n\\nVerfügbarkeitsklasse 2 (N+1): Der Ausfall einer einzelnen Komponente führt hier nicht zum Ausfall des kompletten Versorgungspfads, da eine zusätzliche Ersatzkomponente (+1) einspringen kann. Beispiel: Zur Kühlung des Serverraums werden 5 Umluftklimageräte benötigt. Das sechste, im Raum verbaute Gerät springt im Fall des Ausfalls eines anderen Geräts ein.\\n\\nVerfügbarkeitsklasse 3 (2N): Bei dieser Redundanzkonzeption sind sämtliche Versorgungspfade„doppelt” ausgelegt. Beispiel: Sämtliche Server sind mit zwei Netzteilen ausgerüstet, die über zwei verschiedene Stromphasen mit Strom versorgt (z.B. bei werden. Durch Abschaltung einer Wartungsarbeiten) ist der operative Betrieb im Serverraum weiterhin gewährleistet. kompletten Stromphase\\n\\nVerfügbarkeitsklasse 4 (2N+1): Dies stellt die höchste Verfügbarkeitsklasse dar. Im Gegensatz zur 2N Konzeption kann bei Wartungsarbeiten eines kompletten Versorgungspfades zusätzlich noch die Ausfallsicherheit einer Einzelkomponente sichergestellt werden.\\n\\nDen Verfügbarkeitsklassen sind entsprechende maximale Ausfallzeiten pro zugeordnet:\\n\\nJahr\\n\\nVerfügbar- keitsklasse\\n\\nBezeichnung\\n\\nMindestver- fügbarkeit\\n\\nMax. Ausfallzeit pro Monat\\n\\nMax. Ausfallzeit pro Jahr\\n\\nVK 0\\n\\nOhne zugesicherte Verfügbarkeit\\n\\n--\\n\\n--\\n\\n--\\n\\nVK1\\n\\nNormale Verfügbarkeit\\n\\n99,0 %\\n\\n< 8 h\\n\\n< 88 h\\n\\nVK2\\n\\nErhöhte Verfügbarkeit\\n\\n99,9 %\\n\\n<44 min\\n\\n<9 h\\n\\nVK3\\n\\nHochverfügbarkeit\\n\\n99,99 %\\n\\n<5 min\\n\\n<53 min\\n\\nVK4\\n\\nHöchstverfügbarkeit\\n\\n99,999 %\\n\\n<26 sek.\\n\\n< 6 min\\n\\nTabelle 11: Verfügbarkeitsklassen (VK1 - VK4)\\n\\nverbundenen Mit Verfügbarkeitsklasse steigen auch die Investitionskosten an. Da die zusätzlichen Komponenten oftmals nicht situativ im Falle einer Wartung oder eines Ausfalls hinzugeschaltet werden, sondern vielmehr im sogenannten Halblastparallelbetrieb (mit)laufen, sind bei höherer Redundanzauslegung auch die Effizienzwerte geringer als im Betrieb ohne Redundanzauslegung. Dies wird von den Betreibern jedoch bewusst in Kauf\\n\\neiner\\n\\nhöheren\\n\\nRedundanzauslegung\\n\\nund\\n\\nder\\n\\ndamit\\n\\nGroße KI-Modelle für Deutschland\\n\\n153\\n\\ngenommen, um einen höheren Grad an Ausfallsicherheit zu gewährleisten. Für LEAM wird für die Inference Anwendungen eine Verfügbarkeit von 99 % benötigt.\\n\\nGlasfaserversorgung: Backbone-Netze, Redundanzen, diskriminierungsfreier und Carrier-neutraler Zugang, Nähe und Zugang zu Internetaustauschknoten Ähnlich wie die Stromversorgung von der externen Übertragungsinfrastruktur des Energieversorgers über Mittelspannungs- und Niederspannungsverteilung auf dem Rechenzentrumsgelände zum Server-Netzteil geleitet wird, erfolgt auch die Datenanbindung einer vergleichbaren Verteilstruktur.\\n\\nÜbergabepunkt: Über nationale und internationale Carrier-Anbindungen wird die externe Datenanbindung zu den Rechenzentren über interne Meet-Me Räume (MMR) sichergestellt. Meet-Me Räume sind hierbei der zentrale Ort innerhalb eines Collocation-Rechenzentrums, an dem sich Telekommunikationsunternehmen und Carrier sowie die Kunden des Collocation-Betreibers physisch miteinander verbinden und Daten austauschen können. Oftmals befinden sich die Hochleistungsrouter eines dezentral aufgestellten Internetknotenpunkts in den Meet-Me Räumen der Rechenzentrumspartners („Enabled Sites”) und ermöglichen so den Zugang zu den ‘Connected Networks’ des Knotenbetreibers.\\n\\nStandortverteilung (SV): Vom Meet-Me Raum als zentralen Übergabepunkt in das externe Netz gelangen die Daten über eine Primär/Campusverkabelung zu den einzelnen Stockwerken/Serverräumen.\\n\\nGebäudeverkabelung (GV): Ggf. unterteilt sich die Netzwerkverkabelung in einzelne Stockwerke über entsprechende Etagenverteiler (vertikale Verteilung).\\n\\nTertiärverkabelung (EV): Über eine Tertiär- oder Etagenverteilung erfolgt dann die Zuleitung der Verkabelung an die Serverschränke in den einzelnen Serverräumen.\\n\\nBei der Verteilung auf Serverschrankebene lassen sich zwei Konzepte voneinander unterscheiden:\\n\\nEnd of Row (EoR): Der erste und/oder der letzte Schrank einer Rackreihe ist mit dem Zugangsswitch für die Anbindung aller anderen Schränke der jeweiligen Rackreihe ausgestattet. Der Schrank mit dem EoR Switch muss dabei eine große Anzahl von Patchkabeln für die horizontale Verkabelung über seine Patchpanels unterbringen. Der Vorteil dieser Anordnung besteht im vereinfachten Change- Management, da sämtliche Patchkabel einer Rackreihe an diesem zentralen Ort zusammenlaufen.\\n\\nDas Middle of Row Konzept (MoR) ist mit dem des EoR vergleichbar, nur dass hierbei der zentrale Schrank mit den Zugangsswitchen in der Mitte der Rackreihe positioniert ist.\\n\\nTop of Row (ToR): Bei diesem Konzept befinden sich in jedem Schrank (zumeist oben) eigene Switche. So können die Patchkabel in der Regel sehr kurzgehalten werden. Bei hoher Rack Anzahl sind jedoch viele kleinere (Edge-)Switche erforderlich. Schrankübergreifende Change-Requests sind beim ToR Konzept nicht so leicht zu realisieren. Das Konzept ist kostenintensiver, da in der Regel mehr Switche benötigt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n154\\n\\nSchutzbedarf/Risiko-Analyse Viele Rechenzentren stellen systemrelevante Dienstleistungen zur Verfügung. Collocation- Cloud- und Hosting-Rechenzentren mit einer vertraglich vereinbarten Leistung von mehr als 3,5 MW fallen unter die Verordnung zur Bestimmung Kritischer Infrastrukturen (BSI-KritisV). Unabhängig davon haben alle Rechenzentrumsbetreiber mehr oder weniger strenge Sicherheitskonzepte für ihre Anlagen vorgesehen. Hierzu zählt die Etablierung eines Zonenkonzeptes. Die Gebäudeteile und Abschnitte eines Rechenzentrums(Campus) werden entsprechend ihrer Kritikalität in verschiedene Zonen und Sicherheitsbereiche eingeteilt. Von außen nach innen lassen sich nach einem „Zwiebelschalen-Prinzip” die folgenden fünf Zonen/Bereiche unterscheiden:\\n\\nZone I: Das Grundstück oder Firmengelände ist entweder frei zugänglich oder mit einem Zaun und einer Videoüberwachungsanlage gesichert.\\n\\nZone II: Halböffentlicher Bereich innerhalb des Betriebsgelände, z.B. mit normalen Büroarbeitsplätzen für Mitarbeiter:innen\\n\\nZone II: IT-Nebenräume und administrative Steuerung. Spätestens dieser Bereich ist nur noch einem bestimmten Personenkreis vorbehalten. Oftmals existieren hier Zugangspunkte in Form von Schleusen und Personenvereinzelungsanlagen.\\n\\nZone IV: Direkter Zugang zu technischen Anlagen des Rechenzentrums. Oftmals ist der Zugang in diesem Bereich ausschließlich für das technische Wartungspersonal vorgesehen.\\n\\nZone V: Zugang zu den Serverräumen, dem eigentlichen „Herz” des\\n\\nRechenzentrums. In Hochsicherheitsrechenzentren sind die Zugänge für die Serverräume sowie die Technikräume für Klima/Strom so ausgelegt, dass sich die entsprechenden Mitarbeiter:innen in separaten Gängen bewegen und eine direkte Begegnung ausgeschlossen wird. Dieser Aspekt ist besonders wichtig für den Fall, dass externes Wartungs- und Technikpersonal zum Einsatz kommt.\\n\\nErgänzend zum Zonenkonzept werden in der DIN EN 50600-1 vier unterschiedliche Schutzklassen definiert:\\n\\nSchutzart\\n\\nArt des Zugangs\\n\\nSchutzklasse 1 Öffentlicher oder halböffentlicher Bereich.\\n\\nSchutzklasse 2\\n\\nBereich, der allen autorisierten Personen (Mitarbeiter:innen und Besucher:innen) zugänglich ist.\\n\\nSchutzklasse 3\\n\\nBereich, der festgelegten Mitarbeiter:innen und Besucher:innen vorbehalten ist. Andere Personen mit Zugang zu Schutzklasse 2 müssen von Personen begleitet werden, die Zugang zu Bereichen der Schutzklasse 3 haben.\\n\\nSchutzklasse 4\\n\\nfestgelegten Mitarbeiter:innen, die einen Bereich, der nachgewiesenen Bedarf für den Zugang haben, vorbehalten ist. Andere Personen mit Zugang zu Schutzklasse 2 und 3 müssen von Personen begleitet werden, die Zugang zu Bereichen der Schutzklasse 4 haben.\\n\\nTabelle 12: Schutzklassen nach DIN EN 50600-1\\n\\nGroße KI-Modelle für Deutschland\\n\\n155\\n\\nDie unterschiedlichen Schutzklassen 1 – 4 sind dabei durch geeignete Maßnahmen und Prozesse entsprechend abzusichern. Die wichtigsten Punkte hierbei sind u.a.:\\n\\nSchutzklasse 1:\\n\\no Identifizierbare physische Sperre an der externen (Grundstücks)Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 2 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Physische Trennung des Zugangs von Fußgängern und Fahrzeugen der Schutzklasse 1\\n\\nund 2.\\n\\no Ausgewiesene Parkplätze für nicht autorisierte Fahrzeuge (Besucher:innen).\\n\\nSchutzklasse 2:\\n\\no Identifizierbare physische Barriere an der Grenze zur Schutzklasse 2. o Türen, Fenster, Gitter müssen der Widerstandsklasse 3 (DIN EN 1627:2011) entsprechen. Fenster und Türen müssen so konstruiert sein, dass sie im geschlossenen Zustand außerhalb der Schutzklasse 2 nicht geöffnet werden können. o Physische Trennung des Zugangs von Fußgängern und Fahrzeugen der Schutzklasse 2\\n\\nund 3.\\n\\no Maßnahmen zur Erkennung und Verhinderung unerwünschten und unnötigen\\n\\nZugangs.\\n\\no Jedes Öffnen einer Notausgangstüre muss einen Alarm auslösen, der eine geeignete\\n\\nReaktion auslöst.\\n\\nSchutzklasse 3:\\n\\no Identifizierbare physische Sperre an der externen Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 4 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Begrenzungen im Bereich der Schutzklasse 3 dürfen nicht mit denen der Schutzklasse\\n\\n1 örtlich zusammen angeordnet werden.\\n\\no Begrenzungen im Bereich der Schutzklasse 3, die mit Begrenzungen von Bereichen der Schutzklasse 2 zusammen angeordnet sind, müssen der Summe des Widerstands für Schutzklasse 2 und 3 entsprechen.\\n\\no Folgende Maßnahmen müssen vorhanden sein, um folgende Ereignisse zu erkennen und zu verhindern: Unerwünschter oder unnötiger Zugang zwischen Flächen der Schutzklasse 3 und 4; nicht autorisierter Zugang von einer Fläche der Schutzklasse 3 in Schutzklasse 4; Erkennung aller Personen sowie Materialien und Gerate (z.B. IT- Equipment), die Schutzklasse 3 betreten oder verlassen.\\n\\nSchutzklasse 4\\n\\no Identifizierbare physische Sperre an der externen Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 4 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Begrenzungen im Bereich der Schutzklasse 4 dürfen nicht mit denen der Schutzklasse\\n\\n1 örtlich zusammen angeordnet werden.\\n\\no Begrenzungen im Bereich der Schutzklasse 4, die mit Begrenzungen von Bereichen geringerer Schutzklassen zusammen angeordnet sind, müssen der Summe des Widerstands für alle Schutzklassen entsprechen.\\n\\no Alle Durchbrüche der physischen Begrenzung müssen den Zugang für nicht autorisierte Personen verhindern. Darin eingeschlossen sind z.B. auch Druckentlastungsklappen für Gaslöschanlagen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n156\\n\\nZugangskontrolle Um den kontrollierten Zugang in bzw. aus dem Rechenzentrumsbereich zu gewährleisten sind entsprechenden dahinterliegenden Prozessen installiert. Grundsätzlich lassen sich Online- und Offline- sind permanent mit einer Zugangssysteme unterscheiden. Online-Anlagen Zutrittskontrollzentrale verbunden. Alle Zutrittsbewegungen werden hierbei direkt an eine zentrale Stelle gesendet. Bei Offline-Anlagen ist die Zutrittsberechtigung auf entsprechenden das Zeiterfassungsterminal oder Onlineleser tagesaktuell übertragen werden (Dürr, 2018).\\n\\nin\\n\\nden\\n\\nRechenzentren\\n\\nZugangskontrollsysteme mit\\n\\nZutrittskarten\\n\\ngespeichert\\n\\nund\\n\\nkönnen\\n\\nz.B.\\n\\nüber\\n\\nZur Identifikation und Autorisierung sind unterschiedliche Systeme im Markt erhältlich. Folgende Grundtypen können unterschieden werden:\\n\\nPhysikalische Erkennung: mittels Leser und Ausweisen oder Schlüsselanhänger.\\n\\nLogische Erkennung: mittels Eingabe von Zahlencodes. (Diese Art des Zugangs ist nicht personalisiert und birgt die Gefahr, dass Codes unberechtigt weitergegeben werden können).\\n\\nBiometrische Erkennung: Eindeutige Identifikation einer zugangsberechtigten Iris-Scan, Handrückenerkennung, Person Venenerkennung.\\n\\nZur Erhöhung der Zugangssicherheit lassen sich die beschriebenen Methoden in der Regel auch miteinander kombinieren.\\n\\n8.6 Betrieb eines KI-Rechenzentrums\\n\\nFür den Betrieb eines KI-Rechenzentrums kommen generell drei Optionen in Betracht. Die Zusammenarbeit mit einem Collocation-Anbieter, der Aufbau eines eigenen HPC- Rechenzentrums sowie die Nutzung einer verteilten Infrastruktur. Die drei Optionen werden im folgenden näher beleuchtet.\\n\\nCollocation Ein Collocation-Betreiber ist ein Unternehmen, das Räume und Infrastrukturen für die Unterbringung von Rechenzentren und anderen IT-Anlagen bereitstellt. Collocation- Betreiber bieten ihren Kunden die Möglichkeit, ihre Rechenzentren und IT-Anlagen in Räumen unterzubringen, die speziell für den Betrieb von Rechenzentren ausgestattet sind (vgl. Tabelle 6). Diese Räume sind in der Regel mit Stromversorgungssystemen, Sicherheitsmaßnahmen Kühlungstechnologien, IT-Anlagen ausgestattet, um ordnungsgemäß betrieben werden können. Collocation-Betreiber bieten ihren Kunden auch Dienstleistungen wie Wartung und Support für IT-Anlagen an. Sie tragen somit dazu bei, dass Unternehmen ihre IT-Anlagen sicher und zuverlässig betreiben können, ohne sich um die notwendige Infrastruktur und die Wartung der Geräte zu kümmern. Der Normalfall ist allerdings, dass die Wartung und Installation der Racks durch den Kunden oder von in den Räumen des Collocation-Anbieters durchgeführt wird.\\n\\nNetzwerkverbindungen\\n\\nund\\n\\nsicherzustellen, dass die Rechenzentren und\\n\\nihm beauftragten Personal\\n\\nGroße KI-Modelle für Deutschland\\n\\n157\\n\\nBei der Auswahl eines Collocation Anbieters spielt auch die Erfüllung von Anforderungen an Kühlungssysteme eine wichtige Rolle. Weitere Kriterien sollten der Bezug von grünem Strom sowie ein schlüssiges Konzept zur Abwärmenutzung sein. Der Collocation Anbieter sollte demnach über entsprechende Infrastruktur zur Wasserkühlung verfügen. Die Anforderungen für das Training von Foundation-Modellen wurden bereits im Kapitel 2.2 erläutert.\\n\\n[GSI - Helmholtzzentrum für Schwerionenforschung]\\n\\nDas GSI Helmholtzzentrum für Schwerionenforschung in Darmstadt betreibt eine der weltweit führenden Teilchenbeschleunigeranlagen für die Forschung und den Green IT-Cube, ein fortschrittliches Rechenzentrum mit einer Kapazität von bis zu 12 MW. Dieses verfügt über hohe CPU Compute-Kapazitäten von mehr als 300.000 Cores und 400 GPUs. Es fungiert als Testrechenzentrum und verfügt auf sechs Etagen über eine Fläche von 4.645 m² und bietet Platz für 768 19” Racks à 2,2 Meter Höhe (4 MW und 256 Racks in der ersten Ausbaustufe). Dank eines speziellen Kühlsystems ist der Green Cube besonders energie- und kosteneffizient. Die Power Usage Effectiveness (PUE) beträgt hier weniger als 1,07 und wird über eine passive sowie Wärmetauscher- Wasserkühlung Verdunstungskühltürme erreicht. Dadurch, dass keine Raumluftkühlung nötig ist, sind hier hohe räumliche Leistungsdichten möglich, die für HPC-Systeme mit vielen GPU-Knoten benötigt werden. In einem KI-Cluster sind üblicherweise bis zu 4 NVIDIA DGX H100 Systeme pro Rack verbaut. Die entstehende Abwärme von 36 kW/Rack kann nach Aussage der Expert:innen des GSI mit dem Kühlsystem des Green Cube bewältigt werden.\\n\\nin den Rücktüren der Racks\\n\\nDer Green Cube verfügt über eine N+1 Redundanz und bietet die Möglichkeit der Bereitstellung von Rackspace und Dienstleistungen im Rechenzentrum (Collocation). Laut dem Betreiber könnten in den geplanten Ausbaustufen ausreichende Flächen für einen HPC-Supercomputer mit 4 MW Leistung und entsprechender Kühlung (Wasserkühlung im Rack und perspektivisch auch direct- to-Chip Kühlung) im Rahmen eines Collocation Modells bereitgestellt werden. Als zusätzliche Möglichkeit könnten hier im Rahmen des Testbetriebs des HPC- Clusters wertvolle Daten und wissenschaftliche Erkenntnisse zum Betrieb eines solchen großen, GPU basierten HPC-Systems gesammelt und veröffentlicht werden. Der Green Cube bezieht zu 100 % grünen Strom und wurde, neben anderen Preisen, 2020 von der Bundesregierung mit dem Umweltzeichen Blauer Engel ausgezeichnet. Die Abwärme der Racks wird zum Beheizen des Büro- und Kantinengebäudes auf dem Campus verwendet. Die technische Umsetzbarkeit eines Collocationbetriebs des LEAM KI-Rechenzentrums im Green Cube wurde vom Betreiber GSI bereits bestätigt.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n158\\n\\nDer europäische Collocation Markt lag 2021 bei einem Umsatzvolumen von ca. 10,7 Mrd. US-Dollar und wird von einer großen Anzahl global operierender Anbieter beherrscht, die europaweit bzw. weltweit eine Vielzahl von Rechenzentrumsstandorten unterhalten. Für den Zeitraum von 2021 bis 2027 wird aktuell ein jährliches Marktwachstum (CAGR) von 14.8 % prognostiziert (Research and Markets ltd, 2021). So hat sich z.B. in der Rhein-Main Region die Leistung der jährlich neu hinzugekommenen Collocation-Rechenzentren von 2020 mit 62 MW Leistung im Jahr 2021 auf 139 MW Leistung mehr als verdoppelt (Hintemann et al., 2021).\\n\\nZu den führenden Unternehmen in dieser Branche zählen u.a.:\\n\\nDigital Realty / Interxion unterhält an weltweit über 300 Standorten Collocation- Rechenzentren. In Europa betreibt Digital Realty in 15 Metropolregionen, verteilt auf 13 Länder insgesamt 114 Rechenzentren\\n\\nEquinix betreibt weltweit 240 Rechenzentren in 31 Ländern auf 6 Kontinenten. Insgesamt unterhält das Unternehmen 2,6 Mio. m² Fläche weltweit. Die durchschnittliche Verfügbarkeit der Rechenzentren liegt bei 99.9999 %.\\n\\nNTT Global Data Center unterhält 600.000 m² RZ-Fläche in über 20 Ländern mit einer IT-Leistung von 1.500 MW. In Europa ist das Unternehmen in 15 Metropolregionen mit Collocation-Dienstleistungen tätig.\\n\\nCyxtera betreibt mehr als 60 Rechenzentren in über 30 Märkten. In Europa ist das Unternehmen an den Standorten London, Amsterdam und Frankfurt tätig. • Cyrus One unterhält in Nordamerika und Europa 50 Collocation Standorte. In\\n\\nEuropa ist das Unternehmen in Deutschland, UK, Irland, den Niederlanden sowie Spanien mit 14 Collocation Rechenzentren vertreten.\\n\\nKDDI / Telehouse betreibt über 45 Collocation Rechenzentren weltweit. In Europa ist das Unternehmen an den Standorten London, Paris und Frankfurt mit 9 Rechenzentren präsent.\\n\\nVantage Data Centers betreibt auf 5 Kontinenten 24 Campus-Standorte. In Europa ist das Unternehmen an den Standorten Berlin, Frankfurt, Mailand, Warschau und Zürich und Cardiff mit 8 Lokationen vertreten.\\n\\nIron Mountain unterhält auf drei Kontinenten an insgesamt 21 Standorten über 370.000 m² Brutto RZ-Fläche. In Europa ist das Unternehmen an den Standorten Amsterdam, Frankfurt, London und Madrid präsent.\\n\\nGlobal Switch betreibt 13 Rechenzentren auf 2 Kontinenten. In Europa werden an den Standorten London, Amsterdam, Frankfurt, Paris und Madrid insgesamt 9 Rechenzentren betrieben.\\n\\nCOLT Data Center Services betreibt in Asien (Tokyo, Osaka und Mumbai) sowie in Europa (Frankfurt, London, Paris, Rotterdam) 14 Rechenzentrumsstandorte.\\n\\nPenta Infra betreibt Rechenzentren in den Niederlanden, Dänemark und Deutschland. In Deutschland ist das Unternehmen u.a. mit Rechenzentren in Berlin, Hamburg, Düsseldorf, Köln und Leipzig präsent.\\n\\nNorth C Datacenters betreibt Rechenzentren an insgesamt 14 Standorten in Deutschland, der Schweiz sowie in den Niederlanden. In Deutschland ist das Unternehmen in Nürnberg und München vertreten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n159\\n\\nZu den vorrangig im deutschsprachigen Raum tätigen Collocation-Betreibern zählen weiterhin (Auswahl):\\n\\nnoris network AG mit sieben Rechenzentren an fünf Standorten in Nürnberg, München und Hof.\\n\\nStackIT (Schwarz IT) mit zwei Standorten in Ellhofen sowie Ostermiething (AT). • Data Center One mit Standorten in Düsseldorf, Leverkusen und Stuttgart. • MyLoc Managed IT mit 3.500 m² Fläche an sechs Standorten in Düsseldorf. • Plusserver betreibt in Köln, Düsseldorf und Hamburg eigene Rechenzentren. • ScaleUp Technologies betreibt insgesamt sieben Hochleistungsrechenzentren an den Standorten Hamburg, Berlin und Düsseldorf.\\n\\nMaincubes One betreibt neben einem niederländischen Standort in Amsterdam in Deutschland drei Standorte in Frankfurt sowie einen weiteren Standort in Berlin. • Akquinet betreibt vier Rechenzentren an den Standorten Hamburg, Norderstedt und Itzehoe.\\n\\nCollocationIX betreibt am Standort Bremen ein Hochsicherheits-Collocation- Rechenzentrum.\\n\\nGrass Merkur betreibt am Standort Hannover 3.500 m² RZ-Fläche\\n\\nDarüber hinaus bieten viele Internet Services Provider ebenfalls Collocation Services neben ihrem klassischen IPS-Portfolio an (u.a. M-net, Pfalzkom, Telemaxx, NetCologne, Dokom, EnviaTel).\\n\\nDie Kosten liegen laut der befragten Expert:innen für Collocation Angebote in der geplanten Größenordnung zwischen 100-120 EUR/KW/Monat. Hinzu kommt der Strom, der im Beispiel mit einem Preis von 20 Cent/kWh berechnet wird. Dies entspricht der von LEAM benötigten Größenordnung von ca. 4 MW geschätzten monatlichen Kosten von ca. 400.000 bis 500.000 EUR für den laufenden Betrieb.\\n\\nBau und Betrieb eines eigenen HPC-Rechenzentrums Eine weitere Option ist der Bau und Betrieb eines eigenen HPC-Rechenzentrums mit entsprechender Gebäude-Infrastruktur. Die Kostenstruktur für den Bau und den Betrieb von Rechenzentren richtet sich vorrangig nach der erforderlichen Verfügbarkeitsklasse. Grundstückskosten stellen oftmals eine untergeordnete Rolle in der Total Cost of Ownership (TCO)-Betrachtung dar. Die folgende Beispielrechnung ist als grober Richtwert zu verstehen und kann aufgrund lokaler baulicher Gegebenheiten sowie besonderer technischer Erfordernisse abweichen:\\n\\nGroße KI-Modelle für Deutschland\\n\\n160\\n\\nAbb. 23: Beispielrechnung Bau und Betrieb eines eigenen HPC-Rechenzentrums\\n\\nFür die Inbetriebnahme eines Rechenzentrums mit eigener Gebäudeinfrastruktur rechnen die befragten Expert:innen mit zwei bis drei Jahren, je nachdem wie lange die baurechtlichen Genehmigungsprozesse dauern, die je nach zuständiger Kommune stark abweichen können. Die hier durchgeführte Betrachtung soll exemplarisch die verschiedenen Abhängigkeiten aufzeigen und eine Orientierung zur Planung eines Rechenzentrums geben.\\n\\nGroße KI-Modelle für Deutschland\\n\\n161\\n\\nNutzung einer verteilten Infrastruktur Fraglich ist, ob verteiltes Rechnen eines großen KI-Foundation-Modell mit mehreren zusammengeschalteten HPC-Rechenzentren möglich ist. Grundsätzlich ist verteiltes Rechnen bei der Erstellung eines großen KI-Foundation-Modells ein möglicher Ansatz, der aber noch weitgehend unerprobt ist. Außerdem stellt verteiltes Rechnen erhöhte Anforderungen an Infrastruktur, Netzwerk, Latenz (Ausfallsicherheit) und Sicherheit. Erste Studien zum Thema „Decentralized Training of Foundation-Models in Heterogeneous Environments\" kommen von der Stanford Universität (B. Yuan et al., 2022). Allerdings sind diese Systeme noch sehr neu und noch nicht ausreichend in der Praxis getestet. Somit würde ein solches verteiltes Rechenmodell für LEAM mit sehr hohem Risiko einhergehen, da beim Berechnen eines Foundation-Modells der Trainingsdurchlauf nicht unterbrochen werden sollte. Insbesondere scheitert das Rechnen auf verteilter Infrastruktur häufig noch an der nötigen Bandbreite bei der Vernetzung der einzelnen GPU-Knoten, die untereinander und mit dem Storage mit einer Bandbreite von bis zu 900 Gb/s verschaltet werden.\\n\\n8.6.1 Strompreisentwicklung und Vertragsgestaltung\\n\\nDie Stromkosten machen laut Béla Waldhauser CEO von Telehouse Deutschland GmbH sowie CEO von KDDI Deutschland GmbH mittlerweile in Deutschland 50 % der Kosten für Kunden der Rechenzentren aus (Weidmann & Krüger, 2020). Rechenzentren kaufen je nach Größe den notwendigen Strom entweder vom lokalen Energieversorger ein oder direkt an der Energiebörse. Hier sind die Preise aktuell volatil und betragen 0,20 EUR/KWh für den Normalbetrieb bis zu 0,50 EUR/KWh für den Einkauf bei Spitzenlasten. Im Vergleich dazu liegen die Strompreise für Anbieter in Norwegen, Finnland und Schweden bei ca. 0,05-0,10 EUR/KWh. Außerdem ergeben sich hier durch die niedrigeren nordischen Temperaturen weitere Vorteile im Bereich der Kühlung der Rechenzentren. Schwankungen des Strompreises können erheblichen Einfluss auf die Wirtschaftlichkeit des Geschäftsmodells haben. Dies wird auch durch die Ergebnisse einer Umfrage des Borderstep Instituts bestätigt, in dem die Befragten die Entwicklung des Strompreises als größtes Risiko für den Rechenzentrumsmarkt einstufen (Hintemann et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n162\\n\\nAbb. 24: Delphi-Befragung: Wie beurteilen Sie folgende Risiken für die Entwicklung des Rechenzentrumsmarktes in Deutschland? (Hintemann et al., 2022, S. 37)\\n\\nBei der Vertragsgestaltung zwischen Rechenzentren und ihren Kunden werden üblicherweise unterschiedliche Fristen für eine Strompreisbindung vereinbart. Dabei sichert der Rechenzentrumsbetreiber seinen Collocation-Kunden innerhalb der Laufzeit die Abgabe von Strom zu einem festen Kostensatz zu. Da der Rechenzentrumsbetreiber diesen Strom am Markt zu schwankenden Preisen einkaufen muss, stellen größere Schwankungen ein Risiko dar und gefährden unter Umständen die Wirtschaftlichkeit des Geschäftsmodells des Anbieters.\\n\\n8.6.2 Verfügbarkeiten und Beschaffungszeitraum der erforderlichen IT-\\n\\nRessourcen\\n\\nLaut der befragten Expert:innen liegen die Lieferzeiten beispielsweise für NVIDIA DGX H 100 Systeme momentan bei unter sechs Monaten bei größeren Systemen. Der Hersteller NVIDIA sieht Lieferengpässen zurzeit eher bei den Netzwerkkomponenten, die ca. drei bis sechs Monate betragen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n163\\n\\n8.6.3 Aspekte der Nachhaltigkeit\\n\\nDiese Studie soll zur strategischen Ausrichtung der Planung nicht zuletzt auch die Nachhaltigkeit von Rechenzentren anhand des Beispiels einer wichtigen Initiative differenzierter thematisieren. Im Rahmen des Climate Neutral Data Center Pact setzen sich im Rahmen einer Selbstregulierungsinitiative zahlreiche Verbände und ca. 80 Unternehmen auf europäischer Ebene für die Schaffung von Standards im Bereich des nachhaltigen Betriebs von Rechenzentren ein (Climate Neutral Data Centre Pact – The Green Deal Need Green Infrastructure, o.D.). Im Rahmen einer Selbstverpflichtung haben sich die Mitglieder auf folgende Ziele geeinigt:\\n\\nEnergie Effizienz\\n\\nBis 2025 sollen Rechenzentren mit 50 KW und mehr Energiebedarf in kühlen Klimazonen unter Volllast einen PUE-Wert von 1,3 erreichen. Rechenzentren haben in wärmeren Zonen einen Wert von 1,4. Außerdem sollen neue Effizienz-Metriken für Rechenzentren entwickelt werden.\\n\\nGrüne Energie\\n\\nDie Rechenzentren werden ihren Strombedarf künftig durch den Einkauf von grünem Strom decken. 75 % CO²-neutrale oder erneuerbare Energie bis Ende 2025; 100 % bis Ende 2030.\\n\\nWassernutzung\\n\\nBis 2025 werden sich alle Rechenzentren, die in kühlen Klimazonen in Gebieten mit Wassermangel unter voller Auslastung betrieben werden und Trinkwasser für die Kühlung nutzen, einen maximalen WUE (Water Usage Effectiveness) Wert von 0,4 L/kWh erfüllen. Existierende Rechenzentren, die neue Kühlsysteme installieren, werden bis 2040 die angestrebten WUE-Werte erfüllen.\\n\\nKreislaufwirtschaft\\n\\nDie Wiederverwendung, Reparatur und das Recycling von Servern, elektrischen Geräten und anderen elektrischen Komponenten hat für die Betreiber von Rechenzentren Priorität. Hier werden hohe Standards an das Recycling gesetzt und angestrebt, zukünftig 100 % des eingesetzten Server-Equipments wiederzuverwerten.\\n\\nAbwärmenutzung\\n\\nDie Rechenzentren planen den Ausbau der Einspeisung/Abgabe von Abwärme in die allgemeinen Energieversorgernetze und an andere Abnehmer voranzutreiben. Dies soll möglichst umweltfreundlich und kosteneffizient erfolgen.\\n\\nUm die Entwicklung der Nachhaltigkeit von Rechenzentren weiter voranzutreiben und gleichzeitig Kosten zu senken, beobachtet und evaluiert die Rechenzentrumsbranche momentan außerdem die Themenfelder Refurbished IT, Remanufacturing und Re-Use sowie die Verwendung CO²-armer Baustoffe oder die Wiederverwendung von Beton (Bitkom e.V., 2022b).\\n\\nGroße KI-Modelle für Deutschland\\n\\n164\\n\\n8.7 Zusammenfassung und Empfehlung\\n\\nZusammenfassend lässt sich feststellen, dass die zur Berechnung von großen Sprachmodellen benötigten HPC-Ressourcen in Deutschland/Europa derzeit kurzfristig nicht verfügbar sind. Die hohen Anforderungen an die GPU-Zahlen (ca. 4500) und die entsprechende schnelle Vernetzung der einzelnen GPUs untereinander können Stand heute nicht gewährleistet werden oder würden Berechnungszeiten erfordern, die die leistungsfähigsten deutschen HPC-Zentren für fast zwei Jahre komplett auslasten würden und außerdem die benötigten kurzen Innovationszyklen nicht gewährleisten könnten. Die einschlägigen Cloud-Dienste aus den USA und China können teilweise ausreichende KI- Rechenkapazitäten bereitstellen. Diese lassen sich aber nur schwerlich unter Wahrung der digitalen Souveränität und der europäischen Anforderungen an den Datenschutz nutzen. Zudem werden hier meist so genannte Lock-in Effekte wirksam, die einen späteren Wechsel zu einem anderen Anbieter erschweren.\\n\\nAufgrund der vielen genannten Faktoren kann eine Standortempfehlung nur auf den konkreten Use Case bezogen gegeben werden. Diese Einzelfallbetrachtung kann im Rahmen der Studie nicht geleistet werden. Gespräche mit Vertreter:innen verschiedener Bundesländer sowie Regionalinitiativen haben aber gezeigt, dass es in verschiedenen Bundesländern eine generelle Bereitschaft für den Aufbau eines Rechenzentrums gibt.\\n\\nDer Bau eines eigenen Rechenzentrums wäre generell deutlich teurer und würde einem schnellen Start der Entwicklungsaktivitäten entgegenwirken. Einschließlich der erforderlichen Planungs-, Genehmigungs- und Errichtungsphasen würden bis zu drei Jahre vergehen, bis ein entsprechendes KI-Rechenzentrum den operativen Betrieb aufnehmen könnte. Dies ist wegen des bereits beschriebenen Handlungsbedarfs zu lange. Von der Errichtung eines KI-Rechenzentrums mit eigener baulicher Infrastruktur für die sollte deshalb aus Zeit- und Kostengründen abgesehen werden.\\n\\nEine kurzfristig realisierbare Möglichkeit für den Betrieb eigener Rechenkapazitäten besteht im Rahmen eines Collocation Modells. Wie im Beispiel GSI Helmholtzzentrum für Schwerionenforschung aufgezeigt wurde, gibt es bereits Anbieter, die kurzfristig mit grünem Strom und nachhaltiger Abwärmenutzung betriebene Kapazitäten anbieten, die Anforderungen an die benötigte Wasserkühlung erfüllen und Skalierbarkeit ermöglichen. Die Angebote von Betreibern aus Norwegen, Finnland, Schweden und Island bieten hier zusätzlich über den Strompreis einen Betriebskostenvorteil wegen der niedrigeren durchschnittlichen insbesondere datenschutzrechtlich orientieren sich diese Länder zudem an den europäischen Datenschutzstandards (Schweden und Finnland sind EU-Mitgliedstaaten, Island und Norwegen sind Teil des Europäische Wirtschaftsraums (EWR)) und bekommen deshalb eine Empfehlung von den Autor:innen. Für die Inference-Anwendungen können je nach isländische und Use Case schwedische Anbieter wegen der großen geografischen Entfernungen ggf. nicht erfüllen können.\\n\\nAußentemperaturen.\\n\\nJuristisch\\n\\nund\\n\\njedoch Latenzen benötigt werden, die norwegische,\\n\\nGroße KI-Modelle für Deutschland\\n\\n165\\n\\nEine weitere Möglichkeit ist die Nutzung von neu zu schaffenden HPC-Kapazitäten, die von einem Anbieter, beispielsweise nach GPU-Stunden abgerechnet, bereitgestellt werden könnten. Auf diese Möglichkeit wird in Kapitel 10 im Rahmen der betriebswirtschaftlichen Betrachtungen näher eingegangen.\\n\\nNach Einschätzung der befragten Expert:innen evaluieren die einschlägigen, hier bereits genannten europäischen und internationalen Rechenzentren und Collocation Anbieter bereits größere Investitionen in HPC-Infrastrukturen in Europa, speziell in konzentrierte KI-geeignete Kapazitäten mit sehr großen Anzahlen von bis zu 20000 GPU. Hier werden parallel verschiedene Kooperationsmöglichkeiten evaluiert. Das Projekt Open GPT-X erforscht bereits heute die Entwicklung großer Sprachmodelle in Zusammenarbeit mit dem Jülich Supercomputing Centre (JSC). LEAM wird diese Ergebnisse im Rahmen der engen Zusammenarbeit mit dem Projekt in seine weitere Planung der Infrastruktur einfließen lassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n166\\n\\nDie organisatorische Struktur von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n167\\n\\n9. Die organisatorische Struktur von LEAM Die vorangegangenen Kapitel haben gezeigt, dass der Aufbau eines KI- Hochleistungsrechenzentrums entscheidend für den Technologie- und Industriestandort ist die Frage der Deutschland wirtschaftlichen Machbarkeit eines solchen Zentrums. Die wirtschaftliche Machbarkeit wird in dem folgenden Kapitel näher betrachtet. Auf Basis der Interviews mit Rechenzentrumsbetreibern und der Wirtschaft haben die Autor:innen die (LKS) entwickelt. Welche Organisationsstruktur konkret hinter der Idee des LKS steckt wird im Folgenden genauer erklärt.\\n\\nIdee eines LEAM KI-Servicezentrum\\n\\n9.1 Zielgruppen des LEAM KI-Servicezentrums\\n\\nDas in dieser Studie diskutierte LEAM-KI-Servicezentrum hat zum Ziel, an europäischen Werten orientierte KI-Foundation-Modelle zu entwickeln und insbesondere für die etablierte Industrie und junge Technologieunternehmen nutzbar zu machen. Dabei verfolgt das LKS zwei Zielsetzungen:\\n\\n(1) Kapazitäten des KI-Hochleistungsrechenzentrums bereitzustellen, die von\\n\\nWirtschaft und Wissenschaft für das Training eigener Modelle zur Verfügung stehen.\\n\\n(2) KI-Foundation-Modelle Open Source anzubieten, die mit entsprechend Beratungs- und Serviceleistungen auf die individuellen Bedürfnisse von Industrieunternehmen angepasst werden können (Tuning).\\n\\nDie Kapazitäten und Services sollen vier Zielgruppen angeboten werden. Die Zielgruppen sind:\\n\\nWirtschaft: Unter die Zielgruppe Wirtschaft fallen privatwirtschaftliche Organisationen, die die Services der LKS für die Entwicklung von Modellen, Anwendungen oder Produkten in Anspruch nehmen.\\n\\nKI-Start-ups: Wie bei der Zielgruppe Wirtschaftliche Anwendungen handelt es sich hier um privatwirtschaftliche Organisationen. Sie unterscheiden sich aber von ersterer in zwei Punkten: (1) KI-Startups arbeiten überwiegend an Künstlicher Intelligenz und\\n\\n(2) es handelt sich um junge Unternehmen.\\n\\nStart-ups sollten über spezielle Förderprojekte gezielt unterstützt werden. Ein Beispiel ist ein KI-Compute-Voucher, der Startups Zugang zu den Services des LKAS gewährleisten soll.\\n\\nGroße KI-Modelle für Deutschland\\n\\n168\\n\\nPublic: Öffentliche Institutionen, Behörden, Ministerien und Dienste können die Services des LKS nutzen, um die Verwaltung zu optimieren oder spezifische sicherheitsrelevante Insights auf Basis von großen Datenmengen zu erlangen. Für die Nutzung der LKS Services durch öffentliche Institutionen sind besondere Anforderungen hinsichtlich Datensicherheit, Datenschutz und allgemeine Richtlinien zur Verschwiegenheit zu erfüllen.\\n\\nWissenschaft: Unter die Zielgruppe Forschung und Entwicklung fallen Hochschulen, außeruniversitäre Forschungsinstitute und staatliche Forschungseinrichtungen, die die Services der LKS für die Forschung in Anspruch nehmen und vor allem die Erstellung von Foundation-Modellen unterstützen.\\n\\n9.2 Organisationseinheiten des LEAM KI-Servicezentrums\\n\\nDas Organisationseinheiten strukturiert (Abb. 25).\\n\\nLEAM KI-Servicezentrum\\n\\n(LKS)\\n\\nist\\n\\nin horizontalen und\\n\\nvertikalen\\n\\nDie horizontalen Organisationseinheiten bilden die infrastrukturelle und kapazitive Grundlage des LEAM KI-Servicezentrums. Sie stellen den Betrieb der Infrastruktur und die Bereitstellung der Kernservices sicher. Hierzu gehören:\\n\\nHousing • Infrastruktur-as-a-Service • Training-as-a-Service\\n\\nhorizontalen Die Organisationseinheiten auf und gliedern sich in die vier verschiedenen Kernservices für Kunden und Nutzer:innen. Hierzu gehören:\\n\\nvertikalen\\n\\nOrganisationseinheiten\\n\\nbauen\\n\\nauf\\n\\nden\\n\\nKI-Foundation-Model Development • KI-Model Tuning • Inference • Consulting\\n\\nDaneben „Koordination” als Managementeinheit verantwortlich für den Aufbau des LKS, hält engen Kontakt zum LEAM Board (s. Kapitel 9.3) und weiteren Stakeholdern aus Politik, Wissenschaft und Wirtschaft. Sie kann – je nach gewählter Gesellschaftsform – auch die Aufgaben eines gesellschaftsrechtlich erforderlichen Organs übernehmen, beispielsweise der Geschäftsführung oder des Vorstands.\\n\\nist die Organisationseinheit\\n\\nDie einzelnen Organisationseinheiten können innerhalb einer Gesellschaft abgebildet werden oder entsprechend eines Governance-Konzepts in unterschiedliche juristische Einheiten aufgeteilt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n169\\n\\nAbb. 25: Organisationseinheiten des LEAM-KI-Servicezentrums\\n\\n9.2.1 Koordination\\n\\nDie OE (Organisationseinheit) Koordination dient als Managementeinheit für das gesamte LKS. Sie stellt den kontinuierlichen Aufbau und Betrieb des LKS sicher und erweitert es innerhalb Deutschlands und der EU.\\n\\nDie OE ist eine Anlaufstelle für interessierte Personen aus Wirtschaft und Wissenschaft und hält den Kontakt zur Politik. Sie koordiniert die Interessen der Stakeholder:innen, etabliert und steuert die horizontalen Organisationseinheiten, erstellt Marktstudien und akquiriert Förder- sowie Investitionsmittel. Als steuernde Einheit des LKS unterstützt sie die anderen Services und übernimmt die Vermarktung des LKS.\\n\\nDieser Bereich sollte unabhängig vom gewählten Organisationsszenario zusammen mit dem Bereich Consulting initial aufgebaut werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n170\\n\\n9.2.2 Housing\\n\\nDie OE Housing stellt die Basisinfrastruktur für den Betrieb des Rechenzentrums zur Verfügung. Hierzu gehören bspw. die Gebäudeinfrastruktur, die Strom- und Telekommunikations-Versorgung sowie ein Kühlungskonzept. Das Kapitel 8 fasst die besonderen Anforderungen für die Ausstattung zusammen.\\n\\nDa die Investitionen in die erforderliche Infrastruktur hoch sind und der Neuaufbau eines Rechenzentrums mehrere Jahre dauert, sollte zum jetzigen Zeitpunkt von einem Neubau abgesehen werden. Stattdessen bieten verschiedene Organisationen Housing Services an, die eingekauft oder angemietet werden können. Für den Aufbau des Rechenzentrums muss ein geeigneter Housing Partner gefunden werden.\\n\\nGovernance:\\n\\nAußerhalb LEAM\\n\\nOrganisatorische Schnittstellen:\\n\\nKoordination; Service\\n\\nInfrastruktur-as-a-\\n\\nKosten Collocation HW:\\n\\n1.344.000 EUR per annum\\n\\nTabelle 13: Übersicht über die OE Housing\\n\\n9.2.3 Infrastruktur-as-a-Service (IaaS)\\n\\nZentraler Bestandteil der OE IaaS ist der Aufbau und Betrieb eines KI-Supercomputers. Der Rechner benötigt zum Betrieb die Services der Housing-Infrastruktur. Die Anschaffung des Rechners ist einer der größten Posten des Gesamtbudgets und liegt im dreistelligen Millionen Euro Bereich (s. Kapitel 10). Nähere Infos zu den technischen Anforderungen und weiteren Aspekten des Rechners finden sich in Kapitel 8.\\n\\nFür die organisatorische Einordnung der OE IaaS ergeben sich grundsätzlich zwei Szenarien: Einerseits ist die Anschaffung und Betrieb des Rechners durch das LKS denkbar, andererseits kann die Infrastruktur des LKS auch als Service durch ein externes Unternehmen bereitgestellt werden.\\n\\nAufgrund der hohen Relevanz des gewählten Szenarios für Investitions- und Betriebskosten des LKS sowie organisatorischen Gegebenheiten werden diese beiden Szenarien im Folgenden näher betrachtet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n171\\n\\n9.2.4 IaaS innerhalb LEAM\\n\\nIm ersten Szenario ist das LKS verantwortlich für die Anschaffung des Computers sowie den Betrieb und die Bereitstellung der Infrastruktur als Service. Dies birgt die folgenden Vor- und Nachteile.\\n\\nVORTEILE EINER EIGENEN INFRASTRUKTUR\\n\\nUmfangreiche Kontrolle und Gestaltungsmöglichkeiten Bei der eigenen Anschaffung eines KI-Supercomputers können einzelne Komponenten, Bauteile sowie zugehörige Frameworks und Software selbstständig ausgewählt werden. Das macht eine flexiblere Auslegung und Ausrichtung auf die zukünftige Nutzung des Rechenzentrums möglich. Unterstützung eines eigenständigen Geschäftsmodells Die Investition in einen eigenen KI-Supercomputer macht es möglich, diesen nach der Abschreibungsdauer weiter zu nutzen. Die Abhängigkeit von eventuellen Kostenschwankungen und fremden Dienstleistern begrenzt sich dabei auf die zum Betrieb notwendigen Kosten. Keine Datenweitergabe an Drittanbieter Bei der Nutzung eines eigenen Rechenzentrums werden keine Daten über Nutzung etc. an Dritte weitergegeben. Das vereinfacht eventuelle Fragestellungen in Bezug auf Dritte (z.B. Sicherheitsdienste). Ebenso können auf diese Weise Anforderungen der Cybersicherheit (beispielsweise BSI-Anforderungen), deren Erfüllung für die Teilnahme an bestimmten öffentlichen Förderprogrammen oder öffentlichen Auftragsausschreibungen nachgewiesen werden müssen, bei der Nutzung eines eigenen Rechenzentrums besser nachgewiesen werden. Insbesondere bei einer staatlichen Finanzierung ist zudem Folgendes zu bedenken: Der Einkauf von Leistungen eines externen KI-Supercomputers bedarf einer öffentlichen Ausschreibung. Ein entsprechender Vertrag über den Einkauf kann zwar für einen längeren Zeitraum ausgestaltet sein, muss aber periodisch neu ausgeschrieben werden. Das kann die Investitionsbereitschaft eines externen Dienstleisters einschränken.\\n\\nNACHTEILE EINER EIGENEN INFRASTRUKTUR\\n\\nHohe Investitionskosten Die Anschaffung eines KI-Supercomputers ist mit einer sehr hohen initialen Investition verbunden. Als Abschreibungsdauer wird ein Zeitraum von vier Jahren angenommen. Komplexe Finanzierung Mit der Finanzierung und dem Aufbau des KI-Supercomputers ergeben sich komplexe Fragestellungen bezüglich der Finanzierung, der Besitzverhältnisse sowie den Nutzungsrechten. Aufbau einer eigenen Betriebseinheit Als Betreiber des Rechenzentrums ergeben sich hohe Kosten für Personal und Software. Die Verwaltung und Instandhaltung eines KI-Supercomputers ist komplex und muss durch entsprechendes Personal rund um die Uhr überwacht werden. Darüber hinaus muss man in der Lage sein, eine adäquate IT-Sicherheit herzustellen. Das bedeutet einen erheblichen administrativen Aufwand sowie hohe Kosten. Vor allem bei Förderprojekten oder Aufträgen aus der öffentlichen Verwaltung können zusätzliche Anforderungen und damit verbundene Aufwände beim Nachweis von Cybersicherheitsstandards entstehen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n172\\n\\n9.2.5 IaaS über einen externen Partner\\n\\nIn diesem Szenario kauft das LKS die Rechenkapazitäten von einem externen Partner ein. Der Partner übernimmt dabei die komplette Installation und den Betrieb der Compute Infrastruktur und nutzt seine Erfahrung im Bereich des Hostings aus bestehenden Geschäftsmodellen.\\n\\nVORTEILE EXTERNER INFRASTRUKTUR\\n\\nGeringe Investitionskosten für eigenes Personal, Hardware und Software Bei der Bereitstellung eines KI-Supercomputers über einen externen Partner ist dieser auch für die Verfügbarkeit und Funktionalität der Anlage verantwortlich. und Daher Softwarekosten, lediglich Unterstützungsfunktionen müssen übernommen werden. Flexible Skalierbarkeit des KI-Supercomputers Entsprechend der aktuellen Verfügbarkeit und des Bedarfs können die Kapazitäten, wenn vereinbart, flexibel dazu gebucht oder abgewählt werden. Das macht eine einfachere Anpassung auf den momentanen Bedarf möglich und fängt eventuelle Spitzen ab. Zugriff auf fachliches Know-How und etablierte Prozesse Expert:innen eines externen Partners können gegebenenfalls mittels fachlicher Expertise bei Fragestellungen unterstützen und die Nutzung der Services für Kunden so vereinfachen. Weiter kann bei der Abrechnung und dem Betrieb auf etablierte Prozesse und Erfahrungen - bspw. im Bereich IT-Sicherheitsstandards - des Drittanbieters zurückgegriffen werden.\\n\\nentstehen\\n\\nvergleichsweise\\n\\ngeringe\\n\\nPersonal-, Hardware-\\n\\nNACHTEILE EXTERNER INFRASTRUKTUR\\n\\nWeitergabe unternehmensbezogener Daten an Dritte Bei der Ausführung von Services auf der Infrastruktur eines externen Anbieters werden evtl. Nutzungsdaten an diese weitergegeben. Dabei müssen Abwägungen über die Datensicherheit getroffen und Bestimmungen zur Weitergabe von Daten genau geprüft werden. Abhängigkeit von externen Anbietern Die Nutzung von Infrastruktur externer Anbieter steht und fällt mit der Verfügbarkeit von Kapazitäten dieses Anbieters. Wenn diese, sei es auch nur kurzfristig, nicht gegeben ist, muss mit Umsatzeinbußen und Compliance-Schwierigkeiten gerechnet werden. Um dies zu umgehen, müssen externe Anbieter die Verfügbarkeit von Kapazitäten garantieren können und mit dem Kunden vertraglich vereinbaren. Rollierende, marktabhängige Kosten Infrastruktur eines externen Anbieters fallen Die Kosten zur Nutzung der kontinuierlich an und sind höher als die Betriebskosten eines eigenen Rechenzentrums. Die Kosten können, je nach vertraglichen Regelungen, aufgrund von Marktbewegungen variieren. Gefahr durch Übernahme oder Insolvenz Die Infrastruktur des externen Anbieters steht unter dessen Kontrolle und könnte entweder in ein anderes Land verlagert werden, was sich möglicherweise nachteilig auf Datensicherheitsaspekte auswirkt, oder aufgekauft werden. Ebenfalls besteht die Möglichkeit, dass die Infrastruktur bspw. nach einer Insolvenz nicht mehr zur Verfügung steht\\n\\nGroße KI-Modelle für Deutschland\\n\\n173\\n\\nEmpfehlung Die befragten Expert:innen aus Industrie und Wissenschaft halten es für erstrebenswert, eine eigene, leistungsstarke und flexibel nutzbare Supercomputing-Infrastruktur aufzubauen, da die bisher verfügbare private und wissenschaftliche Infrastruktur diese Kriterien nicht erfüllt. Dies haben die Interviews im Rahmen dieser Machbarkeitsstudie, aber auch die Empfehlungen von wissenschaftlichen Initiativen, bspw. des European Language Equality Projekts, deutlich gemacht. 19\\n\\nEine Entscheidungsgrundlage über die Finanzierung der Infrastruktur (eigene Hardware oder externe Infrastruktur) muss auch unter Berücksichtigung der hohen Investitionskosten weiter analysiert und vorbereitet werden.\\n\\n9.2.6 Training-as-a-Service\\n\\nAuf Basis der IaaS und damit bereitgestellten Compute-Kapazitäten werden Prozesse etabliert, um den Nutzer:innen und Kunden von LEAM umfangreiche Services für höher liegende Ebenen bereitzustellen. So ermöglicht das LKS potentiellen Nutzer:innen, die Infrastruktur zum Training ihrer Modelle zu nutzen.\\n\\nFür das Training von KI-Foundation-Modellen sind umfangreiche Maßnahmen zum Aufbau und Betrieb der entsprechenden Prozesse und Software-Infrastrukturen erforderlich (s. Kapitel 7).\\n\\nDie Aktivitäten können auf unterschiedlichen Ebenen (Layern) kategorisiert werden. Zu den Aktivitäten gehören u.a.:\\n\\nSystem Layer\\n\\nManagement der KI-Beschleuniger ○ Bereitstellung von (Open Source) Software und Softwarebibliotheken\\n\\nData Layer\\n\\nSicherstellung von effizienten und stress-resilienten Datenspeicher- und Laderoutinen\\n\\nBeachtung von gesetzlichen und regulatorischen Anforderungen an Datenschutz, Datenqualität und Datensicherheit\\n\\nTraining and Application Layer\\n\\nAufbau eines effektiven Optimierungssystems zur optimalen Auslastung der GPUs während des Trainingsprozesses\\n\\nAufbau eines Systems zum Management der Trainingsjobs ○ Implementation von Evaluations- und Benchmarkingprozessen\\n\\n19 “Current LT research also requires flexible access to High Performance Computing (HPC) facilities in the form of clusters of high capacity GPUs. There are many EU initiatives offerring HPC: EuroHPC JU, PRACE, national computing facilities, etc. However, it is unclear if these initiatives are ready to provide the computing support that the European LT research community currently needs for developing state-of-the-art language models for all languages, domains, tasks and modalities.” (ELE Consortium 2022, S. 23).\\n\\nGroße KI-Modelle für Deutschland\\n\\n174\\n\\nFramework & Service Layer\\n\\nNormalisierung der Trainingsumgebung durch Bereitstellung von Container- Umgebungen\\n\\nImplementierung von benutzerfreundlichen Interfaces für Entwickler:innen und Administrator:innen\\n\\nUm diesen Service anbieten zu können, muss ein fachkundiges Team bestehend aus Data Science, Data Engineering, Machine Learning Engineering und DevOps Expert:innen aufgebaut werden. Dieses wird unter dem Team Services vereint und bildet Schnittstellen zum Team Consulting.\\n\\nGovernance:\\n\\nInnerhalb LEAM\\n\\nOrganisatorische Schnittstellen:\\n\\nKoordination; Housing, Training-as-a- Service, externe Kunden\\n\\nKosten Büroräumlichkeiten:\\n\\n180.000 EUR per annum\\n\\nKosten Team Services:\\n\\n3.000.000 EUR per annum\\n\\nTabelle 14: Übersicht über die Training-as-a-Service\\n\\n9.2.7 KI-Foundation-Model Development\\n\\nDer Service KI-Foundation-Model Development ist der Kernservice des LKS. Unter diesem Service werden alle Aktivitäten zusammengefasst, die direkt mit der Entwicklung neuer KI-Foundation-Modelle zusammenhängen.\\n\\nLEAM wird den Service KI-Foundation-Model Development privatwirtschaftlichen, öffentlichen und wissenschaftlichen Einrichtungen anbieten. Es ist jedoch davon Investitionskosten das auszugehen, dass aufgrund der hohen Komplexität und grundlegende KI-Foundation-Modell Development überwiegend von Forschung- und Kooperationsprojekten aus Forschung und Wirtschaft genutzt wird. Die Interviews mit der Forschung haben gezeigt, dass ein großes Interesse darin besteht, neue und konkurrenzfähige KI-Foundation-Modelle zu entwickeln. Darüber hinaus werden sie den Service nutzen, um wissenschaftliche Fragestellungen zu beantworten.\\n\\nSollte die LKS (teil-)öffentlich finanziert werden, müssen mindestens 80 % der Modelle Open Source verfügbar gemacht werden. Maximal 20 % können von Unternehmen mit Exklusivrechten genutzt werden. Prinzipiell eine Chance, denn unsere Befragung von KI- Unternehmen hat gezeigt, dass der Mangel von Open Source KI-Foundation-Modellen eines der Haupthindernisse für KI-Unternehmen ist, diese produktiv nutzen zu können. Zusammen mit der Verfügbarkeit von Daten und hohen Kosten wurde dies von 58 % der befragten Unternehmen als Hindernis angegeben.\\n\\nForschungsprojekte können sich dann über ein noch zu definierendes Verfahren auf Rechenzeit bewerben. Dabei soll auf die Erfahrungen der im Gauss Centre for Supercomputing organisierten Rechenzentren zurückgegriffen werden. Um allerdings den Besonderheiten von LEAM gerecht zu werden, sollten einige Punkte beachtet werden:\\n\\nGroße KI-Modelle für Deutschland\\n\\n175\\n\\n\\uf0fc Interessierten Projekten soll durchgehend die Möglichkeit gegeben werden, sich auf Projekte zu bewerben. Dies bietet Forschungsprojekten die notwendige Flexibilität, um in der schnelllebigen KI-Forschung zeitschonend zu forschen. Darüber hinaus ist dies eine Möglichkeit, Spitzen in der Nutzung der Infrastruktur abzuschwächen, da nicht alle Projekte zur gleichen Zeit mit der Berechnung ihrer Modelle starten.\\n\\n\\uf0fc Die Bewerbungs- und Bewertungsverfahren sollen so einfach und flexibel wie\\n\\nmöglich gestaltet werden. Ein häufiger Kritikpunkt an der aktuellen HPC-Landschaft ist, dass die Antragsphase zu lange dauert und wichtige Ressourcen bindet.\\n\\n\\uf0fc Anwendungsbezogenen Forschungsprojekten soll ein Vorrang vor\\n\\nGrundlagenforschung gegeben werden.\\n\\nIm Bereich KI-Foundation-Model Development werden folgende Services angeboten:\\n\\nErstellung und Bereitstellung allgemeiner Trainingsdatensätze: Ein allgemeiner Trainingsdaten-Pool wird aufgebaut, entsprechend der Datenschutzvorgaben und Qualitätskriterien gepflegt und interessierten Organisationen zur Verfügung gestellt.\\n\\nBereitstellung von Basis-Algorithmen: In einem Repository werden Code-Basen existierender (Open Source) Programme, erforderliche Hilfs-Tools und weitere Frameworks zur Verfügung gestellt.\\n\\nVerwaltung und Bereitstellung von trainierten Foundation-Modellen: Die trainierten Foundation-Modelle werden zur weiteren Nutzung in einem Repository abgelegt und verwaltet.\\n\\nUm interessierten Organisationen den bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellung von Mitarbeiter:innen: Für die Begleitung und die Überwachung des Trainings werden Mitarbeiter:innen eingestellt und entsprechend ausgebildet.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Training von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es wird ein Abrechnungsmodell für das Training der Modelle entwickelt. Hierbei erfolgt eine Orientierung an bestehenden Services im HPC-Bereich oder im kommerziellen Cloud-Services Umfeld.\\n\\nVerwaltung des Trainingsdaten-Pools: Trainingsdatensätze werden in Repositorys gesammelt und Nutzer:innen zur Verfügung gestellt, um damit KI-Foundation- Modelle zu entwickeln. Mitarbeiter:innen des LKS unterstützen bei der Sammlung und Pflege von relevanten Datensätzen.\\n\\nSupport von Frameworks: Für das Training von KI-Foundation-Modellen werden relevante Frameworks in Repositorys gesammelt und den Nutzer:innen zur Verfügung gestellt. LEAM-Mitarbeiter:innen unterstützen die Nutzer:innen bei der\\n\\nAnwendung dieser im Zusammenhang mit dem KI-Supercomputer.\\n\\nGroße KI-Modelle für Deutschland\\n\\n176\\n\\n9.2.8 Model Tuning\\n\\nNeben der Entwicklung von KI-Foundation-Modellen werden Ressourcen und Infrastruktur für das Tuning von Modellen bereitgestellt. Dies ist nötig, um die allgemeinen KI-Foundation-Modelle um domänenspezifisches Wissen zu ergänzen und so für konkrete Anwendungen zu nutzen und zu optimieren.\\n\\nDer größte Teil der befragten Unternehmen ist an konkreten Anwendungen auf Basis von KI-Foundation-Modellen interessiert. 51 % der befragten KI-Unternehmen arbeiten bereits mit KI-Foundation-Modellen und 18 % planen die Nutzung von KI-Foundation- Modellen in der Zukunft (s. Kapitel 4).\\n\\nBereits vorhandene Modelle sollen durch Model-Tuning erweitert und für spezifische Zwecke nutzbar gemacht werden. Aktuell tun dies nur 27 % der befragten Unternehmen, die sich mit Foundation-Modellen auseinandersetzen. Somit ist es absehbar, dass die Nachfrage an Model-Tuning zeitnah steigen wird. LEAM kann dies der deutschen Industrie substantiell vereinfachen, indem es Expertise, Modelle, Daten und Infrastruktur bündelt und es Unternehmen erlaubt, Model-Tuning ohne großen Mehraufwand zu betreiben. Ein besonderer Fokus sollte hier darauf liegen, Start-ups aus dem Bereich KI, die ihre Modelle für Anwendungen in der Industrie weiterentwickeln möchten, Rechenkapazität zur Verfügung zu stellen.\\n\\nDaneben sind auch wissenschaftliche Institute daran interessiert, den Model-Tuning- Service zu nutzen. Entsprechend ist auch für die Wissenschaft die Kombination an Expertise, Modellen und Daten, die das LKS bietet, von Interesse.\\n\\nIm Bereich Tuning sollen folgende Services angeboten werden:\\n\\nTuning-as-a-Service: Kunden können das Tuning von Modellen beim Rechenzentrum in Auftrag geben. In Kooperation mit den Kunden passen die Mitarbeiter:innen des Rechenzentrums die Foundation-Modelle an.\\n\\nBeratung und fachliche Unterstützung bei der Auswahl von Modellen, Daten und Algorithmen.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellung von Mitarbeiter:innen: Zur Beratung und Unterstützung der Nutzer:innen in der Bedienung der Infrastruktur und zur Auswahl des Modells sowie der Datensätze stellt die LKS Mitarbeiter:innen ein.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Tuning von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es muss ein Abrechnungsmodell für das Tuning der Modelle entwickelt werden. Hierbei bietet sich beispielsweise ein Modell GPU/Stunde an.\\n\\nGroße KI-Modelle für Deutschland\\n\\n177\\n\\n\\n\\nVerwaltung des Trainingsdaten-Pools: Trainingsdatensätze werden in Reopsitorys gesammelt und Nutzer:innen zur Verfügung gestellt, um damit KI-Foundation- Modelle zu entwickeln. Mitarbeiter:innen von LEAM sammeln relevante Datensätze und halten diese instand.\\n\\nSPOTLIGHT SAP SE An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie SAP SE ist ein börsennotierter internationaler Softwarekonzern mit Sitz in Walldorf, Baden- Württemberg. Als ein Marktführer für Geschäftssoftware unterstützt SAP Unternehmen jeder Größe und Branche dabei, ihre Ziele bestmöglich zu erreichen: SAP-Kunden generieren 87 % des gesamten weltweiten Handels.\\n\\nDr. Feiyu Xu, Vizepräsidentin und Global Head of AI, SAP\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use- Case? Foundation-Modelle werden eine sehr wichtige Rolle in der Enterprise AI spielen und zu disruptiven Innovationen im Prozessmanagement führen. Dazu gehören neue Anwendungen für business process mining, business\\n\\nprocess prediction, business process composition und Prozessoptimierung.\\n\\nDaneben werden die angepassten großen Sprachmodelle auch zur besseren Mensch- Maschine-Interaktion via Chatbot oder Digital-Assistenten beitragen und auch die semantische Verarbeitung der Business-Dokumente, insbesondere Informationsextraktion und Entity-Linking, erheblich verbessern.\\n\\nEin weiteres Anwendungsfeld ist die Generierung von Programmcode durch die Foundation-Modelle. Das steigert die Effizienz der Programmierer. Weil sich dann auch die Anwender neue Werkzeuge oder Erweiterungen von Softwareprogrammen von der KI erzeugen lassen können, führt das auch zu einer Demokratisierung der Softwareentwicklung.\\n\\nDie potentiellen Use Cases kann man nach den jeweils benötigten Datenstrukturen ihrer I/Os klassifizieren:\\n\\n1. Text2Text oder Speech2Speech:\\n\\nChatbots und Digitale Assistenten sind wichtige Anwendungen für SAP, durch die Benutzer:innen, z.B. Angestellte einer Kunden-Firma, natürlichsprachliche Fragen stellen können und Antworten über Fakten oder Transaktionen erhalten. Digitalassistenten lassen sich für Kundendienste einsetzen.\\n\\nÜbersetzung und Lokalisierung der Software, Dienste und Business-Dokumente • Zusammenfassungen von Textdokumenten oder Meeting-Transkripten\\n\\nGroße KI-Modelle für Deutschland\\n\\n178\\n\\n2. Text2Prozess und Prozess2Text\\n\\nDie Integration der BPMN (Business Prozess Modelling Notation) in die Sprachmodelle ermöglicht die natürlichsprachliche Anfragen für die Generierung, Validierung und Ausführung der Business-Prozesse. In der anderen Richtung können zu bereits bestehenden Prozessmodellen textuelle Prozessbeschreibungen generiert werden, die z.B. für Schulungen oder Zertifizierungen benötigt werden.\\n\\n3. Prozess2Prozess\\n\\nHierzu gehören Prozesskomposition, Prozessmodifikation, Prozessvalidierung und Prozessoptimierung.\\n\\n4. Text2Code: low-code/no-code\\n\\nDie automatische Generierung von Programmcode steigert die Effizienz und Produktivität der Programmierung\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle führen einerseits zu großen inkrementellen Verbesserungen, da bessere Ergebnisse mit deutlich weniger (bis gar keinen) Trainingsdaten erzielt werden. Sie haben aber auch das Potential, neue und modularisierte End-to-End Geschäftsprozesse zu ermöglichen und so die Gesamtheit der Enterprise Resource Planning Landschaft zu revolutionieren. Wir geben ein erstes Beispiel aus dem Bereich Businessdokumentverarbeitung. Die Anpassung bestehender Deep Learning Modelle erfordert momentan einen erheblichen Aufwand. Kunden müssen für ihre spezifischen Dokumentformate große Mengen an Trainingsdaten bereitstellen. Auch die Lokalisierung der Modelle in weiteren Sprachen ist ohne Trainingsdaten nicht machbar. KI-Foundation-Modelle haben das Potential, den Aufwand in beiden Bereichen erheblich zu reduzieren. Durch Foundation-Modelle können wir eine neue Art von Angebot an Kunden machen: Verarbeitung ihrer spezifischen Formate in den für sie relevanten Sprachen, mit wenig Trainingsdaten oder sogar out-of- the-box.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Europa benötigt eine eigene KI-Computing-Infrastruktur, um nicht in Abhängigkeit zu geraten. Zudem ist der Zugang zu Daten, inklusive mehrsprachiger Inhalte und explizit kodiertem Wissen, essenziell. KI in Europa kann nur langfristig erfolgreich sein, wenn es uns gelingt, KI-Talente auszubilden und bei uns zu halten. Eine wichtige Rahmenbedingung für Europa ist außerdem eine KI-freundliche Policy.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Die Sprachabdeckung ist für uns essentiell, ebenso wie Transparenz zu verwendeten Daten. Nötig ist eine Infrastruktur, mit der KI-Modelle unter Gewährleistung der Transparenz und unter Einhaltung europäischer Datenschutzstandards generiert und Open Source bereitgestellt werden können. Bias ist in vielen Geschäftsbereichen ein kritisches Thema, etwa im Personalwesen und bei der Verarbeitung von Bewerbungen. Hier und auch in anderen KI-Anwendungen können KI-Modelle nur eingesetzt werden, wenn sie zu entsprechenden Regulierungen wie dem AI Act der Europäischen Union konform sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n179\\n\\n9.2.9 Inference\\n\\nDas LEAM-KI-Servicezentrum soll auch dafür genutzt werden, die Modelle für potentielle Kunden bereitzustellen. Der Service wird zu marktüblichen Preisen angeboten, rund 5-10 % der Compute-Infrastruktur sollen für das Bereitstellen der Modelle genutzt werden.\\n\\nUnternehmen sind vor allem am produktiven Einsatz von Anwendungen, die auf KI- Foundation-Modellen basieren, interessiert (s. Kapitel 4). Insbesondere Unternehmen, die keine eigenständige KI-Fachabteilung vorweisen und daher keine eigenen Modelle entwickeln bzw. anpassen können, werden auf diese Möglichkeit der Nutzung zurückgreifen. Da aktuell nur 50 % der befragten Unternehmen, die KI einsetzen, auch Foundation-Modelle nutzen, besteht hier eine relevante Zielgruppe für diesen Service. Für die Wissenschaft, die grundlegende Fragen zu KI-Foundation-Modellen beantworten möchte, ist dieser Service von geringerer Bedeutung.\\n\\nKI-Unternehmen können auf Basis der Inference-Services eigene KI-Produkte und Anwendung entwickeln und anbieten und diese ihren Kunden z.B. über eine API und spezifische Abrechnungsmodelle zur Verfügung stellen.\\n\\nIm Bereich Inference werden verschiedene Services angeboten:\\n\\nHosting-as-a-Service: Kunden können das Bereitstellen von Anwendungen beim Rechenzentrum in Auftrag geben. Rund 5 bis 10 % der gesamten Infrastruktur werden für diesen Service reserviert. Mitarbeiter:innen des Rechenzentrums unterstützen und koordinieren die Vorhaben.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellen von Mitarbeiter:innen: Für die Einrichtung und den Betrieb von Inference-APIs stellt die LKS Mitarbeiter:innen ein.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Tuning von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an die Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es muss ein Abrechnungsmodell für die Bereitstellung der Modelle entwickelt werden. Hierbei bietet eine Berechnungsmethode des Entgelts basierend auf der Menge an genutzten Tokens (einzelne Anfragen an das Modell) an.\\n\\nSchulung und Training von Nutzer:innen: Um die Infrastruktur nutzen zu können, müssen potentielle Nutzer:innen geschult werden. Dafür muss ein Training vorbereitet und angeboten werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n180\\n\\n9.2.10 Consulting\\n\\nIm Bereich Consulting erfolgt eine Beratung, die Kunden aus den verschiedenen Zielgruppen bei der Entwicklung, Optimierung sowie Implementierung von KI- Anwendungen unterstützt. Dieser Service erfolgt unabhängig von der Bereitstellung von Rechenzentrums-Infrastruktur-Leistungen.\\n\\nZielgruppe für Consulting Services sind alle wissenschaftlichen Institutionen, Start-ups und Unternehmen, die eigene Foundation-Modelle entwickeln oder existierende Foundation-Modelle für ihre spezifischen Anforderungen anpassen wollen.\\n\\nEine enge Zusammenarbeit mit Beratungs-Unternehmen aus der Wirtschaft und deren Befähigung, im Umfeld von KI-Foundation-Modellen Dienstleistungen anzubieten, gehört ebenfalls zu den Aktivitäten dieser OE.\\n\\nIm Bereich Consulting werden folgende Services angeboten:\\n\\nBeratung: An KI-Anwendungen interessierte Kunden können eine Beratung in Anspruch nehmen. Diese wird die Organisationen bei der Entwicklung, Optimierung sowie Implementierung von KI-Anwendungen auf Basis von KI- Foundation-Modellen unterstützen.\\n\\nSchulungen & Workshops: Interessierte Unternehmen sowie\\n\\nForschungseinrichtungen werden gezielt auf die Gegebenheiten der Entwicklung von Foundation-KI-Modellen mittels des LEAM-KI-Supercomputers vorbereitet.\\n\\nTraining von externen Beratungsunternehmen: Um externe Beratungen mit den spezifischen Gegebenheiten und der Technologie vertraut zu machen, müssen diese zunächst geschult werden. Dies wird durch enge Zusammenarbeit zwischen externer Beratung und dem internen Personal des LKS oder bereits beratender externer Unternehmen erreicht.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nAufbau und Betrieb eines Beratungsteams: Um diesen Service anbieten zu können, muss ein Team aus fachkundigen Berater:innen angeboten werden. Dieses sollte aus Fachexpert:innen für Data Science und Machine Learning bestehen. Alternativ können externe Beratung herangezogen werden.\\n\\nPractice Work: Das Consulting-Team des LKS entwickelt eigene Fähigkeiten im Rahmen der Entwicklung von Foundation-KI-Modellen stetig weiter, um im Rahmen der Beratung stets die aktuell wichtigsten Bereiche und Technologien abdecken zu können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n181\\n\\n9.3 Das LEAM-Board\\n\\nAbb. 26: Das LEAM-Board als zentrale Governance-Einheit des LKS\\n\\nDas LEAM-Board überwacht und steuert die strategische Ausrichtung, Ziele sowie die Weiterentwicklung sich Entscheidungsträger:innen und Expert:innen aus Wissenschaft, Politik, Unternehmen und Start-ups. Weiterführend entscheidet das LEAM-Board über Investitionen sowie die Vergabe von Compute-Ressourcen an Forschung und Entwicklung.\\n\\ndes\\n\\nLKS.\\n\\nInnerhalb\\n\\ndes\\n\\nLEAM-Boards\\n\\nbefinden\\n\\nGroße KI-Modelle für Deutschland\\n\\n182\\n\\n9.4 Zusammenfassung\\n\\nDas LKS soll Kunden vier Kern-Services anbieten, die sich insbesondere an Unternehmen richten. Dabei kommt dem KI-Foundation-Modell Development die größte Bedeutung zu. Ziel muss es sein, Kunden optimal bei der Entwicklung und produktiven Nutzung von KI- Foundation-Modellen zu unterstützen. Dabei hilft der Service Consulting als Unterstützung bei der Entwicklung eines KI-Modells bis hin zum produktiven Einsatz mittels des Inference-Services. Interessierte Unternehmen sowie Forschung können jedoch auch einzelne Services wie Model Tuning, Inference oder Infrastruktur in Anspruch nehmen. Damit ist die Zielgruppe des KI-Rechenzentrums groß und der KI- Supercomputer wird flexibel genutzt. Ein Team aus LEAM-Mitarbeiter:innen steht dabei beratend und unterstützend zur Seite und begleitet Unternehmen auf ihrem Weg zur Entwicklung von KI-Foundation-Modellen.\\n\\nAls Grundlage für diese Services dienen die Organisationseinheiten Housing, Infrastruktur-as-a-Service sowie teilweise Training-as-a-Service. Die Einheit Housing wird dabei größtenteils an externe Partner übergeben. Dies spart der LKS hohe Investitionskosten und reduziert die Zeit bis zum Start des LKS. Die Einheit Koordination unterstützt die anderen Einheiten und koordiniert den Betrieb des LKS.\\n\\nGroße KI-Modelle für Deutschland\\n\\n183\\n\\nBetriebswirtschaftliche Aspekte\\n\\nGroße KI-Modelle für Deutschland\\n\\n184\\n\\n10. Betriebswirtschaftliche Aspekte Das LEAM-KI-Servicezentrum wird über die trainierten und bereitgestellten Foundation- Models einzigartige Wachstumsimpulse auslösen und branchenübergreifend enorme Effizienzgewinne in der Wirtschaft erreichen. Nach der anfänglich benötigten Investition, die entweder als öffentliche, private, gemischte Vollfinanzierung oder im Falle von externer IaaS mit Hilfe eines bestenfalls GAIA-X konformen Infrastrukturbetreibers erfolgt, wird die Recheninfrastruktur zu erheblichen Teilen der Wissenschaft und Unternehmen, die im Open-Source-Verfahren entwickeln, zur Entwicklung von Foundation-Modellen zur Verfügung gestellt. Die Entwickler:innen trainieren auf dieser Infrastruktur Foundation-Modelle der neuesten Generation und etablieren Wege zu standardisierten Fine-Tuning Prozessen. Die Modelle sowie die Verfahren und Prozesse zum Trainieren der Modelle werden entweder Open Source oder zu möglichst geringen Selbstkosten zur Verfügung gestellt.\\n\\nDie etablierte Wirtschaft sowie Start-ups können diese Technologien schnell in ihre Produkte einbauen, da sie von verlässlichen Partnern stammen und nach europäischen Standards entwickelt wurden. Durch die Bereitstellung zum Selbstkostenpreis sind Unternehmen aus der Wirtschaft damit unabhängig von amerikanischen HyperScalern. Infolgedessen wird für Wirtschaftsunternehmen die Barriere reduziert, diese Modelle in ihre Produkte einzubauen oder neue Produkte auf dieser Basis anzubieten. Zudem profitiert die Wirtschaft von den entwickelten standardisierten Blueprint-Prozessen in den Bereichen Training, Tuning und Bereitstellung. Durch die Bündelung von Know-How innerhalb des LKS können Wirtschaft und Start-ups von erfahrenen Expert:innen entlang (Entwicklung, Training, Tuning, des kompletten KI-Foundation-Model-Lifecycles Bereitstellung, Anwendung) lernen und sich kompetent beraten lassen.\\n\\nDamit leistet LEAM durch den Aufbau von Erfahrungswissen und den zu erwartenden Spillover-Effekten einen erheblichen Beitrag zur Gewinnung rarer Talente. Gleichzeitig erfolgt ein Wissenstransfer über die Schlüsseltechnologie „KI” in die Wirtschaft, die vor dem Aufbau eigener Teams LEAM Services im Finetuning und Beratungskompetenz bei der Implementierung im Bereich Inference nutzen kann.\\n\\n10.1 Kosten\\n\\nDer Finanzierungsbedarf für das LEAM-KI-Servicezentrum setzt sich zusammen aus\\n\\nInvestitionskosten (CapEx)\\n\\nBetriebskosten (OpEx)\\n\\nDie jährliche Kostensituation ist u.a. abhängig von der Abschreibungsdauer der Investitionskosten, die im Folgenden mit vier Jahren angenommen wird.\\n\\nIm Fall des Aufbaus der Infrastruktur über einen externen Partner und Einkauf von Rechenzentrumsleistung ist die Kostenkalkulation entsprechend anzupassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n185\\n\\nDie anfallenden Kosten sind abhängig von einer Vielzahl von Parametern, die in dieser Machbarkeitsstudie lediglich abgeschätzt werden können.\\n\\nDie folgende Tabelle zeigt die für die Kostenkalkulation relevanten Parameter, die getroffenen Annahmen sowie die darüber hinaus gehenden möglichen Wertebereiche:\\n\\nUnit\\n\\nAnnahme Kommentare - Range\\n\\nHardware\\n\\nAbschreibungszeitraum\\n\\nJahre\\n\\n4\\n\\nGenerell 3-5 Jahre\\n\\nHardware (inkl. Speicher, Netw. Support, SW)\\n\\nkEUR\\n\\n260.000\\n\\nVorbild NVIDIA Selene\\n\\nAnzahl DGXA100\\n\\nAnzahl\\n\\n560\\n\\nAlternative HW: Cerebras, Graphcore, Intel, AMD, Nvidia H100\\n\\nDGXA100 per Rack\\n\\nAnzahl\\n\\n2\\n\\ngem. Info von NVIDIA\\n\\nBetriebskosten Infrastruktur\\n\\nMiete Kolokation HW\\n\\n[EUR/ Rack- Monate]\\n\\n400\\n\\nDurchschnittswert\\n\\nMiete Büroraum\\n\\n[EUR / qm] 30\\n\\n15 - 40 EUR/m2\\n\\nBüroraum\\n\\nqm\\n\\n600\\n\\nDurchschnittlich 10 qm / MA\\n\\nStromverbrauchs- Spitzenwert\\n\\nkw\\n\\n4.200\\n\\ngem. Info von NVIDIA\\n\\nStrompreis\\n\\nEUR/kwh\\n\\n0,35\\n\\naufgrund der aktuellen geopolitischen Lage ein hoher Unsicherheitsfaktor\\n\\nEffektivität Energienutzung\\n\\nder\\n\\nFaktor\\n\\n1,50\\n\\nrelativ hoher Wert, kann in einem Green Data Center niedriger sein\\n\\nDauerlast Stromaufnahme\\n\\nvon max\\n\\n%\\n\\n65\\n\\n50 %-90 %\\n\\nTeam Operationen\\n\\nFTE\\n\\n20\\n\\nAdministration Hardware und SW- Infrastruktur\\n\\nKosten Organisation und Services\\n\\nTeam Administration\\n\\nBusiness\\n\\nFTE\\n\\n20\\n\\nGovernance, Marketing\\n\\nVerwaltung,\\n\\nVertrieb,\\n\\nTeam Service\" & Consulting\\n\\n\"Training-as-a-\\n\\nFTE\\n\\n20\\n\\nImplementierung Prozessen, Dienstleistungen\\n\\nvon\\n\\nOperationen,\\n\\nPersonalkosten\\n\\nkEUR/FTE/ Jahr\\n\\n150\\n\\nDurchschnitt\\n\\nTabelle 15: Übersicht der Kosten des LEAM-KI-Servicezentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n186\\n\\nAnmerkung: Die in diesem Berechnungsmodell zugrunde liegende Architektur auf Basis von NVIDIA A100 wird durch die Hersteller in 2023 durch die neuere, leistungsfähigere Linie H100 ersetzt. Dadurch wird es auch zu einer Verbesserung des Performance- & Preisverhältnisses kommen.\\n\\nDamit ergibt sich folgende Gesamtkostenstruktur auf der Basis eines Betriebes und einer Abschreibungsdauer von vier Jahren:\\n\\nInvest\\n\\nInfrastruktur\\n\\nJahr 1 kEUR 88.732\\n\\nJahr 2 kEUR 88.732\\n\\nJahr 3 kEUR 88.732\\n\\nJahr 4 kEUR 88.732\\n\\nSumme kEUR 354.927\\n\\nHW-Abschreibung\\n\\n65.000\\n\\n65.000\\n\\n65.000\\n\\n65.000\\n\\n260.000\\n\\nHW Collocation\\n\\n1.344\\n\\n1.344\\n\\n1.344\\n\\n1.344\\n\\n5.376\\n\\nStromverbrauch\\n\\n19.316\\n\\n19.316\\n\\n19.316\\n\\n19.316\\n\\n77.263\\n\\nTeam Operationen\\n\\n3.000\\n\\n3.000\\n\\n3.000\\n\\n3.000\\n\\n12.000\\n\\nBüroraum 1/3\\n\\n72\\n\\n72\\n\\n72\\n\\n72\\n\\n288\\n\\nOrganisation und Services\\n\\n6.144\\n\\n6.144\\n\\n6.144\\n\\n6.144\\n\\n24.576\\n\\nPersonal\\n\\n6.000\\n\\n6.000\\n\\n6.000\\n\\n6.000\\n\\n24.000\\n\\nBüroraum2/3\\n\\n144\\n\\n144\\n\\n144\\n\\n144\\n\\n576\\n\\nTotal\\n\\n94.876\\n\\n94.876\\n\\n94.876\\n\\n94.876\\n\\n379.503\\n\\nTabelle 16: Gesamtkostenstruktur des LEAM-KI-Servicezentrums bei einer Abschreibungsdauer von vier Jahren\\n\\nInvestitionskosten Der zentrale Teil der Supercomputers in Anspruch genommen.\\n\\nInvestitionskosten wird durch die Anschaffung eines KI-\\n\\nFür die Berechnung dieser Infrastrukturkosten wird folgendes Szenario angenommen:\\n\\nAufbau Betrieb eines KI-Supercomputers in der Größenordnung des NVIDIA Selene (Wikipedia Contributors, 2022).\\n\\nDie Dauer eines Trainingslaufs für ein Modell der Größenordnung GPT-3 beträgt auf Selene ca. 1-1,5 Wochen.\\n\\nKernstück von Selene ist die NVIDIA Superpod-Architektur auf der Basis der DGX A100.\\n\\nDie Größenordnung einer NVIDIA Selene liegt bei 506 DGX A100 Nodes mit je 8 GPUs, in Summe.\\n\\nDie Anschaffungskosten liegen im Bereich von 260 Millionen Euro.\\n\\nAuf dieser Basis wird hier lediglich eine Beispielrechnung durchgeführt. Zum Zeitpunkt der Anschaffung der Infrastruktur bzw. deren Ausschreibungen werden alternative Lösungen (z.B. Graphcore, Cerebras, AMD, Intel) bzw. die neueste Architektur von NVIDIA (H100) analysiert und evaluiert.\\n\\nGroße KI-Modelle für Deutschland\\n\\n187\\n\\nBetriebskosten Die Betriebskosten des KI-Supercomputers setzen sich aus Collocation, Energiebedarf, Mietkosten und Personalkosten zusammen.\\n\\nMiete Collocation HW: Um den KI-Supercomputer zu betreiben, sind entsprechend ausgestattete Räumlichkeiten und Serverracks nötig. Diese müssen extern angemietet und mit der anzuschaffenden KI-Hardware ausgestattet werden. Pro Serverrack können zwei DGX A100 eingebaut werden. Bei Mietkosten in Höhe von 400 EUR pro Monat pro Rack ergeben sich jährliche Kosten in Höhe von 1.344.000 EUR.\\n\\nEnergiekosten: Der Energiebedarf der Anlage beläuft sich auf circa 4.200 kWh. Er ist stark abhängig von der Auslastung der Anlage. Es ist von einer durchschnittlichen Dauerlast der Anlage von 65 % auszugehen. In der aktuellen Situation sind die Stromkosten höchst volatil. Auch wenn der Strompreis für Industriekunden aktuell noch niedriger liegt, wird aktuell mit 0,35 EUR/kWh geplant. Dadurch ergeben sich jährliche Kosten in Höhe von 19.316.000 EUR.\\n\\nTeam Infrastruktur: Das Team Infrastruktur kümmert sich um den Betrieb der Hardware und SW-Infrastruktur. Dabei wird von einer Teamstärke von 20 FTE (Full- Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nMiete Büro: Um Mitarbeiter:innen von LEAM einen Arbeitsplatz zu bieten, müssen Büroräumlichkeiten mit entsprechender Ausstattung angemietet werden. Es wird aktuell mit einem 600 m² großen Büro zu 30 EUR pro Quadratmeter geplant. Dies entspricht jährlichen Kosten in Höhe von 60.000 EUR .\\n\\nTeam Koordination: Das Team Business Administration beschäftigt sich mit Governance, Sales, Marketing sowie administrativen Tätigkeiten rund um das Projekt LEAM. Dabei wird von einer Teamstärke von 20 FTE (Full-Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nTeam Services und Consulting: Dieses Team beschäftigt sich mit der Implementierung von Prozessen und Services rund um die Services, die LEAM anbietet. Dabei wird von einer Teamstärke von 20 FTE (Full-Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nMiete Büro: Um Mitarbeiter:innen von LEAM einen Arbeitsplatz zu bieten, müssen Büroräumlichkeiten mit entsprechender Ausstattung angemietet werden. Es wird aktuell mit einem 600 m² großen Büro zu 30 EUR pro Quadratmeter geplant. Dies entspricht jährlichen Kosten in Höhe von 60.000 EUR.\\n\\nGroße KI-Modelle für Deutschland\\n\\n188\\n\\nAlternative Kostensituation bei Einkauf GPU-RZ-Leistungen Beim Einkauf von Rechenleistung fallen die initialen Investitionskosten für LKS durch die fehlende Notwendigkeit der Beschaffung von KI-Hardware fast vollständig weg. Voraussetzung hierfür ist die Bereitschaft eines Unternehmens aus dem Bereich Cloud- folgende Service-Providing, Rahmenbedingungen berücksichtigt werden müssen:\\n\\nin\\n\\ndie\\n\\nInfrastruktur\\n\\nzu\\n\\ninvestieren, wobei\\n\\nEs muss sichergestellt sein, dass ein ausreichend großes Compute-Cluster zur Verfügung gestellt wird (ca. 4500 GPUs).\\n\\nLKS wird über den Zeitraum von vier Jahren eine Mindest-Abnahmemenge von Rechenkapazität (z.B. 60 %) garantieren.\\n\\nDer Ankauf von Rechenleistung bzw. GPU-Stunden erfolgt nach marktüblichen Preisen (s.u.).\\n\\nEinzelheiten der Kalkulation und Vertragsgestaltung sind zu definieren.\\n\\nDie Auswahl eines Cloud-Service-Providers erfolgt evtl. im Rahmen einer öffentlichen Ausschreibung, deren Details zu definieren sind.\\n\\nIn diesem Szenario fallen folgende Kostenpositionen gemäß Tabelle 17 an:\\n\\nAnnahme\\n\\nJährl Kosten in kEUR\\n\\nAnkauf von Rechenzentrumskapazitäten / GPU Stunden\\n\\n60 % der für den eigenen Betrieb errechneten Kosten\\n\\n53.239\\n\\nOrganisation, Training-as-a- Service und Consulting\\n\\nEntsprechend der Kalkulation mit eigenem RZ\\n\\n6.144\\n\\nSumme\\n\\n59.383\\n\\nTabelle 17: Kosten des LEAM-KI-Servicezentrums bei einem Einkauf der GPU-RZ-Leistung\\n\\nGroße KI-Modelle für Deutschland\\n\\n189\\n\\n10.2 Einnahmen\\n\\nDurch die angebotenen Services des LKS lassen sich verschiedene Einnahmequellen definieren:\\n\\n\\uf0fc Verkauf von Rechenzentrums-Kapazität (GPU-Stunden) \\uf0fc Services für das Training von Foundation-Modellen (Training-as-a-Service) \\uf0fc Services für das Tuning von maßgeschneiderten Modellen (Training-as-a-Service) \\uf0fc \\uf0fc Allgemeine Beratungs-Tätigkeiten (Consulting)\\n\\nInference Service (GPU-Stunden)\\n\\nDie Möglichkeiten zur Generierung von Umsatz sind zielgruppenspezifisch zu differenzieren:\\n\\nWirtschaft: Corporates und KMU werden Leistungen zu marktüblichen Preisen angeboten, wenn sie Modelle für den privatwirtschaftlichen Betrieb entwickeln. Bei Forschungsprojekten, die im Open-Source-Verfahren arbeiten, erhält die Wirtschaft Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nWissenschaft: Projekte, die im Open-Source-Verfahren arbeiten, erhalten Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nPublic Sector: Öffentliche Einrichtungen erhalten Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nStart-ups: Junge Technologieunternehmen können für Leistungen von der öffentlichen Hand bereitgestellte Kontingente (z.B. KI-Compute-Voucher) zur Nutzung der LKS-Services beantragen.\\n\\nVerkauf von Rechenzentrumskapazität Basis für die Services KI-Foundation-Model Development, Tuning und Inference ist die Nutzung von Rechenzentrumskapazität nach GPU-Stunde.\\n\\nDie Kosten für eine GPU-Stunde auf Basis des o.a. Kostenszenarios berechnet sich unter Annahme der Vollauslastung wie folgt:\\n\\nJährliche Kosten des Infrastrukturbetriebs gem. Tabelle 16: Anzahl GPUs Anzahl Stunden pro Jahr Auslastung\\n\\n88.732.000 EUR\\n\\n4.480 8.765 100 %\\n\\nKosten pro GPU-Stunde\\n\\n2,25 EUR\\n\\nDies liegt im Bereich der derzeit marktüblichen Preise (s. Anhang C).\\n\\nGroße KI-Modelle für Deutschland\\n\\n190\\n\\nUnter der Annahme, dass 20 % der Rechenkapazität zu diesen Preisen am Markt verkauft werden können, ergibt sich ein möglicher Jahresumsatz (kostendeckend, ohne Marge) von:\\n\\nPreis pro GPU-Stunde 20 % von 4480 GPUs * 8765 h\\n\\n2,25EUR 7.848.960h\\n\\nJährlicher Umsatz ca.\\n\\n14,464 Mio EUR\\n\\nDiese grobe Kalkulation muss bei einer detaillierten Ausgestaltung des Szenarios angepasst werden, vor allem hinsichtlich der Parameter-Auslastung, Verfügbarkeit, Marge sowie angepassten Infrastrukturkosten.\\n\\nKI-Foundation-Model-Training und -Tuning Die Services KI-Foundation-Model-Development und -Tuning stellen eine Kombination aus Beratung und der Nutzung der Computer-Kapazitäten dar. Die beratenden Tätigkeiten sowie die tatsächliche Entwicklung und das Tuning werden dabei durch das Team Services und Consulting des LKS erbracht. Abgerechnet werden dabei projektspezifisch übliche Tagessätze zwischen 1.200 EUR und 2.500 EUR pro Berater:in/Entwickler:in.\\n\\nAnnahme\\n\\nVerfügbare Kapazitäten\\n\\n10 FTE bei 80 % Chargeability & 200 Tagen\\n\\n1.600 Tage/Jahr\\n\\nDurchschnittlicher Tagessatz\\n\\nzwischen 1.200 & 2.500 Tagessatz\\n\\n1.600EUR\\n\\nSumme\\n\\n2,56 Mio EUR/Jahr\\n\\nTabelle 18: Übersicht der Einnahmen durch das Model-Training\\n\\nGroße KI-Modelle für Deutschland\\n\\n191\\n\\nBeratung Im Bereich Consulting sollen Beratungsleistungen nach marktüblichen Preisen abgerechnet werden. Diese werden anhand von Personentagen sowie der Seniorität und Expertise des Beratenden berechnet. Je nach der angefragten Leistung an das LKS wird dabei ein unterschiedlicher Umfang an Beratung nötig.\\n\\nAnnahme\\n\\nVerfügbare Kapazitäten\\n\\n10 FTE bei 80% Chargeability & 200 Tagen\\n\\n1.600 Tage/Jahr\\n\\nDurchschnittlicher Tagessatz\\n\\nzwischen 1.200 & 2.500 Tagessatz\\n\\n1.600EUR\\n\\nSumme\\n\\n2,56 Mio EUR/Jahr\\n\\nTabelle 19: Übersicht der Einnahmen durch die Beratung\\n\\nInference-Service Der Inference-Service bietet Unternehmen die Möglichkeit, KI-Modelle mit den Compute- Ressourcen des LKS zu nutzen. Abgerechnet wird hier nach der Menge genutzter Compute-Ressourcen, ähnlich der Vermietung von GPU-Stunden. Eventuell fallen weitere Kosten an, insofern das Team Services oder Consulting Modelle ein Modell zunächst für die Nutzung mittels Compute-Ressourcen des LKS vorbereiten muss. Diese zusätzlich nötigen Aktivitäten werden analog zum KI-Foundation-Model-Development abgerechnet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n192\\n\\nFinanzierungsmodelle von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n193\\n\\n11. Finanzierungsmodelle von LEAM Zur Finanzierung des Aufbaus und Betriebs des LKS bieten sich drei verschiedene Modelle an: die öffentliche und private Finanzierung sowie die Finanzierung als Public-Private- Partnership. Abhängig von möglichen Förderungen und damit einhergehenden rechtlichen Auflagen an LEAM muss die genaue Finanzierungsstruktur weiter evaluiert werden. Im Folgenden werden die möglichen Finanzierungsmodelle zunächst überblicksartig dargestellt. In Kapitel 11.5 findet sich eine Betrachtung der spezifischen rechtlichen Überlegungen denkbaren Finanzierungsmodelle. Public-Private- Partnership\\n\\nQuellen\\n\\nBund • Länder\\n\\nCorporates • Venture Capitals • Stiftungen\\n\\nKombination aus öffentlichen und privaten Quellen\\n\\nVehikel\\n\\nStaatliche Gesellschaft • Fördermittel (Förderung zur Gründung, Institutionelle Förderung, Projektförderung) • Garantierte Abnahme von Rechenleistung\\n\\nEigenkapital • Fremdkapital • Garantierte Abnahme von Rechenleistung\\n\\nKombination aus den Vorgenannten\\n\\n(Kredite) + maximale\\n\\nUnabhängigkeit + Konsens zwischen\\n\\n+ Flexibilität in der\\n\\nUnternehmensgestaltung\\n\\n+ Public als Ankerinvestor + Flexibleres operatives\\n\\nBewertung\\n\\nöffentl. Interesse und EU-konformer Entwicklung von KI- Modellen\\n\\n– Starre Strukturen und\\n\\n+ Einfaches und flexibles Anwerben von Personal – Komplexes Fundraising\\n\\nund eventuelle Governance\\n\\nGeschäft bei Einhaltung der Rahmenbedingungen für LEAM\\n\\n– Wettbewerbsrechtliche\\n\\nProzesse\\n\\n– Einschränkungen in\\n\\nAgilität, Personalaufbau etc.\\n\\n– Erschwerte Kooperation mit Wissenschaft durch hohe Kosten\\n\\nBeschränkungen\\n\\n– Kompliziertes Verfahren\\n\\nzur Gründung\\n\\nKommentar\\n\\nBerücksichtigung des EU-Beihilferechts mit Privilegierungen für Open-Source-KI- Projekte für Unternehmen und Wissenschaft\\n\\nBeispiel: DFKI\\n\\nTabelle 20: Gegenüberstellung der drei Finanzierungsszenarien für das LKS\\n\\nGroße KI-Modelle für Deutschland\\n\\n194\\n\\n11.1 Öffentliche Finanzierung\\n\\nIn der Wissenschaftslandschaft Deutschlands existiert eine Reihe von öffentlich finanzierten Rechenzentren, die zum Teil zur Weltspitze gehören (s. Kapitel 8). In der Regel teilen sich hier Bund und das zugehörige Bundesland die Finanzierung der Investitions- und laufenden Kosten. Es wird ein Grundstock an Personal zur Erhaltung des Betriebs finanziert sowie Planstellen zur Forschung. Üblicherweise erhalten diese Rechenzentren erhebliche Anteile ihrer Finanzierung über Drittmittelprojekte, also kompetitiv bei Förderern (DFG, Bund, Länder, Industrie) eingeworbene Personal- und Sachkosten für spezifische Forschungsprojekte. Zu unterscheiden sind hier in der Regel die institutionelle Förderung und die Projektförderung: Die institutionelle Förderung wird wiederkehrend jährlich gewährt; sie mag in der Höhe abhängig von verschiedenen Faktoren schwanken, bildet aber eine sichere Grundlage für den wirtschaftlichen Betrieb der Einrichtung. Daneben tritt die bereits zuvor erwähnte Projektförderung, auf die sich die Antragsteller:innen selbst (oft im kompetitiven Wettbewerb mit anderen Einrichtungen) bewerben können.\\n\\nVorteile öffentlicher Finanzierung Der Vorteil der öffentlichen Finanzierung für LEAM besteht in der engen Verzahnung zwischen öffentlichen Interessen eines an europäischen Werten orientierten Foundation- Models. Gleichzeitig bestehen nach erfolgter Finanzierungszusage eine hohe Verlässlichkeit und Planungssicherheit für die öffentlich geförderten Projekte. Damit könnte das Projekt über einen gewissen Zeitraum verlässlich wirtschaften und wäre unabhängig von der aktuellen Marktlage. Die entsprechenden Anreize für Start-ups könnten in dieser Finanzierungsform ebenfalls angeboten werden.\\n\\nNachteile öffentlicher Finanzierung finanzierte Rechenzentren aus EU- In der Regel unterliegen rein öffentlich steuerrechtlichen Gründen beihilferechtlichen, wettbewerbsrechtlichen Beschränkungen in der Nutzung durch privatwirtschaftliche Akteure. So stellt bspw. das HLRS der Uni Stuttgart, das dezidiert Rechenzeit für die Industrie zur Verfügung anbietet, nur einen Bruchteil der möglichen Rechenzeit zur Verfügung (s. Kapitel 8.5). Eine Nutzung einer öffentlich finanzierten LKS durch die eher wirtschaftlich organisierten LEAM Services würde also Beschränkungen unterliegen und es könnte womöglich aus rechtlichen Gründen nicht genügend Rechenzeit zur Verfügung gestellt werden. Dies gilt auch für Start-ups, die ebenfalls um die knappen Rechenressourcen mit den anderen wirtschaftlichen Einheiten konkurrieren. Dies könnte den Aufbau von wirtschaftlichen Applikationen auf den Foundation-Modellen hemmen. Gleichzeitig unterliegen rein öffentlich finanzierte Projekte umfangreichen Genehmigungen und Auflagen und somit in der Regel langwierigen Abstimmungsprozessen sowie bei wechselnden politischen Mehrheiten auch sich ändernden politischen Gegebenheiten. Öffentliche Unternehmen sind in der Regel tarifgebunden oder lehnen sich an Tarifverträge an, was die Flexibilität bei der Gewinnung der für den Betrieb und die Services erforderlichen hochqualifizierten Mitarbeiter:innen erschweren kann.\\n\\noder\\n\\nGroße KI-Modelle für Deutschland\\n\\n195\\n\\n11.2 Private Finanzierung\\n\\nFür eine privatwirtschaftliche Finanzierung von LEAM kommen vor allem zwei Szenarien in Betracht:\\n\\n\\n\\nJoint-Venture von großen Unternehmen (Cloud-Service-Provider und Anwender). Hierfür wurde in den geführten Interviews von verschiedenen Unternehmen eine generelle Bereitschaft und Interesse signalisiert.\\n\\nFinanzierung über Risikokapital durch klassische Venture Capital (VC)- Gesellschaften oder Private-Equity-Investoren.\\n\\nDa das Business-Modell einer rein privatwirtschaftlichen Gestaltung vom LKS mit hohen Risiken verbunden ist (u.a. auch durch die Dynamik der technologischen Entwicklungen und deren Auswirkungen auf den Markt), ist eine hundertprozentige Finanzierung durch private Unternehmen und Kapitalgeber nicht sehr wahrscheinlich.\\n\\nHier kann die öffentliche Hand durch Werkzeuge wie einer Anschubfinanzierung, Darlehen o.ä. helfen und das finanzielle Risiko abschwächen. Im Gegenzug kann vereinbart werden, dass z.B. ein gewisser Teil der Infrastruktur für nicht-wirtschaftliche Open-Source-Projekte von Unternehmen oder der Wissenschaft reserviert wird. Inwieweit diese Variante mit einer teilweisen oder gar überwiegenden wirtschaftlichen Nutzung auch in subventionsrechtlicher Hinsicht kompatibel ist, ist zu prüfen.\\n\\nDie Wissenschaft und kooperierende Unternehmen, die bereit sind, im Open-Source- im Rahmen von Verfahren zusammenzuarbeiten, würden Auftragsforschung oder einer Projektförderung die Rechenzeit nutzen, um die Open- Source Modelle im LEAM KI-Servicezentrum zu trainieren. Im Falle der Auftragsforschung kämen als Auftraggeber für die Wissenschaft dabei sowohl LEAM selbst als auch die öffentliche Hand in Betracht.\\n\\nin diesem Szenario\\n\\nVorteile privater Finanzierung Der Vorteil einer privaten Finanzierung liegt darin, dass Training und Verwertung ohne die regulatorischen Erfordernisse einer öffentlichen Finanzierung in einer Gesellschaft organisiert werden können. Die Investitionskosten könnten je nach Finanzierungsart auf viele Schultern verteilt werden, was das individuelle Risiko der Gesellschafter:innen minimieren würde.\\n\\nDie Gesellschaft könnte zudem unbegrenzte Gewinne erwirtschaften und wäre unter Berücksichtigung der Interessen der Kapitalgeber frei bei der Wahl der Investitionen. Die Gesellschaft wäre durch frei wählbare Vergütungen auch flexibler darin, entsprechendes Personal anzuwerben und könnte so einen Vorteil beim Know-How Aufbau erlangen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n196\\n\\nNachteile privater Finanzierung Die private Finanzierung birgt die Herausforderung, Geldgeber für einen erheblichen Investitionsbetrag zu finden. Die Wahrscheinlichkeit, dass ein einzelnes Unternehmen ins Risiko geht, die hohe Finanzierungslast auf sich zu nehmen, ist gering.\\n\\nEs müssten Joint-Ventures aus mehreren Unternehmen gebildet werden, wodurch die Gestaltung der Gesellschaft und deren Governance komplex und langwierig werden kann.\\n\\nIm Bereich der Venture Capital-Finanzierung übersteigen die erforderlichen Finanzierungssummen die zum Teil erheblich die Finanzierungspraxis der meisten deutschen und europäischen Fonds. So stellt der High-Tech-Gründerfonds (HTGF) nur Anschubfinanzierungen in einstelliger Millionenhöhe bereit.\\n\\nEin weiterer Nachteil privater Finanzierung in der Kooperation mit den wissenschaftlichen Partnern, die in diesem Modell nicht direkt, sondern nur im Rahmen von Aufträgen und Projekten an LEAM beteiligt sind. Das Training der Foundation- Modelle, die selbst keinen Profit generieren, aber viel Rechenzeit benötigen, stünde in diesem Fall in enger Konkurrenz mit dem Kerngeschäft der Betreibergesellschaft Inference und Tuning. Rechenzeit würde somit prioritär für die kommerziellen Produkte von LEAM verwendet werden, wodurch das Training der Foundation-Modelle eher keinen experimentellen Charakter hätte. Zudem müssten Unternehmen und/oder die Wissenschaft, die bereit sind, im Open-Source-Modell zu arbeiten, Marktpreise für die Nutzung zahlen.\\n\\nliegt\\n\\nDer größte Nachteil ist jedoch, dass die LEAM Zielstellung, KI-Foundation-Modelle im öffentlichen Interesse bereitzustellen, mit dem Gewinnstreben der Organisation im Wettbewerb stehen würde. So würden Trainingsdaten für die Foundation-Modelle in diesem Szenario auch stärker unter der Perspektive einer ökonomischen Verwertbarkeit ausgewählt, weshalb seltene Sprachen auf Grund der schlechten Skalierbarkeit am Markt und des höheren Aufwands bei der Beschaffung von Trainingsdaten ähnlich schlecht repräsentiert würden, wie bei den bisher am Markt angebotenen Modellen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n197\\n\\n11.3 Public-Private-Partnership\\n\\nBei einer Public-Private-Partnership erfolgt die Finanzierung durch die öffentliche Hand und private Unternehmen in jeweils zu definierenden Anteilen.\\n\\nEin Beispiel für eine erfolgreiche PPP im Bereich der Künstlichen Intelligenz ist das Deutsche Forschungszentrum für Künstliche Intelligenz (DFKI). Das DFKI wurde 1988 als gemeinnützige Public-Private Partnership (PPP) gegründet. Es unterhält Standorte in Kaiserslautern, Saarbrücken, Bremen, Niedersachsen, Labore in Berlin und Darmstadt sowie Außenstellen in Lübeck und Trier. Die Finanzierung erfolgt über Zuwendungen öffentlicher Fördermittelgeber wie der Europäischen Union, dem Bundesministerium für Bildung und Forschung (BMBF), dem Bundesministerium für Wirtschaft und Klimaschutz (BMWK), den Bundesländern und der Deutschen Forschungsgemeinschaft (DFG) sowie durch Entwicklungsaufträge aus der Industrie.\\n\\nWichtig bei der Gestaltung der PPP ist, die jeweiligen Vorteile von öffentlicher/privater Finanzierung zu maximieren und deren Nachteile zu minimieren.\\n\\nDie Herausforderung bei der Realisierung von LEAM als PPP steckt dabei im Charakter LEAMs als Infrastruktureinrichtung und den entsprechend hohen Investitionskosten, die für den Aufbau notwendig sind. Die Gesellschaft würde in diesem Szenario aus öffentlichen Mitteln (Bund und ggf. Land) die Infrastruktur beschaffen. Es müssten Mittel und Wege gefunden werden, die Mittel für die notwendige Infrastruktur zwischen den öffentlichen und privaten Partnern aufzuteilen. Grundsätzlich lassen sich in diesem Szenario auch die eher wirtschaftlich orientierten Services des Trainings-as-a-Service, Consulting und Inference/Tuning besser zusammen in einer Gesellschaft mit der Infrastruktur abbilden.\\n\\nVorteile einer Public-Private-Partnership Eine PPP ist ein denkbares Modell, in dem die Interessen der Wirtschaft nach optimalen Verwertungsmöglichkeiten eines Open Source Foundation-Modells sowie die Interessen der Politik und Gesellschaft nach einem digital souveränen Europa miteinander in Einklang gebracht werden, da beide Seiten an einer Gesellschaft beteiligt wären. Darüber hinaus wird das Projekt finanziell leichter zu realisieren, da die Investitionskosten geteilt werden. Gleichzeitig wäre der öffentliche Haushalt entlastet, da die Privatwirtschaft ebenfalls für einen Teil der Investitionskosten für die Infrastruktur aufkommt. Die entsprechenden Anreize für Start-ups könnten in dieser Finanzierungsform ebenfalls angeboten werden.\\n\\nNachteile einer Public-Private-Partnership Nachteil eines PPP besteht darin, dass die Gesellschaft nicht in derselben Weise privatwirtschaftlich agieren könnte, wie ein Wirtschaftsunternehmen, da der Bund bei seinen Beteiligungen wettbewerbsrechtlichen Beschränkungen unterworfen ist. Darüber hinaus ist sowohl mit Blick auf die genaue organisationstechnische Ausgestaltung der PPP als auch mit Blick auf die Spezifikationen der Foundation-Modelle mit einem intensiven Aushandlungsprozess zwischen den Vertretern der Privatwirtschaft und der öffentlichen Hand zu rechnen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n198\\n\\nSPOTLIGHT TUI Deutschland An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie TUI Group ist einer der weltweit führenden Touristikkonzerne. Zum Konzern gehören über 400 Hotels und Resorts und 16 eigene Kreuzfahrtschiffe, außerdem europaweit führende Veranstaltermarken und Online-Vermarktungsplattformen, fünf Fluggesellschaften und über 1.000 Reisebüros. Neben dem Ausbau des Kerngeschäfts mit Hotels, Kreuzfahrten über erfolgreiche Joint Ventures und Aktivitäten in den Urlaubsdestinationen setzt die TUI verstärkt auf den Ausbau digitaler Plattformen.\\n\\nHenning von Roon, Common Analytics Capabilities Lead, TUI Deutschland\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Bei der TUI kommen vortrainierte Modell bei der\\n\\nBilderkennung und Textklassifizierung zum Einsatz. Mittels dieser Modelle können wir beispielsweise die Produktpräsentation optimieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Durch den Einsatz von KI-Foundation-Modellen können viele Geschäftsprozesse leichter automatisiert, erweitert oder verbessert werden. Gerade bei der Personalisierung von Angeboten für unsere Millionen von Kunden pro Jahr können wir mit KI-Foundation-Modellen einen großen Mehrwert schaffen, ohne die Entwicklungskosten massiv zu steigern. Personalisierung auf dieser Größenordnung wäre ansonsten kaum umzusetzen.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Unsere hohen Ansprüche in Europa an Datenschutz und Datensicherheit werden oftmals durch außerhalb der EU bereitgestellte Modelle nicht erfüllt. Um keine Abstriche bei Datenschutz und Datensicherheit zu machen und gleichzeitig von den wirtschaftlichen Vorteilen des Einsatzes von KI-Foundation-Modellen zu profitieren, muss Europa deshalb eigene KI-Foundation-Modelle entwickeln. Zusätzlich ist die Übertragbarkeit häufig nicht gegeben, da die\\n\\nTrainingsdaten nicht die europäischen Sprachen und Verhaltensweisen widerspiegeln. Außerdem könnten Abhängigkeiten entstehen, welche sich in einer ungünstigen Preis- und Lizenzgestaltung abbilden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n199\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Open Source Modelle sorgen für Transparenz und erlauben eine bessere Interpretation der Ergebnisse. Zudem haben sie den Vorteil, dass wir sie entsprechend unserer spezifischen Bedürfnisse weiterentwickeln können. Europäische Modelle unterliegen von Anfang an den hiesigen Standards und bilden die lokalen Gegebenheiten deutlich besser ab, welches den Einsatz für uns attraktiver macht.\\n\\n11.4 Rechtliche Rahmenbedingungen\\n\\nDie Umsetzung der Finanzierung des LEAM KI-Supercomputers wirft zahlreiche Rechtsfragen auf, die im Rahmen dieser Machbarkeitsstudie weder abschließend noch tatsächlichen Gegebenheiten und vollständig behandelt werden können. Die Maßnahmen im Bezug auf rechtliche Rahmenbedingungen müssen daher im Nachgang tiefergehend evaluiert werden. Für die weitere Betrachtung sind jedoch grundsätzlich drei besondere Rechtsbereiche zu unterscheiden, die die geplante Struktur und den Betrieb des LKS mitprägen und bei der Strukturplanung mit in den Blick genommen werden müssen. Dabei handelt es sich hierbei um das EU-Beihilferecht, das (EU-)Vergaberecht sowie das öffentliche Dienst- und Vergütungsrecht. Nachfolgend sollen diese Rechtsmaterien zunächst überblicksmäßig dargestellt werden Anschließend sollen die Rechtsmaterien und ihre Auswirkungen auf die drei grundsätzlich zu unterscheidenden Finanzierungsmodelle im Hinblick auf die Besonderheiten des LEAM-Projekts dargestellt werden, um hieraus in einer vergleichenden Rechtsbetrachtung die Vor- und Nachteile zusammenzufassen.\\n\\nDie Bedeutung des EU-Beihilferechts Eine staatliche Förderung für das Projekt LEAM sowie der Betrieb des LKS müssen mit dem EU-Beihilferecht vereinbar sein. Dieses ist bindend und steht dem deutschen Recht vor. Die Vorschriften über staatliche Beihilfen – Art. 107 bis 109 des Vertrages über die Arbeitsweise der Europäischen Union (AEUV) – zählen zu den wichtigsten europäischen Wettbewerbsregeln. Die beihilferechtlichen Regelungen verfolgen das Ziel, Wettbewerbsverzerrungen innerhalb des europäischen Binnenmarkts durch Beihilfen, die Mitgliedstaaten Unternehmen gewähren, zu verhindern. Daher normiert Art. 107 Abs. 1 AEUV ein grundsätzliches Beihilfeverbot, von dem jedoch Ausnahmemöglichkeiten bestehen.\\n\\nDas EU-Beihilferecht wird – vereinfacht – wie folgt geprüft:\\n\\nAuf der ersten Ebene wird geprüft, ob überhaupt begrifflich eine – nach dem soeben Gesagten grundsätzlich unzulässige – Beihilfe vorliegt. Häufig ist dies durch eine geschickte Ausgestaltung einer staatlichen Förderung bereits nicht der Fall – mit sehr günstigen Rechtsfolgen. Liegt begrifflich eine staatliche Beihilfe vor, so muss auf einer zweiten Ebene geprüft werden, ob diese aufgrund bestimmter Regeln der EU zulässig ist, ohne dass die EU der Beihilfe im Einzelnen ausdrücklich zustimmen muss. Eine Beihilfe Allgemeinen kann\\n\\ninsbesondere\\n\\nzulässig\\n\\nsein, wenn\\n\\ndie\\n\\nRegeln\\n\\nder\\n\\nGroße KI-Modelle für Deutschland\\n\\n200\\n\\nGruppenfreistellungsverordnung (AGVO) greifen. Wenn solche generell geregelten Ausnahmen nicht einschlägig sind, muss auf der dritten Stufe eine Genehmigung der EU- Kommission für die Beihilfe eingeholt werden (sog. Notifizierung). Das ist oft langwierig und komplex und sollte für Projekte, die in besonderem Maße auf Schnelligkeit ausgerichtet sind, daher vermieden werden. Nicht mit dem Europarecht konforme Beihilfen darf der deutsche Staat nicht vergeben und ist verpflichtet, dennoch gewährte Gelder zurückzufordern. Daher muss das EU-Beihilferecht zwingend eingehalten werden.\\n\\nWichtig ist für das LEAM Projekt zunächst die Prüfung, ob überhaupt „begrifflich\" eine staatliche Beihilfe vorliegt. Eine solche ist gegeben, wenn die folgenden vier Merkmale kumulativ erfüllt sind. Eine Beihilfe ist\\n\\neine staatliche Maßnahme,\\n\\ndie eine Begünstigung,\\n\\neines bestimmten Unternehmens darstellt und\\n\\ndadurch zum Eintritt einer (jedenfalls potenziellen) Wettbewerbsverfälschung und Beeinträchtigung des innergemeinschaftlichen Handels führt.\\n\\nEine staatliche Förderung der Infrastruktur des Projekts LEAM stellt, ohne dass hierauf im Einzelnen eingegangen werden soll, eine staatliche Maßnahme dar, die zu einer Begünstigung führen und auch Auswirkungen auf den europäischen Markt haben kann, da einer solchen großen Infrastruktur schon ihrem Sinn und Zweck nach europaweite Bedeutung zukommen kann.\\n\\nAllerdings stellt sich – unabhängig von der Rechtsform – die Frage, ob das Projekt LEAM auch ein „Unternehmen\" im Sinne des EU-Beihilferechts darstellt. Der Begriff des Unternehmens im EU-Beihilferecht ist dabei nicht von der Rechtsform einer Einheit abhängig, er ist tätigkeitsbezogen. Maßgeblich ist immer die Frage, ob die Einheit eine wirtschaftliche Tätigkeit im Sinne des Beihilferechts ausübt. Nach ständiger EuGH- Rechtsprechung ist eine Tätigkeit wirtschaftlich, wenn sie darin besteht, Güter oder Dienstleistungen auf einem bestimmten Markt anzubieten, 20 unabhängig von ihrer Rechtsform, der Art ihrer Finanzierung sowie einer Gewinnerzielungsabsicht. 21 Irrelevant ist daher, ob das Unternehmen in privater oder öffentlicher Trägerschaft betrieben wird. Ein Beispiel sind Forschungsinfrastrukturen: Wenn sie – vereinfacht ausgedrückt – für die Allgemeinheit forschen, sind sie kein Unternehmen im Sinne des EU-Beihilferechts. Wenn sie dagegen Auftragsforschung für andere Unternehmen als Auftraggeber erbringen, sind sie beihilferechtlich ein Unternehmen und es gelten die Beschränkungen des EU- Beihilferechts. Ein und dieselbe Einheit kann daher sowohl „Unternehmen\" wie auch „Nicht-Unternehmen\" im Sinne des EU-Beihilferechts sein, abhängig von der jeweils ausgeübten Tätigkeit.\\n\\n20 Vgl. EuGH, Urteil vom 16.06.1987, Rs. 118/85, Rn. 7 – Kommission/Italien; vom 12.09.2000, verbundene Rsen. C-180/98 bis C-184/98, Rn. 75 – Pavlov u.a. und vom 01.07.2006, Rs. C-49/07, Rn. 22 – MOTOE. 21 EuGH, Urteil vom 23.04.1991, C-41/90, Slg. 1991, I-1979 Rn. 21 – Höfer u. Elser/Macroton; von Wallenberg/Schütte, in: Grabitz/Hilf/Nettesheim, Das Recht der Europäischen Union, Werkstand: 67. EL Juni 2019, Art. 107 AEUV, Rn. 39.\\n\\nGroße KI-Modelle für Deutschland\\n\\n201\\n\\nZusammengefasst bedeutet dies auch für das LKS:\\n\\n\\n\\nSoweit dieses nicht-wirtschaftliche Tätigkeiten im Sinne des EU-Beihilferechts ausübt, stellt die staatliche Förderung bzw. die Verwendung der staatlich geförderten Mittel keine Beihilfe dar.\\n\\nWenn das LKS wirtschaftliche Tätigkeiten im Sinne des EU-Beihilferechts ausübt, ist es ein Unternehmen und die Beschränkungen des EU-Beihilferechts gelten für die staatliche Förderung bzw. die Verwendung der staatlich geförderten Mittel.\\n\\nFür die Abgrenzung von nichtwirtschaftlichen und wirtschaftlichen Tätigkeiten, die, wie hier, einen Bezug zur Wissenschaft haben, ergeben sich Besonderheiten bei dieser Abgrenzung. Günstig ist, dass das Projekt LEAM einen großen Teil der Kapazitäten für die Entwicklung von KI-Foundation-Modellen zur Verfügung stellen will, die im Wege des Open-Source-Verfahrens der Allgemeinheit (bzw. diskriminierungsfrei der Wirtschaft) zur Verfügung gestellt werden sollen. Insbesondere ist bei der Auslegung und Anwendung des EU-Beihilferechts auf dem Gebiet der Entwicklung von KI-Modellen zur freien Nutzung der „Unionsrahmen für staatliche Beihilfen zur Förderung von Forschung, Entwicklung und Innovation“ vom 19.10.2022, 2022/C 7388 (Aktualisierung der Vorgängerversion vom 21.05.2014, 2014/C 198/01; nachfolgend „FuE-Rahmen“) maßgeblich. Dieser enthält die wichtigsten Unterscheidungen zwischen wirtschaftlichen und nichtwirtschaftlichen Tätigkeiten im Wissenschaftskontext sowie auch Privilegierungen für den Einsatz von Infrastruktur in der Wissenschaft, die dem LEAM-Projekt zugutekommen können.\\n\\nVereinfacht kann folgende Unterscheidung vorgenommen werden:\\n\\nSoweit das LKS mit der Wissenschaft, im Open-Source-Verfahren arbeitenden Unternehmen oder Konsortien aus beiden Bereichen zusammenarbeitet und insbesondere KI-Foundation-Modelle entwickelt, die im Open-Source-Verfahren wiederum allgemein der Wissenschaft und Wirtschaft zur Verfügung gestellt werden, können solche allgemein die (europäische) Wissenschaft und Wirtschaft bereichernde Tätigkeiten und damit nicht-wirtschaftliche Tätigkeiten im Sinne des FuE-Rahmens sein (vgl. Rz. 20 FuE-Rahmen). KI hat dabei im neuen FuE-Rahmen 2022 eine ausdrückliche Erwähnung gefunden: Der Begriff der experimentellen Entwicklung wird in Rz. 16 lit. k) definiert als (verkürzt) den Erwerb, die Kombination, Gestaltung und Nutzung vorhandener wissenschaftlicher, technischer, wirtschaftlicher und sonstiger einschlägiger Kenntnisse und Fertigkeiten mit dem Ziel, in beliebigen Bereichen, Technologien, Branchen oder Wirtschaftszweigen neue oder verbesserte Produkte, Verfahren oder Dienstleistungen einschließlich digitaler Produkte, Verfahren oder Dienstleistungen zu entwickeln. Hierbei wird die Entwicklung Künstlicher Intelligenz ausdrücklich als ein Anwendungsbeispiel genannt. Damit dürften die KI-Foundation-Modelle (sowie sonstige KI-Modelle) unter den Begriff der Entwicklung im Rahmen des FuE-Rahmens zu subsumieren sein. Zudem wird die Entwicklung an Software mit dem Ziel, diese als Open- Source-Software zur Verfügung zu stellen, ausdrücklich als nicht-wirtschaftliche und damit nicht beihilferelevante Tätigkeit eingestuft (Ziff. 20 FuE-Rahmen).\\n\\nWie aus der deutschen Förderlandschaft bekannt, können sich Unternehmen auf solche Projekte bewerben, auch in Verbund mit der Wissenschaft, wenn sie bereit sind, die Software im Open-Source-Verfahren am Ende zur Verfügung zu stellen. Das LEAM-Projekt kann hierbei mehrere Wege wählen, um eine entsprechende beihilfekonforme Ausgestaltung zu erreichen; so kann sie beispielsweise die Software im Open-Source-\\n\\nGroße KI-Modelle für Deutschland\\n\\n202\\n\\nVerfahren zusammen mit Unternehmen oder der Wissenschaft entwickeln, wobei ihr wesentlicher Beitrag die Zurverfügungstellung der Infrastruktur und der technische Support sein kann (siehe beispielsweise Rz. 20, lit. a) ii) des FuE-Rahmens, \"wirksame Zusammenarbeit\"), oder eben allgemein die Entwicklung solcher KI-Foundations-Modelle im Wege einer Ausschreibung/diskriminierungsfreien Vergabe auch durch Unternehmen ermöglichen (siehe die Logiken in Rz. 20, lit. a) iii) und lit. b) des FuE-Rahmens, wonach die öffentliche Ausschreibung sogar von Dienstleistungen durch Unternehmen im Rahmen solcher Projekte den nichtwirtschaftlichen Charakter nicht berührt). Unternehmen können also an der Entwicklung von KI-Foundations-Modelle zusammen mit dem Projekt LEAM beteiligt werden, soweit sie im Open-Source-Verfahren arbeiten und sich damit bereit erklären, entwickelte Software offen und diskriminierungsfrei (jedenfalls im EU- Raum) zur Verfügung zu stellen. Wie dies letztendlich konkret ausgestaltet ist, hängt von der gewünschten Organisationsform ab.\\n\\nFür das Projekt LEAM und seine Ziele kann dies eine erhebliche beihilferechtliche Privilegierung darstellen und eine öffentliche Finanzierung des Projekts begünstigen. Das bedeutet vereinfacht zusammengefasst: Eine staatliche Finanzierung der Infrastruktur des LEAM-Projekts ist jedenfalls dann beihilferechtlich möglich, wenn die Infrastruktur im Open-Source- weit überwiegend der Wissenschaft und/oder kooperierenden, Verfahren arbeitenden Unternehmen, in nicht-diskriminierender Weise zur Verfügung gestellt wird. Abzuklären wäre dabei im Rahmen der Umsetzung noch, ob eine zwingende Verpflichtung für die nutzende Wissenschaft bestehen muss, entwickelte KI-Modelle bzw.- Lösungen im Open Source-Verfahren wiederum der Allgemeinheit zur Verfügung zu stellen - dies wäre wohl jedenfalls beihilferechtlich der „sicherste\" Weg. Bei entwickelnden Unternehmen dürfte nach dem Beihilferahmen viel dafür sprechen, dass eine Open- Source-Nutzung verpflichtend ist, wobei Einzelheiten bei der konkreten Ausgestaltung festzulegen wären.\\n\\nSelbst wenn beispielsweise generierte Modelle oder Wissen im Rahmen des Wissenstransfers in die Wirtschaft veräußert werden, kann dies eine nichtwirtschaftliche Tätigkeit sein, sofern der Gewinn aus der Veräußerung dem nichtwirtschaftlichen Betrieb zugute kommt (ebenfalls Rz. 20 FuE-Rahmen). Das bedeutet also, dass beispielsweise die Weitergabe von Lizenzen für entwickelte Software oder KI-Modelle durch das LEAM- Projekt gegen eine entsprechende Gebühr die öffentliche Förderung im Sinne des EU- Beihilferechts nicht gefährdet, wenn die Gewinne aus diesen Lizenzen wiederum dem nicht-wirtschaftlichen Bereich des LEAM-Projekts zugutekommen (also etwa mehr Rechenzeit oder bessere Infrastruktur für die Wissenschaft bzw. im Open-Source-Modell arbeitende Unternehmen zur Verfügung zu stellen). Insoweit ergibt sich dann kein beihilferechtliches Problem.\\n\\nAndere geplante Tätigkeiten des Projekts LEAM, wie zum Beispiel die Consulting-Angebote oder die Vermietung der Infrastruktur, sind dagegen wirtschaftliche Tätigkeiten. Sie unterliegen den Beschränkungen des EU-Beihilferechts.\\n\\nSofern bei der öffentlichen Finanzierung des LEAM-Projekts danach doch das EU- Beihilferecht eingreift, kommt es zweiten Ebene auf Ausnahmetatbestände an, die ausdrücklich geregelt sind und bei denen keine Genehmigung der EU-Kommission notwendig ist. Welche Ausnahmetatbestände dies sind, lässt sich nicht im Einzelnen vorab festlegen, sondern hängt von der konkreten\\n\\nzunächst auf der\\n\\nGroße KI-Modelle für Deutschland\\n\\n203\\n\\nAusgestaltung des Projekts und der gewünschten Finanzierung ab. Während die De- minimis-Verordnung keine Rolle spielen wird (die Fördergrenzen sind mit max. 200.000 Euro zu niedrig, um für das Projekt relevant zu werden), kann die sogenannte Allgemeine Gruppenfreistellungsverordnung eine Rolle spielen.\\n\\nDie Allgemeinen Gruppenfreistellungsverordnung (Verordnung Nr. 652/2014 der EU- Kommission vom 17.06.2014, ABl. L 187/1 vom 26.06.2014; „AGVO“) enthält in größerem Umfang Tatbestände, bei denen eine begrifflich vorliegende Beihilfe an ein Unternehmen auch ohne Notifizierung (Genehmigung) durch die EU-Kommission beihilferechtskonform ist. Die Regeln sind dabei sehr dezidiert und müssen eingehalten werden, damit keine Notifizierung erforderlich ist. Sie sollen nachfolgend, soweit sie in Betracht kommen, bei den einzelnen Modellen vorgestellt werden.\\n\\nDaneben können Maßnahmen zugunsten von Unternehmen, die mit sogenannten Interesse“ befasst sind, auf „Dienstleistungen von allgemeinem wirtschaftlichen des des Grundlage Freistellungsbeschlusses der EU-Kommission (Beschluss der Kommission (2012/21/EU) vom 20.12.2011, ABl. Nr. L 7/3 vom 11.01.2012) vom EU-Beihilfeverbot ausgenommen sein. Es ist jedoch unwahrscheinlich, dass das LEAM-Projekt (trotz seiner möglichen Bedeutung für die deutsche Wirtschaft) hierunter fallen kann, da diese Rechtsprechung sich eher auf soziale Bedürfnisse (Öffentlicher Nahverkehr, Gesundheitsversorgung o.ä.) bezieht.\\n\\nder\\n\\nAltmark-Trans-Rechtsprechung\\n\\nEuGH\\n\\nbzw.\\n\\nSofern dann auch Regeln wie die AGVO nicht greifen, um die Beihilfe zu rechtfertigen, kommt es auf eine Notifizierung (mithin ein Genehmigungsverfahren) für die Beihilfe bei der EU-Kommission an. Dieses ist rechtlich komplex und kann eine längere Zeit in Anspruch nehmen, wobei die EU-Kommission in der Regel auch Änderungswünsche an der Finanzierungsstruktur bzw. an sonstigen Aspekten der Verwirklichung der geplanten Struktur hat. Einzelheiten eines solchen Verfahrens können an dieser Stelle nicht dargestellt werden und würden den Umfang dieser Studie erheblich überschreiten. Zudem ist die Frage, nach welchen Regeln die EU-Kommission notifiziert, auch maßgeblich davon abhängig, wie die Finanzierungsstruktur des Projekts ausgestaltet wird und welche Zwecke es im Kern verfolgen soll. Sofern die notifizierungspflichtige Beihilfe nach der Zweckrichtung des LEAM-Projekts unter den FuE-Rahmen fallen kann, prüft die EU- Kommission, ob die staatliche Beihilfe zur Förderung von Forschung und Entwicklung als mit dem Binnenmarkt vereinbar angesehen werden kann. Dazu untersucht die Kommission, ob die Beihilfemaßnahme die Entwicklung eines bestimmten Wirtschaftszweigs fördert und ob sie die Handelsbedingungen in einer Weise verändert, die dem gemeinsamen Interesse zuwiderläuft (Rz. 38 FuE-Rahmen). Entscheidend ist hierbei, ob eine Beihilfe eine wesentliche Verbesserung bewirkt, die der Markt selbst nicht herbeiführen kann, insbesondere wenn es im Übrigen ein Marktversagen gibt. Dabei verlangt die Kommission andererseits wiederum, dass die Höhe und die Intensität der Beihilfe auf ein Minimum begrenzt sein muss, was in der Praxis regelmäßig umfangreich geprüft wird. Dies sind nur einige – überblicksmäßige – Zusammenfassungen des Prüfungsprogramms der EU-Kommission im Rahmen eines Notifizierungsverfahrens.\\n\\nGroße KI-Modelle für Deutschland\\n\\n204\\n\\nDie Bedeutung des Vergaberechts Wenn die öffentliche Hand sich Güter und Dienstleistungen beschafft, ist sie in ihrer Auswahl, anders als private Unternehmen, nicht frei, sondern unterliegt den Beschränkungen des Vergaberechts. Dieses dient der Wirtschaftlichkeit und Transparenz staatlichen Handelns sowie einem fairen Wettbewerb um öffentliche Aufträge. Hierzu gibt das Vergaberecht vor, welches Verfahren bei der Auftragsvergabe einzuhalten ist und wie die Auswahl zwischen möglichen Vertragspartnern zu erfolgen hat.\\n\\ninsbesondere öffentliche Verpflichtet, öffentliche Aufträge auszuschreiben, sind Auftraggeber im Sinne von §§ 98, 99 des Gesetzes gegen Wettbewerbsbeschränkungen (GWB). Hierunter fallen nach § 99 Nr. 2 lit. a) GWB unter anderem juristische Personen des öffentlichen und des privaten Rechts, die zu dem besonderen Zweck gegründet wurden, im Allgemeininteresse liegende Aufgaben nichtgewerblicher Art zu erfüllen, sofern sie überwiegend von Gebietskörperschaften, das heißt, etwa von Bund und Ländern, durch Beteiligung oder in sonstiger Weise finanziert werden. Je nach konkreter Ausgestaltung der Finanzierung kann dies auf das LEAM-Projekt zutreffen. Diese Frage wird unten jeweils bezogen auf die einzelnen Finanzierungsmodelle erörtert.\\n\\nAbgesehen von Bagatellgeschäften mit Wert von (je nach Bundesland bzw. Bund) 500 EUR bzw. 1000 EUR (netto) müssen Aufträge öffentlicher Auftraggeber ausgeschrieben werden, und zwar je nach geschätztem Auftragsvolumen entweder bundes- oder – bei liegt der Überschreitung bestimmter Schwellenwerte – europaweit. Derzeit Schwellenwert für Liefer- und Dienstleistungen für öffentliche Auftraggeber bei 215.000 EUR (netto). Es hat eine Veröffentlichung der Ausschreibung zu erfolgen und Bieter können ein Angebot abgeben. Nach der Angebotsauswahl, die bestimmten Vorgaben unterliegt, wird der Auftrag einem Bieter zugeschlagen.\\n\\nKommt das Vergaberecht zur Anwendung, sind seine Vorgaben unbedingt einzuhalten, da Verstöße hiergegen erhebliche finanzielle Konsequenzen verursachen und den zeitlichen Ablauf von Projekten stark verzögern können. Durch die der staatlichen Kontrolle unterliegenden ist die Güter- und Leistungsbeschaffung durch öffentliche Auftraggeber zeitlich und inhaltlich deutlich weniger frei als für rein private Unternehmen.\\n\\nvergaberechtlichen Vorgaben\\n\\nDie Bedeutung des öffentlichen Dienst- und Vergütungsrecht Bei der Entscheidung für ein Finanzierungsmodell sind auch Belange des öffentlichen Dienst- und Vergütungsrechts zu beachten. Im öffentlichen Dienst existieren spezielle Anforderungen an die Personalgewinnung sowie vergütungsrechtliche Bestimmungen. Aber auch für private Unternehmen, die staatliche Förderungen in Anspruch nehmen, können in diesem Bereich Einschränkungen gelten.\\n\\nFür die Personalgewinnung im öffentlichen Dienst ist Art. 33 Abs. 2 Grundgesetz (GG) maßgeblich, der einen gleichen Zugang zu öffentlichen Ämtern garantiert. Daher hat die Stellenbesetzung im öffentlichen Dienst nach dem Prinzip der Bestenauslese zu erfolgen, was zu besonderen Anforderungen an die Ausgestaltung von Bewerbungsverfahren und die Personalauswahl führt. Auch bei der Personalgewinnung sind öffentliche Arbeitgeber daher weniger frei als private Unternehmen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n205\\n\\nWeiter gilt für beim Bund Beschäftigte der Tarifvertrag für den öffentlichen Dienst (TVöD), für Beschäftigte der Länder außerhalb von Branchen mit eigenen Tarifverträgen der Tarifvertrag für hochqualifizierte Kräfte nach diesen Tarifverträgen bleiben weit hinter ihren Verdienstmöglichkeiten in der freien Wirtschaft zurück und sind damit finanziell eher unattraktiv.\\n\\nfür den öffentlichen Dienst der Länder\\n\\n(TV-L). Die Gehälter\\n\\nFür Empfänger hochvolumiger Beihilfen greift oftmals das sog. Besserstellungsverbot. Dabei wird der Bewilligungsbescheid für die Zuwendung mit einer Auflage versehen, die besagt, dass der Zuwendungsempfänger seine Arbeitnehmer:innen nicht besser entlohnen darf als vergleichbare Arbeitnehmer:innen des Zuwendungsgebers. Beispielhaft darf der Empfänger einer Beihilfe des Bundes in einem solchen Fall seine Angestellten nicht besser entlohnen, als es nach dem TVöD der Fall wäre.\\n\\n11.5 Auswirkungen der Rechtsmaterien auf die Finanzierungsmodelle\\n\\nNachfolgend werden die Auswirkungen dieser Rechtsmaterien auf die einzelnen drei grundsätzlichen Finanzierungsmodelle dargestellt.\\n\\nDas Modell der öffentlichen Finanzierung Vorab ein Hinweis zur grundsätzlichen Zulässigkeit: Grundsätzlich ist der Bund befugt, Forschungsinfrastrukturen in eigener Verantwortung zu errichten oder sich an ihrer Finanzierung zu beteiligen. Neben der notwendigen Haushaltsermächtigung durch den Bundestag kommt es auf die konkrete Ausgestaltung der Struktur an, um dahinter liegende verfassungsrechtliche Fragen entsprechend beantworten zu können.\\n\\nIn Art. 91b des Grundgesetzes (GG) wurde verfassungsrechtlich abgesichert, dass Bund und Länder in Fällen überregionaler Bedeutung bei der Förderung von Wissenschaft und insbesondere für die Forschung zusammenwirken können. Die Regelung wurde Finanzierung großer Forschungsinfrastrukturen geschaffen und bietet dem Bund – dann in Zusammenwirken mit einem Sitzland – auch die Möglichkeit, das LEAM-Projekt öffentlich zu finanzieren.\\n\\nBei einem Modell der (rein) öffentlichen Finanzierung des LEAM-Projekts finden alle drei zuvor dargestellten Rechtsmaterien die stärkste Wirkung. Im Einzelnen:\\n\\nEU-Beihilferecht Bei einer rein oder jedenfalls weit überwiegenden öffentlichen Finanzierung gelten die oben genannten Darstellungen zum EU-Beihilferecht. Wie dort dargelegt, kann auf Basis des FuE-Rahmens voraussichtlich argumentiert werden, dass wesentliche geplante Einsätze des KI-Supercomputers nicht-wirtschaftliche Tätigkeiten darstellen und damit die öffentliche insoweit nicht den Beihilfetatbestand erfüllt. Das wäre insoweit vorteilhaft, als einerseits ein komplexes Notifizierungsverfahren bei der EU-Kommission entfällt und andererseits auch die Beschränkungen der AGVO, die in der Regel keine volle staatliche „Durchfinanzierung“ zulässt, nicht greifen würden. In einer öffentlichen Grundfinanzierung könnte, soweit es auch politisch gewollt ist, der Bund zusammen mit einem Sitzland damit grundsätzlich\\n\\nFinanzierung der\\n\\ngeplanten\\n\\nInfrastruktur\\n\\nGroße KI-Modelle für Deutschland\\n\\n206\\n\\nFinanzmittel für den Aufbau der Infrastruktur sowie deren Betrieb (einschließlich Personal) zur Verfügung stellen.\\n\\nDabei sind auch Tätigkeiten aus dieser Infrastruktur heraus, die wirtschaftlicher Natur im Sinne des EU-Beihilferechts sind (z.B. das geplante Consulting-Angebot), nicht von vornherein ausgeschlossen. Der FuE-Rahmen erlaubt auch solche Tätigkeiten, wenngleich auch mit gewissen Anforderungen. Diese sollen nachfolgend überblicksmäßig dargestellt werden:\\n\\nDas wesentlichste Gebot ist die Einführung einer sogenannten Trennungsrechnung. Das bedeutet Folgendes: Übt das LEAM-Projekt sowohl wirtschaftliche als auch fällt die öffentliche Förderung der nichtwirtschaftliche Tätigkeiten aus, so nichtwirtschaftlichen Tätigkeiten nicht unter das Beihilfeverbot, wenn die nichtwirtschaftlichen und die wirtschaftlichen Tätigkeiten und die Kosten, Finanzierung und Erlöse klar voneinander getrennt werden können, sodass keine Gefahr der Quersubventionierung der wirtschaftlichen Tätigkeiten besteht. Das Geld, das vom Bund bzw. einem Land stammt, darf nicht verwendet werden, um Verluste des wirtschaftlichen Bereichs auszugleichen (Verbot der Quersubventionierung und zugleich Gebot der sog. Trennungsrechnung, Ziff. 19 FuE-Rahmen). Eine solche Trennungsrechnung kann im Jahresabschluss durchgeführt werden, bedeutet aber auch, dass Einnahmen und Aufwand für nichtwirtschaftliche und wirtschaftliche Tätigkeiten in der Buchhaltung streng voneinander getrennt werden müssen und stets klar sein muss, ob ein Aufwand (gleich, ob Personal oder Infrastrukturnutzung) dem einen oder dem anderen Bereich zugeordnet werden kann. Das erfordert in der Praxis einiges an Verwaltungsaufwand, ist aber lösbar.\\n\\nDeshalb darf der wirtschaftliche Bereich (also z.B. das Consulting) keinen Verlust erwirtschaften. Ebenso darf der wirtschaftliche Bereich nicht eine Hauptanwendung der öffentlich geförderten Infrastruktur werden, bei der der nichtwirtschaftliche Bereich in den Hintergrund gedrängt wird. Hierzu enthält der FuE-Rahmen noch eine Privilegierung für Forschungseinrichtungen bzw. -infrastrukturen: Wenn die Forschungseinrichtung oder Forschungsinfrastruktur fast ausschließlich für eine nichtwirtschaftliche Tätigkeit genutzt wird, kann ihre Förderung ganz aus dem Anwendungsbereich des Beihilferechts herausfallen, sofern die wirtschaftliche Nutzung eine reine Nebentätigkeit darstellt, die mit dem Betrieb der Infrastruktur unmittelbar verbunden und dafür erforderlich ist oder die in untrennbaren Zusammenhang mit der nichtwirtschaftlichen Haupttätigkeit steht und ihr Umfang begrenzt ist. Das ist der Fall, wenn für die wirtschaftlichen Tätigkeiten dieselben Inputs (wie Material, Ausstattung, Personal und Anlagekapital) eingesetzt werden, wie für die nichtwirtschaftlichen Tätigkeiten und wenn die für die betreffende wirtschaftliche Tätigkeit jährlich zugewiesene Kapazität nicht mehr als 20 % der jährlichen Kapazität der betreffenden Einrichtung bzw. Infrastruktur beträgt (Ziff. 21 FuE-Rahmen). Das würde, sehr grob vereinfacht, für das LEAM KI-Projekt beispielsweise bedeuten, dass 80 % der Rechenkapazität des KI-Supercomputers für nichtwirtschaftliche Aktivitäten benutzt werden dürfen (z.B. für die Entwicklung großer KI Foundation-Modelle mit Open Access für die Wirtschaft bzw. Wissenschaft), 20 % dann für wirtschaftliche Zwecke. Einzelheiten sind dann für den tatsächlichen Betrieb auszuarbeiten. Es zeigt sich aber, dass auch bei einer rein öffentlichen Finanzierung wirtschaftliche Tätigkeiten dem Projekt LEAM nicht untersagt sind und im Geschäftsmodell weiter eingeplant werden können. In\\n\\nGroße KI-Modelle für Deutschland\\n\\n207\\n\\nder Praxis gibt es solche Fälle zum Beispiel bei Hochschulen, deren Infrastruktur öffentlich finanziert ist, die aber gleichwohl wirtschaftliche Angebote wie Auftragsforschung oder forschungsnahe Dienstleistungen haben.\\n\\nNachdem eine Abgrenzung von wirtschaftlicher zu nichtwirtschaftlicher Tätigkeit vorgenommen wurde, muss sichergestellt werden, dass wirtschaftliche Tätigkeiten nicht zu einer Quersubventionierung führen. Dies bemisst sich am sog. Private-Investor-Test. Dabei wird das wirtschaftliche Handeln der staatlichen Stelle mit dem hypothetischen Verhalten eines Privatinvestors verglichen. Würde ein solcher den Vorteil nicht oder zu ungünstigeren Konditionen anbieten, liegt eine Beihilfe nach Art. 107 Abs. 1 AEUV vor. Der FuE-Rahmen enthält insoweit dezidierte Regelungen (vgl. Ziff. 26f. FuE-Rahmen), wonach die Preisermittlung sich entweder nach dem Marktpreis oder nach einer Vollkostenkalkulation mit Gewinnaufschlag richten muss. Gleichwohl können die Kosten, die das LEAM-Projekt für die Wirtschaft für solche wirtschaftlichen Tätigkeiten berechnet, betriebswirtschaftlich deutlich niedriger liegen als vergleichbare Angebote aus den USA, da sich auch beihilferechtlich das LEAM-Projekt bei diesen wirtschaftlichen Aktivitäten nicht an dem Gedanken der Profitmaximierung orientieren muss, sondern an dem Gedanken Vollkosten + angemessener Gewinnaufschlag. Letzterer dürfte deutlich liegen als bei Angeboten von US-Anbietern, die gegebenenfalls eine niedriger monopolähnliche Stellung haben.\\n\\nZusammengefasst bedeutet dies mithin, dass eine öffentliche Finanzierung des Projekts LEAM und deren Infrastruktur beihilferechtlich vor allem dann denkbar ist, wenn die geschaffene Infrastruktur überwiegend für nichtwirtschaftliche Zwecke genutzt und der Wissenschaft bzw. kooperierenden Unternehmen zur Verfügung gestellt wird. Eine gewisse Kapazität des Supercomputers und des Personals kann gleichwohl auch für wirtschaftliche Tätigkeiten zur Verfügung gestellt werden, wobei diese wirtschaftlichen Tätigkeiten in der Regel keinen Verlust erwirtschaften und zu einem Marktpreis angeboten werden müssen. Eine Trennungsrechnung muss buchhalterisch und im Jahresabschluss klarstellen, dass keine Quersubventionierung wirtschaftlicher Tätigkeiten erfolgt.\\n\\nVergaberecht Das Vergaberecht ist bei einer rein öffentlichen Finanzierung voll einschlägig.\\n\\nWird das LEAM-Projekt nach dem ersten in Betracht kommenden Modell vollständig öffentlich finanziert, dann handelt es sich bei dem für das LEAM-Projekt zu schaffenden Rechtsträger um einen öffentlichen Auftraggeber im Sinne von §§ 98, 99 GWB. In diesem Fall sind die in Kapitel 11.4 aufgeführten Voraussetzungen von § 99 Nr. 2 lit. a) GWB erfüllt, denn es handelt sich in diesem Fall um eine juristische Person, die zu dem besonderen Zweck gegründet wurden, im Allgemeininteresse liegende Aufgaben nichtgewerblicher Art zu erfüllen und die mindestens überwiegend – hier: vollständig – von Gebietskörperschaften finanziert wird.\\n\\nDabei ist das Merkmal der Aufgaben nichtgewerblicher Art selbst dann erfüllt, wenn neben der Kernaufgabe der KI-Forschung und -Entwicklung im Rahmen des LEAM- Projekts auch Consulting oder die Vermietung von Rechenzeit angeboten werden, da es für die Frage der Nichtgewerblichkeit auf eine Gesamtschau ankommt, die Faktoren wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n208\\n\\neine Entkoppelung von Wettbewerb und Marktmechanismen und das Vorliegen einer treten Nebenbetätigungen wie Gewinnerzielungsabsicht berücksichtigt. Consulting und Rechenzeitvermietung hinter der weit überwiegenden, nicht auf Gewinnerzielung ausgerichteten Haupttätigkeit zurück.\\n\\nInsoweit\\n\\nÖffentliches Dienst- und Vergütungsrecht Beim Modell der rein öffentlichen Finanzierung gelten Art. 33 Abs. 2 GG und die Einschränkungen des öffentlichen Vergütungsrechts uneingeschränkt.\\n\\nDie Geltung des verfassungsrechtlichen Grundsatzes der Bestenauslese führt dazu, dass eine rechtssichere Gestaltung von Bewerbungsverfahren einen gewissen Mehraufwand bedeutet. So muss beispielsweise für zu besetzende Stellen ein Anforderungsprofil festgelegt und Fragen im Bewerbungsgespräch hieran ausgerichtet werden und es gelten Dokumentationspflichten. Stellenausschreibungen müssen zudem fristgebunden sein.\\n\\nJe nach Struktur des Projekts kommen zudem TVöD bzw. TVL zur Anwendung mit der Folge, nur vergleichbar unattraktiv entlohnen zu können. Für Angestellte bei rein oder überwiegend staatlich finanzierten Unternehmen, auch wenn diese eine privatrechtliche ist einzuhalten bei einer Form haben, greift das Besserstellungsverbot. Dieses institutionellen Förderung (Zuwendungen zur Deckung der gesamten oder eines nicht abgegrenzten Teils der Ausgaben des Zuwendungsempfängers) sowie bei Projektförderung, wenn die Gesamtausgaben des Zuwendungsempfängers überwiegend aus Zuwendungen der öffentlichen Hand bestritten werden (vgl. § 8 Haushaltsgesetz 2021). Der Bundesfinanzminister kann bei Vorliegen zwingender Gründe zwar Ausnahmen erlassen, dies wird jedoch restriktiv gehandhabt.\\n\\nFür das EU-Beihilferecht und das öffentliche Dienst- und Vergütungsrecht kann jedoch noch ein gemeinsamer Hinweis gegeben werden: Selbst bei einer rein oder überwiegend ist es möglich, dass neben der öffentlichen Finanzierung des LEAM-Projekts Trägerstruktur des LEAM-Projekts auch eine Service-GmbH errichtet wird, die sich auf wirtschaftliche Aktivitäten fokussiert und zu deren Erbringung Infrastruktur des LKS anmietet (unter Berücksichtigung der oben dargestellten Kapazitätsbeschränkungen). Die Ausgliederung von wirtschaftlichen Aktivitäten im Sinne des EU-Beihilferechts in eine wirtschaftliche Einheit ist beihilferechtlich bei Einhaltung der Regeln zum Verbot der Quersubventionierung und der Kapazitätsbeschränkungen für die wirtschaftliche Nutzung geförderter Infrastrukturen grundsätzlich zulässig.\\n\\nGroße KI-Modelle für Deutschland\\n\\n209\\n\\nSie ist in der deutschen Wissenschaftslandschaft erprobt und bietet verschiedene Vorteile:\\n\\nDie Trennungsrechnung, die die geförderte Einrichtung führen muss, wird deutlich vereinfacht, wenn alle wirtschaftlichen Tätigkeiten in eine dafür eingerichtete GmbH ausgegliedert sind.\\n\\nDie rechtlich von der geförderten Struktur getrennte wirtschaftliche Einheit (also beispielsweise eine gesonderte Service-GmbH) genießt größere Handlungsspielräume in Fragen der Vergütung und Preisbestimmung. Insbesondere kann sie Beschäftigte der geförderten nichtwirtschaftlichen Einheit beispielsweise in Nebentätigkeit beschäftigen und vergüten, um auf diese Weise im Rahmen des rechtlich Zulässigen ein interessantes Vergütungspaket zu bieten. Da zudem die Beschäftigten dann mit der Nebentätigkeit noch eigene Anstrengungen unternehmen müssen, um diese Zusatzvergütung zu erreichen, ist in der Regel auch in der Außendarstellung und in der Politik eine Akzeptanz solcher Lösungen erreichbar. Diese Vorgehensweise ist insbesondere in der Wissenschaft erprobt. Rechtliche Parameter müssen im Einzelnen ausgearbeitet werden.\\n\\n\\n\\nIn der Umsetzung würden Infrastruktur/Kapazitäten der wirtschaftlich orientierten Einheit/GmbH von der geförderten Einheit angemietet, was beihilferechtlich grundsätzlich zulässig ist und eine klare Trennung zwischen wirtschaftlichen und nichtwirtschaftlichen Bereichen ermöglicht.\\n\\nIm Rahmen der konkreten Ausgestaltung kann es vor diesem Hintergrund sinnvoll sein, die Trennung von nichtwirtschaftlichen und wirtschaftlichen Aktivitäten des Projekts LEAM in zwei rechtlich getrennten Strukturen zu untersuchen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n210\\n\\nSPOTLIGHT Ubermetrics Technologies GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie Ubermetrics Content Intelligence-Plattform basiert auf einer für die Bedürfnisse von Kommunikatoren speziell entwickelten und trainierten Künstlichen Intelligenz, welche öffentlich verfügbare Inhalte und Insights für effektive Daten Kommunikationsstrategien der umwandelt. Mit Verarbeitung von über 50.000 Artikeln pro Minute und ist Inhalte von mehr als 460 Millionen Quellen Ubermetrics die Intelligence Plattform für Marketing- und PR-Experten.\\n\\nin aussagekräftige\\n\\nführende Content\\n\\nPatrick Bunk, Founder und CEO von Ubermetrics.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use- Case?\\n\\nUbermetrics extrahiert Texte aus Millionen von Internetquellen und wertet diese unter dem Einsatz menschlicher Experten und automatisiert durch KI-Verfahren für Kunden aus. Wir setzen Foundation-Modelle bislang begrenzt in einigen Teilen der Ergebnisdarstellung ein. Beispielsweise setzen wir ein RoBERTa Modell ein, um automatisiert Entitäten (bspw. Personen, Orte, Produkte etc.) in Texten zu erkennen, um diese für die weitere Analyse nutzbar zu machen (sog. Named Entity Recognition / -Linking). Allerdings schöpfen wir damit die Möglichkeiten der Modelle für unsere Produkte erst ansatzweise aus.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle sind von überragender Bedeutung für die Wettbewerbsfähigkeit und den Erfolg von Ubermetrics, bzw. UNICEPTA. Bislang mussten wir aufwändig für jeden einzelnen Aspekt in unserer Produktionskette und teilweise für jede Sprache ein neues KI-Verfahren implementieren oder selbst erforschen (bspw. Sentimentanalyse). Diese könnten zu einem großen Teil durch auf Foundation-Modellen basierten Verfahren ersetzt werden- bei gleichzeitiger Verbesserung der Treffsicherheit. Das betrifft (bspw. sowohl die Textsammlung als auch die Aufbereitung Spracherkennung) und die Auswertung. Foundation-Modelle erlauben uns nun eine umfassende Basis, auf der wir die Entwicklung besserer Verfahren und die Entwicklung neuer Produkte aufsetzen können. Für uns sinkt also einerseits der Aufwand in der Produktion und wir werden andererseits in die Lage versetzt, bessere bzw. für uns gänzlich neue Produkte und Features anbieten zu können. Bspw. könnten wir Summaries über mehrere Artikel hinweg und Abstracts von Artikeln teilautomatisieren, um unsere Wissensarbeiter:innen bei weniger anspruchsvollen Tätigkeiten maschinell zu unterstützen. Wir schätzen den wirtschaftlichen Wert durch die Implementation auf 60 % unseres Umsatzes ein.\\n\\nGroße KI-Modelle für Deutschland\\n\\n211\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Die größten Schwierigkeiten und Probleme liegen in dem Verlust digitaler Souveränität und in der schwächeren Unterstützung relevanter europäischer Sprachen. Sollten wir gezwungen sein, außereuropäische Modelle über APIs zu nutzen, müssten wir dazu relevante und zum Teil sensible Geschäftsdaten preisgeben. Dies ist nur schwer vereinbar mit europäischen Datenschutzstandards und ein Hauptgrund für unseren bisher nur sporadischen Einsatz solcher Modelle.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Von europäischen Open-Source-Modellen erhoffen wir uns neben der besseren Sprachabdeckung u.a. durch die europäischen Datenschutzstandards eine größere Akzeptanz unserer Kunden. Gleichzeitig erwarten wir eine bessere Planbarkeit in Bezug auf Weiterentwicklungen.\\n\\nDas Modell der privaten Finanzierung Im Gegensatz zur rein öffentlichen Kapitalbeschaffung steht grundsätzlich das Modell der privaten Finanzierung bzw. der überwiegenden privaten Beherrschung der gewählten Struktur.\\n\\nAngenommen, es werden keine staatlichen Mittel in Anspruch genommen, sondern das Modell wird rein privat finanziert und betrieben, so gelten die oben genannten Beschränkungen nicht. Das LEAM-Projekt muss zwar in diesem Fall letztlich „sich selbst tragen\" bzw. (wenn nicht die gemeinnützige Form gewählt wird) und zum wirtschaftlichen Überleben sogar Gewinne erwirtschaften, doch greifen in diesem Fall weder beihilferechtliche noch vergaberechtlich noch tarifliche Beschränkungen. Das LEAM- Projekt kann (und muss) dann „frei am Markt\" agieren. Aufträge können schnell vergeben werden, am Markt übliche Gehälter bezahlt werden. Das gilt selbst dann, wenn aus steuerrechtlichen Gründen die Wahl auf eine gemeinnützige Gesellschaft fällt (mit bestimmten Einschränkungen des Gemeinnützigkeitsrechts, die hier nicht abschließend dargestellt werden können).\\n\\nEU-Beihilfenrecht Einer solchen privat strukturierten bzw. überwiegend privat finanzierten Einrichtung ist es dann wiederum möglich, sich allgemein auf Fördermittel für wissenschaftliche Infrastruktur und Projekte, im Wege der Projektförderung zum Beispiel bei der DFG, bei dem Bundesministerium für Forschung und Bildung oder bei der EU zu bewerben – so wie jeder anderen öffentlichen oder privaten Einrichtung auch. Diese Projektförderung wird jedoch nicht nur kompetitiv für begrenzte Laufzeiten vergeben (das heißt die Mittel sind zeitlich beschränkt und ihr Erhalt aufgrund des Wettbewerbscharakters öffentlicher Ausschreibungen nicht gesichert), sondern ist dann auch gegebenenfalls wieder mit öffentlich-rechtlichen Pflichten verbunden. Das können beispielsweise wiederum Beschränkungen in der Vergütung sein (gegebenenfalls gilt das Besserstellungsverbot, s. oben) oder es sind bei der Vergabe der Mittel an Unternehmen durch die geförderte Infrastruktur (also hier dann das LEAM-Projekt, das beispielsweise Rechenkapazitäten einkaufen will) öffentliche Ausschreibungen durchzuführen. Das hängt von dem\\n\\nGroße KI-Modelle für Deutschland\\n\\n212\\n\\njeweiligen Fördertopf, der in Anspruch genommen werden soll, und der Förderintensität ab.\\n\\nEs ist jedoch nicht ausgeschlossen, dass auch der Bund oder ein Land dann wiederum Finanzierungshilfe leisten, wenn die zugrundeliegende Einrichtung ihrem Wesen nach privat finanziert und in der Gesellschafterstruktur auch betrieben werden soll. Deutschland kennt zahlreiche Förderinstrumente für Start-Ups und Unternehmen mit Technologiebezug, die in ihrer Bandbreite hier jedoch nicht alle dargestellt werden können. Auch der bekannte High-Tech-Gründerfonds gehört dazu, ebenso wie Kreditprogramme der staatlichen Förderbank KfW auf Bundesebene oder der Förderbanken auf Landesebene. Unabhängig davon, dass sich hieraus auch wiederum beihilferechtliche Prüfungen ergeben können (dazu sogleich), können solche Mittel oftmals nur eine „Anschubfinanzierung\" von beschränktem Umfang sein, neben die erhebliche Privatmittel treten müssen (siehe auch die Ausführungen zur AGVO und zum Public-Private-Partnership, dazu sogleich).\\n\\nSolche Finanzierungen müssen sich dann, wenn sie einen erheblichen Umfang haben sollen, voraussichtlich als „Beihilfen\" für ein Unternehmen vor allem an den Regelungen der AGVO messen lassen, soll ein langwieriges Notifizierungsverfahren vermieden werden. Beihilfen, die auf Grundlage der AGVO gewährt werden, müssen bestimmte Transparenzvorgaben erfüllen und einen Anreizeffekt haben. Außerdem müssen Informationen zu den Beihilfen auf einer „Beihilfe-Website“ des Mitgliedstaates bzw. seiner handelnden Körperschaft veröffentlicht und der Kommission mitgeteilt werden. Die Mitgliedstaaten sind zum Monitoring verpflichtet. Für die Förderung des LKS könnten insbesondere folgende Kategorien der AGVO relevant sein:\\n\\nArt. 17 AGVO: Investitionsbeihilfen für KMU Bis zu 20 % der Investitionskosten kleiner bzw. 10 % der Investitionskosten mittlerer Unternehmen für immaterielle und materielle Investitionsgüter. Darunter fallen u. a. die Errichtung einer neuen Produktionsstätte sowie die Deckung von Lohnkosten der durch das Investitionsvorhaben geschaffenen Arbeitsplätze.\\n\\nArt. 18 AGVO: Investitionsbeihilfen für KMU bei der Inanspruchnahme von Beratungsdiensten Mit einer Höhe von bis zu 50 % der Kosten dürfen Beratungsleistungen externer Berater bezuschusst werden, insofern es sich dabei nicht um gewöhnliche Werbungskosten und fortlaufende Dienstleistungen handelt.\\n\\nArt. 26 AGVO: Investitionsbeihilfen für Forschungsinfrastrukturen Bau und Ausbau von Forschungsinfrastrukturen können mit bis zu 50 % der Kosten als bevorzugte Beihilfe nach den Regeln der AGVO gefördert werden. Der Preis für Betrieb und Nutzung der so geförderten Infrastruktur muss dem Marktpreis entsprechen. Die Infrastruktur muss außerdem mehreren Nutzer:innen offenstehen und der Zugang muss transparenten und diskriminierungsfreien Bedingungen gewährt werden. zu Unternehmen, die mindestens 10 % der Investitionskosten der Infrastruktur finanziert haben, können einen bevorzugten Zugang erhalten. Wenn eine Forschungsinfrastruktur ist eine sowohl wirtschaftliche als auch nichtwirtschaftliche Tätigkeiten ausübt, Trennungsrechnung zwingend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n213\\n\\nArt. 28 AGVO: Innovationsbeihilfen für KMU Bis zu 50 % der Kosten, die kleinen und mittleren Unternehmen u. a. für die Abordnung hochqualifizierten Personals für Innovationsberatungsdienste innovationsunterstützende Dienstleistungen entstehen, dürfen als AGVO-Beihilfen finanziert werden. Bis zu einem Betrag von 200.000 EUR in drei Jahren dürfen auch bis zu 100 % der Kosten für Innovationsberatungsdienste und innovationsunterstützende Dienstleistungen finanziert werden.\\n\\nfür Forschungs- und Wissensverbreitung oder oder\\n\\nVergaberecht Bei einer rein privaten Finanzierung ist das LEAM-Projekt kein öffentlicher Auftraggeber, sodass bei seiner Beschaffung von Leistungen das Vergaberecht nicht zur Anwendung kommt. Das führt zu deutlich freieren Beschaffungsvorgängen.\\n\\nGegebenenfalls kann es im Rahmen einer von einer privaten Einheit beantragten Projektfinanzierung dazu kommen, dass die Fördermittelbedingungen Vorschriften zur öffentlichen Vergabe anzuschaffender Infrastruktur enthalten. Das kann, muss aber nicht der Fall sein.\\n\\nÖffentliches Dienstrecht Das öffentliche Dienstrecht kommt in diesem Fall in Bezug auf die Personalgewinnung nicht zur Anwendung. Da das Besserstellungsverbot nur bei überwiegend öffentlicher Finanzierung greift, entfällt dieses bei einer (überwiegend) privaten Finanzierung. In Bezug auf die Arbeitnehmer:innengewinnung ist eine rein private Finanzierung insgesamt deutlich freier und eine höhere Vergütung für Arbeitnehmer:innen attraktiver.\\n\\nDas Modell der Public-Private-Partnership Die Mischform zu den vorhergehenden Modellen stellt das Konzept der Public-Private- Partnership, also der (gesellschafts-)rechtlich statuierten Zusammenarbeit zwischen Privaten und dem Staat dar. In der Praxis sind hier verschiedenste Ausprägungen denkbar.\\n\\nfinanziellen und/oder\\n\\nBevor die Anwendbarkeit und Bedeutung von Beihilfen- Vergabe- und öffentlichem Dienstrecht auf das Modell der Public-Private-Partnership dargestellt werden kann, ist zunächst zu erklären, was eine solche ist, welche Formen möglich sind und inwieweit der Staat sich an einer solchen beteiligen darf.\\n\\nBegriffserklärung Eine sog. Public-Private-Partnership (nachfolgend auch „PPP\"), zu Deutsch Öffentlich- private-Partnerschaft, kurz ÖPP, stellt eine Mischform zwischen einer rein öffentlichen Finanzierung und Struktur und einer ausschließlich privaten Ausgestaltung dar.\\n\\nEs handelt sich bei einer PPP um eine partnerschaftliche Zusammenarbeit zwischen der öffentlichen Hand und privaten Unternehmen zur Realisierung eines öffentlichen Projekts oder Erbringung von Leistungen, die der Erfüllung einer öffentlich-rechtlichen Aufgabe dienen.\\n\\nFür die Betrachtung der rechtlichen Möglichkeiten, Anforderungen und einschlägigen Vorschriften kommt es auf die konkrete Ausgestaltung der jeweiligen Kooperation an. Für\\n\\nGroße KI-Modelle für Deutschland\\n\\n214\\n\\nPPP existieren nur lückenhafte Regelungen in verschiedenen Gesetzen, aber keine umfassenden gesetzlichen Vorgaben.\\n\\nAusgestaltungsmodelle Für die konkrete Ausgestaltung von PPP kommen zahlreiche verschiedene Organisationsmodelle in Frage. Wiederum sind diese nicht gesetzlich definiert, sondern haben sich in der Praxis herausgebildet. Dabei erfolgt die Kategorisierung von Organisationsmodellen häufig uneinheitlich.\\n\\nAnzahl, Namen und genaue Beschreibung der Organisationsformen unterscheiden sich je nach Betrachtung. Im Groben kann aber folgende Kategorisierung erfolgen:\\n\\nKonzessionsmodell Beim Konzessionsmodell überträgt die öffentliche Hand einem privaten Unternehmen eine öffentliche Aufgabe. Der Private betreibt die betroffene Einrichtung dabei in eigenem Namen, auf eigene Rechnung und auf eigenes wirtschaftliches Risiko. Nur er tritt hierbei gegenüber Dritten auf. Er generiert Einnahmen dadurch, dass er von Dritten für die Nutzung einer Sache bzw. die Inanspruchnahme einer Leistung ein Entgelt verlangt. Im Falle von LEAM wäre insoweit die Vermietung von Rechenzeit sowie Consulting möglich. Zum Teil wird bei Anwendung des Konzessionsmodells zudem vereinbart, dass die öffentliche Hand als Konzessionär an den Privaten eine Konzessionsabgabe entrichtet. Bei diesem Modell ist allerdings anzumerken, dass für eine solche Konzessionsvergabe eine europaweite Ausschreibung nötig ist.\\n\\nDie Problematik besteht bei einem Konzessionsmodell jedoch darin, dass es sich bei dem Betrieb des KI-Hochleistungsrechenzentrums denklogisch um eine (zwingende) öffentliche Aufgabe handeln muss, die dem Staat obliegt (im Gegensatz zu einer staatlichen Förderung von wünschenswerten Aktivitäten, zum Beispiel in der Wissenschaft). Das erscheint derzeit nur schwer darstellbar, bedarf jedoch auch einer Erörterung mit politisch Verantwortlichen, ob sie gegebenenfalls einen solchen Tatbestand durch Verordnung oder Gesetz schaffen wollen. Bis dahin sind Zweifel an der Anwendbarkeit des Konzessionsmodells gegeben.\\n\\nLeasingmodell Beim Leasingmodell beauftragt die öffentliche Hand eine Privatperson oder eine Leasinggesellschaft mit der Planung, Errichtung und Finanzierung eines Vorhabens gegen Zahlung einer vereinbarten Leasingrate. Die Privatperson wird Eigentümer:in bzw. Inhaber:in des Objekts, aber gewährt der öffentlichen Hand das Nutzungsrecht während der vereinbarten Leasingdauer. Hierzu erfolgt gegebenenfalls ein Widmungsakt, mit dem das Objekt zu einer öffentlichen Sache wird.\\n\\nBetreibermodell Beim Betreibermodell verwirklicht eine private Organisation vertraglich festgelegte Teilaufgaben in Bezug auf Planung, Entwicklung, Betrieb und Finanzierung und erhält hierfür ein vereinbartes Entgelt. Dabei ist sowohl möglich, dass der Private eher als technischer Erfüllungsgehilfe nur im Innenverhältnis zum Verwaltungsträger auftritt, als auch, dass er, näher am Konzessionsmodell, auch gegenüber Dritten agiert und von diesen ein Entgelt erhebt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n215\\n\\nBetriebsführungsmodell Beim Betriebsführungsmodell betreibt eine private Organisation eine öffentliche Einrichtung namens und im Auftrag der öffentlichen Hand auf deren Rechnung und deren Risiko. Er tritt gegenüber Dritten also nur in fremden Namen auf.\\n\\nDie vorgenannten Modelle können unter dem Oberbegriff der „Vertragsmodelle\" zusammengefasst werden. Ihnen ist gemein, dass öffentliche Hand und Private hier auf vertraglicher Basis miteinander kooperieren, es jedoch nicht zur Schaffung eines gemeinsamen Rechtssubjektes kommt.\\n\\nBeteiligungs- oder Kooperationsmodell Anders liegt es bei dem sog. Beteiligungs- oder Kooperationsmodell. Bei diesem gründen die öffentliche Hand und private Akteure eine gemeinsame Gesellschaft. An dieser beteiligen sich sowohl öffentliche Stellen (z.B. Bund und/oder Bundesländer) als auch private Unternehmen als Gesellschafter:innen. Aufgrund dieser Schaffung einer neuen Gesellschaft als von ihren Gesellschaftern zu unterscheidende eigenständige Rechtspersönlichkeit wird dieses Modell auch als „institutionelle PPP\" bezeichnet, aufgrund der gemeinsamen Inhaberschaft von öffentlichen und privaten Rechtsträgern auch als „gemischtwirtschaftliches Unternehmen\". Es wird ein Gesellschaftsvertrag abgeschlossen. Neben der Rechtsform der Gesellschaft (s. Kapitel 12) festgelegt, wie groß der Anteil der Gesellschaftsanteile ist, den die jeweiligen Gesellschafter:innen tragen. Dabei kann der Anteil der öffentlichen Hand unterschiedlich groß sein. Sie kann, was in der Praxis häufig vorkommt, Mehrheitsanteilseignerin sein; möglich ist aber auch eine Beteiligung zu einem Anteil von weniger als 50 %.\\n\\nist\\n\\nin diesem zunächst\\n\\nDer Gesellschaftsvertrag enthält weitere Regelungen zur Geschäftsführung und Vertretung der Gesellschaft, zu Rechten und Pflichten der Gesellschafter:innen, zur Verteilung von Gewinn und Verlust, zur Besetzung von Gesellschaftsorganen. Von den gesellschaftsrechtlichen Einflussnahmemöglichkeiten verspricht man sich auch eine verbesserte Steuerungs- und Kontrollmöglichkeiten durch die öffentliche Hand Zu Vor- und Nachteilen verschiedener Gesellschaftsformen (s. Kapitel 12).\\n\\nNach dem Verständnis von LEAM als auf Dauer angelegtes Projekt erscheint die Gründung einer den möglichen Ausgestaltungsformen von PPPs. Daher fokussieren sich die folgenden Ausführungen auf die rechtlichen Auswirkungen einer solchen Ausgestaltung.\\n\\ngemeinsamen Gesellschaft\\n\\nals\\n\\npassendste\\n\\nunter\\n\\nVoraussetzungen für öffentlich-private Kooperationen Bei der Erfüllung seiner Aufgaben hat der Staat grundsätzlich einen weiten Gestaltungsspielraum. Das betrifft auch die Frage, ob er in privatrechtlicher Form tätig wird und inwieweit er private Akteure zur Wahrnehmung staatlicher Aufgaben heranziehen kann. Abgesehen von für LEAM nicht einschlägigen Ausnahmebereichen ist daher auch die Beteiligung an Public-Private-Partnerships möglich.\\n\\nAuch weitere haushaltsrechtliche Regeln für die Beteiligung der öffentlichen Hand an Unternehmen – etwa die Begrenzung der Einzahlungsverpflichtung auf einen bestimmten in den Betrag und die Sicherung eines angemessenen staatlichen Einflusses Überwachungsorganen der Gesellschaft können durch das LEAM-Projekt eingehalten werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n216\\n\\nEU-Beihilfenrecht Im Hinblick auf das EU-Beihilfenrecht ergeben sich für Public-Private-Partnerships keine speziellen Rechtsfragen. Durch die Beteiligung des Staates und die Finanzierung des Projekts mit staatlichen Mitteln bleibt das Beihilferecht grundsätzlich anwendbar, wobei sich jedoch für die Finanzierung nichtwirtschaftlicher Aktivitäten die gleichen Maßstäbe ergeben wie bei der rein öffentlichen Finanzierung.\\n\\nFür die Unterscheidung zwischen wirtschaftlichen und nichtwirtschaftlichen Tätigkeiten – die entscheidend für die Frage des Vorliegens einer staatlichen Beihilfe ist – gelten, da es insoweit nicht auf die Rechtsform und die Art der beteiligten Gesellschafter:innen ankommt, sondern die Abgrenzung tätigkeitsbezogen erfolgt, die in Kapitel 11.4 ausgeführten Grundsätze. Auch insoweit ist also der FuE-Rahmen maßgeblich. Dessen Anforderungen, insb. das Gebot der Einführung einer Trennungsrechnung und das Verbot einer Quersubventionierung wirtschaftlicher Tätigkeiten sind auf PPP zu übertragen. Möglich ist allerdings, die Struktur hier bei einer Finanzierung Privater so auszugestalten, dass im stärkeren Umfang wirtschaftliche Aktivitäten möglich sind; dies insbesondere dann, wenn die privaten Investoren sich bereit erklären, etwaige Verluste der wirtschaftlichen Tätigkeiten selbst auszugleichen (so dass aus staatlichen Mitteln keine Quersubventionierung erfolgt). Einzelheiten müssen dann bei der konkreten Ausgestaltung evaluiert werden.\\n\\nSoweit es sich begrifflich um eine Beihilfe handelt, kann auch beim PPP- Finanzierungsmodell eine Ausnahmevorschrift greifen. Insoweit kommen insb. die oben aufgeführten AGVO-Beihilfen in Betracht. Liegt eine nicht unter Ausnahmevorschriften fallende staatliche Beihilfe vor, ist ein Notifizierungsverfahren durchzuführen.\\n\\nVergaberecht Für die Frage, inwieweit das Vergaberecht auf PPPs Anwendung findet, sind verschiedene Themenkomplexe voneinander abzugrenzen: Zu unterscheiden sind einerseits eine Zusammenarbeit, die einem der oben unter dargestellten Vertragsmodelle (Konzessions- , Leasing-, Betreiber- oder Betriebsführungsmodell) zuzuordnen ist, andererseits PPPs nach dem Beteiligungsmodell, bei dem eine gemeinsame Gesellschaft gegründet wird. Bei diesen wiederum ist danach zu differenzieren, ob es um die Anwendbarkeit von Vergaberecht auf die Gesellschaftsgründung oder um die Beschaffung von Leistungen durch die bestehende Gesellschaft geht.\\n\\nFür Kooperationen nach einem der Vertragsmodelle kann aufgrund der Vielfältigkeit möglicher Modelle im derzeitigen Stadium keine vollständige vergaberechtliche Bewertung erfolgen. Da ihnen aber gemein ist, dass der Staat hier alle oder bestimmte Teile der Projektführung auf Private überträgt, wird es sich bei dem Projektvertrag häufig um einen öffentlichen Auftrag im Sinne von § 103 Abs. 1 GWB oder eine Konzession nach § 105 Abs. 1 GWB handeln. Daher muss dem Abschluss eines Projektvertrags mit einem privaten Kooperationspartner in aller Regel ein formelles Vergabeverfahren vorausgehen, aufgrund des hohen Projektvolumens mit europaweiter Bekanntmachung.\\n\\nFür die Gründung einer Joint Venture (Beteiligungsmodell) stellt sich die Rechtslage folgendermaßen dar: Die Gründung der gemischtwirtschaftlichen Gesellschaft selbst stellt grundsätzlichen keinen beschaffungsrelevanten Vorgang dar, unterliegt also nicht\\n\\nGroße KI-Modelle für Deutschland\\n\\n217\\n\\ndem Vergaberecht. Anders liegt es jedoch, wenn mit dem Gründungsakt zugleich eine unmittelbare Übertragung von Aufgaben von der öffentlichen Hand an die Gesellschaft einhergeht. In diesem Fall wird das beteiligte private Unternehmen nicht nur Gesellschafter, sondern zugleich auch Leistungserbringer gegenüber dem Staat. Im Rahmen einer vorzunehmenden Gesamtbetrachtung stellt sich dieses Gesamtkonstrukt insgesamt als ausschreibungspflichtiger Vorgang in Form eines öffentlichen Auftrags im Sinne von § 103 Abs. 1 GWB bzw. als Konzession nach § 105 Abs. 1 GWB dar, da Gesellschaftsgründung und Aufgabenwahrnehmung ein unteilbares Ganzes sind. Dasselbe gilt, wenn ein gemischtwirtschaftliches Unternehmen durch die öffentliche Hand und private Unternehmen gegründet wird, um eigene Aufgaben in einem entsprechend definierten Bereich zu übernehmen und die Leistungen im Anschluss durch die Gesellschafter erbracht werden.\\n\\nWerden später – von der Gesellschaftsgründung zu unterscheiden – durch die Gesellschaft für sie erforderliche Aufträge in einem nachgelagerten Schritt an Dritte vergeben, so ist die Gesellschaft in der Regel selbst als öffentlicher Auftraggeber nach § 99 Nr. 2 lit. a) GWB einzuordnen. Beschafft sie sich Leistungen am Markt, so unterliegt dieser Beschaffungsakt in aller Regel den vergaberechtlichen Vorgaben. Die oben in Kapitel 11.4 dargestellten Voraussetzungen dieser Vorschrift liegen vor. Insbesondere folgende sind gegeben: Wie auch im Rahmen der Ausführungen zum Vergaberecht bei rein öffentlicher Finanzierung dargestellt ist bei den im LEAM-Projekt geplanten im Aktivitäten Allgemeininteresse liegenden Aufgaben nichtgewerblicher Art auszugehen. Insoweit ergeben sich keine Unterschiede. Die weitere Voraussetzung des § 99 Nr. 2 lit. a) GWB, dass die Gesellschaft überwiegend von Gebietskörperschaften – hier voraussichtlich Bund und Standortland – finanziert wird, liegt vor, wenn – wie zu erwarten – die Finanzierung zu mehr als 50 % staatlich ist.\\n\\nim Rahmen der vorzunehmenden Gesamtschau\\n\\ninsgesamt von\\n\\nÖffentliches Dienst- und Vergütungsrecht Auch bei von der öffentlichen Hand beherrschten Public-Private-Partnerships (also solchen mit staatlicher Beteiligung von mehr als 50 %) muss die Stellenbesetzung nach Art. 33 Abs. 2 GG erfolgen. Der bisherigen Rechtsprechung des Bundesarbeitsgerichts zu öffentlichen Betrieben in privater Rechtsform ist zu entnehmen, dass es auch für öffentlich beherrschte gemischtwirtschaftliche Unternehmen von einer Geltung der Vorschrift ausgehen wird.\\n\\nVergütungsrechtliche Einschränkungen gelten nur, wenn aufgrund der Beihilfehöhe das Besserstellungsverbot zur Anwendung kommt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n218\\n\\n11.6 Abschließende Übersicht\\n\\nDie nachfolgende Tabelle stellt überblicksmäßig die rechtlichen Vor- und Nachteile der einzelnen Modelle in Bezug auf die drei grundsätzlichen Finanzierungsarten dar:\\n\\nÖffentlich finanziert\\n\\nPrivat finanziert\\n\\nPublic-Private- Partnership\\n\\n– mögliche\\n\\nEU-Beihilferecht\\n\\n– voll anwendbar – Trennungsrechnung – Beschränkung\\n\\nwirtschaftlicher Tätigkeiten\\n\\n– FuE-Rahmen 2022\\n\\nmit vielen Privilegien\\n\\n– nicht anwendbar\\n\\noder nur eingeschränkt\\n\\n– Beihilfen nach AGVO\\n\\ndenkbar\\n\\nBeschränkung wirtschaftlicher Tätigkeit und Trennungsrech- nung; FuE-Rahmen\\n\\n– im Übrigen Beihilfen nach AGVO denkbar\\n\\n– anwendbar auf den\\n\\nVergaberecht\\n\\n– voll anwendbar – öffentliche\\n\\nAusschreibungen notwendig\\n\\n– grundsätzlich nicht\\n\\nanwendbar\\n\\nGründungsakt, wenn damit zugleich Aufgabenübertragu ng auf den privaten Partner und bei Konzessionierung\\n\\n– in aller Regel\\n\\nanwendbar auf Leistungsbeschaffu ng durch die gemischtwirtschaftl iche Gesellschaft\\n\\n– Mitarbeitergewinn-\\n\\nÖffentliches Dienst- und Vergütungs- recht\\n\\n– voll anwendbar – Besserstellungs- verbot greift ein\\n\\n– in der\\n\\nMitarbeiterauswahl und Vergütung frei (ggf. steuerliche Einschränkungen bei Gemeinnützigkeit) – Marktvergütun-gen\\n\\nmöglich\\n\\nung unterliegt Vorgaben der Bestenauslese, wenn staatliche Beteiligung über 50 %\\n\\n– Besserstellungs- verbot je nach Beihilfenhöhe möglich\\n\\nTabelle 21: Übersicht der Vor- und Nachteile der drei Finanzierungsmodelle mit Hinblick auf die rechtlichen Rahmenbedingungen\\n\\nGroße KI-Modelle für Deutschland\\n\\n219\\n\\nGesellschaftsstruktur von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n220\\n\\n12. Gesellschaftsstruktur von LEAM\\n\\nDie Gesellschaftsstruktur von LEAM ist stark abhängig von der Finanzierungsart des Projektes. Hier bieten verschiedene Gesellschaftsformen an. Im Rahmen der Finanzierung sollten diese genauer evaluiert und bewertet werden. Generell ist aufgrund der Struktur des Projektes die Gründung einer Projektentwicklungsgesellschaft denkbar.\\n\\nsich\\n\\nje nach\\n\\nFinanzierungsstruktur\\n\\nGrundsätzlich können verschiedene Gesellschaftsformen unterschieden werden. Diese sollen anhand der Finanzierungsform kurz beleuchtet werden.\\n\\n12.1 Öffentliche Finanzierung\\n\\nSofern eine rein öffentliche Finanzierung gewählt wird, kommen neben den Rechtsformen des öffentlichen Rechts (vor allem der Anstalt des öffentlichen Rechts) grundsätzlich mehrere privatrechtliche Formen in Betracht. Auch in der deutschen Förder- und Wissenschaftspraxis gibt es Einrichtungen, die als e.V. organisiert sind (zum Beispiel Max-Planck-Gesellschaft oder Fraunhofer) oder auch als GmbHs. Insbesondere die GmbH – gegebenenfalls auch in Form der gemeinnützigen GmbH mit dem besonderen steuerrechtlichen Einschlag – kann in der Praxis heute eine Rolle für die Aufgabenerfüllung der öffentlichen Hand spielen. Dem Bund oder einem Land ist hierbei eine Gesellschaftsbeteiligung nach dem deutschen Haushaltsrecht im Umfang der Investition möglich (wenn also beispielsweise der Bund 90 % der Investition trägt, würde er auch 90 % der Geschäftsanteile an der GmbH enthalten). Dies folgt auch aus der Logik, dass das EU-Beihilferecht und das Vergaberecht sich mit ihren Möglichkeiten und Beschränkungen nicht an der Rechtsform, sondern vielmehr an der tatsächlich ausgeübten Tätigkeit orientieren. Das GmbH-Recht ist zudem hinreichend flexibel, um auch besondere Organstrukturen abzubilden und gleichzeitig eine gewisse Flexibilität der Handelnden, insbesondere der Geschäftsführung, zu erlauben. Bei einer GmbH- Konstruktion kann es sich zudem anbieten, eine eher nicht-wirtschaftlich orientierte Träger-GmbH für die Infrastruktur (ggf. als gGmbH) mit einer wirtschaftlich orientierten GmbH zu verbinden.\\n\\nDie Aktiengesellschaft und die Genossenschaft erscheinen dagegen in diesem Modell nicht sinnvoll, da sie haushaltsrechtlich für den Bund und ein beteiligtes Bundesland nicht darstellbar erscheinen und zudem auch in der rechtlichen Flexibilität Probleme auslösen können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n221\\n\\n12.2 Private Finanzierung\\n\\nIm Rahmen einer privaten Finanzierung sind grundsätzlich alle Gesellschaftsformen, die oben dargestellt sind, denkbar.\\n\\nDie Rechtsform der AG ist im Hinblick auf die Übertragbarkeit von Anteilen und die – auch kurzfristige – Kapitalbeschaffung flexibel. Durch strenge gesetzliche Vorgaben wird indes der unternehmerische Gestaltungsspielraum reduziert. Zudem ist eine AG eher für ein kapitalmarktorientiertes Unternehmen sinnvoll. Übertragbarkeit und kurzfristige Kapitalbeschaffung sind bei einer GmbH im Vergleich dazu erschwert. Im Gegensatz zum Vorstand einer AG ist die Geschäftsführung einer GmbH an Weisungen der Gesellschafter:innen gebunden. Formale Vorgaben sind weniger streng. Die demokratischen Strukturen und Flexibilität der Genossenschaft können je nach Sichtweise als Vor- oder Nachteil begriffen werden. Nachteil einer Stiftung ist der Wegfall des Zugriffs auf das eingeflossene Vermögen. Der Betrieb eines wirtschaftlichen Unternehmens mit Gewinnerzielungsabsicht als eingetragener Verein ist so nicht möglich.\\n\\n12.3 Public-Private-Partnership\\n\\nAuch hier sind grundsätzlich verschiedene Gesellschaftsformen denkbar.\\n\\nFür PPPs wird häufig die Rechtsform der GmbH gewählt. Für diese spricht, dass sie aufgrund ihrer Satzungsfreiheit und der Weisungsbindung der Geschäftsführung gute Voraussetzungen bietet, um die haushaltsrechtlichen Pflichten der öffentlichen Hand zur Einwirkung und Kontrolle gegenüber der Geschäftsführung hinreichend Rechnung zu tragen. In Form der GmbH schützt die öffentliche Hand zudem vor untragbaren Haftungsrisiken.\\n\\nim der Der gemischtwirtschaftlichen Unternehmen auch die wichtige Bedeutung zu, der Gefahr einer Anfechtung von am Gemeinwohl orientierten Gesellschafterbeschlüssen durch überstimmte Dritte wegen Treuepflichtverstoßes oder der Verfolgung von Sondervorteilen zu begegnen.\\n\\nsatzungsmäßigen\\n\\nFestlegung\\n\\nöffentlichen\\n\\nAufgabe\\n\\nkommt\\n\\nVorteil der GmbH gegenüber der AG ist im Rahmen von Public-Private-Partnerships, dass sie mehr Steuerungsmöglichkeiten der Gesellschafter:innen zulässt. Das GmbH-Recht ist insoweit gestaltungsoffener, weil es die Regelung der gesellschaftsvertraglichen Rechtsverhältnisse weitgehend den Gesellschafter:innen überlässt.\\n\\nEine GmbH ermöglicht es bei einer PPP zudem, die Geschäftsanteile zwischen Bund, Land und Privatwirtschaft angemessen aufzuteilen und gleichzeitig der Geschäftsführung der GmbH gewisse Freiheiten und Flexibilität bei der Umsetzung der Ziele des Projekts LEAM zu gewährleisten.\\n\\nNicht ausgeschlossen ist es andererseits, gemischtwirtschaftliche Unternehmen in öffentlicher Rechtsform zu bilden. Hierbei sind allerdings erhöhte Anforderungen geboten, weil für öffentliche Rechtsformen das verfassungsrechtliche Legitimationsgebot umfassend gilt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n222\\n\\nSzenario für ein LEAM KI-Servicezentrum\\n\\nGroße KI-Modelle für Deutschland\\n\\n223\\n\\n13. Szenario für ein LEAM KI-Servicezentrum Auf Basis der vorangegangenen Ausführungen lassen sich verschiedene Szenarien für die Gestaltung und Implementierung des LKS ableiten. Im Folgenden wird eine mögliche Konzeption entwickelt und erläutert. Hierbei ist zu berücksichtigen, dass es in allen Dimensionen noch Gestaltungsoptionen und Alternativen geben kann. Diese müssen dann im weiteren Verlauf der Konzeption und Verhandlungen detaillierter spezifiziert und zur Entscheidung gebracht werden.\\n\\nDie Governance-Struktur des LKS ist in Abbildung 27 schematisch dargestellt und besteht aus folgenden Komponenten:\\n\\nLEAM-Institut Das LEAM-Institut übernimmt folgende Aufgaben:\\n\\nGesamtkoordination der LEAM-Aktivitäten und des LKS\\n\\nSteuerung der Entwicklung von KI-Foundation-Modellen\\n\\nBereitstellung von Foundation-Modellen als Open Source\\n\\nEntwicklung und Umsetzung des Dienstes “Training-as-a-Service”\\n\\nDie Finanzierung erfolgt über öffentliche Mittel unter Berücksichtigung der Beihilfe- Konformität mit Gehaltsstrukturen des öffentlichen Dienstes (TVöD). Die Anmietung von Infrastrukturen (Housing, Office, evtl. Rechenkapazitäten) bzw. die Anschaffung eines KI- Supercomputer muss dafür öffentlich ausgeschrieben werden. Um dem EU-Beihilferecht gerecht zu werden, erfolgt die Nutzung der Kapazitäten des Supercomputers zu 80% für nicht-wirtschaftliche Zwecke - das heißt Forschung und Entwicklung durch Wirtschaft und Wissenschaft im Open-Source-Verfahren - und zu 20% für wirtschaftliche Zwecke.\\n\\nLEAM-Servicegesellschaft Die LEAM-Servicegesellschaft übernimmt folgende Aufgaben:\\n\\nAngebot eines Services für Model-Tuning mit Fokus auf Kunden aus der Wirtschaft\\n\\nBereitstellung von Kapazitäten für den Betrieb von KI-Anwendungen (Inference)\\n\\nBereitstellung von GPU Rechenkapazitäten\\n\\nBeratungsdienstleistungen (Consulting)\\n\\nDiese Services werden in der Regel kommerziell angeboten.\\n\\nMögliche Finanzierungsquellen für die Servicegesellschaft sind:\\n\\nFinanzinvestoren\\n\\nUnternehmen bzw. Joint Ventures\\n\\nEine Public-Private Partnership (PPP) mit Beteiligung des Bundes / eines Landes\\n\\nGroße KI-Modelle für Deutschland\\n\\n224\\n\\nDie Möglichkeit (z.B. Anschubfinanzierung) ist im weiteren Verlauf zu prüfen und die Rechtsform sowie die kommerzielle Ausrichtung (gewinnorientiert / gemeinnützig) zu definieren.\\n\\nfür eine weitere Unterstützung durch öffentliche Mittel\\n\\nDa die Gesellschaft nicht dem öffentlichen Dienst- und Vergütungsrecht unterliegt, kann sie marktübliche Gehälter bezahlen. Dadurch steigt die Attraktivität für hochqualifizierte Talente.\\n\\nInfrastruktur Die Bereitstellung der notwendigen Supercomputing-Infrastruktur folgenden zwei alternativen Ansätzen gestalten (s. Kapitel 9.2):\\n\\nlässt sich mit\\n\\nAnschaffung eines KI-Supercomputers\\n\\nEinkauf von GPU RZ-Leistungen bei einem externen Provider, der einen KI- Supercomputer bereitstellt\\n\\nHousing Die notwendige Housing Infrastruktur wird von einem externen Dienstleister angemietet (s. Kapitel 9.2).\\n\\nFörderprojekt Ein erstes Foundation-Modell wird im Rahmen eines öffentlich geförderten Ankerprojekts erstellt, an dem Wirtschaft und Wissenschaft gemeinsam arbeiten. Die Gestaltung dieses Projektes kann sich an dem bestehenden Projekt OpenGPT-X orientieren und die Ergebnisse von OpenGPT-X übernehmen und weiterentwickeln.\\n\\nIm Rahmen dieses Ankerprojekts werden folgende Zielsetzungen erreicht:\\n\\nAufbau und Pflege eines Daten-Korpus für Sprachmodelle\\n\\n\\n\\nImplementierung und Test der notwendigen Training-as-a-Service Prozesse\\n\\nBereitstellung des entwickelten Foundation-Modells als Open Source für die Wirtschaft\\n\\nKI-Compute-Voucher Start-ups sollen im Rahmen der LEAM-Initative die Möglichkeit erhalten, sich an der Entwicklung von Foundation-Modellen zu beteiligen sowie - vor allem - bereitgestellte Foundation-Modelle zu tunen und auf dieser Basis eigene KI-Anwendungen und Geschäftsmodelle zu entwickeln.\\n\\nUm diese Zielsetzung zu unterstützen, stellt die öffentliche Hand sog. KI-Compute- Voucher bereit. Hiermit können (auch kleinere) Entwicklungsprojekte für Start-ups gefordert werden. Die genaue Gestaltung und der Umfang dieses Programms sind im weiteren Verlauf zu definieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n225\\n\\nGroße KI-Modelle für Deutschland\\n\\ns m u r t n e z e c i v r e S - I K - M A E L\\n\\ns e d r u t k u r t S e d r ü f o i r a n e z S\\n\\ni\\n\\n:\\n\\n7 2\\n\\n.\\n\\nb b A\\n\\n226\\n\\nFazit LEAM Machbarkeitsstudie\\n\\nGroße KI-Modelle für Deutschland\\n\\n227\\n\\n14. Fazit Die Umfragen und Interviews mit Expert:innen aus Wirtschaft und Wissenschaft zeigen ein eindeutiges Bild: KI-Foundation-Modelle stellen die nächste Entwicklung in der Erfolgsgeschichte der Künstlichen Intelligenz dar. Dabei sind die aktuell populären Sprachmodelle nur der erste Schritt. In den nächsten Jahren werden noch weitaus performantere und auf noch vielfältigere Daten trainierte Modelle den Markt weiter revolutionieren.\\n\\nDie deutsche Wirtschaft hat diesen Trend erkannt und evaluiert bereits Möglichkeiten, die KI-Foundation-Modellen effektiv in internen Prozessen und als Produkte zu nutzen. Dabei ist sie aktuell aber weitgehend von proprietären, amerikanischen Foundation-Modellen abhängig. Dies stellt die Unternehmen vor große Herausforderungen in den Bereichen Datenschutz, Qualität und Zugriff auf die Modelle. Ein entscheidender Wettbewerbsnachteil gegenüber der ausländischen Konkurrenz droht. Europäischen Standards entsprechende, mit hochwertigen und vielfältigen Daten trainierte und Open Source verfügbare Foundation-Modelle würden diese Herausforderungen bewältigen und dazu beitragen, dass die deutsche Wirtschaft umfänglich von KI-Foundation- Modellen profitiert.\\n\\nDabei wurden Kernherausforderungen identifiziert:\\n\\nsowohl\\n\\nvon Wirtschaft als auch der Wissenschaft drei\\n\\n(1) Es bedarf einer Vielzahl an Expert:innen für das Thema KI-Foundation-Modelle.\\n\\n(2) Diese Expert:innen benötigen Zugriff auf qualitative hochwertige Daten\\n\\nverschiedener Arten sowie\\n\\n(3) Zugriff auf eine hinreichend mächtige Infrastruktur, die für die aktuelle KI-\\n\\nTechnologie und die Prozesse der KI-Entwicklung optimiert ist.\\n\\nEs gilt, diese Herausforderungen in einer gemeinsamen Anstrengung der Wirtschaft, der Wissenschaft und des Staates zu lösen.\\n\\n14.1 Beurteilung der Machbarkeit\\n\\nDiese Studie hat die Notwendigkeit, die Chancen und den Bedarf von KI-Foundation- Modellen in Deutschland untersucht. Dabei wurde vor allem die Machbarkeit für den Aufbau und den Betrieb eines dedizierten KI-Rechenzentrums beleuchtet. Dafür wurden in dieser Studie die Bereiche Software, Hardware, bauliche Infrastruktur, Personal, Organisationsstruktur sowie Finanzierung betrachtet. Die Machbarkeit in diesen Bereichen soll hier noch einmal abschließend beurteilt werden.\\n\\nSoftware Die für das Training und Entwicklung der KI-Foundation-Modelle benötigten Softwareframeworks und -tools stellen keine zentrale Herausforderung dar. Die notwendigen Technologien sind bereits vorhanden und überwiegend als Open-Source- Software verfügbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n228\\n\\nHardware Es gibt aktuell in Europa kein dediziertes KI-Rechenzentrum, das für die Entwicklung international kompetitiver Foundation-Modelle ausreicht. Um zum aktuellen Stand der amerikanischen Hyperscaler aufzuschließen, müssen rund 4.500 GPU im Rechenzentrum verbaut werden. Hersteller und Collocation-Anbieter haben signalisiert, dass der Aufbau eines solchen Clusters hardwareseitig zeitnah möglich ist.\\n\\nBauliche Infrastruktur Von dem Aufbau einer eigenen baulichen Infrastruktur sollte aufgrund der Kosten und der zu erwartenden Bauzeit abgesehen werden. Verschiedene Collocation-Anbieter haben aber bereits signalisiert, dass sie in der Lage wären, in ihrer Infrastruktur zu betreiben. Es sollte daher auf diese Möglichkeit zurückgegriffen werden.\\n\\nPersonal Für den Betrieb eines KI-Rechenzentrums wird ein Team aus hochspezialisierten Expert:innen benötigt. Der Aufbau des Teams für den Aufbau und den Betrieb der Services ist kurzfristig eine Herausforderung. Mittel- und langfristig eine große Chance, um talentierte Wissenschaftler zu halten.\\n\\nOrganisationsstruktur Für den Betrieb eines KI-Rechenzentrums schlagen die Autor:innen die Einrichtung einer eigenen Organisation, dem LEAM KI-Servicezentrum, vor. Dieses LKS wird den Aufbau der Infrastruktur begleiten und den identifizierten Zielgruppen spezialisierte Services bereitstellen. Der Fokus soll darauf liegen, der Wirtschaft KI-Foundation-Modelle für die Entwicklung von Anwendungen bereitzustellen.\\n\\nFinanzierung Die Autor:innen kalkulieren für den Aufbau und Betrieb eines KI-Rechenzentrums über vier Jahre einen Bedarf von rund 380 Millionen Euro. Für die Finanzierung wurde ein Modell entwickelt, das öffentliche und privatwirtschaftliche Mittel berücksichtigt.\\n\\nZusammenfassend lässt sich festhalten, dass der Aufbau von LEAM-Infrastruktur und -Services für die deutsche Wirtschaft ein entscheidender Wirtschaftsfaktor ist. Die Umsetzung ist mit der Beteiligung der öffentlichen Hand, der Forschung und Wissenschaft sowie der Wirtschaft realisierbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n229\\n\\n14.2 Ausblick\\n\\nIn engem Austausch mit der Politik, der Wirtschaft, potentiellen Standorten, Anbieter:innen und Anwender:innen sollte das Thema nun weiter vorangetrieben werden. Dabei gilt es vor allem, die Finanzierungsmöglichkeiten weiter zu präzisieren.\\n\\nAktuelle Entwicklungen wie der Hype um ChatGPT zeigen, wie dynamisch sich die Forschung und Anwendung von Foundation-Modellen entwickelt. Die Gefahr, dass sich in den USA monopolartige Strukturen - ähnlich Google bei Suchmaschinen - bilden, wird fortlaufend größer. Damit Deutschland nicht weiter zurückfällt, ist eine zeitnahe Umsetzung des Konzeptes für die LEAM-Infrastruktur nötig. Das Momentum, das im Rahmen der Erstellung dieser Machbarkeitsstudie gewonnen wurde - durch eine enge Zusammenarbeit zwischen Forschung, Wissenschaft, Wirtschaft und Start-ups - sollte genutzt werden, um die Zielsetzung weiter voranzutreiben.\\n\\nEine Möglichkeit ist die zeitnahe Gründung einer Projektentwicklungsgesellschaft, welche die Grundlagen für den Aufbau dieses strategischen KI-Leuchtturmprojekts erarbeitet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n230\\n\\nVerzeichnisse & Methodik der LEAM Machbarkeitsstudie\\n\\nTitelseite\\n\\nGroße KI-Modelle für Deutschland\\n\\n231\\n\\nI. Quellenverzeichnis\\n\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,\\n\\nKudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Zheng, X. (2016). TensorFlow: A system for large-scale machine learning. Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, 265–283. https://dl.acm.org/doi/10.5555/3026877.3026899\\n\\nAbseil Python Common Libraries. (2022). [Python]. Abseil. https://github.com/abseil/abseil-py (Original work\\n\\npublished 2017)\\n\\nAgarwal, O., Ge, H., Shakeri, S., & Al-Rfou, R. (2021). Knowledge Graph Based Synthetic Corpus Generation for\\n\\nKnowledge-Enhanced Language Model Pre-training (arXiv:2010.12688). arXiv. https://doi.org/10.48550/arXiv.2010.12688\\n\\nAI accelerator. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=AI_accelerator&oldid=1123373022\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., … Simonyan, K. (2022). Flamingo: A Visual Language Model for Few-Shot Learning. In A. H. Oh, A. Agarwal, D. Belgrave, & K. Cho (Eds.), Advances in Neural Information Processing Systems. https://openreview.net/forum?id=EbMuimAbPbs\\n\\nAMD. (o.D.). AMD ROCmTM Open Ecosystem. Retrieved 28 November 2022, from\\n\\nhttps://www.amd.com/en/graphics/servers-solutions-rocm\\n\\nAMD. (2021, August 11). New AMD InstinctTM MI200 Series Accelerators Bring Leadership HPC and AI Performance\\n\\nto Power Exascale Systems and More. https://www.amd.com/en/press-releases/2021-11-08-new-amd- instinct-mi200-series-accelerators-bring-leadership-hpc-and-ai\\n\\nAn updated set of basic linear algebra subprograms (BLAS). (2002). ACM Transactions on Mathematical\\n\\nSoftware, 28(2), 135–151. https://doi.org/10.1145/567806.567807\\n\\nAn, W., Guo, Y., Bian, Y., Ma, H., Yang, J., Li, C., & Huang, J. (2022). MoDNA: Motif-oriented pre-training for\\n\\nDNA language model. Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, 1–5. https://doi.org/10.1145/3535508.3545512 Apache Flink. (2022). [Java]. The Apache Software Foundation. https://github.com/apache/flink (Original work\\n\\npublished 2014)\\n\\nApache Hadoop. (2022). [Java]. The Apache Software Foundation. https://github.com/apache/hadoop\\n\\n(Original work published 2014)\\n\\nApache Spark. (2022). [Scala]. The Apache Software Foundation. https://github.com/apache/spark (Original\\n\\nwork published 2014)\\n\\nArakelyan, G., Soghomonyan, G., & The Aim team. (2020). Aim (3.9.3) [TypeScript].\\n\\nhttps://doi.org/10.5281/zenodo.6536395\\n\\nASHRAE. (2022). Data Center Power Equipment Thermal Guidelines and Best Practices. ASHRAE Technical Committee (TC) 9.9 Mission Critical Facilities, Data Centers, Technology Spaces, and Electronic Equipment.\\n\\nBaek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G. R., Wang, J., Cong, Q., Kinch, L. N., Schaeffer, R. D., Millán, C., Park, H., Adams, C., Glassman, C. R., DeGiovanni, A., Pereira, J. H., Rodrigues, A. V., van Dijk, A. A., Ebrecht, A. C., … Baker, D. (2021). Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557), 871–876. https://doi.org/10.1126/science.abj8754\\n\\nBaevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning\\n\\nof speech representations. Proceedings of the 34th International Conference on Neural Information Processing Systems, 12449–12460.\\n\\nBannour, N., Ghannay, S., Névéol, A., & Ligozat, A.-L. (2021). Evaluating the carbon footprint of NLP methods: A\\n\\nsurvey and analysis of existing tools. 11–21. https://doi.org/10.18653/v1/2021.sustainlp-1.2\\n\\nBasic Linear Algebra Subprograms. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Basic_Linear_Algebra_Subprograms&oldid=1120747813\\n\\nBenaic, N., & Hogarth, I. (2022). State of AI 2022. https://www.stateof.ai\\n\\nGroße KI-Modelle für Deutschland\\n\\n232\\n\\nBitkom e.V. (2022a, January 3). IT-Fachkräftelücke wird größer: 96.000 offene Jobs | Bitkom e.V.\\n\\nhttps://www.bitkom.org/Presse/Presseinformation/IT-Fachkraefteluecke-wird-groesser\\n\\nBitkom e.V. (2022b). Bitkom Postionspapier: Perspektiven für eine nachhaltige Rechenzentren-Wirtschaft bis 2030.\\n\\nhttps://www.bitkom.org/sites/main/files/2022-10/2210-Positionspapier-Nachhaltige- Rechenzentren.pdf\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut,\\n\\nA., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., … Liang, P. (2021). On the Opportunities and Risks of Foundation Models (arXiv:2108.07258). arXiv. https://doi.org/10.48550/arXiv.2108.07258\\n\\nBoroditsky, L. (2012, March 15). Linguistik: Wie die Sprache das Denken formt. Spektrum.de.\\n\\nhttps://www.spektrum.de/news/linguistik-wie-die-sprache-das-denken-formt/1145804\\n\\nBranch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., Iglesias, D. del C., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples (arXiv:2209.02128). arXiv. https://doi.org/10.48550/arXiv.2209.02128\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\n\\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural Information Processing Systems, 33, 1877–1901. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a- Abstract.html\\n\\nBrundage, M., Mayer, K., Eloundou, T., Agarwal, S., Adler, S., Krueger, G., Leike, J., & Mishkin, P. (2022).\\n\\nLessons learned on language model safety and misuse. OpenAI. https://openai.com/blog/language- model-safety-and-misuse/\\n\\nBundesministerium für Digitales und Verkehr. (2022). Digitalstrategie Deutschland. https://digitalstrategie-\\n\\ndeutschland.de/medien/\\n\\nCampa, C., Kawalek, C., Vo, H., & Bessoudo, J. (2020, May 14). Defining AI Innovation with NVIDIA DGX A100. NVIDIA Technical Blog. https://developer.nvidia.com/blog/defining-ai-innovation-with-dgx-a100/\\n\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., &\\n\\nErlingsson, U. (2021). Extracting training data from large language models. 30th USENIX Security Symposium (USENIX Security 21), 2633–2650. https://www.usenix.org/system/files/sec21-carlini- extracting.pdf\\n\\nCarpintero, A. G. (2021, November 21). MLOps with Docker and Jenkins: Automating Machine Learning Pipelines.\\n\\nMedium. https://towardsdatascience.com/mlops-with-docker-and-jenkins-automating-machine- learning-pipelines-a3a4026c4487\\n\\nCen, S., & Shah, D. (2021). Regulating algorithmic filtering on social media. Advances in Neural Information\\n\\nProcessing Systems, 34, 6997–7011. https://proceedings.neurips.cc/paper/2021/hash/38b4f06e27fd4f6fdcceabc6f5c068ea-Abstract.html\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision Transformer: Reinforcement Learning via Sequence Modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems (Vol. 34, pp. 15084–15097). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\\n\\nBrockman, G., & others. (2021). Evaluating large language models trained on code. ArXiv Preprint ArXiv:2107.03374.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways (arXiv:2204.02311). arXiv. https://doi.org/10.48550/arXiv.2204.02311\\n\\nChowdhury, R., Bouatta, N., Biswas, S., Floristean, C., Kharkar, A., Roy, K., Rochereau, C., Ahdritz, G., Zhang, J.,\\n\\nChurch, G. M., Sorger, P. K., & AlQuraishi, M. (2022). Single-sequence protein structure prediction using a language model and deep learning. Nature Biotechnology, 40(11), Article 11. https://doi.org/10.1038/s41587-022-01432-w\\n\\nClimate Neutral Data Centre Pact – The Green Deal need Green Infrastructure. (o.D.). Retrieved 19 December\\n\\n2022, from https://www.climateneutraldatacentre.net/\\n\\nCodeCarbon. (2020). CodeCarbon—CodeCarbon 2.0.0 documentation. https://mlco2.github.io/codecarbon/\\n\\nGroße KI-Modelle für Deutschland\\n\\n233\\n\\nCremers, A. B., Englander, A., Gabriel, M., Hecker, D., Mock, M., Poretschkin, M., Julia Rosenzweig, J.,\\n\\nRostalski, F., Volmer, J., & Voosholz, J. (2019). Vertrauenswürdiger Einsatz von Künstlicher Intelligenz. Handlungsfelder aus philosophischer, ethischer, rechtlicher und technologischer Sicht als Grundlage für eine Zertifizierung von Künstlicher Intelligenz. Fraunhofer-Institut Für Intelligente Analyse-Und Informationssysteme (IAIS). https://www.iais.fraunhofer.de/content/dam/iais/KINRW/Whitepaper_KI- Zertifizierung.pdf\\n\\nCSTB Releases Report Fostering Responsible Computing Research: Foundations and Practices » CCC Blog. (2022, May 16). https://cccblog.org/2022/05/16/cstb-releases-report-fostering-computing-research- foundations-and-practices/\\n\\nDask. (2022). [Python]. dask. https://github.com/dask/dask (Original work published 2015) Data protection in the EU. (o.D.). [Text]. European Commission - European Commission. Retrieved 28\\n\\nNovember 2022, from https://ec.europa.eu/info/law/law-topic/data-protection/data-protection- eu_en\\n\\nDean, J., & Ghemawat, S. (2004). MapReduce: Simplified Data Processing on Large Clusters. OSDI’04: Sixth\\n\\nSymposium on Operating System Design and Implementation, 137–150.\\n\\nDeep Lake. (2022). [Python]. Activeloop. https://github.com/activeloopai/deeplake (Original work published\\n\\n2019)\\n\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers\\n\\nfor Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\\n\\nDodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., & Gardner, M. (2021). Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 1286–1305. https://doi.org/10.18653/v1/2021.emnlp-main.98\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\\n\\nM., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2022, March 23). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations. https://openreview.net/forum?id=YicbFdNTTy\\n\\nDürr, B. (2018). IT-Räume und Rechenzentren planen und betreiben: Handbuch der baulichen Maßnahmen und\\n\\nTechnischen Gebäudeausrüstung (Vol. 2). Verlag Bau+Technik.\\n\\nELE Consortium. (2022). Digital Language Equality in Europe by 2030: Strategic Agenda and Roadmap.\\n\\nhttps://european-language-equality.eu/agenda/\\n\\nFan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A., Goyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary, V., Goyal, N., Birch, T., Liptchinsky, V., Edunov, S., Grave, E., Auli, M., & Joulin, A. (2022). Beyond english-centric multilingual machine translation. The Journal of Machine Learning Research, 22(1), 4839–4886.\\n\\nFFCV. (2022). [Python]. FFCV. https://github.com/libffcv/ffcv (Original work published 2021) Frostig, R., Johnson, M., & Leary, C. (2018). Compiling machine learning programs via high-level tracing.\\n\\nhttps://mlsys.org/Conferences/doc/2018/146.pdf\\n\\nGehlhaus, D., & Koslosky, L. (2022). Training Tomorrow’s AI Workforce. Center for Security and Emerging Technology. https://cset.georgetown.edu/publication/training-tomorrows-ai-workforce/ Gehlhaus, D., Koslosky, L., Goode, K., & Perkins, C. (2021). U.S. AI Workforce: Policy Recommendations.\\n\\nCenter for Security and Emerging Technology. https://cset.georgetown.edu/publication/u-s-ai- workforce-policy-recommendations/\\n\\nGehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating Neural\\n\\nToxic Degeneration in Language Models (arXiv:2009.11462). arXiv. https://doi.org/10.48550/arXiv.2009.11462\\n\\nGlaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C., … Irving, G. (2022). Improving alignment of dialogue agents via targeted human judgements (arXiv:2209.14375). arXiv. https://doi.org/10.48550/arXiv.2209.14375 GlusterFS. (2022). [C]. Gluster.org. https://github.com/gluster/glusterfs (Original work published 2011) Gopani, A. (2021, July 16). JAX Vs TensorFlow Vs PyTorch: A Comparative Analysis. Analytics India Magazine.\\n\\nhttps://analyticsindiamag.com/jax-vs-tensorflow-vs-pytorch-a-comparative-analysis/\\n\\nHansell, S. (2002, April 8). Google’s Toughest Search Is for a Business Model. The New York Times.\\n\\nhttps://www.nytimes.com/2002/04/08/business/google-s-toughest-search-is-for-a-business- model.html\\n\\nGroße KI-Modelle für Deutschland\\n\\n234\\n\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), Article 7825. https://doi.org/10.1038/s41586-020-2649-2\\n\\nHensel, M., & Ostler, U. (2020, November 22). Die beliebtesten Anbieter von Technik für das High-\\n\\nPerformance Computing. Datacenter Insider. https://www.datacenter-insider.de/die-beliebtesten- anbieter-vontechnik-fuer-das-high-performance-computing-a-980532/\\n\\nHickmann, H., & Koneberg, F. (2022). Die Berufe mit den aktuell größten Fachkräftelücken. 67.\\n\\nhttps://www.iwkoeln.de/studien/helen-hickmann-filiz-koneberg-die-berufe-mit-den-aktuell- groessten-fachkraefteluecken.html\\n\\nHintemann, Dr. R., Hinterholzer, S., Graß, M., & Grothey, T. (2022). Bitkom-Studie: Rechenzentren in\\n\\nDeutschland 2021 – Aktuelle Marktentwicklungen. Borderstep Institut. https://www.bitkom.org/sites/main/files/2022-02/10.02.22-studie-rechenzentren.pdf\\n\\nHintemann, Dr. R., Hinterholzer, S., & Grothey, T. (2021). Herausforderungen und Chancen durch den Boom\\n\\nbeim Neubau von Rechenzentren. Hessische Staatskanzlei, Ministerin für Digitale Strategie und Entwicklung. https://digitales.hessen.de/sites/digitales.hessen.de/files/2022- 05/rechenzentrumsmarkt_hessen.pdf\\n\\nHintemann, R. (2020). Data centers 2018. Efficiency gains are not enough: Data center energy consumption\\n\\ncontinues to rise significantly - Cloud computing boosts growth. https://doi.org/10.13140/RG.2.2.26033.40800\\n\\nHintemann, R., & Clausen, J. (2018). Bedeutung digitaler Infrastrukturen in Deutschland. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network (arXiv:1503.02531). arXiv.\\n\\nhttps://doi.org/10.48550/arXiv.1503.02531\\n\\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\\n\\n& Salimans, T. (2022). Imagen Video: High Definition Video Generation with Diffusion Models (arXiv:2210.02303). arXiv. https://doi.org/10.48550/arXiv.2210.02303\\n\\nHugging Face. (o.D.). Model Parallelism. Retrieved 28 November 2022, from\\n\\nhttps://huggingface.co/docs/transformers/v4.15.0/parallelism\\n\\nHydra. (2022). [Python]. Meta Research. https://github.com/facebookresearch/hydra (Original work\\n\\npublished 2019)\\n\\nIntel. (2022, June 29). Second-Gen Habana Gaudi2 Outperforms Nvidia A100. Intel.\\n\\nhttps://www.intel.com/content/www/us/en/newsroom/news/second-gen-habana-gaudi2- outperforms-nvidia-a100.html\\n\\nISO - ISO/IEC 27001 and related standards—Information security management. (o.D.). ISO. Retrieved 28 November 2022, from https://www.iso.org/isoiec-27001-information-security.html\\n\\nISO 27017 and ISO 27018 Certification | DEKRA. (o.D.). Retrieved 6 December 2022, from\\n\\nhttps://www.dekra.com/en/iso-27017-and-iso-27018-certification/\\n\\nIzacard, G., & Grave, E. (2021). Leveraging Passage Retrieval with Generative Models for Open Domain Question\\n\\nAnswering (arXiv:2007.01282). arXiv. https://doi.org/10.48550/arXiv.2007.01282\\n\\nJawahar, R. (2021, October 14). Teaching AI to perceive the world through your eyes. Meta AI.\\n\\nhttps://ai.facebook.com/blog/teaching-ai-to-perceive-the-world-through-your-eyes/ Joseph, E., Riddle, M., Sorensen, T., & Conway, S. (2022). The Economic and Societal Benefits of Linux\\n\\nSupercomputers. https://davidbader.net/publication/2022-hyperionresearch/\\n\\nJülich Forschungszentrum. (2022, June 15). Erster europäischer Exascale-Superrechner kommt nach Jülich. https://www.fz-juelich.de/de/aktuelles/news/pressemitteilungen/2022/exascale-standort- entscheidung\\n\\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R.,\\n\\nŽídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera- Paredes, B., Nikolov, S., Jain, R., Adler, J., … Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), Article 7873. https://doi.org/10.1038/s41586-021- 03819-2\\n\\nKahneman, D. (2011). Thinking, fast and slow. Macmillan. Kale, M., Siddhant, A., Al-Rfou, R., Xue, L., Constant, N., & Johnson, M. (2021). nmT5—Is parallel data still relevant for pre-training massively multilingual language models? Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\\n\\nGroße KI-Modelle für Deutschland\\n\\n235\\n\\nNatural Language Processing (Volume 2: Short Papers), 683–691. https://doi.org/10.18653/v1/2021.acl- short.87\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\\n\\nKeras. (2022). [Python]. Keras. https://github.com/keras-team/keras (Original work published 2015) Khan, S. M., & Mann, A. (2020, April). AI Chips: What They Are and Why They Matter. Center for Security and\\n\\nEmerging Technology. https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they- matter/\\n\\nKosson, A., Chiley, V., Venigalla, A., Hestness, J., & Köster, U. (2021). Pipelined Backpropagation at Scale:\\n\\nTraining Large Models without Batches (arXiv:2003.11666). arXiv. https://doi.org/10.48550/arXiv.2003.11666\\n\\nKubeflow. (2022). [Jsonnet]. Kubeflow. https://github.com/kubeflow/kubeflow (Original work published 2017) Kubernetes (K8s). (2022). [Go]. Kubernetes. https://github.com/kubernetes/kubernetes (Original work\\n\\npublished 2014)\\n\\nLamonica, M. (2014, June 11). HP’s Water-Cooled Supercomputer is Designed for the Hydrophobic. IEEE\\n\\nSpectrum. https://spectrum.ieee.org/a-watercooled-supercomputer-for-the-hydrophobic-\\n\\nLi, C. (2020, June 3). OpenAI’s GPT-3 Language Model: A Technical Overview.\\n\\nhttps://lambdalabs.com/blog/demystifying-gpt-3\\n\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta- Navas, D., Hudson, D. A., … Koreeda, Y. (2022). Holistic Evaluation of Language Models (arXiv:2211.09110). arXiv. https://doi.org/10.48550/arXiv.2211.09110\\n\\nLin, S., Hilton, J., & Evans, O. (2021). TruthfulQA: Measuring How Models Mimic Human Falsehoods\\n\\n(arXiv:2109.07958). arXiv. https://doi.org/10.48550/arXiv.2109.07958\\n\\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., Costa, A. dos S., Fazel-Zarandi, M., Sercu, T., Candido, S., & Rives, A. (2022). Evolutionary-scale prediction of atomic level protein structure with a language model (p. 2022.07.20.500902). bioRxiv. https://doi.org/10.1101/2022.07.20.500902\\n\\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L. (2020). Multilingual\\n\\nDenoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8, 726–742. https://doi.org/10.1162/tacl_a_00343\\n\\nLiu, Y., Liu, P., Radev, D., & Neubig, G. (2022). BRIO: Bringing Order to Abstractive Summarization.\\n\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2890–2903. https://doi.org/10.18653/v1/2022.acl-long.207\\n\\nMaszke, S. (2022). Torchdatasets [Python]. https://github.com/szymonmaszke/torchdatasets (Original work\\n\\npublished 2019)\\n\\nMerkel, D. (2014). Docker: Lightweight Linux containers for consistent development and deployment. Linux\\n\\nJournal, 2014(239), 2:2.\\n\\nMessage Passing Interface. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&oldid=1112449606 Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., & Sivic, J. (2019). HowTo100M: Learning a Text- Video Embedding by Watching Hundred Million Narrated Video Clips. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2630–2640. https://doi.org/10.1109/ICCV.2019.00272\\n\\nMishkin, P., Ahmad, L., Brundage, M., Krueger, G., & Sastry, G. (2022). DALL·E 2 Preview—Risks and Limitations.\\n\\n[https://github.com/openai/dalle-2-preview/blob/main/system- card.md](https://github.com/openai/dalle-2-preview/blob/main/system-card.md)\\n\\nMLflow: A Machine Learning Lifecycle Platform. (2022). [Python]. MLflow. https://github.com/mlflow/mlflow\\n\\n(Original work published 2018)\\n\\nMo, S., Fu, X., Hong, C., Chen, Y., Zheng, Y., Tang, X., Lan, Y., Shen, Z., & Xing, E. (2021, September 24). Multi-\\n\\nmodal Self-supervised Pre-training for Large-scale Genome Data. NeurIPS 2021 AI for Science Workshop. https://openreview.net/forum?id=fdV-GZ4LPfn\\n\\nMoritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., &\\n\\nStoica, I. (2018). Ray: A Distributed Framework for Emerging AI Applications (arXiv:1712.05889). arXiv. https://doi.org/10.48550/arXiv.1712.05889\\n\\nGroße KI-Modelle für Deutschland\\n\\n236\\n\\nMudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., Yang, J. A., Gao, L., Ivchenko, D., Basant, A., Hu, Y., Yang, J., Ardestani, E. K., Wang, X., … Rao, V. (2022). Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models (arXiv:2104.05158). arXiv. https://doi.org/10.48550/arXiv.2104.05158\\n\\nMujkanovic, N., Sivalingam, K., & Lazzaro, A. (2020). Optimising AI Training Deployments using Graph Compilers\\n\\nand Containers (arXiv:2008.11675). arXiv. https://doi.org/10.48550/arXiv.2008.11675\\n\\nNagrani, A., Seo, P. H., Seybold, B., Hauth, A., Manen, S., Sun, C., & Schmid, C. (2022). Learning Audio-Video\\n\\nModalities from Image Captions. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.), Computer Vision – ECCV 2022 (pp. 407–426). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-19781-9_24\\n\\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., & Schulman, J. (2021). WebGPT: Browser-assisted question-answering with human feedback (arXiv:2112.09332). arXiv. https://doi.org/10.48550/arXiv.2112.09332\\n\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V. A., Vainbrand, D.,\\n\\nKashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., & Zaharia, M. (2021). Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM (arXiv:2104.04473). arXiv. https://doi.org/10.48550/arXiv.2104.04473\\n\\nNguyen, T. T., Trahay, F., Domke, J., Drozd, A., Vatai, E., Liao, J., Wahib, M., & Gerofi, B. (2022). Why Globally\\n\\nRe-shuffle? Revisiting Data Shuffling in Large Scale Deep Learning. 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 1085–1096. https://doi.org/10.1109/IPDPS53621.2022.00109\\n\\nNichol, A. (2022, June 28). DALL·E 2 Pre-Training Mitigations. OpenAI. https://openai.com/blog/dall-e-2-pre-\\n\\ntraining-mitigations/\\n\\nNVIDIA. (o.D.). NCCL and MPI — NCCL 2.15.5 documentation. Retrieved 28 November 2022, from\\n\\nhttps://docs.nvidia.com/deeplearning/nccl/user-guide/docs/mpi.html\\n\\nNVIDIA Developer. (2013, July 2). CUDA Toolkit—Free Tools and Training. NVIDIA Developer.\\n\\nhttps://developer.nvidia.com/cuda-toolkit\\n\\nOfeidis, I., Kiedanski, D., & Tassiulas, L. (2022). An Overview of the Data-Loader Landscape: Comparative\\n\\nPerformance Analysis (arXiv:2209.13705). arXiv. https://doi.org/10.48550/arXiv.2209.13705\\n\\nOfer, D., Brandes, N., & Linial, M. (2021). The language of proteins: NLP, machine learning & protein\\n\\nsequences. Computational and Structural Biotechnology Journal, 19, 1750–1758. https://doi.org/10.1016/j.csbj.2021.03.022\\n\\nOpen MPI. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_MPI&oldid=1120683830 OpenAI. (2022a, July 18). Reducing Bias and Improving Safety in DALL·E 2. OpenAI.\\n\\nhttps://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/\\n\\nOpenAI. (2022b, November 30). ChatGPT: Optimizing Language Models for Dialogue. OpenAI.\\n\\nhttps://openai.com/blog/chatgpt/\\n\\nPaaß, G., & Giesselbach, S. (2023). Foundation Models for Natural Language Processing. Springer Cham.\\n\\nhttps://link.springer.com/book/9783031231896\\n\\nPapers with Code—Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (o.D.).\\n\\nRetrieved 10 January 2023, from https://paperswithcode.com/paper/exploring-the-limits-of-transfer- learning\\n\\nParliament, E. (2018). European Parliament resolution of 11 September 2018 on language equality in the digital\\n\\nage (2018/2028(INI)). https://www.europarl.europa.eu/doceo/document/TA-8-2018-0332_DE.html\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L.,\\n\\nDesmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library (arXiv:1912.01703). arXiv. https://doi.org/10.48550/arXiv.1912.01703\\n\\nPatterson, D., Gonzalez, J., Hölzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., &\\n\\nDean, J. (2022). The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. Computer, 55(7), 18–28. https://doi.org/10.1109/MC.2022.3148714\\n\\nPhilippe Lorenz & Kate Saslow. (2019). Demystifying AI & AI Companies. What foreign policy makers need to\\n\\nknow about the global AI industry. https://www.stiftung-nv.de/de/publikation/demystifying-ai-ai- companies-what-foreign-policy-makers-need-know-about-global-ai\\n\\nGroße KI-Modelle für Deutschland\\n\\n237\\n\\nPiloto, L. S., Weinstein, A., Battaglia, P., & Botvinick, M. (2022). Intuitive physics learning in a deep-learning\\n\\nmodel inspired by developmental psychology. Nature Human Behaviour, 6(9), Article 9. https://doi.org/10.1038/s41562-022-01394-8\\n\\nPoretschkin, M. (2022). ZERTIFIZIERTE KI | Qualität sichern. Fortschritt gestalten. ZERTIFIZIERTE KI.\\n\\nhttps://www.zertifizierte-ki.de/\\n\\nPoretschkin, M., Mock, M., & Wrobel, S. (2021). Zur Systematischen Bewertung der Vertrauenswürdigkeit von\\n\\nKI-Systemen. Regulierung Für Algorithmen Und Künstliche Intelligenz, 175–202. https://doi.org/10.5771/9783748927990\\n\\nPushkarna, M., Zaldivar, A., & Kjartansson, O. (2022). Data Cards: Purposeful and Transparent Dataset\\n\\nDocumentation for Responsible AI (arXiv:2204.01075). arXiv. https://doi.org/10.48550/arXiv.2204.01075\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\\n\\nKrueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 38th International Conference on Machine Learning, 8748–8763. https://proceedings.mlr.press/v139/radford21a.html\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring\\n\\nthe limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1–67.\\n\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation\\n\\nwith CLIP Latents (arXiv:2204.06125). arXiv. https://doi.org/10.48550/arXiv.2204.06125\\n\\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., & de Freitas, N. (2022). A Generalist Agent (arXiv:2205.06175). arXiv. https://doi.org/10.48550/arXiv.2205.06175\\n\\nRen, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T.-Y. (2022). FastSpeech 2: Fast and High-Quality End-to-\\n\\nEnd Text to Speech (arXiv:2006.04558). arXiv. https://doi.org/10.48550/arXiv.2006.04558\\n\\nResearch and Markets ltd. (2021, June). Europe Data Center Colocation Market: Market Size, Forecast, Insights,\\n\\nand Competitive Landscape. https://www.researchandmarkets.com/reports/5511065/europe-data- center-colocation-market-market\\n\\nReuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., & Kepner, J. (2022). AI and ML Accelerator Survey\\n\\nand Trends (arXiv:2210.04055). arXiv. https://doi.org/10.48550/arXiv.2210.04055\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models (arXiv:2112.10752). arXiv. https://doi.org/10.48550/arXiv.2112.10752\\n\\nSchödwell, B., Zarnekow, D. R., Liu, R., Gröger, J., & Wilkens, M. (2018). Kennzahlen und Indikatoren für die Beurteilung der Ressourceneffizienz von Rechenzentren und Prüfung der praktischen Anwendbarkeit.\\n\\nSchreiner, M. (2022, January 29). Meta’s AI chief: Three major challenges of artificial intelligence. THE DECODER.\\n\\nhttps://the-decoder.com/metas-ai-chief-three-major-challenges-of-artificial-intelligence/\\n\\nSchuhmann, C. (2021, August 8). LAION-400-MILLION OPEN DATASET | LAION. LAION.\\n\\nhttps://laion.ai/blog/laion-400-open-dataset\\n\\nSevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., & Villalobos, P. (2022). Compute Trends Across Three\\n\\nEras of Machine Learning (arXiv:2202.05924). arXiv. https://doi.org/10.48550/arXiv.2202.05924\\n\\nShuster, K., Poff, S., Chen, M., Kiela, D., & Weston, J. (2021). Retrieval Augmentation Reduces Hallucination in\\n\\nConversation (arXiv:2104.07567). arXiv. https://doi.org/10.48550/arXiv.2104.07567 Simons, G. J., & Frese, A. (2021). Zukunft regional – digital: Das Rheinische Revier; Machbarkeitsstudie\\n\\nDateninfrastrukturen im Rheinischen Revie. Ministerium für Wirtschaft, Innovation, Digitalisierung und Energie des Landes Nordrhein-Westfalen. https://www.wirtschaft.nrw/sites/default/files/documents/machbarkeitsstudie_dateninfrastrukturen _lang_de.pdf\\n\\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D.,\\n\\nGupta, S., & Taigman, Y. (2022). Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv:2209.14792). arXiv. https://doi.org/10.48550/arXiv.2209.14792\\n\\nSohofi, A., Yu, T., Aribal, A., Loetzsch, W., Team, S. D., & Wollmann, T. (2022). Squirrel [Python]. https://github.com/merantix-momentum/squirrel-core (Original work published 2022)\\n\\nSonnenburg, S., Braun, M. L., Ong, C. S., Bengio, S., Bottou, L., Holmes, G., LeCun, Y., Müller, K.-R., Pereira, F.,\\n\\nRasmussen, C. E., R&#228, G., tsch, Schölkopf, B., Smola, A., Vincent, P., Weston, J., & Williamson, R. (2007). The Need for Open Source Software in Machine Learning. Journal of Machine Learning Research, 8(81), 2443–2466.\\n\\nGroße KI-Modelle für Deutschland\\n\\n238\\n\\nStobbe, Dr. L., Proske, M., Zedel, H., Hintemann, Dr. R., Clausen, Dr. J., & Beucker, Dr. S. (2015). Entwicklung\\n\\ndes IKT-bedingten Strombedarfs in Deutschland [Abschlussbericht]. Fraunhofer-Institut für Zuverlässigkeit und Mikrointegration. https://www.bmwk.de/Redaktion/DE/Downloads/E/entwicklung-des-ikt-bedingten-strombedarfs-in- deutschland-abschlussbericht.pdf?__blob=publicationFile&v=3\\n\\nStöcker, C., & Dambeck, H. (2006, December 19). Deutsch-französische Suchmaschine: Quaero ist geplatzt. Der Spiegel. https://www.spiegel.de/netzwelt/web/deutsch-franzoesische-suchmaschine-quaero-ist- geplatzt-a-455558.html\\n\\nStreim, A. (2022, November 16). Trotz Krieg und Krisen: In Deutschland fehlen 137.000 IT-Fachkräfte |\\n\\nPresseinformation | Bitkom e.V. https://www.bitkom.org/Presse/Presseinformation/Deutschland- fehlen-137000-IT-Fachkraefte\\n\\nSuzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H.,\\n\\nZhou, D., & Wei, J. (2022). Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them (arXiv:2210.09261). arXiv. https://doi.org/10.48550/arXiv.2210.09261\\n\\nSystem and Organization Controls: SOC Suite of Services. (o.D.). AICPA. Retrieved 28 November 2022, from\\n\\nhttps://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html\\n\\nTachyum. (2022, October 4). Tachyum Unveils Details of Architecture and Prodigy Design in Overview White Paper | Tachyum. https://www.tachyum.com/media/press-releases/2022/10/04/tachyum-unveils-details-of- architecture-and-prodigy-design-in-overview-white-paper/\\n\\nTamkin, A., & Ganguli, D. (2021, February 5). How Large Language Models Will Transform Science, Society, and AI. Stanford HAI. https://hai.stanford.edu/news/how-large-language-models-will-transform-science- society-and-ai\\n\\nTerraform. (2022). [Go]. HashiCorp. https://github.com/hashicorp/terraform (Original work published 2014) THE NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH RESOURCE TASK FORCE (NAIRRTF). (o.D.). National Artificial\\n\\nIntelligence Initiative. Retrieved 10 January 2023, from https://www.ai.gov/nairrtf/\\n\\nThe role of data centers in an interconnected world—DECIX – Without You. (o.D.). DECIX – Without You.\\n\\nRetrieved 19 December 2022, from https://withoutyou.de-cix.net/the-role-of-data-centers/ Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., … Le, Q. (2022). LaMDA: Language Models for Dialog Applications (arXiv:2201.08239). arXiv. https://doi.org/10.48550/arXiv.2201.08239\\n\\nUng, M., Xu, J., & Boureau, Y.-L. (2022). SaFeRDialogues: Taking Feedback Gracefully after Conversational\\n\\nSafety Failures. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 6462–6481. https://doi.org/10.18653/v1/2022.acl-long.447 van Rossum, G. (1995). Python reference manual (R 9525). Article R 9525. https://ir.cwi.nl/pub/5008 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).\\n\\nAttention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems (Vol. 30, pp. 5998–6008). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\n\\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). OFA: Unifying\\n\\nArchitectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework (arXiv:2202.03052). arXiv. https://doi.org/10.48550/arXiv.2202.03052\\n\\nWannemacher, K., & Bodmann, L. (2021). Künstliche Intelligenz an den Hochschulen. Potenziale und\\n\\nHerausforderungen in Forschung, Studium und Lehre sowie Curriculumentwicklung: Arbeitspapier Nr. 59. Berlin: Hochschulforum Digitalisierung. https://hochschulforumdigitalisierung.de/sites/default/files/dateien/HFD_AP_59_Kuenstliche_Intellige nz_Hochschulen_HIS-HE.pdf\\n\\nWeidmann, Dr. R. E., & Krüger, Dr. T. (2020, November 30). Dr. Béla Waldhauser, Telehouse Deutschland:\\n\\nDigitale Effizienz in Rechenzentren. https://detecon.com/de/journal/dr-bela-waldhauser-telehouse- deutschland-digitale-effizienz-rechenzentren\\n\\nWikipedia contributors. (2022). Selene (supercomputer)—Wikipedia, The Free Encyclopedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Selene_(supercomputer)&oldid=1109224992\\n\\nWu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., & Duan, N. (2022). NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.),\\n\\nGroße KI-Modelle für Deutschland\\n\\n239\\n\\nComputer Vision – ECCV 2022 (pp. 720–736). Springer Nature Switzerland. https://doi.org/10.1007/978- 3-031-19787-1_41\\n\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2021). mT5: A\\n\\nmassively multilingual pre-trained text-to-text transformer (arXiv:2010.11934). arXiv. https://doi.org/10.48550/arXiv.2010.11934\\n\\nYazdani-Jahromi, M., Yousefi, N., Tayebi, A., Kolanthai, E., Neal, C. J., Seal, S., & Garibay, O. O. (2022).\\n\\nAttentionSiteDTI: an interpretable graph-based model for drug-target interaction prediction using NLP sentence-level relation classification. Briefings in Bioinformatics, 23(4). https://doi.org/10.1093/bib/bbac272\\n\\nYin, P., Neubig, G., Yih, W., & Riedel, S. (2020). TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8413–8426. https://doi.org/10.18653/v1/2020.acl-main.745\\n\\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple Linux Utility for Resource Management. In D. Feitelson, L. Rudolph, & U. Schwiegelshohn (Eds.), Job Scheduling Strategies for Parallel Processing (pp. 44–60). Springer. https://doi.org/10.1007/10968987_3\\n\\nYuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models.\\n\\n27th International Conference on Intelligent User Interfaces, 841–852. https://doi.org/10.1145/3490099.3511105\\n\\nYuan, B., He, Y., Davis, J. Q., Zhang, T., Dao, T., Chen, B., Liang, P., Re, C., & Zhang, C. (2022). Decentralized\\n\\nTraining of Foundation Models in Heterogeneous Environments (arXiv:2206.01288). arXiv. https://doi.org/10.48550/arXiv.2206.01288\\n\\nZellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., & Choi, Y. (2022).\\n\\nMERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound (arXiv:2201.02639). arXiv. https://doi.org/10.48550/arXiv.2201.02639\\n\\nZhang, D., Maslej, N., Brynjolfsson, E., Etchemendy, J., Lyons, T., Manyika, J., Ngo, H., Niebles, J. C., Sellitto, M., Sakhaee, E., Shoham, Y., Clark, J., & Perrault, R. (2022). Artificial Intelligence Index Report 2022. Stanford Institute for Human-Centered AI, Stanford University. https://aiindex.stanford.edu/wp- content/uploads/2022/03/2022-AI-Index-Report_Master.pdf\\n\\nZhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., & Chen, W. (2021). Poolingformer: Long document modeling with pooling attention. International Conference on Machine Learning, 12437–12446. https://arxiv.org/abs/2105.04371\\n\\nZhang, Y., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang, R., Le, Q. V., & Wu, Y. (2020). Pushing the Limits of Semi-\\n\\nSupervised Learning for Automatic Speech Recognition (arXiv:2010.10504). arXiv. https://doi.org/10.48550/arXiv.2010.10504\\n\\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2022). Automatic Chain of Thought Prompting in Large Language Models\\n\\n(arXiv:2210.03493). arXiv. https://doi.org/10.48550/arXiv.2210.03493\\n\\nGroße KI-Modelle für Deutschland\\n\\n240\\n\\nII. Abbildungsverzeichnis Abbildungs- nummer\\n\\nName\\n\\nAbb. 1\\n\\nAbb. 2\\n\\nAbb. 3\\n\\nAbb. 4\\n\\nAbb. 5\\n\\nAbb. 6\\n\\nAbb. 7\\n\\nAbb. 8\\n\\nAbb. 9\\n\\nAbb. 10\\n\\nAnzahl der Parameter großer KI-Sprachmodelle seit GPT-3 (Open Source Modelle rot markiert). Quelle: state of ai Report 2022 (Benaic & Hogarth, 2022) Verdeutlichung der Self-Attention am Satz \"Die Bank verleiht Geld\". Das Token Bank (unten) hat eine hohe Korrelation mit dem Token Geld (oben), wobei die Korrelation zu den anderen Token geringer ausfällt. Zentraler Bestandteil der Foundation-Modelle sind Schichten mit Self-Attention Blöcken (blau), die kontextsensitive Einbettungsvektoren (violett) von Eingabetoken (grün) berechnen. Die logistische Schicht L prognostiziert die Wahrscheinlichkeit der Ausgabetoken. Beim Training werden die Parameter so optimiert, dass die Wahrscheinlichkeiten der korrekten fehlenden bzw. nächsten Token (gelb) möglichst hoch sind. Ein Foundation-Modell kann Informationen aus verschiedenen Datenquellen verschiedener Modalitäten berücksichtigen. Dieses eine Modell kann dann eine Vielzahl von nachgelagerten Aufgaben lösen (Bommasani et al., 2021). Die Genauigkeit des „few-shot\"-Lernens von GPT-3 wird durch die Erweiterung der Modellgröße und der Anzahl der präsentierten Beispiele erhöht (Brown et al., 2020). Von einem Foundation-Modell mit Hilfe von Retrieval gefundene Antwort auf eine Frage im Natural Question Benchmark. Aktuelle Modelle erreichen eine Genauigkeit (F1) von 80% (Zhanag et al., 2021). Zusammenfassung eines Textes von 800 Wörtern durch das Modell BRIO im Vergleich zu der von Experten erstellten Zusammenfassung (Liu et al., 2022). Zu unterschiedlichen Texten von DALL-E 2 erzeugte Bilder (Ramesh et al., 2022) Zu unterschiedlichen Texten von CogVideo erzeugte Videos (Hong et al., 2022) Das Gato-Modell generiert aus den aktuellen Zuständen (hellblau) neue Aktionen (dunkelblau). Die Umgebung produziert daraus neue Zustände, usw. Das Modell kann Texte, Messwerte, Bilder, usw. verarbeiten. Um 2016 tauchte ein neuer Trend zu sehr großen Modellen auf, die von großen Internetfirmen trainiert wurden (rot). Diese waren in der Lage waren, die notwendigen Investitionen zu finanzieren. Die untere blaue Linie\\n\\nAbb. 11\\n\\nGroße KI-Modelle für Deutschland\\n\\nSeite\\n\\n19\\n\\n24\\n\\n27\\n\\n29\\n\\n31\\n\\n35\\n\\n36\\n\\n40\\n\\n41\\n\\n44\\n\\n55\\n\\nAbb. 12\\n\\nAbb. 13\\n\\nAbb. 14\\n\\nAbb. 15\\n\\nAbb. 16\\n\\nAbb. 17\\n\\nAbb. 18\\n\\nAbb. 19\\n\\nAbb. 20\\n\\nAbb. 21\\n\\nAbb. 22\\n\\nAbb. 23\\n\\nAbb. 24\\n\\nAbb. 25 Abb. 26 Abb. 27\\n\\n241\\n\\nveranschaulicht den Berechnungsaufwand der anderen Modelle, z.B. von Universitäten (Sevilla et al., 2022). Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach Ländern Trainingsleistung (1018 FLOPs = 1 ExaFLOPs) gegenüber Veröffentlichungsjahr nach Ländern. Der Abbildung (intern)1.12: Großteil der Foundation-Modelle wurden in den USA entwickelt. Aus Deutschland und anderen EU Ländern wurden tendenziell kleinere Foundation-Modelle mit niedrigerem Trainingsaufwand veröffentlicht. Das BLOOM Modell stellt eine Ausnahme dar und wurde von einem Wissenschaftskollektiv bestehend aus über 250 Institutionen auf dem Jean Zay Supercomputer in Frankreich trainiert. Ergebnisse der Umfrage mit KMUs zu deren Einsatz von Foundation-Modellen Ergebnisse der Umfrage mit KMUs zu Hindernissen beim Einsatz von Foundation-Modellen Ergebnisse der Umfrage mit KMUs zur Bedeutung von unterschiedlichen Aspekten der Foundation-Modell- Entwicklung Ergebnisse der Umfrage mit KMUs zur Relevanz von unterschiedlichen Arten von KI-Modellen bei der Foundation-Modell-Entwicklung (Antworten mit einer Antwortrate von weniger als 20 % wurden ausgelassen. Die vollständigen Antworten befinden sich in Anhang A.2.) Simplifizierte Darstellung der Hard- und Software- Infrastruktur von HPCs Die Architektur des Trainings- & Applikations-Layers im Detail Die Architektur des System- und Data-Storage & Loading- Layers im Detail Die Architektur des Framework- & Service-Layers im Detail MLPerf hardware: accelarators (Zhang et al., 2022, S.18)\\n\\nBeispielrechnung Bau und Betrieb eines eigenen HPC- Rechenzentrums Delphi-Befragung: Wie beurteilen Sie folgende Risiken für die Entwicklung des Rechenzentrumsmarktes in Deutschland? (Hintemann et al., 2022, S. 37) Organisationseinheiten des LEAM-KI-Servicezentrums Das LEAM-Board als zentrale Governance-Einheit des LKS Szenario für die Struktur des LEAM-KI-Servicezentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n56\\n\\n57\\n\\n69\\n\\n70\\n\\n72\\n\\n73\\n\\n110\\n\\n113\\n\\n120\\n\\n122\\n\\n138\\n\\n160\\n\\n162\\n\\n169 181 225\\n\\n242\\n\\nIII. Tabellenverzeichnis\\n\\nTabellen- nummer\\n\\nName\\n\\nTabelle 1\\n\\nTabelle 2\\n\\nTabelle 3\\n\\nTabelle 4\\n\\nTabelle 5\\n\\nTabelle 6 Tabelle 7 Tabelle 8\\n\\nEine Auswahl möglicher Anwendungen auf Basis von Sprachmodellen Eine Auswahl möglicher Anwendungen auf Basis von multimodalen Modellen Übersicht der wichtigsten Information zu der Umfrage für die Wirtschaft Befragte Experten aus der Wirtschaft Im Betrieb von LEAM werden für Training, Tuning und Inference Tausende GPUs benötigt Beispiele für Rechenzentren Größen von Rechenzentren Übersicht über die Kühlmöglichkeiten in Rechenzentren Compute Anforderungen für die Berechnung eines Foundationmodells\\n\\nTabelle 9\\n\\nTabelle 10 HPC-Standorte in Deutschland Tabelle 11 Tabelle 12 Tabelle 13 Übersicht über die OE Housing Tabelle 14 Übersicht über die Training-as-a-Service Tabelle 15 Übersicht der Kosten des LEAM-KI-Servicezentrums\\n\\nVerfügbarkeitsklassen (VK1 - VK4) Schutzklassen nach DIN EN 50600-1\\n\\nTabelle 16\\n\\nTabelle 17\\n\\nGesamtkostenstruktur des LEAM-KI-Servicezentrums bei einer Abschreibungsdauer von vier Jahren Kosten des LEAM-KI-Servicezentrums bei einem Einkauf der GPU-RZ-Leistung\\n\\nTabelle 18 Übersicht der Einnahmen durch das Model-Training Tabelle 19 Übersicht der Einnahmen durch die Beratung\\n\\nTabelle 20\\n\\nTabelle 21\\n\\nGegenüberstellung der drei Finanzierungsszenarien für das LKS Übersicht der Vor- und Nachteile der drei Finanzierungsmodelle mit Hinblick auf die rechtlichen Rahmenbedingungen\\n\\nTabelle 22 Übersicht über die drei Online-Umfragen Tabelle 23 Übersicht der Interviewpartner:innen im Bereich Wirtschaft\\n\\nTabelle 24\\n\\nÜbersicht der Interviewpartner:innen im Bereich Rechenzentrum und Hardware\\n\\nTabelle 25 Übersicht der Interviewpartner:innen im Bereich Wissenschaft Tabelle 26 Übersicht der sonstigen Interviewpartner:innen\\n\\nTabelle 27\\n\\nKosten für eine GPU-Stunde auf einer NVIDIA A100 Tensor Core GPU 80 GB nach Anbieter. Preise in US-Dollar wurden in Euro umgerechnet zu einem Kurs von $1 = 0,948768EUR (Dollarkurs am 06.12.2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\nSeite\\n\\n33\\n\\n39\\n\\n67\\n\\n68\\n\\n111\\n\\n128 129 132\\n\\n137\\n\\n142 152 154 170 174 185\\n\\n186\\n\\n188\\n\\n190 191\\n\\n193\\n\\n218\\n\\n320 321\\n\\n322\\n\\n323 324\\n\\n369\\n\\n243\\n\\nIV. Abkürzungsverzeichnis\\n\\nAbb\\n\\nAbbildung\\n\\nAbs\\n\\nAbsatz\\n\\nAEUV\\n\\nVertrag über die Arbeitsweise der Europäischen Union\\n\\nAI\\n\\nArtificial Intelligence - Künstliche Intelligenz\\n\\nAPI\\n\\nApplication Programming Interface\\n\\nArt\\n\\nArtikel\\n\\nB2B\\n\\nBusiness to Business\\n\\nBERT\\n\\nBidirectional Encoder Representations from Transformers\\n\\nBMWK\\n\\nBundesministerium für Wirtschaft und Klimaschutz\\n\\nbspw\\n\\nbeispielsweise\\n\\nbzgl\\n\\nbezüglich\\n\\nbzw\\n\\nbeziehungsweise\\n\\nca\\n\\ncirca\\n\\nDMZ\\n\\nDemilitarized Zone\\n\\nDNA\\n\\nDesoxyribonukleinsäure\\n\\nEU\\n\\nEuropäische Union\\n\\nF&E\\n\\nForschung und Entwicklung\\n\\nFLOP\\n\\noder FLOPs - Floating Point Operations\\n\\nFTE\\n\\nFull-Time-Equivalents\\n\\nGG\\n\\nGrundgesetz\\n\\nggf\\n\\nGegebenenfalls\\n\\nGPT\\n\\nGenerative Pre-trained Transformer\\n\\nGWB\\n\\nGesetz gegen Wettbewerbsbeschränkungen\\n\\nHPC\\n\\nHigh Performance Computing\\n\\nIaaS\\n\\nInfrastruktur-as-a-Service\\n\\nIT\\n\\nInformation Technologies\\n\\nGroße KI-Modelle für Deutschland\\n\\nKI\\n\\nKMU\\n\\nLEAM\\n\\nLKS\\n\\nMio\\n\\nML\\n\\nMrd\\n\\nNLP\\n\\nNLU\\n\\no.D.\\n\\nOE\\n\\nOSS\\n\\nPers\\n\\nPPP\\n\\nPUE\\n\\nRZ\\n\\nS\\n\\nSeq2Seq\\n\\nsog\\n\\nTVöD\\n\\nu.a.\\n\\nUSV\\n\\nusw\\n\\nvgl\\n\\nVPN\\n\\nz.B.\\n\\nZiff\\n\\n244\\n\\nKünstliche Intelligenz\\n\\nkleine und mittlere Unternehmen\\n\\nLarge European Artificial Intelligence Models\\n\\nLEAM KI-Servicezentrum\\n\\nMillionen\\n\\nMachine Learning\\n\\nMilliarden\\n\\nNatural Language Processing\\n\\nNatural Language Understanding\\n\\nohne Datum\\n\\nOrganisationseinheit\\n\\nOpen Source Software\\n\\nPerson\\n\\nPublic-Private-Partnership\\n\\nPower usage effectiveness\\n\\nRechenzentrum\\n\\nSeite\\n\\nSequence to Sequence\\n\\nsogenannte\\n\\nTarifvertrag für den öffentlichen Dienst\\n\\nunter anderem\\n\\nunterbrechungsfreie Stromversorgung\\n\\nUnd so weiter\\n\\nvergleiche\\n\\nVirtual Private Network\\n\\nzum Beispiel\\n\\nZiffer\\n\\nGroße KI-Modelle für Deutschland\\n\\n245\\n\\nV. Methodik der Machbarkeitsstudie\\n\\nIm Folgenden werden die bei der Durchführung der Machbarkeitsstudie angewandten Methoden beschrieben. Sie lassen sich in drei übergeordnete Bereiche einteilen: Literatur- und Internetrecherche, die Erhebung von Primärdaten in qualitativer und quantitativer Form sowie deren Analyse.\\n\\nZur besseren Nachvollziehbarkeit für den/die Leser:in können die einzelnen Schritte der methodischen Vorgehensweise wie folgt umrissen werden:\\n\\n1. Literatur-\\n\\nKI-Foundation-Modellen, Softwareanforderungen, Hochleistungsrechenzentren sowie organisatorische und finanzielle Rahmenbedingungen Identifikation relevanter Zielgruppen für die quantitative Online-Umfrage:\\n\\nund\\n\\nInternetrecherche\\n\\nzu\\n\\n2.\\n\\na. Kleine und mittlere KI-Unternehmen sowie KI-Initiativen b. Großunternehmen mit KI-Abteilungen c. KI-Forscher:innen\\n\\n3. Erstellung, Versendung und Auswertung einer Online-Umfrage pro Zielgruppe 4. a. Großunternehmen mit KI-Abteilungen b. Betreiber von Rechenzentren und Expert:innen von Hardware c. KI-Forscher:innen d. Sonstige (bspw. KI-Initiativen, Vertreter der Bundesländer)\\n\\n5. Erstellung mehrerer Leitfäden/ Fragenkataloge basierend auf den Zielgruppen 6. Durchführung und Auswertung der leitfadengestützten Interviews 7. Beurteilung der Machbarkeit anhand aller vorliegenden Informationen\\n\\nInternet- und Literaturrecherche Die Autor:innen der einzelnen Kapitel nutzten Internet- und Literaturrecherche als Ausgangspunkt für die Studie. Sie bietet in vielen Fällen einen geeigneten Einstieg in die Thematik. Die Erkenntnisse flossen außerdem in die Online-Umfrage sowie die Interviewleitfäden ein.\\n\\nIm Rahmen der Recherchen wurden relevante Quellen aus der Literatur genutzt, um grundlegendes Wissen ihren Eigenschaften und Besonderheiten sowie momentan existierenden Modellarten zu gewinnen. Außerdem wurde der aktuelle Entwicklungsstand und Einsatz von KI-Foundation-Modellen im internationalen Vergleich recherchiert.\\n\\nzu KI-Foundation-Modellen,\\n\\nIm Bereich der Softwareanforderungen wurden aktuelle Studien und das Expertenwissen der Autor:innen im Bereich der Foundation-Modellentwicklung herangezogen, um eine ausführliche Übersicht über die benötigte Software und das Personal zu erhalten. Darüber hinaus wurde die bestehende Literatur verwendet, um einen Überblick über bestehende Rechenzentren und die Herausforderungen für das KI-Training zu gewinnen. Schließlich bot die Recherche auch einen Einstieg in eine mögliche Governance, Gesellschaftsform sowie Finanzierungsmöglichkeiten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n246\\n\\nDatenerhebung\\n\\nDie Datenerhebung bestand aus zwei Aspekten: quantitativen Online-Umfragen und qualitativen Experteninterviews. Die quantitativen Daten wurden in Form einer webbasierten Online-Umfrage erhoben und an drei Akteursgruppen versendet:\\n\\n(1) Kleine und mittlere KI-Unternehmen & KI-Start-Ups, (2) Großunternehmen und (3) KI-Wissenschaftler:innen.\\n\\nDas Ziel der qualitativen Datenerhebung war der Gewinn eines allgemeinen Überblicks über den wirtschaftlichen Bedarf von KI- Foundation-Modellen und deren technischen Voraussetzungen. Die qualitativen, durch wissenschaftlichen und leitfadengestützten Interviews wurden mit ausgewählten Expert:innen aus den Bereichen Wissenschaft, Wirtschaft, Politik und Recht durchgeführt. In den Interviews konnte tiefer auf Erkenntnisse aus der quantitativen Umfrage sowie neu aufkommende Themen eingegangen werden.\\n\\nOnline-Umfragen Die Online-Umfrage wurde maßgeblich aus der Internet- und Literaturrecherche abgeleitet und dient dazu, wesentliche Faktoren wie beispielsweise die Relevanz, Nutzung und Entwicklung verschiedener KI-Foundation-Modelle quantitativ messbar zu machen. Die Befragung fand vom 31. Oktober bis zum 23. November 2022 statt.\\n\\nDie drei Fragenkataloge wurden von den Projektpartnern der Machbarkeitsstudie unter Koordination des KI Bundesverbands ausgearbeitet. Sie bestehen aus 20 - 46 Fragen (s. Tabelle 22), die überwiegend in Multiple-Choice-Form aufgebaut sind. Bei der Auswertung ihrer Ergebnisse ist somit zu berücksichtigen, dass pro Frage meist mehrere Antwortmöglichkeiten gegeben waren. Zusätzlich zu den Multiple-Choice Fragen wurden demographische Angaben erfragt und zum Schluss durch ein offenes Kommentarfeld die Möglichkeit gegeben, weitere Anmerkungen zu den Anforderungen von KI-Foundation- Modellen oder zu der Machbarkeitsstudie zu machen. Im Anhang A.1 sind die Umfragen im Detail zu finden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n247\\n\\nUm eine hohe quantitative Fallzahl zu erlangen, wurden die drei Umfragen an ein breites Spektrum an Kontakten gesendet. Insgesamt gab es 71 Rückmeldungen von den kleinen und mittleren KI-Unternehmen und 21 Rückmeldungen von KI-Wissenschaftler:innen. Aufgrund der geringen Rücklaufquote bei den Großunternehmen wurden ausgewählte Antwortpartner:innen der Online-Umfrage zusätzlich um Interviews gebeten. Somit liegt bei dieser Auswertung der Fokus auf den Interviews.\\n\\nTabelle 22: Übersicht über die drei Online-Umfragen\\n\\nGroße KI-Modelle für Deutschland\\n\\n248\\n\\nExperteninterviews Den Interviews lagen Leitfäden zu Grunde, die eine offene Beantwortung der Fragen vorsahen. Dabei gab es für jede Zielgruppe eigene Leitfäden mit Fokus auf deren Expertise. Die Interviews wurden per Videokonferenz durchgeführt. Der Zeitraum der Befragung war vom 25. Oktober 2022 bis zum 12. Januar 2023.\\n\\nBei der Auswahl der Interviewpartner:innen wurde zunächst eine Identifikation und Einstellung relevanter Akteursgruppen vorgenommen. Daraus hervorgehend wurden die folgenden Gruppen bzw. Institutionen herausgestellt:\\n\\nGroßunternehmen\\n\\nKI-Forscher:innen\\n\\nRechenzentrums- und Hardwareexpert:innen\\n\\nSonstige Expert:innen (bspw. regionale Vertreter, KI-Initiativen, etc.)\\n\\nEs wurden insgesamt Interviews mit 71 Gesprächspartner:innen durchgeführt. Die vollständige Liste der Interviewpartner:innen befindet sich im Anhang B.1.\\n\\nDie Auswertung der unterschiedlichen Daten wurde von den verantwortlichen Projektpartner:innen unternommen und mit den Ergebnissen der Literatur- und Internetrecherche verbunden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n249\\n\\nAuswertung der Online-Umfragen Zur Auswertung der quantitativen Datenerhebung wurden die Ergebnisse der Online- Umfrage von KI-Unternehmen/KI-Initiativen, auf der einen Seite, und KI-Forscher:innen, auf der anderen, jeweils gesammelt, ausgewertet und grafisch aufbereitet. Die offenen Fragen wurden zur Quantifizierbarkeit auf spezifische Schlüsselwörter durchsucht. Eine grafische Aufarbeitung der Multiple-Choice Fragen befindet sich in Anhang A.2. Aufgrund der geringen Teilnehmerzahl fand keine Auswertung der Umfrage an Großunternehmen statt.\\n\\nAuswertung der Experteninterviews Um die qualitativen Daten in Form der leitfadengestützten Interviews auszuwerten, wurden die Interviews transkribiert und in Ergebnisprotokollen zusammengefasst. Im Falle der KI-Forscher:innen wurden die Ergebnisse in einer Interviewmatrix für eine interne Auswertung zusammengefasst. Die Interviewten aus der Wirtschaft wurden gebeten, die Ergebnisprotokolle freizugeben. Die bis zum Redaktionsschluss freigegebenen Ergebnisprotokolle befinden sich im Anhang B.3.\\n\\nfür den Druck\\n\\nGroße KI-Modelle für Deutschland\\n\\n250\\n\\nD O W N L O A D D E R K O M P L E T T E N S T U D I E M I T A L L E N A N H Ä N G E N U N T E R:\\n\\nhttps://leam.ai/feasibility-study-leam-2023/\\n\\nLEAM Machbarkeitsstudie V1/01_2023\\n\\nGroße KI-Modelle für Deutschland\\n\\n251\\n\\nAnhang der LEAM Machbarkeitsstudie\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n252\\n\\nAnhang\\n\\nAnhang A - Zusätzliche Information zu den Umfragen\\n\\nAnhang A.1 - Die Umfragen\\n\\nUmfrage 1: Kleine und mittlere Unternehmen\\n\\nFoundation-Modelle\\n\\n1. Arbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n2. Inwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation-Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\na. Welche Voraussetzungen müssen erfüllt werden, damit Sie Foundation-\\n\\nModelle nutzen würden?\\n\\nb. Inwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation-\\n\\nModellen? Inwiefern passen Sie existierende Foundation-Modelle an (Tuning)?\\n\\nc. d. Inwiefern entwickeln Sie eigene Foundation-Modelle?\\n\\n3. In welchen Bereichen sind für Sie Foundation-Modelle in Nutzung und Entwicklung relevant?\\n\\nGesamtwirtschaftliche Bedeutung\\n\\n4. Welche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n5. Welche Bedeutung messen Sie den Foundation-Modellen für die KI und damit für gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n6. Wie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\nFoundation-Modell Entwicklung\\n\\n7. Welche Bedeutung hat für Sie die Verfügbarkeit von Foundation-Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n8. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenschaftlichen Institutionen?\\n\\n9. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Start- ups?\\n\\n10. Haben Sie weitere Anmerkungen zu den Themen Bedeutung von, Gefahren durch und wissenschaftliche Fragestellungen zu Foundation-Modellen?\\n\\nTechnologische Anforderungen\\n\\n11. Rechenleistung: Wie viele GPUs und andere Beschleuniger werden z.B. für Training und Inferenz von state-of-the-art Foundation-Modellen benötigt?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n253\\n\\n12. Verfügbarkeit: Welche Kriterien müssen Service Level Agreements (SLA) erfüllen, um die Entwicklung von Foundation-Modellen möglich zu machen?\\n\\n13. Anforderungen der Entwicklung von Foundation-Modellen an die Software- Infrastruktur\\n\\n14. Scheduling Infrastructure: Welche Scheduling Infrastructures sollen bei der Entwicklung eingesetzt werden?\\n\\n15. Deployment Infrastructure: Welche Deployment Infrastructures müssen bei der Entwicklung unterstützt werden?\\n\\n16. Access Control Levels & Datenhoheit: Wie soll das Laden der Daten in der Entwicklung angesteuert werden?\\n\\n17. Anforderungen der Entwicklung von Foundation-Modellen an die Trainings- Software\\n\\n18. Frameworks: Welche Frameworks sollen bei der Entwicklung von Foundation- Modellen eingesetzt werden?\\n\\n19. Inference: Welche Voraussetzungen sollen Service Level Agreements (SLAs) mindestens erfüllen?\\n\\na. Inference: Welche Voraussetzungen sollen Service Level Agreements\\n\\n(SLAs) bei der Verfügbarkeit mindestens erfüllen?\\n\\nb. Inference: Welche Voraussetzungen sollen Service Level Agreements\\n\\n(SLAs) bei der erlaubten Latenz mindestens erfüllen?\\n\\n20. Anforderungen der Entwicklung von Foundation-Modellen an den Data-Storage- Layer\\n\\n21. Internet- & Bandbreite: Wie schnell muss die Internetverbindung für die Entwicklung von Foundation-Modellen mindestens sein?\\n\\n22. Speicherplatz für Daten: Wie hoch ist der Speicherbedarf für die Entwicklung eines Foundation-Modells?\\n\\n23. Compliance: Welche regulatorischen Voraussetzungen müssen erfüllt werden, um Foundation-Modelle zu entwickeln?\\n\\na. Können sie die ISO/Normen benennen, die beachtet werden müssen?\\n\\nAnforderungen der Entwicklung von Foundation-Modellen an Daten\\n\\n24. Beschäftigen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n25. Wissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n26. Ist Ihnen das Konzept der Model Cards / Data Set Cards im Bezug zur Erstellung neuer Daten geläufig?\\n\\na. Ist das Konzept für Ihre Domäne relevant oder hilfreich?\\n\\nAnforderungen der Entwicklung von Foundation-Modellen an Mitarbeiter:innen\\n\\n27. Welche Personalrollen (oder äquivalent) sind für die Entwicklung von Foundation- Modellen erforderlich?\\n\\n28. In welchen dieser Personalrollen beobachten oder antizipieren Sie einen Mangel, um Foundation-Modelle zu entwickeln?\\n\\nWirtschaftliche Anforderungen\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n254\\n\\n29. Welche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n30. Welche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n31. In welchem Zeitraum erwarten Sie, diese Investitionskosten zu amortisieren? 32. Wie erwarten Sie, dass sich diese Investitionskosten amortisieren könnten?\\n\\nZivilgesellschaftliche Anforderungen\\n\\n33. Wie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n34. Empfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n35. Wie sollten Ihrer Meinung nach Datensätze für die Entwicklung von Foundation- Modellen erhoben werden?\\n\\n36. Wie hoch empfinden Sie Awareness, Relevanz und Akzeptanz bzgl. Foundation- Modellen innerhalb Ihrer Organisation?\\n\\n37. Haben Sie weitere Anmerkungen zu den technologischen, wirtschaftlichen und zivilgesellschaftlichen Anforderungen der Entwicklung von KI-Foundation- Modellen?\\n\\nMetadaten\\n\\n38. Was ist Ihr Name? 39. Was ist Ihre E-Mail Adresse? 40. Wie groß ist Ihr Unternehmen? 41. Welcher Branche gehört Ihr Unternehmen an? 42. Was ist Ihre Position im Unternehmen? 43. Stehen Sie für einen weiteren Austausch zu den Themen Foundation-Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\nEnde\\n\\n44. Abschließend möchten wir Ihnen die Gelegenheit bieten, uns Reaktionen, Ideen, Wünsche, Prioritäten oder Warnungen mitzuteilen, die für unsere weitere Arbeit wichtig sein könnten.\\n\\nUmfrage 2: Große Unternehmen\\n\\nFoundation-Modelle\\n\\n1. Inwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation-Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\nNutzung\\n\\n2. In welchen Bereichen sind für Sie Foundation-Modelle in Nutzung und Entwicklung relevant?\\n\\n3. Inwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation- Modellen?\\n\\n4. Inwiefern passen Sie existierende Foundation-Modelle an (Tuning)?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n255\\n\\n5. Inwiefern entwickeln Sie eigene Foundation-Modelle?\\n\\nGesamtwirtschaftliche Bedeutung\\n\\n6. Welche Bedeutung messen Sie der Datenanalyse und KI gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n7. Welche Bedeutung messen Sie den Foundation-Modellen für die KI und damit für gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n8. Wie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\nFragen der Entwicklung\\n\\n9. Welche Bedeutung hat für Sie die Verfügbarkeit von Foundation-Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n10. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation- Modellen, die Zusammenarbeit mit wissenschaftlichen Institutionen?\\n\\n11. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation- Modellen, die Zusammenarbeit mit spezialisierten KMUs und Start-ups?\\n\\nLEAM Ziel der LEAM Initiative ist der Aufbau eines dedizierten KI-Servicezentrums für die Wissenschaft und Industrie zur Erstellung von Foundation-Modellen und der Entwicklung darauf aufbauender Anwendungen.\\n\\n12. Inwiefern würden Sie die Services von LEAM nutzen? 13. Inwiefern würden Sie bzw. Ihr Unternehmen in den Aufbau eines LEAM KI- Servicezentrums investieren (unter noch zu klärenden Voraussetzungen)?\\n\\nMetadaten\\n\\n14. Wie groß ist Ihr Unternehmen? 15. Welcher Branche gehört Ihr Unternehmen an? 16. Was ist Ihre Position im Unternehmen? 17. Was ist Ihr Name? 18. Was ist Ihre E-Mail Adresse? 19. Stehen Sie für einen weiteren Austausch zu den Themen Foundation-Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n20. Welche Anregungen, Ideen oder auch Kritikpunkte möchten Sie uns mitgeben?\\n\\nUmfrage 3: Wissenschaft\\n\\nPersönliche Daten\\n\\n1. Bitte füllen Sie Ihre persönlichen Daten aus\\n\\nFoundation-Modelle\\n\\n2. Wie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wissenschaft ein?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n256\\n\\n3. Wie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wirtschaft ein?\\n\\n4. Arbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n5. Arbeiten Sie bereits an oder wollen Sie mit Foundation-Modellen arbeiten?\\n\\nKönnen Sie uns darüber etwas erzählen? Welche Daten werden in Ihrer Organisation aktuell für das Training von Foundation-Modellen verwendet?\\n\\nWas hindert Sie daran, bereits heute Foundation-Modelle einzusetzen? ○ Wie hoch schätzen Sie die Wahrscheinlichkeit ein, Foundation-Modelle für Ihre Arbeit zu nutzen?\\n\\n6. Wie schätzen Sie die Bedeutung verschiedener Arten von Foundation-Modellen in Gegenwart und Zukunft ein?\\n\\n7. Gibt es weitere Arten von Foundation-Modellen, die Sie als bedeutend einschätzen?\\n\\n8. Wie schätzen Sie die Gefahren ein, die mitunter im Zusammenhang mit Foundation-Modellen genannt wurden?\\n\\n9. Sehen Sie zusätzliche Arten von potentiellen Gefahren? 10. Fast alle existierenden Foundation-Modelle, deren Architekturen und alle anderen wichtigen Neuerungen auf dem Gebiet kommen aus den USA und aus China. Warum? Was fehlt uns? Wo stehen wir in Deutschland im Vergleich in Bezug auf notwendige Voraussetzungen?\\n\\n11. Sehen Sie weitere Voraussetzungen, die uns fehlen? 12. Welche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zur Architektur?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Daten und Datenverarbeitung?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zum Pre-Training?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zum Fine-Tuning (Nachtraining)?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Multimedia?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Sonstigen Themen?\\n\\n13. Welche weiteren relevanten Fragestellungen sehen Sie für die Forschung - auch besonders in Ihrem eigenen Fachgebiet? (Siehe Beispiele)\\n\\n14. Wie schätzen Sie die Bedeutung von Foundation-Modellen für die folgenden Anwendungen ein?\\n\\n15. Sehen Sie weitere Anwendungen, für die Foundation-Modelle bedeutend sein werden?\\n\\n16. Haben Sie weitere Anmerkungen zu den Themen Bedeutung von, Gefahren durch und wissenschaftliche Fragestellungen zu Foundation-Modellen?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n257\\n\\nBias und Diskriminierung\\n\\n17. Beschäftigen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n18. Wissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\nWirtschaftliche Anforderungen\\n\\n19. Welche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n20. Welche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\nZivilgesellschaftliche Anforderungen\\n\\n21. Wie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n22. Empfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\nEnde\\n\\n23. Abschließend möchten wir Ihnen die Gelegenheit bieten, uns Reaktionen, Ideen, Wünsche, Prioritäten oder Warnungen mitzuteilen, die für unsere weitere Arbeit wichtig sein könnten.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n258\\n\\nAnhang A.2 - Graphische Auswertung der Umfragen\\n\\nDie detaillierte Auswertung der Umfragen sind zu finden unter:\\n\\nUmfrage 1: Kleine und Mittlere Unternehmen\\n\\nhttps://form.typeform.com/report/VoH1BI5d/WHsdAKz0nUHNr76M\\n\\nUmfrage 2: Große Unternehmen\\n\\nhttps://form.typeform.com/report/eDjpXmDx/RVEnkGRn1K7fhNja\\n\\nUmfrage 3: Wissenschaft\\n\\nhttps://form.typeform.com/report/G3MBzf8d/iG33eUmp26emWKiC\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n259\\n\\n259\\n\\nLEAM Umfrage KMU\\n\\n71 Antworten\\n\\nArbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n71\\n\\nWir setzen bereits KI bzw. darauf basierende Applikationen ein\\n\\n64 Antw. 90.1%\\n\\nWir planen aktuell, KI bzw. darauf basierende Applikationen einzusetzen\\n\\n4 Antw. 5.6%\\n\\nWir beabsichtigen aktuell nicht, KI bzw. darauf basierende Applikationen einzusetzen\\n\\n2 Antw. 2.8%\\n\\nWir haben angefangen, uns mit KI bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\n1 Antw. 1.4%\\n\\nWir kennen das Thema noch nicht\\n\\n0%\\n\\n0 Antw.\\n\\nInwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\n71 von 71 Personen haben diese Frage beantwortet\\n\\nGroße KI-Modelle für Deutschland\\n\\nWir setzen bereits Foundation Modelle bzw. darauf basierende Applikationen ein\\n\\nWir beabsichtigen aktuell nicht, Foundation Modelle bzw. darauf basierende Applikationen einzusetzen\\n\\nWir planen aktuell, Foundation Modellen bzw. darauf basierende Applikationen einzusetzen\\n\\nWir haben angefangen, uns mit Foundation Modellen bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\nWir kennen das Thema noch nicht\\n\\n36 Antw. 50.7%\\n\\n13 Antw. 18.3%\\n\\n11 Antw. 15.5%\\n\\n9 Antw. 12.7%\\n\\n2 Antw. 2.8%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n261\\n\\nWelche Voraussetzungen müssen erfüllt werden, damit Sie Foundation-Modelle nutzen würden?\\n\\n24\\n\\nNiedrige Kosten\\n\\n14 Antw. 58.3%\\n\\nOpen-Source Modelle\\n\\n14 Antw. 58.3%\\n\\nVerfügbare Daten\\n\\n14 Antw. 58.3%\\n\\nAusreichend Recheninfrastruktur\\n\\n9 Antw. 37.5%\\n\\nHoher Datenschutz beim Einsatz der Modelle\\n\\n8 Antw. 33.3%\\n\\nQualifizierte Mitarbeiter\\n\\n25%\\n\\n6 Antw.\\n\\nVerfügbare europäische Modelle\\n\\n25%\\n\\n6 Antw.\\n\\nOther\\n\\n5 Antw. 20.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n262\\n\\nInwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation Modellen?\\n\\n56\\n\\nIn der Entwicklung\\n\\n25 Antw. 44.6%\\n\\nBereits im produktiven Einsatz\\n\\n17 Antw. 30.4%\\n\\nIn Planung\\n\\n5 Antw. 8.9%\\n\\nKeine Planungen vorhanden\\n\\n5 Antw. 8.9%\\n\\nIn der Evaluation\\n\\n4 Antw. 7.1%\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n263\\n\\nInwiefern passen Sie existierende Foundation Modelle an (Tuning)?\\n\\n56\\n\\nIn der Entwicklung\\n\\n21 Antw. 37.5%\\n\\nBereits im produktiven Einsatz\\n\\n15 Antw. 26.8%\\n\\nIn der Evaluation\\n\\n8 Antw. 14.3%\\n\\nIn Planung\\n\\n6 Antw. 10.7%\\n\\nKeine Planungen vorhanden\\n\\n5 Antw. 8.9%\\n\\nGenerell nicht interessant\\n\\n1 Antw. 1.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n264\\n\\nInwiefern entwickeln Sie eigene Foundation Modelle?\\n\\n56\\n\\nKeine Planungen vorhanden\\n\\n25 Antw. 44.6%\\n\\nIn der Evaluation\\n\\n9 Antw. 16.1%\\n\\nBereits im produktiven Einsatz\\n\\n6 Antw. 10.7%\\n\\nGenerell nicht interessant\\n\\n6 Antw. 10.7%\\n\\nIn Planung\\n\\n6 Antw. 10.7%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nIn der Entwicklung\\n\\n4 Antw. 7.1%\\n\\nIn welchen Bereichen sind für Sie Foundation Modelle in Nutzung und Entwicklung relevant?\\n\\n56\\n\\nSprachmodelle\\n\\n40 Antw. 71.4%\\n\\nMultilinguale Sprachmodelle\\n\\n29 Antw. 51.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nMultimodale Modelle\\n\\nGeschä!s- und Fertigungsprozesse\\n\\nRobotik\\n\\nJura und Recht\\n\\nBiomed (Omiks)\\n\\nMaterialwissenscha!\\n\\nAusbildung/Pädagogik\\n\\nChemie\\n\\nKlima/Meteorologie\\n\\nAstronomie\\n\\nGeologie\\n\\nOther\\n\\n21 Antw. 37.5%\\n\\n19 Antw. 33.9%\\n\\n11 Antw. 19.6%\\n\\n8 Antw. 14.3%\\n\\n7 Antw. 12.5%\\n\\n4 Antw. 7.1%\\n\\n3 Antw. 5.4%\\n\\n3 Antw. 5.4%\\n\\n2 Antw. 3.6%\\n\\n1 Antw. 1.8%\\n\\n0 Antw.\\n\\n0%\\n\\n10 Antw. 17.9%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n266\\n\\nWelche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n71\\n\\nGroße Bedeutung\\n\\n64 Antw. 90.1%\\n\\nMittlere Bedeutung\\n\\n7 Antw. 9.9%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nNiedrige Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n267\\n\\nWelche Bedeutung messen Sie den Foundation Modellen für die KI und damit für gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n56\\n\\nGroße Bedeutung\\n\\n41 Antw. 73.2%\\n\\nMittlere Bedeutung\\n\\n13 Antw. 23.2%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 3.6%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n268\\n\\nWie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\n56\\n\\nGroße Bedeutung\\n\\n46 Antw. 82.1%\\n\\nMittlere Bedeutung\\n\\n6 Antw. 10.7%\\n\\nKeine Bedeutung\\n\\n2 Antw. 3.6%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 3.6%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n269\\n\\nWelche Bedeutung hat für Sie die Verfügbarkeit von Foundation Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n46\\n\\nGroße Bedeutung\\n\\n39 Antw. 84.8%\\n\\nMittlere Bedeutung\\n\\n4 Antw. 8.7%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 4.3%\\n\\nKeine Bedeutung\\n\\n1 Antw. 2.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n270\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenscha!lichen Institutionen?\\n\\n46\\n\\nGroße Bedeutung\\n\\n24 Antw. 52.2%\\n\\nMittlere Bedeutung\\n\\n18 Antw. 39.1%\\n\\nNiedrige Bedeutung\\n\\n3 Antw. 6.5%\\n\\nKeine Bedeutung\\n\\n1 Antw. 2.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n271\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Startups?\\n\\n46\\n\\nGroße Bedeutung\\n\\n31 Antw. 67.4%\\n\\nMittlere Bedeutung\\n\\n11 Antw. 23.9%\\n\\nKeine Bedeutung\\n\\n2 Antw. 4.3%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 4.3%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n272\\n\\nRechenleistung: Wie viele GPUs und andere Beschleuniger werden z.B. für Training und Inferenz von state-of-the-art Foundation-Modellen benötigt?\\n\\n15\\n\\n>100\\n\\n7 Antw. 46.7%\\n\\n5-10\\n\\n4 Antw. 26.7%\\n\\n1-4\\n\\n2 Antw. 13.3%\\n\\n10-49\\n\\n2 Antw. 13.3%\\n\\n50-100\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n273\\n\\nVerfügbarkeit: Welche Kriterien müssen Service Level Agreements (SLA) erfüllen, um die Entwicklung von Foundation-Modellen möglich zu machen?\\n\\n16\\n\\nOn-demand\\n\\n50%\\n\\n8 Antw.\\n\\nPre-ordered <24h\\n\\n6 Antw. 37.5%\\n\\nPre-ordered <60min\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n274\\n\\nScheduling Infrastructure: Welche Scheduling Infrastructures sollen bei der Entwicklung eingesetzt werden?\\n\\n15\\n\\nRay\\n\\n10 Antw. 66.7%\\n\\nkubeflow\\n\\n60%\\n\\n9 Antw.\\n\\nSLURM\\n\\n40%\\n\\n6 Antw.\\n\\nOther\\n\\n20%\\n\\n3 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n275\\n\\nDeployment Infrastructure: Welche Deployment Infrastructures müssen bei der Entwicklung unterstützt werden?\\n\\n16\\n\\nDocker\\n\\n15 Antw. 93.8%\\n\\nKubernetes\\n\\n10 Antw. 62.5%\\n\\nTerraform\\n\\n25%\\n\\n4 Antw.\\n\\nFlyter\\n\\n2 Antw. 12.5%\\n\\nSeldon\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n276\\n\\nAccess Control Levels & Datenhoheit: Wie soll das Laden der Daten in der Entwicklung angesteuert werden?\\n\\n16\\n\\nDMZ + VPN\\n\\n11 Antw. 68.8%\\n\\nZentral, temporär gelagerte Daten\\n\\n6 Antw. 37.5%\\n\\nEnclave computing\\n\\n3 Antw. 18.8%\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n277\\n\\nFrameworks: Welche Frameworks sollen bei der Entwicklung von Foundation-Modellen eingesetzt werden?\\n\\n16\\n\\nTensorFlow\\n\\n14 Antw. 87.5%\\n\\nPyTorch\\n\\n75%\\n\\n12 Antw.\\n\\nKeras\\n\\n9 Antw. 56.2%\\n\\nJax\\n\\n2 Antw. 12.5%\\n\\nMXNet\\n\\n1 Antw. 6.2%\\n\\nFlux\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n278\\n\\nInternet- & Bandbreite: Wie schnell muss die Internetverbindung für die Entwicklung von Foundation- Modellen mindestens sein?\\n\\n16\\n\\n>100 Gbit/s\\n\\n50%\\n\\n8 Antw.\\n\\n< 1 Gbit/s\\n\\n3 Antw. 18.8%\\n\\n< 100 Gbit/s\\n\\n3 Antw. 18.8%\\n\\n< 10 Gbit/s\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n279\\n\\nSpeicherplatz für Daten: Wie hoch ist der Speicherbedarf für die Entwicklung eines Foundation- Modells?\\n\\n15\\n\\n10-100 TB\\n\\n5 Antw. 33.3%\\n\\n>1 PB\\n\\n4 Antw. 26.7%\\n\\n1-10 TB\\n\\n20%\\n\\n3 Antw.\\n\\n100-1000 TB\\n\\n20%\\n\\n3 Antw.\\n\\n< 1 TB\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n280\\n\\nCompliance: Welche regulatorischen Voraussetzungen müssen erfüllt werden, um Foundation- Modelle zu entwickeln?\\n\\n14\\n\\nISO/Normen\\n\\n9 Antw. 64.3%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nHIPAA\\n\\n4 Antw. 28.6%\\n\\nSOC 3\\n\\n4 Antw. 28.6%\\n\\nOther\\n\\n2 Antw. 14.3%\\n\\nBeschä!igen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n15\\n\\nJa\\n\\n8 Antw. 53.3%\\n\\nNein\\n\\n7 Antw. 46.7%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n281\\n\\nWissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n9\\n\\nJa\\n\\n6 Antw. 66.7%\\n\\nNein\\n\\n3 Antw. 33.3%\\n\\nIst Ihnen das Konzept der Model Cards / Data Set Cards im Bezug zur Erstellung neuer Daten geläufig?\\n\\n16\\n\\nJa\\n\\n9 Antw. 56.2%\\n\\nNein\\n\\n7 Antw. 43.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n282\\n\\nIst das Konzept für Ihre Domäne relevant oder hilfreich?\\n\\n9\\n\\nJa\\n\\n7 Antw. 77.8%\\n\\nNein\\n\\n2 Antw. 22.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n283\\n\\nWelche Personalrollen (oder äquivalent) sind für die Entwicklung von Foundation-Modellen erforderlich?\\n\\n16\\n\\nDevOps, MLOps\\n\\n14 Antw. 87.5%\\n\\nMachine Learning Engineer\\n\\n14 Antw. 87.5%\\n\\nMachine Learning Researcher\\n\\n13 Antw. 81.2%\\n\\nSo!ware Engineer\\n\\n9 Antw. 56.2%\\n\\nSysAdmin\\n\\n9 Antw. 56.2%\\n\\nSite Reliability Engineer\\n\\n5 Antw. 31.2%\\n\\nOther\\n\\n1 Antw. 6.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n284\\n\\nIn welchen dieser Personalrollen beobachten oder antizipieren Sie einen Mangel, um Foundation- Modelle zu entwickeln?\\n\\n13\\n\\nMachine Learning Engineer\\n\\n8 Antw. 61.5%\\n\\nDevOps, MLOps\\n\\n6 Antw. 46.2%\\n\\nMachine Learning Researcher\\n\\n6 Antw. 46.2%\\n\\nSo!ware Engineer\\n\\n6 Antw. 46.2%\\n\\nSite Reliability Engineer\\n\\n3 Antw. 23.1%\\n\\nSysAdmin\\n\\n3 Antw. 23.1%\\n\\nOther\\n\\n2 Antw. 15.4%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n285\\n\\nWelche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n16\\n\\n5-10 Mio. Euro\\n\\n5 Antw. 31.2%\\n\\n> 50 Mio. Euro\\n\\n5 Antw. 31.2%\\n\\n25-50 Mio. Euro\\n\\n25%\\n\\n4 Antw.\\n\\n1-5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n10-25 Mio. Euro\\n\\n0%\\n\\n0 Antw.\\n\\n<1 Mio. Euro\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n286\\n\\nWelche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n16\\n\\n> 5 Mio. Euro\\n\\n7 Antw. 43.8%\\n\\n1-3 Mio. Euro\\n\\n25%\\n\\n4 Antw.\\n\\n3-5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n<0.5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n0,5-1 Mio. Euro\\n\\n1 Antw. 6.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n287\\n\\nIn welchem Zeitraum erwarten Sie, diese Investitionskosten zu amortisieren?\\n\\n16\\n\\n5-10 Jahre\\n\\n6 Antw. 37.5%\\n\\n1-3 Jahre\\n\\n5 Antw. 31.2%\\n\\n>10 Jahre\\n\\n3 Antw. 18.8%\\n\\n3-5 Jahre\\n\\n2 Antw. 12.5%\\n\\n< 1 Jahr\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n288\\n\\nWie erwarten Sie, dass sich diese Investitionskosten amortisieren könnten?\\n\\n16\\n\\nMaßgebliche Entwicklung eines neuen Marktsegments\\n\\n11 Antw. 68.8%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nLeasing des KI-Modells an andere Entwickler\\n\\n10 Antw. 62.5%\\n\\nEroberung eines maßgeblichen Marktanteils existierender Marktsegmenten\\n\\n9 Antw. 56.2%\\n\\nOther\\n\\n0%\\n\\n0 Antw.\\n\\nWie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n16\\n\\nNeutral, es gibt Unklarheiten\\n\\n7 Antw. 43.8%\\n\\nSchlecht\\n\\n5 Antw. 31.2%\\n\\nGut, keine Unklarheiten\\n\\n25%\\n\\n4 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n289\\n\\nEmpfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n10\\n\\nEnabler\\n\\n70%\\n\\n7 Antw.\\n\\nDisabler\\n\\n30%\\n\\n3 Antw.\\n\\nWie sollten Ihrer Meinung nach Datensätze für die Entwicklung von Foundation-Modellen erhoben werden?\\n\\n16\\n\\nDonation\\n\\n13 Antw. 81.2%\\n\\nScraping\\n\\n75%\\n\\n12 Antw.\\n\\nCentralized Third-Party collection\\n\\n50%\\n\\n8 Antw.\\n\\nOther\\n\\n3 Antw. 18.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n290\\n\\nWie hoch empfinden Sie Awareness, Relevanz und Akzeptanz bzgl. Foundation-Modellen innerhalb Ihrer Organisation?\\n\\n16\\n\\nHoch\\n\\n9 Antw. 56.2%\\n\\nNeutral\\n\\n6 Antw. 37.5%\\n\\nNiedrig\\n\\n1 Antw. 6.2%\\n\\nWeiß ich nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n291\\n\\nWie groß ist Ihr Unternehmen?\\n\\n71\\n\\n< 100 Mitarbeiter\\n\\n64 Antw. 90.1%\\n\\n100-1000 Mitarbeiter\\n\\n6 Antw. 8.5%\\n\\n1000-10.000 Mitarbeiter\\n\\n1 Antw. 1.4%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n10.000 -100.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\n> 100.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nWelcher Branche gehört Ihr Unternehmen an?\\n\\n71\\n\\nGroße KI-Modelle für Deutschland\\n\\nInformationstechnologie\\n\\nDienstleistung\\n\\nIndustrie\\n\\nMedien\\n\\nFinanzwirtscha!\\n\\nGesundheit\\n\\nHandel\\n\\nMedien\\n\\nAutomobil\\n\\nImmobilien\\n\\nTourismus\\n\\nOther\\n\\n56 Antw. 78.9%\\n\\n18 Antw. 25.4%\\n\\n8 Antw. 11.3%\\n\\n8 Antw. 11.3%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n2 Antw. 2.8%\\n\\n1 Antw. 1.4%\\n\\n1 Antw. 1.4%\\n\\n8 Antw. 11.3%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n293\\n\\nStehen Sie für einen weiteren Austausch zu den Themen Foundation Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n71\\n\\nJa\\n\\n61 Antw. 85.9%\\n\\nNein\\n\\n10 Antw. 14.1%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n294\\n\\nLEAM Umfrage Große Unternehmen\\n\\n10 Antworten\\n\\nInwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation Modellen oder darauf basierenden Applikationen auseinandergesetzt?\\n\\n10\\n\\nWir setzen bereits Foundation Modelle bzw. darauf basierende Applikationen ein\\n\\n50%\\n\\n5 Antw.\\n\\nWir haben angefangen, uns mit Foundation Modellen bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\n30%\\n\\n3 Antw.\\n\\nWir beabsichtigen aktuell nicht, Foundation Modelle bzw. darauf basierende Applikationen einzusetzen\\n\\n10%\\n\\n1 Antw.\\n\\nWir planen aktuell, Foundation Modellen bzw. darauf basierende Applikationen einzusetzen\\n\\n10%\\n\\n1 Antw.\\n\\nWir kennen das Thema noch nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n295\\n\\nIn welchen Bereichen sind für Sie Foundation Modelle in Nutzung und Entwicklung relevant?\\n\\n8\\n\\nGeschä!s- und Fertigungsprozesse\\n\\n75%\\n\\n6 Antw.\\n\\nMultilinguale Sprachmodelle\\n\\n75%\\n\\n6 Antw.\\n\\nSprachmodelle\\n\\n75%\\n\\n6 Antw.\\n\\nMultimodale Modelle\\n\\n50%\\n\\n4 Antw.\\n\\nRobotik\\n\\n25%\\n\\n2 Antw.\\n\\nBiomed (Omiks)\\n\\n0%\\n\\n0 Antw.\\n\\nChemie\\n\\n0%\\n\\n0 Antw.\\n\\nKlima/Meteorologie\\n\\n0%\\n\\n0 Antw.\\n\\nMaterialwissenscha!\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n1 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n296\\n\\nInwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation Modellen?\\n\\n10\\n\\nBereits im produktiven Einsatz\\n\\n40%\\n\\n4 Antw.\\n\\nIn der Entwicklung\\n\\n30%\\n\\n3 Antw.\\n\\nKeine Planungen vorhanden\\n\\n20%\\n\\n2 Antw.\\n\\nIn Planung\\n\\n10%\\n\\n1 Antw.\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nIn der Evaluation\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n297\\n\\nInwiefern passen Sie existierende Foundation Modelle an (Tuning)?\\n\\n9\\n\\nBereits im produktiven Einsatz\\n\\n3 Antw. 33.3%\\n\\nIn der Entwicklung\\n\\n3 Antw. 33.3%\\n\\nKeine Planungen vorhanden\\n\\n2 Antw. 22.2%\\n\\nIn der Evaluation\\n\\n1 Antw. 11.1%\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nIn Planung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n298\\n\\nInwiefern entwickeln Sie eigene Foundation Modelle?\\n\\n10\\n\\nKeine Planungen vorhanden\\n\\n50%\\n\\n5 Antw.\\n\\nIn der Entwicklung\\n\\n20%\\n\\n2 Antw.\\n\\nGenerell nicht interessant\\n\\n10%\\n\\n1 Antw.\\n\\nIn der Evaluation\\n\\n10%\\n\\n1 Antw.\\n\\nIn Planung\\n\\n10%\\n\\n1 Antw.\\n\\nBereits im produktiven Einsatz\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n299\\n\\nWelche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n10\\n\\nGroße Bedeutung\\n\\n90%\\n\\n9 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nMittlere Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n300\\n\\nWelche Bedeutung messen Sie den Foundation Modellen für die KI und damit für gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n10\\n\\nGroße Bedeutung\\n\\n70%\\n\\n7 Antw.\\n\\nMittlere Bedeutung\\n\\n20%\\n\\n2 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n301\\n\\nWie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\n9\\n\\nGroße Bedeutung\\n\\n7 Antw. 77.8%\\n\\nMittlere Bedeutung\\n\\n1 Antw. 11.1%\\n\\nNiedrige Bedeutung\\n\\n1 Antw. 11.1%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n302\\n\\nWelche Bedeutung hat für Sie die Verfügbarkeit von Foundation Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n10\\n\\nGroße Bedeutung\\n\\n70%\\n\\n7 Antw.\\n\\nMittlere Bedeutung\\n\\n20%\\n\\n2 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n303\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenscha!lichen Institutionen?\\n\\n10\\n\\nGroße Bedeutung\\n\\n60%\\n\\n6 Antw.\\n\\nMittlere Bedeutung\\n\\n30%\\n\\n3 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n304\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Startups?\\n\\n9\\n\\nGroße Bedeutung\\n\\n4 Antw. 44.4%\\n\\nMittlere Bedeutung\\n\\n3 Antw. 33.3%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 22.2%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n305\\n\\nInwiefern würden Sie die Services von LEAM nutzen?\\n\\n10\\n\\nZum Tuning von eigenen Modellen auf Basis von Foundation Modellen\\n\\n60%\\n\\n6 Antw.\\n\\nZum Trainieren von Foundation Modellen\\n\\n40%\\n\\n4 Antw.\\n\\nGar nicht\\n\\n30%\\n\\n3 Antw.\\n\\nZum Betrieb von Foundation Modellen\\n\\n20%\\n\\n2 Antw.\\n\\nOther\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n306\\n\\nInwiefern würden Sie bzw. Ihr Unternehmen in den Aufbau eines LEAM KI-Servicezentrums investieren (unter noch zu klärenden Voraussetzungen)?\\n\\n9\\n\\nMit geringer Wahrscheinlichkeit\\n\\n4 Antw. 44.4%\\n\\nMit mittlerer Wahrscheinlichkeit\\n\\n4 Antw. 44.4%\\n\\nMit hoher Wahrscheinlichkeit\\n\\n1 Antw. 11.1%\\n\\nDefinitiv nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n307\\n\\nWie groß ist Ihr Unternehmen?\\n\\n10\\n\\n10.000 -100.000 Mitarbeiter\\n\\n60%\\n\\n6 Antw.\\n\\n> 100.000 Mitarbeiter\\n\\n40%\\n\\n4 Antw.\\n\\n100-1000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n1000-10.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\n< 100 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nWelcher Branche gehört Ihr Unternehmen an?\\n\\n10\\n\\nGroße KI-Modelle für Deutschland\\n\\nAutomobil\\n\\nHandel\\n\\nFinanzwirtscha!\\n\\nIndustrie\\n\\nInformationstechnologie\\n\\nDienstleistung\\n\\nMedien\\n\\nMedien\\n\\nGesundheit\\n\\nImmobilien\\n\\nTourismus\\n\\nOther\\n\\n3 Antw.\\n\\n3 Antw.\\n\\n2 Antw.\\n\\n2 Antw.\\n\\n2 Antw.\\n\\n1 Antw.\\n\\n1 Antw.\\n\\n1 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n30%\\n\\n30%\\n\\n20%\\n\\n20%\\n\\n20%\\n\\n10%\\n\\n10%\\n\\n10%\\n\\n0%\\n\\n0%\\n\\n0%\\n\\n0%\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n309\\n\\nStehen Sie für einen weiteren Austausch zu den Themen Foundation Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n10\\n\\nJa\\n\\n60%\\n\\n6 Antw.\\n\\nNein\\n\\n40%\\n\\n4 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n310\\n\\nLEAM Umfrage - KI-Forscher\\n\\n21 responses\\n\\nWie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wissenscha! ein?\\n\\n21\\n\\nHoch\\n\\n11 resp. 52.4%\\n\\nEher hoch\\n\\n9 resp. 42.9%\\n\\nEher niedrig\\n\\n1 resp. 4.8%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n0%\\n\\n0 resp.\\n\\nNiedrig\\n\\n0%\\n\\n0 resp.\\n\\nWeder hoch noch niedrig\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n311\\n\\nWie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wirtscha! und andere Bereiche der Gesellscha! ein?\\n\\n19\\n\\nHoch\\n\\n12 resp. 63.2%\\n\\nEher hoch\\n\\n5 resp. 26.3%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 5.3%\\n\\nWeder hoch noch niedrig\\n\\n1 resp. 5.3%\\n\\nEher niedrig\\n\\n0%\\n\\n0 resp.\\n\\nNiedrig\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n312\\n\\nArbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n21\\n\\nBereits recht viel\\n\\n81%\\n\\n17 resp.\\n\\nSchon etwas\\n\\n2 resp. 9.5%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nNein\\n\\n1 resp. 4.8%\\n\\nEs ist geplant\\n\\n0%\\n\\n0 resp.\\n\\nVielleicht in der Zukun!\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n313\\n\\nArbeiten Sie bereits an oder wollen Sie mit Foundation-Modellen arbeiten?\\n\\n21\\n\\nBereits recht viel\\n\\n7 resp. 33.3%\\n\\nSchon etwas\\n\\n5 resp. 23.8%\\n\\nEs ist geplant\\n\\n3 resp. 14.3%\\n\\nVielleicht in der Zukun!\\n\\n3 resp. 14.3%\\n\\nNein\\n\\n2 resp. 9.5%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n314\\n\\nWas hindert Sie daran, bereits heute Foundation-Modelle einzusetzen?\\n\\n5\\n\\nFehlende Daten\\n\\n40%\\n\\n2 resp.\\n\\nFehlender Datenschutz beim Einsatz amerikanischer und chinesischer Modelle\\n\\n40%\\n\\n2 resp.\\n\\nInfrastrukturelle Hürden\\n\\n40%\\n\\n2 resp.\\n\\nTechnische Limitierungen: Breakthroughs benötigt, z.B. Interpretable AI\\n\\n40%\\n\\n2 resp.\\n\\nHohe Investitionskosten\\n\\n20%\\n\\n1 resp.\\n\\nKeine Anwendungsfälle\\n\\n20%\\n\\n1 resp.\\n\\nMangel an Talenten\\n\\n20%\\n\\n1 resp.\\n\\nUnklarheit in der Regulation\\n\\n20%\\n\\n1 resp.\\n\\nFehlende Unterstützung der deutschen Sprache in den Modellen\\n\\n0%\\n\\n0 resp.\\n\\nOther\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n315\\n\\nWie hoch schätzen Sie die Wahrscheinlichkeit ein, Foundation-Modelle (intensiver) für Ihre Arbeit zu nutzen?\\n\\n21\\n\\nHoch\\n\\n9 resp. 42.9%\\n\\nEher hoch\\n\\n5 resp. 23.8%\\n\\nWeder hoch noch niedrig\\n\\n5 resp. 23.8%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nNiedrig\\n\\n1 resp. 4.8%\\n\\nout of 21 answered\\n\\nEher niedrig\\n\\n0%\\n\\n0 resp.\\n\\nBeschä!igen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n21\\n\\nNein\\n\\n12 resp. 57.1%\\n\\nJa\\n\\n9 resp. 42.9%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n316\\n\\nWissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n7\\n\\nJa\\n\\n4 resp. 57.1%\\n\\nNein\\n\\n3 resp. 42.9%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n317\\n\\nWelche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n16\\n\\n10-25 Mio. Euro\\n\\n25%\\n\\n4 resp.\\n\\n5-10 Mio. Euro\\n\\n25%\\n\\n4 resp.\\n\\n1-5 Mio. Euro\\n\\n3 resp. 18.8%\\n\\n25-50 Mio. Euro\\n\\n3 resp. 18.8%\\n\\n> 50 Mio. Euro\\n\\n2 resp. 12.5%\\n\\n<1 Mio. Euro\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n318\\n\\nWelche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n17\\n\\n3-5 Mio. Euro\\n\\n5 resp. 29.4%\\n\\n1-3 Mio. Euro\\n\\n4 resp. 23.5%\\n\\n> 5 Mio. Euro\\n\\n4 resp. 23.5%\\n\\nout of 21 answered\\n\\n0,5-1 Mio. Euro\\n\\n3 resp. 17.6%\\n\\n<0.5 Mio. Euro\\n\\n1 resp. 5.9%\\n\\nWie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n21\\n\\nNeutral, es gibt Unklarheiten\\n\\n14 resp. 66.7%\\n\\nSchlecht\\n\\n7 resp. 33.3%\\n\\nGut, keine Unklarheiten\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n319\\n\\nEmpfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n12\\n\\nEnabler\\n\\n8 resp. 66.7%\\n\\nDisabler\\n\\n4 resp. 33.3%\\n\\nGroße KI-Modelle für Deutschland\\n\\n320\\n\\nAnhang B - Zusätzliche Information zu den Interviews\\n\\nAnhang B.1 - Die Interviewpartner:innen\\n\\nWirtschaft\\n\\nWolfgang\\n\\nHauner\\n\\nAllianz SE\\n\\nDr. Maik\\n\\nFriedel\\n\\nBASF SE\\n\\nDr. Marion\\n\\nLegler\\n\\nBayer AG\\n\\nDr. Hans-Jörg\\n\\nVögel\\n\\nBMW Group\\n\\nDr. Michael\\n\\nFausten\\n\\nRobert Bosch GmbH\\n\\nJean-Paul\\n\\nSchmetz\\n\\nBurda Media\\n\\nMario\\n\\nDeng\\n\\nBWI GmbH\\n\\nDr.\\n\\nCorina\\n\\nApachiţe\\n\\nContinental AG\\n\\nDr. Matthias\\n\\nDorner\\n\\nDATEV eG\\n\\nStephan\\n\\nKaulbach\\n\\nDeutsche Bahn AG\\n\\nDr.\\n\\nFrank\\n\\nSäuberlich\\n\\nEnBW Energie Baden-Württemberg AG\\n\\nDr.\\n\\nSebastian\\n\\nKaiser\\n\\nErgo Group AG\\n\\nThomas\\n\\nWolf\\n\\nHugging Face, Inc.\\n\\nDr.\\n\\nSabine\\n\\nDonauer\\n\\nInfineon Technologies AG\\n\\nNico\\n\\nKelling\\n\\nInfineon Technologies AG\\n\\nRainer\\n\\nSträter\\n\\nIonos SE\\n\\nChristian\\n\\nSpannbauer\\n\\nLufthansa Group\\n\\nJochen\\n\\nKaiser\\n\\nMercedes-Benz Group AG\\n\\nDr.\\n\\nStephan\\n\\nMeyer\\n\\nMunich RE\\n\\nDr. Michael\\n\\nMüller-Wünsch\\n\\nOtto GmbH & Co KG\\n\\nDr.\\n\\nArmin\\n\\nKurrle\\n\\nPorsche AG\\n\\nDr.\\n\\nLorenz\\n\\nDetermann\\n\\nRewe Group\\n\\nDr.\\n\\nFeiyu\\n\\nXu\\n\\nSAP SE\\n\\nDr.\\n\\nAndreas\\n\\nWierse\\n\\nsicos BW GmbH\\n\\nDr. Michael\\n\\nMay\\n\\nSiemens AG\\n\\nDr. Dirk\\n\\nDr.\\n\\nSebastian\\n\\nDr.\\n\\nPatrick\\n\\nSchlesinger\\n\\nHallensleben\\n\\nvan der Smagt\\n\\nTÜV Süd AG VDE Verband der Elektrotechnik Elektronik Informationstechnik e. V. Volkswagen AG\\n\\nDr.\\n\\nAlexander\\n\\nBorek\\n\\nZalando SE\\n\\nTabelle 23: Übersicht der Interviewpartner:innen im Bereich Wirtschaft\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n321\\n\\nRechenzentrums- und Hardwareexpert:innen\\n\\nDr.\\n\\nWolfgang\\n\\nStefan\\n\\nChristmann\\n\\nRüping\\n\\nchristmann informationstechnik + medien GmbH & Co. KG Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme IAIS\\n\\nDr.\\n\\nStefan\\n\\nKesselheim\\n\\nForschungszentrum Jülich GmbH\\n\\nDr. Helmut\\n\\nKreiser\\n\\nGSI Helmholtzzentrum für Schwerionenforschung\\n\\nJan\\n\\nSeiler\\n\\nGigabyte Technology\\n\\nAndreas\\n\\nHerden\\n\\nGreen Mountain Datacenter GmbH\\n\\nDr. Bastian\\n\\nKoller\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nDennis\\n\\nHoppe\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nOleksandr\\n\\nShcherbakov\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nVolker\\n\\nLudwig\\n\\nInterxion Deutschland GmbH\\n\\nMarco\\n\\nMaslon\\n\\nNorthern Data AG\\n\\nBedrettin\\n\\nAltay\\n\\nNoya Group Holding GmbH\\n\\nVolker\\n\\nMeschonat\\n\\nNvidia Corporation\\n\\nMarkus\\n\\nHacker\\n\\nNvidia Corporation\\n\\nOlaf\\n\\nDalmer\\n\\nOneFiber Interconnect Germany GmbH\\n\\nWolfgang\\n\\nDreyer\\n\\nOracle Corporation\\n\\nDr.\\n\\nThorsten\\n\\nHennrich\\n\\nPlusServer GmbH\\n\\nMax\\n\\nSchulze\\n\\nSDIA - Sustainable Digital Infrastructure Alliance e.V.\\n\\nAlexander\\n\\nHauser\\n\\nTTSP HWP Planungsgesellschaft mbH\\n\\nTabelle 24: Übersicht der Interviewpartner:innen im Bereich Rechenzentrum und Hardware\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n322\\n\\nWissenschaft\\n\\nProf.\\n\\nAlexander\\n\\nLöser\\n\\nBeuth Hochschule für Technik Berlin\\n\\nProf.\\n\\nDr.\\n\\nProf.\\n\\nDr.\\n\\nDr.\\n\\nProf.\\n\\nSabine\\n\\nKirchmeier\\n\\nJoachim\\n\\nKöhler\\n\\nStefan\\n\\nWrobel\\n\\nNarges\\n\\nAhmidi\\n\\nHolger\\n\\nKarl\\n\\nAlexander Waibel\\n\\nEuropean Federation of National Institutions for Language (EFNIL) Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS) Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS) Fraunhofer-Institut für Kognitive Systeme (IKS). Hasso-Plattner-Institut Karlsruher Institut für Technologie (KIT) und Carnegie Mellon University, Pittsburgh King Abdullah University of Science and Technology (KAUST)\\n\\nProf.\\n\\nJürgen\\n\\nSchmidhuber\\n\\nProf.\\n\\nHinrich\\n\\nSchütze\\n\\nLudwig-Maximilians-Universität München\\n\\nProf.\\n\\nVolker\\n\\nTresp\\n\\nLudwig-Maximilians-Universität München\\n\\nDietmar\\n\\nHarhoff\\n\\nMax-Planck-Institut für Innovation und Wettbewerb\\n\\nDr.\\n\\nPeter\\n\\nNorvig\\n\\nStanford University, Google Inc., NASA\\n\\nProf.\\n\\nIryna\\n\\nGurevych\\n\\nTechnische Universität Darmstadt\\n\\nProf.\\n\\nKristian\\n\\nKersting\\n\\nTechnische Universität Darmstadt\\n\\nProf.\\n\\nAndreas\\n\\nDengel\\n\\nTechnische Universität Kaiserslautern\\n\\nProf.\\n\\nDaniel\\n\\nCremers\\n\\nTechnische Universität München\\n\\nProf.\\n\\nJosef\\n\\nvan Genabith\\n\\nUniversität des Saarlandes\\n\\nProf.\\n\\nAnette\\n\\nFrank\\n\\nUniversität Heidelberg\\n\\nProf.\\n\\nProf.\\n\\nSepp\\n\\nLeo\\n\\nHochreiter\\n\\nWanner\\n\\nUniversität Linz und Linz Institute of Technology (LIT) Universitat Pompeu Fabra Barcelona (UPF)\\n\\nProf.\\n\\nRalf\\n\\nHerbrich\\n\\nUniversität Potsdam\\n\\nTabelle 25: Übersicht der Interviewpartner:innen im Bereich Wissenschaft\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n323\\n\\nSonstige\\n\\nPhilipp\\n\\nDr. Daniel\\n\\nGünther\\n\\nGille\\n\\nBerlin Partner für Wirtschaft und Technologie GmbH Agentur für Innovation in der Cybersicherheit GmbH\\n\\nRisto\\n\\nUuk\\n\\nFuture of Life Institute\\n\\nDr.\\n\\nTina\\n\\nKlug\\n\\nHessische Staatzkanzlei Ministerin für Digitale Strategie und Entwicklung\\n\\nOlly\\n\\nSalzmann\\n\\nKI Park e.V.\\n\\nDr.\\n\\nPeter\\n\\nChristian\\n\\nSebastian\\n\\nPhilipp\\n\\nJörg\\n\\nMendler\\n\\nDinnus\\n\\nLey\\n\\nDenker\\n\\nSchaub\\n\\nMinisterium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen PD - Berater der öffentlichen Hand GmbH Wirtschaftsinitiative FrankfurtRheinMain e.V.\\n\\nTabelle 26: Übersicht der sonstigen Interviewpartner:innen\\n\\nAnhang B.2 - Die Leitfragen\\n\\nLeitfragen für die Interviews mit Wirtschaftsvertreter:innen:`\\n\\n1)\\n\\nInwiefern nutzen Sie bereits KI-Foundation-Modelle? Welche Modelle nutzen Sie und für welche Anwendungen?\\n\\n2) Welche Bedeutung messen Sie Foundation-Modellen aktuell und in der Zukunft zu? 3) Wie wichtig ist aus Ihrer Sicht der Aufbau eines europäischen Ökosystems rund um 4) Foundation-Modelle, inkl. eigener europäischer Modelle? 5) Würden Sie die Services der LEAM Initiative nutzen? In welchem Umfang?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n324\\n\\nAnhang B.3 - Ergebnisprotokolle\\n\\nInterviewprotokoll Allianz\\n\\nInterviewter:\\n\\nWolfgang Hauner, Head of Group Data Analytics, Allianz SE Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Zukunft der KI Liegt in Foundation-Modellen. Die Initiative LEAM ist daher sehr wichtig.\\n\\nDie Möglichkeiten der KI und der Foundation Modelle werden Geschäftsmodelle\\n\\nverändern, ähnlich der Situation beim Aufkommen des Internets und e-\\n\\nCommerce.\\n\\nFür interne Anwendung (bspw. in der Kommunikation und dem Kundenservice)\\n\\nkönnen die bestehenden Modelle bereits gut eingesetzt werden.\\n\\nDie Nutzung von KI-Services über eine API (und der Transfer der Daten in die\\n\\nUSA) ist aufgrund des Einsatzes sensibler Daten keine Option.\\n\\nEs gibt wichtige Anforderungen für versicherungsspezifische Modelle, die es\\n\\naktuell aber noch nicht gibt, z.B. in den Bereichen:\\n\\nVerbesserung des Kundenservices, indem Fragen zur Police automatisch\\n\\nausgelesen werden oder\\n\\nGenerierung von individuellen Policen auf Basis der Foundation-Modellen (Hierfür müssen die Modelle aber verlässlicher und rechtlich abgesichert werden).\\n\\nOhne die Verfügbarkeit dieser Modelle wird es mittelfristig zu Nachteilen für Versicherungsunternehmen im internationalen Wettbewerb kommen..\\n\\nV.a. bei der Verwendung sensibler Versicherungsdaten braucht es lokale Modelle,\\n\\num den Datenschutz zu gewährleisten.\\n\\nDie Herausforderung liegt nicht im traditionellen Versicherungsmarkt, der stark reguliert ist. Stattdessen besteht die Gefahr, den Schritt bei neuen, digitalen Geschäftsmodellen zu verpassen, die keine nationalen Grenzen kennen. Ähnliche\\n\\nEntwicklungen gab es bspw. bei Amazon und Google.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n325\\n\\n2. Zusammenarbeit mit LEAM ● Einzelne Unternehmen in Deutschland sind nicht in der Lage die notwendigen Investitionen alleine zu tragen. Es braucht die Unterstützung der Politik.\\n\\nDer Möglichkeit einer PPP bzw. eines Joint Ventures steht die Allianz offen\\n\\ngegenüber. Eine Zusammenarbeit der DAX Unternehmen hierfür ist durchaus\\n\\nrealistisch.\\n\\nInterviewprotokoll BASF\\n\\nInterviewter:\\n\\nMaik Friedel, Principal Scientist for Artificial Intelligence Chemistry & Leiter der Initiative “Generative AI”, BASF Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● BASF erforscht bereits intensiv den Einsatz von Foundation-Modellen und hat hierfür kürzlich die Initiative „Generative AI“ ins Leben gerufen\\n\\nZiel ist, neben dem Model Tuning von bereitgestellten Modellen auch eigene\\n\\nFoundation Modelle zu tunen.\\n\\nHierzu gehören neben großen Sprachmodellen auch domänenspezifische\\n\\nModelle im Chemie Bereich\\n\\nBASF besitzt bereits eine eigene Supercomputing-Recheninfrastruktur (Quriosity) und plant diese weiter auszubauen. Der Vorteil einer eigenen Infrastruktur liegt in der Datensicherheit.\\n\\nMitarbeiter nutzen bereits privat Dienste wie GPT-3 oder ChatGPT. Das ist kaum kontrollierbar und kann ein potentielles Daten-Sicherheitsrisiko darstellen.\\n\\nDas Trainieren bzw. Tunen von Modellen durch BASF auf Basis von eigenen\\n\\nDaten ist eine strategisch, wichtige Capability. Hierfür können außereuropäische\\n\\nServices (z.T. auch Cloud-Services generell) aus Datensicherheitsgründen nur\\n\\nbedingt, oder gar nicht in Anspruch genommen werden.\\n\\nWenn bei BASF bzw. in Deutschland/Europa entsprechende Services und\\n\\nFoundation Modelle nicht bereitgestellt werden können, entwickelt sich hieraus\\n\\nein großer Wettbewerbsnachteil.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n326\\n\\n2. Zusammenarbeit mit LEAM ● Aus den oben genannten Gründen wäre ein LEAM Service eine gute Unterstützung um eigene Foundation-Modelle zu erstellen und existierende\\n\\nModelle zu tunen\\n\\nInterviewprotokoll Bayer Pharma\\n\\nInterviewter:\\n\\nMarion Legler, Head of Decision Science & Advanced Analytics at Bayer Pharma Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Bayer arbeitet intensiv mit Foundation-Modellen, v.a. im Bereich Sprache (Natural Language Processing) und der Bildverarbeitung (Computer Vision)\\n\\n\\n\\nIm Bereich Natural Language Processing greift Bayer auf den Medizinbereich\\n\\nspezialisierte Modelle wie SciBERT, BioBERT und PubMedBERT zurück.\\n\\nAber: Die aktuellen Modelle erfüllen noch nicht alle Erwartungen, bspw. ist nicht\\n\\ndie gesamte medizinische Terminologie eingebunden. Ebenso können die\\n\\naktuellen Language Models die Nuancen der medizinischen Inhalte, die für Laien\\n\\noft nicht erkennbar sind, noch nicht differenzieren. Größere Modelle im\\n\\nmedizinischen Bereich könnten dort weiterhelfen.\\n\\nEin europäisches Foundation-Modell für die Medizin wäre für Bayer sehr\\n\\ninteressant. Dieses könnte Bayer dann speziell für seine Ansprüche, d.h. konkrete\\n\\nIndikationsgebiete von Interesse, anpassen.\\n\\nAufgrund des Datenschutzes ist es für Bayer keine Option, Modelle von US-\\n\\nProvidern auf amerikanischen Servern zu nutzen bzw. anzupassen, wenn dabei\\n\\nwertvolle (Patienten-)daten geteilt werden.\\n\\nGleichzeitig besteht beim „finetuning“ die Gefahr, dass ein gewisser Bias, der aus dem Ursprungsmodell hervorgeht, bestehen bleibt. Insofern besteht hier ein großes Interesse an großen Sprachmodellen, die von der Basis aus auf\\n\\neuropäischen Daten trainiert wurden und somit europäische\\n\\nSprachgepflogenheiten aber auch unterschiedliche europäische Sprachen per se\\n\\nberücksichtigt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n327\\n\\nNeben Sprachmodellen spielen auch Foundation-Modelle im Bereich Computer\\n\\nVision eine wichtige Rolle.\\n\\nAuch hier lassen sich Modelle, die bspw. auf amerikanischen Patientendaten\\n\\ntrainiert wurden, nicht immer mit derselben Qualität auf europäische\\n\\nPatientendaten anwenden. Der Grund dafür liegt hier in den unterschiedlichen\\n\\nethnischen Zugehörigkeiten der PatientInnen, die sich dann auch in minimalen\\n\\nUnterschieden bspw. der CT-Scans oder anderen Bilddaten niederschlagen, die\\n\\njedoch für die Performance der Modelle ausschlaggebend sind. Modelle, die auf\\n\\neiner Population trainiert sind, sind somit meist nicht generalisierbar und auf\\n\\nandere PatientInnenpopulationen anwendbar. Aus diesem Grund besteht auch\\n\\nhier ein großes Interesse an europäischen Modellen.\\n\\nBayer kann sich gut vorstellen, das Serviceangebot von LEAM im Bereich Model- Tuning und evtl. auch in der Erstellung von Foundation Modellen zu nutzen.\\n\\n2. Zusammenarbeit mit LEAM ● Bayer könnt sich prinzipiell auch ein Engagement an LEAM im Rahmen eines Joint-Ventures / einer PPP vorstellen.\\n\\nEin anzustrebendes Ziel wäre, gemeinsam mit anderen Akteuren aus dem\\n\\nBereich Health ein Foundation-Modell speziell für Anwendungen in der Medizin\\n\\nzu entwickeln. Das Foundation Modell könnte dann als wertvolle Basis für\\n\\nfirmen-spezifische Domän-Modelle dienen.\\n\\nInterviewprotokoll Berlin Partner\\n\\nInterviewter:\\n\\nPhilipp Günther, Berlin Partner Interviewer: Vanessa Cann, KI Bundesverband Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 30. November 2022\\n\\nZusammenfassung\\n\\n1. Berlin Partner ●\\n\\nInformationen zu Berlin Partner\\n\\nDie Berlin Partner für Wirtschaft und Technologie GmbH ist die\\n\\nWirtschaftsförderung des Landes Berlin.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n328\\n\\nMit ca. 220 Mitarbeitenden unterstützt das Public-Private-Partnership\\n\\nUnternehmen, Wissenschaftseinrichtungen und NPOs am Standort.\\n\\n\\n\\nIm Innovationsfeld Künstliche Intelligenz unterstützt Berlin Partner\\n\\nAkteure in Förderfragen, vernetzt sie untereinander und in\\n\\nAnwendungsbranchen und sorgt für mehr Sichtbarkeit über Berlins\\n\\nGrenzen hinaus.\\n\\nEntwicklung KI-Foundation Modelle in Europa\\n\\nSie sehen die Entwicklung eigener Foundation-Modelle als entscheidende Grundlage für deutsche KI-Innovationen innerhalb der europäischen Wertegemeinschaft.\\n\\n\\n\\nIn ihrer Rolle als Wirtschaftsförderung unterstützen sie Akteure in der\\n\\nAnsiedlung, Identifikation von Fördermöglichkeiten, Stakeholder-Dialog,\\n\\nKommunikation und Transfer.\\n\\nBeispielprojekte sind:\\n\\n■ ResKriVer unter Leitung des Fraunhofer Fokus ■ WHO Hub for Pandemic and Epidemic Intelligence ■ EU Testing and Experimentation Facilities (TEF) Health unter\\n\\nLeitung der Charité Berlin\\n\\n■ www.ki-berlin.de: eine Plattform auf der News, Events und\\n\\nErfolgsgeschichten aus dem Berliner KI-Ökosystem dargestellt\\n\\nwerden.\\n\\nProjekte zu Foundation-Modellen wurden bisher nicht enger betreut.\\n\\nGovernance\\n\\nBerlin Partner ist selbst eine PPP. ○ aus dem Protokoll des vorherigen Gesprächs zum PPP-Modell: ■ Berlin Partner bestreitet öffentliche Aufgaben, die vom Land\\n\\nvergeben werden. Sie erhalten eine institutionelle Zuwendung des\\n\\nLandes und keine Finanzierung über eine Projektstruktur.\\n\\n■ Die Partner für Berlin Holding Gesellschaft für Hauptstadt-\\n\\nMarketing mbH ist zur Hälfte privat, zur Hälfte aus der Stadt\\n\\nfinanziert. Sie hält 28% an Berlin Partner.\\n\\nIBB (Investitionsbank Berlin) Unternehmensverwaltung AöR - 31,5%\\n\\n■ ■ Technologiestiftung Berlin - 30,0%\\n\\n■\\n\\njeweils zu 3,5%: Handwerkskammer Berlin, IHK Berlin, Vereinigung\\n\\nder Unternehmensverbände Berlin und Brandenburg e.V.\\n\\n■\\n\\nInsgesamt hält der öffentliche Sektor weniger als 50% der Anteile.\\n\\n\\n\\nJe mehr Partner im Projekt sind, desto mehr Kompetenzen gibt man auch\\n\\nab.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n329\\n\\nEine Möglichkeit ist die Finanzierung über eine Betreiberkonzession, wie\\n\\nes bspw. bei Messegeländen der Fall ist.\\n\\n2. Wie kann LEAM mit den Projekten zusammenarbeiten? ● Generelle Einschätzung zu LEAM\\n\\nLEAM kann eine Grundlage für KI-Innovationen in Berlin, Deutschland und\\n\\nEuropa sein.\\n\\nFür Berlin Partner zeigt das Interesse des Ökosystems, dass LEAM\\n\\nunterstützt werden sollte. Dabei ist vor allem die europäische Perspektive\\n\\ninteressant.\\n\\nNeben dem Interesse bestehender KI-Akteure in der Hauptstadtregion, gibt es auch Gespräche mit internationalen Akteuren, die nach Berlin kommen wollen, von LEAM profitieren könnten und sich vermutlich\\n\\neinbringen würden. LEAM würde auch die Attraktivität des Standorts\\n\\nweiter steigern.\\n\\nDer Standort Berlin\\n\\nBerlin bietet ein dynamisches und diverses KI-Ökosystem mit internationaler Strahlkraft sowie Akteuren, die auf Augenhöhe kooperieren und offen für neue Partner sind.\\n\\nBerlin ist Innovationsstandort. Auch Unternehmen, deren Hauptstandort woanders liegt, haben häufig Innovation Labs oder Entwicklungsteams in der Hauptstadt.\\n\\n\\n\\nIn Berlin findet exzellente KI-Forschung statt – in den Grundlagen, sowie\\n\\nanwendungsnah. Schwerpunkte sind u.a. NLP sowie erklärbare und\\n\\nvertrauenswürdige KI.\\n\\nBerlin ist KI-Startup-Hauptstadt. ○ Berlin zieht als internationaler Hub viele internationale Talente an, die auch für LEAM benötigt würden.\\n\\nMögliche Standorte und Rechenzentren:\\n\\n■ Mit NTT und Penta Infra gibt es Akteure in Berlin, die ohne\\n\\nZeitverzug ein LEAM Rechenzentrum integrieren und hosten\\n\\nkönnten.\\n\\nDie 11 Zukunftsorte Berlins sowie Innovationsparks genießen\\n\\nFörderprivilegien, haben bereits relevante Infrastruktur und könnten\\n\\nebenfalls interessante Partner und Standorte sein.\\n\\nFinanzierung:\\n\\n■ Die Gesprächsbereitschaft der Berliner Landesregierung wird aus\\n\\nSicht von Berlin Partner als hoch eingeschätzt. Ein enger Austausch\\n\\nmit der Senatsverwaltung für Wirtschaft, Energie und Betriebe zu\\n\\ndiesem Thema besteht bereits.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n330\\n\\n■ Generell gilt, dass eine Bundesförderung, an der sich ein Land\\n\\nbeteiligen kann, die Möglichkeit einer schnelleren Förderung durch\\n\\ndas Land deutlich begünstigt.\\n\\n■ Beispiel: AI Quality & Testing Hub / TEF Health\\n\\nBerlin hat als erstes der involvierten Bundesländer\\n\\nFördermittel für die bundesweite Initiative zugesagt.\\n\\nBerlin Partner konnte außerdem EU-Förderung sowie Partner vermitteln, um ein EU-weites Konsortium zur Bewerbung für Testing and Experimentations Facilities (TEF)\\n\\naufzustellen. Dabei stemmen im Erfolgsfall das Land Berlin,\\n\\nsowie die Charité Berlin die nationale Ko-Finanzierung.\\n\\nZusammenarbeit mit Berlin Partner\\n\\nBerlin Partner kann selbst keine finanzielle Unterstützung leisten. ○ Sie unterstützt aber bei: ■ der Suche nach geeigneten Standorten und entsprechenden\\n\\nPartnern (KI Park, Penta-Infra, Zukunftsorte uvm.).\\n\\n■ der Initiierung und Begleitung der Kommunikation mit\\n\\nMinisterialverwaltungen auf Landesebene.\\n\\n■ der Identifikation von weiteren Fördermöglichkeiten (Landes-,\\n\\nBundes- und EU-Mittel).\\n\\n■ der Vernetzung mit weiteren potentiellen Partnern und Kunden\\n\\naus ihrem Netzwerk.\\n\\n■ durch einen Erfahrungsaustausch zu einer möglichen\\n\\nGovernancestruktur und dem Finanzierungsmodell eines PPP.\\n\\n■ Stakeholder-Dialog auf regionaler Ebene. ■ Kommunikation & Transfer, über Area Managements und\\n\\nAuslandsbüros auch weltweit.\\n\\nInterviewprotokoll BMW\\n\\nInterviewter:\\n\\nHans-Jörg Vögel, Manager AI, Robotics, and Cognitive Systems, BMW Group Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n331\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Zielsetzung von LEAM ist sinnvoll und sehr plausibel. ● BMW ist als Associate Partner in OpenGPT-X involviert. ● Es gibt sehr viele Anwendungsfälle für Foundation-Models z.B. im Bereich Benutzerhandbücher, Kunden- und Fahrerdialoge.\\n\\nMultimodale Modelle sind ebenfalls in Planung. Hier gibt es Forschung im Bereich Innenraumüberwachung und Sprachdialogsysteme. Die Fragestellung, inwiefern Foundation-Modelle / Transformer-Modelle zu Durchbrüchen auch beim\\n\\nAutonomen Fahren führen könnten, ist offen.\\n\\nWichtig ist die Berücksichtigung von Datenschutz, Datensicherheit und\\n\\nInformationsschutz, GDPR-Compliance, Unterstützung von Internationalisierung\\n\\n(zweistellige Anzahl von Sprachen für Märkte weltweit) und die Bereitstellung\\n\\neiner mandantenfähigen Plattform. All dies kann durch US-amerikanische\\n\\ngeneral-purpose Services (derzeit) nicht gewährleistet werden.\\n\\nDie Services von ChatGPT sind bereits jetzt schon kritisch, da Nutzung durch\\n\\nMitarbeiter mit Unternehmensdaten schwer kontrollierbar.\\n\\nFür die digitale Souveränität in Deutschland ist es wichtig, die Modelle zu\\n\\nverstehen und über die Bereitstellung von entsprechender Infrastruktur und\\n\\nMöglichkeiten Experten und Talente im Land zu halten. Forschung und Transfer\\n\\nder Forschungsergebnisse in die Wirtschaft ist dabei essenziell.\\n\\nDie intensive Nutzung von KI und die Möglichkeit, Foundation Modelle zu nutzen, ist sehr wettbewerbsrelevant. Im Moment gibt es hierfür in Deutschland nicht die erforderliche Infrastruktur.\\n\\n2. Zusammenarbeit mit LEAM ● BMW ist auf jeden Fall ein potenzieller Nutzer von LEAM. Betriebssicherheit, Qualität und wirtschaftliche Wettbewerbsfähigkeit der Services sind dafür eine\\n\\nVoraussetzung.\\n\\nDie Beteiligung von BMW an einen Joint Venture / einer PPP kann diskutiert\\n\\nwerden, passende Rahmenbedingungen vorausgesetzt.\\n\\nInterviewprotokoll Bosch Center for AI\\n\\nInterviewter:\\n\\nMichael Fausten, SVP AI and Systems at Robert Bosch GmbH, Bosch Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n332\\n\\nAlex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● KI-Foundation-Modelle sind ein relevantes Thema bei Bosch. Aktuell klärt Bosch die Potenziale der Technologie.\\n\\nNeben Sprach- und Bilddaten sind auch weitere Industriedaten spannend. ● Bei der Nutzung amerikanischer Modelle kann es zu Abhängigkeiten von den USA kommen. Darüber hinaus gibt es das Problem, dass aufgrund geopolitischer Entwicklungen die amerikanischen Modellen eventuell in anderen Märkten -\\n\\nbspw. China - nicht mehr nutzbar sind.\\n\\nEuropäische Modelle müssen genauso leistungsstark sein wie die\\n\\namerikanischen.\\n\\n2. Zusammenarbeit mit LEAM ● Bosch betreibt selbst ein Rechenzentrum. Für die Entwicklung von Foundation Modellen ist ein LEAM Service durchaus attraktiv. Daneben besteht vor allem Interesse an der Möglichkeit des Modell Tunings.\\n\\nBosch steht einer finanziellen Beteiligung bei LEAM grundsätzlich offen\\n\\ngegenüber. Dafür müssten jedoch noch einige offene, insbes. kommerzielle\\n\\nFragen geklärt werden. Ein Konsortium aus mehreren Unternehmen klingt nach\\n\\neinem denkbaren Weg.\\n\\nEine zeitnahe Realisierung ist entscheidend. In fünf Jahren sind wir bereits zu\\n\\nspät.\\n\\nInterviewprotokoll BWI GmbH\\n\\nInterviewter:\\n\\nMario Deng, Lead Service Manager Data Analytics, BWI GmbH Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 10. Januar 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n333\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Services, die auf Basis von KI-Foundation-Modellen entwickelt werden, haben einen Mehrwert. Die BWI möchte diese Anwendungen in ihr Portfolio\\n\\nübernehmen.\\n\\nDie Nutzung von US-amerikanischen AI-Services über API ist für die BWI nicht realistisch, da bei sicherheitskritischen Daten besondere Schutzvorkehrungen gelten. Die Technologie dahinter ist aber nutzungswert.\\n\\nEs müssen Use Cases entwickelt werden, die über das Thema Chatbot\\n\\nhinausgehen.\\n\\nEs ist ineffizient, wenn jede Organisation ihr eigenes Rechenzentrum baut und KI-\\n\\nFoundation-Modelle berechnet. Stattdessen sollten gemeinsam wenige\\n\\nFoundation-Modelle berechnet werden, die dann individuell angepasst werden\\n\\nkönnen.\\n\\n2. Zusammenarbeit mit LEAM ● Aufgrund hoher Sicherheitsanforderungen darf die BWI viele Anbieter nicht nutzen.\\n\\nDie BWI hat Bedarf an den Modellen und den von LEAM geplanten Services. ● Entscheidend sind die Sicherheitsvorkehrungen der Infrastruktur sowie die Sicherheit beim Transfer der Daten.\\n\\nEs muss ausgearbeitet werden, wie diese Sicherheitsvorkehrungen\\n\\nauszusehen haben.\\n\\nDaneben benötigt das BWI für eine Nutzung der Infrastruktur folgende\\n\\nVoraussetzung:\\n\\nEine saubere Dokumentation bei der Entwicklung der Modelle ○ Transparenz bei den für Modelle genutzten Daten und den beteiligten Personen\\n\\nEin Verrechnungsmodell, das eine einfache Kostenplanung ermöglicht.\\n\\nInterviewprotokoll Continental\\n\\nInterviewter:\\n\\nDr. Corina Apachiţe, Programm-Leiterin für Künstliche Intelligenz und Daten Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n334\\n\\n14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Foundation-Modelle ● Die Fähigkeiten von großen Sprachmodellen wie GPT-3 sind faszinierend. ● Diese Fähigkeiten sind auch für Continental wichtig, allerdings müsste hierzu ein domainspezifisches Tuning auf Basis von „Ingenieursprache und spezifischen Inhalten“ erfolgen. Dies kann derzeit nicht über GPT-3 oder ähnliche Modelle\\n\\nabgewickelt werden.\\n\\nDie Befähigung ein eigenes große Sprachmodell zu erstellen bzw. zu tunen ist\\n\\nsehr wichtig für die Zukunftsfähigkeit der deutschen und europäischen\\n\\nDigitalindustrie.\\n\\n2. Multimodale Modelle ● Es besteht ein großer Bedarf auch an neuen multimodalen Modellen u.a. für das autonome Fahren.\\n\\nAuf Basis bestehender Modelle sind derzeit eher inkrementelle Verbesserungen\\n\\nmöglich.\\n\\nWenn ein Anbieter in der Lage ist, auf Basis von großen Foundation Modellen ein sehr viel leistungsfähigeres autonomes Fahren anzubieten, würde dies den Markt stark verändern. Gerade in Deutschland müssen wir uns für diese Disruption\\n\\nstärken und die nutzbare Datenbasis vergrößern.\\n\\n3. Gemeinsame Aktivitäten ● Die Industrie muss zusammenarbeiten und befähigt werden, unter Einhaltung von Regulierung und Gesetzen Foundation Modelle zu entwickeln bzw.\\n\\nbereitgestellte Foundation Modelle zu tunen.\\n\\nEin einzelnes Unternehmen kann diese Herausforderungen aufgrund der unzureichenden Datenmenge, Mangel an Personal und Erfahrung nicht stemmen.\\n\\nDie Erfahrungen aus bestehenden Projekten (bspw. Catena-X) muss genutzt\\n\\nwerden.\\n\\nWir brauchen in Deutschland eine neu aufgebaute „Supply-Chain“ für Modelle\\n\\nund Daten.\\n\\nDie Möglichkeit, große Modelle zu erstellen, wird auch ein Katalysator für den\\n\\nAufbau von Datenpools darstellen.\\n\\nEbenfalls kann durch die Befähigung, große Modelle zu erstellen und durch\\n\\nführende Forschungsaktivitäten in diesem Bereich ein attraktives Betätigungsfeld\\n\\nfür Top-Talente gestaltet werden und so einem „Brain-Drain“ entgegen gewirkt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n335\\n\\nAll diese Punkte sind enorm wichtig für das geostrategische Setup und die\\n\\nWettbewerbsfähigkeit der deutschen Wirtschaft und Gesellschaft.\\n\\nInterviewprotokoll Cyberagentur\\n\\nInterviewter:\\n\\nDaniel Gille, Leiter Sicherheit durch KI und Sicherheit für KI, Cyberagentur Michael Lindner, Pressesprecher, Cyberagentur Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 09.12.2022\\n\\nZusammenfassung\\n\\n1. Cyberagentur ● Die Cyberagentur ist eine GmbH des Bundes. Alleinige Gesellschafterin ist die Bundesrepublik Deutschland, vertreten durch das Bundesministerium des Innern\\n\\nund für Heimat sowie das Bundesministerium der Verteidigung.\\n\\nSie forscht nicht selbst, sondern beauftragt Grundlagenforschung im Bereich disruptiver Technologieansätze mit Bezug zu Innerer und/oder Äußerer Cybersicherheit.\\n\\nKünstliche Intelligenz ist aus Sicht der Cyberagentur eine Schlüsseltechnologie und wird entsprechend in der Forschungsstrategie als eines der relevanten Themenfelder behandelt.\\n\\nDie SPRIND, die im Auftrag des Bundesministeriums für Wirtschaft und\\n\\nKlimasschutz sowie des Bundesministeriums für Bildung und Forschung arbeitet,\\n\\nist die “Schwesteragentur” der Cyberagentur.\\n\\n2. Nutzung von LEAM ● Es lässt sich aktuell noch nicht sagen, ob die Cyberagentur selbst die Kapazitäten von LEAM bräuchte, insbesondere da sie selbst nicht im KI-Bereich forscht und entwickelt. Für Forschungs- und Entwicklungsaufgaben mit Sicherheitsbezug\\n\\nmüssten in jedem Fall hohe Security-Anforderungen erfüllt werden.\\n\\nInsbesondere müssten LEAM-Infrastrukturen relevante VS-Kriterien erfüllen.\\n\\nEine zukünftige Nutzung hochperformanter KI-Infrastrukturen durch forschende Auftragnehmer:innen im Rahmen der Umsetzung ihres Forschungsvorhabens ist\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n336\\n\\ndurchaus vorstellbar. Allerdings kann die Cyberagentur ihren\\n\\nAuftragnehmer:innen hinsichtlich zu nutzender externer Ressourcen keine\\n\\nkonkreten Empfehlungen aussprechen oder Vorgaben machen.\\n\\nDas Thema KI-Foundation-Modelle ist in seiner Relevanz erkannt und auf der\\n\\nForschungsroadmap entsprechend hoch priorisiert. Seine Bedeutung wird in der\\n\\nZukunft noch steigen.\\n\\nFür die Cyberagentur ist bei der Betrachtung von Security-Fragestellungen jeder\\n\\nSchritt im ML-Lebenszyklusrelevant.\\n\\nDie Nutzung nicht-europäischer Modelle kann unter Umständen ein\\n\\nSicherheitsrisiko für die Bundesrepublik Deutschland darstellen. Aktuell stehen\\n\\nwir noch am Anfang der Entwicklung, aber bereits in wenigen Jahren sind\\n\\nmöglicherweise viele Anwendungsbereiche, Wertschöpfungsprozesse und\\n\\nGeschäftsmodelle zumindest in Teilen auf Foundation Models angewiesen, über\\n\\nderen Funktionsweise, Entwicklung, Trainingsdaten etc. nur unzureichende\\n\\nTransparenz besteht. Als mahnendes Beispiel sei auf die Hintertüren-Diskussion\\n\\num 5G und Huawei verwiesen.\\n\\n3. Infrastruktur ● Der Bedarf nach einer übergreifenden KI-Infrastruktur ist nach persönlicher Einschätzung DG gegeben, um die technologische Souveränität bei der Entwicklung und Anwendung großer KI-Modelle sicherzustellen.\\n\\nInterviewprotokoll Deutsche Bahn\\n\\nInterviewter:\\n\\nStephan Kaulbach, Head of Data Intelligence Center, Deutsche Bahn AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Bedeutung des Themas Foundation-Modelle wird in der Zukunft wachsen. ● Aktuell nutzt DB KI-Modelle v.a. für Optimierungen. Es gibt aber erste Bestrebungen, Foundation-Modelle bspw. bei der Angleichung von Handbüchern\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n337\\n\\nfür Lokführer und die Instandhaltung. Im Bereich Customer Support (bspw.\\n\\nChatbots) wird das Thema ebenfalls wichtiger werden.\\n\\nAktuell kann niemand abschätzen, was mit den Daten passiert, die für die Nutzung der amerikanischen Modelle über API in deren Cloud-Services übertragen werden. geschieht. Das stellt die Deutsche Bahn vor\\n\\nHerausforderungen, da sie nicht verhindern können, dass die Mitarbeiter bereits\\n\\njetzt Anwendungen wie ChatGPT nutzen.\\n\\nDie DB arbeitet bereits mit dem deutschen Startup Deepl zusammen. Sie haben in ihren Sprachendienst das Bahnlexikon integriert. Bei dieser Kooperation wurde vor allem Wert auf die Einhaltung von Datenschutzstandards gelegt..\\n\\nGenerell sind europäische Modelle aufgrund des Datenschutzes einfacher zu\\n\\nintegrieren als amerikanische Modelle.\\n\\n2. Zusammenarbeit mit LEAM ● Die Initiative LEAM ist unterstützenswert.Die DB wäre sehr an der Nutzung von LEAM-Modellen interessiert.\\n\\nDie Beteiligung an einem Joint Venture bzw. einer PPP ist grundsätzlich sinnvoll.\\n\\nDie Umsetzung müsste, vor allem hinsichtlich der Struktur der DB, geklärt\\n\\nwerden.\\n\\nEntscheidend ist, dass beim Thema KI-Foundation-Modelle zeitnah etwas\\n\\npassiert.\\n\\nInterviewprotokoll EnBW\\n\\nInterviewter:\\n\\nDr. Frank Säuberlich, Chief Data Officer, EnBW Energie Baden-Württemberg AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Aktuell werden KI-Modelle bereits intensiv in klassischen Vorhersagen wie Predictive Maintenance etc. eingesetzt.\\n\\nGroße Sprachmodelle könnten zukünftig ein wichtiges Thema u.a. zur\\n\\nAnalyse/Verarbeitung von unstrukturierten Daten werden.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n338\\n\\nEs gibt auch erste Ansätze, wie mehr Rechenkapazität bzw. Quantencomputing die aktuell genutzten Modelle verbessern könnten, hier denken wir vor allem in Kontext von Optimierungsmodellen, bspw. für Layout von Offshore Windparks.\\n\\nHierfür sind leistungsfähige Rechner erforderlich. Als Anbieter kritischer\\n\\nInfrastruktur ist die Nutzung von amerikanischen Cloud-Service Providern hier\\n\\nmit entsprechenden Risiken / Unwägbarkeiten verbunden. Das Thema\\n\\nDatensicherheit ist wichtig. Die EnBW schaut sehr genau auf diese Themen, wenn\\n\\nModelle außerhalb Europas gehostet werden.\\n\\nEine europäische Alternativen würde hierbei sehr helfen, daher wird die LEAM\\n\\nInitiative begrüßt.\\n\\nNeben Rechenkapazität spielen auch hochwertige Daten und ausreichend\\n\\nPersonal eine Rolle.\\n\\n2. Zusammenarbeit mit LEAM ● Einer Beteiligung am Projekt LEAM steht die EnBW offen gegenüber. Für eine Zusage müssten allerdings weitere Personen involviert und die praktische Umsetzung konkretisiert werden.\\n\\nInterviewprotokoll Ergo\\n\\nInterviewter:\\n\\nSebastian Kaiser, Head of Machine Learning, ERGO Group AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 16. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Das Thema KI-Foundation-Modelle ist von strategischer Bedeutung. Die hierdurch realisierbaren Produktivitätssteigerungen und Service-Verbesserungen\\n\\nkönnen zu Wettbewerbsvorteilen führen.\\n\\nDie größte Herausforderung bei der Nutzung amerikanischer Modelle ist der Datenschutz. Der Zugang zu GPT-3 über API auf einen Rechner in den USA erlaubt keine Verarbeitung von sensiblen Versicherungsdaten .\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n339\\n\\nEin großer Nachteil der aktuellen Foundation-Modelle ist, dass sie nicht auf\\n\\nversicherungsspezifischen Daten trainiert sind. Andere Sprachmodelle (z.B. Wu\\n\\nDao) enthalten mehr versicherungsspezifische Dokumente und sind daher\\n\\nbesser auf die Domäne anwendbar.\\n\\n2. Zusammenarbeit mit LEAM ● Das Serviceangebot von LEAM trifft genau die Bedürfnisse der Ergo hinsichtlich der Nutzung von großen KI-Modellen.\\n\\nWichtig ist aber auch der Vergleich der Kosten ggü. Anbietern (auch aus den\\n\\nUSA).\\n\\nEin kompetitives europäisches Foundation-Modell würde die Diskussionen rund\\n\\num den Datenschutz enorm vereinfachen.\\n\\nEine mögliche finanzielle Beteiligung der ERGO im Rahmen eines Joint Ventures /\\n\\nPublic Private Partnership ist prinzipiell denkbar\\n\\nDie Muttergesellschaft der Ergo, Munich Re, ist der Zusammenarbeit mit anderen Organisationen in diesem Bereich grundsätzlich aufgeschlossen. Allerdings liegt die Entscheidung hier bei der Munich Re.\\n\\nInterviewprotokoll Forschungszentrum Jülich\\n\\nInterviewter:\\n\\nStefan Kesselheim, Head of AI Consultants Team, FZ Jülich Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 14.11.2022\\n\\nZusammenfassung\\n\\n1. Governance des FZ Jülich ● Finanzierung:\\n\\nDie Finanzierung erfolgte über zwei Stränge:\\n\\n■ zu ca. 50% über die EU über die Organisation PRACE - Partnership\\n\\nfor Advanced Computing in Europe\\n\\n■ zu ca. 50% über das BMBF und das Land NRW über den Verein\\n\\nGauss Centre for Supercomputing (GCS)\\n\\n■ PRACE wird von EuroHPC abgelöst\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n340\\n\\n■ weitere Communities (z.b. Helmholtz-Gemeinschaft) sind z.T. an den Rechnern beteiligt und können Ressourcen frei nutzen\\n\\nOrganisationsform:\\n\\nDas FZ Jülich ist eine GmbH des öffentlichen Rechts. ○ Sie wirtschaftet daher nicht wie eine standardmäßige GmbH.\\n\\nDie Recheninfrastruktur ist Eigentum des FZ Jülich\\n\\nAber das FZ Jülich muss den größten Teil der Rechenzeit für externe\\n\\nProjekte zur Verfügung stellen.\\n\\nUnd nur ein kleiner Teil steht dem FZ Jülich zur freien Verfügung.\\n\\nZugang zu Rechenpower:\\n\\nHalbjährlich werden Projektaufrufe auf Rechenzeit veröffentlicht. ○ Projekte werden anhand eines objektiven, peer-review Verfahrens bewertet.\\n\\nBei kleineren Tier 2 Zentren (bspw. NHR-Verbund) gibt es rollierende\\n\\nAufrufe.\\n\\n● Personal:\\n\\nIn der Regel laufen die Jobs max 24 Stunden\\n\\nEine Aussage zur Anzahl des Personals ist schwierig, da es sich in viele\\n\\nverschiedene Bereiche zergliedert.\\n\\n\\n\\nInsgesamt arbeiten am JSC ca. 300 Personen. Ein kleiner Teil ist direkt mit\\n\\ndem Rechnerbetrieb befasst. Ein Großteil betreibt angewandte Forschung\\n\\nz.B. zu großskaligen Simulation und künstlicher Intelligenz. Diese Expertise\\n\\nist ein Schlüsselfaktor für den Erfolg.\\n\\nAlle Mitarbeiter sind über den öffentlichen Dienst eingestellt. Das bringt\\n\\neinige Herausforderungen mit sich:\\n\\n■ Es gibt wenig Flexibilität beim Gehalt, ■ Kündigungen sind so gut wie ausgeschlossen und ■ es ist schwierig, Dauerstellen zu schaffen.\\n\\nWichtig ist, dass auch das Betreibermodell KI-Expertise besitzen muss.\\n\\nKunden:\\n\\n\\n\\nIm FZ Jülich werden v.a. Simulation für wissenschaftliche Domänen wie\\n\\nKlimaforschung und Quantenphysik berechnet.\\n\\n\\n\\nIndustriepartner spielen nur eine kleine Rolle. Sie zahlen für die Nutzung.\\n\\n2. Technik ● Nutzung\\n\\n\\n\\nIn der Regel liegt die Auslastung über 90%. Die Verfügbarkeit ist\\n\\ntypischerweise über 80%. Dies ist auf geplante Wartungen sowie\\n\\ngelegentlich auftretende Schwierigkeiten mit der Hardware\\n\\nzurückzuführen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n341\\n\\nZeitweise liegt die Auslastung bei weniger als 90%. Dies ist vor allem\\n\\ndarauf zurückzuführen, dass Projekte gleichzeitig starten, aber zum Start\\n\\noft noch keine Rechenzeit benötigen.\\n\\nLimitierung:\\n\\nEs gibt eine maximale Berechnungszeit von 24 Stunden. Dies ist für eine gleichmäßig hohe Auslastung sehr entscheidend. In Ausnahmefällen und größeren Projekten sind Jobketten und Reservierung möglich.\\n\\nManagementsoftware\\n\\nDie Jobvergabe erfolgt über das Queuing-System SLURM. ○ Die verwendete Managementsoftware ist eine Eigenentwicklung des FZ Jülich.\\n\\nInterviewprotokoll Future of Life Institute\\n\\nInterviewter:\\n\\nRisto Uuk, Policy Researcher, Future of Life Institute Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 05.12.2022\\n\\nZusammenfassung\\n\\nHinweis: Das Interview wurde auf Englisch geführt.\\n\\n1. Thema KI-Foundation-Modelle ● Das Future of Life Institute hat einen Artikel veröffentlicht zum Thema: Emerging Non-European Monopolies in the Global AI Market\\n\\nAktuell entwickelt lediglich AlephAlpha Foundation-Modelle in Europa. Sie können\\n\\naber nicht mit den amerikanischen Modellen konkurrieren.\\n\\nEuropa braucht eigene Modelle. Die Hoffnung ist, dass diese vertrauenswürdiger sind, da sie auf GDPR-konformen Daten und weiteren vertrauenswürdigen EU Gesetzen und Guidelines beruhen.\\n\\nDas Rennen um die besten Modelle hat negative Auswirkungen auf die\\n\\nSicherheit. Es geht den Unternehmen darum, schnellstmöglich neue Modelle zu\\n\\nproduzieren und nicht das sicherste Modell.\\n\\nDie Sicherheitsvorkehrungen bei GPT-3 sind bspw. einfach zu umgehen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n342\\n\\n\\n\\nIm AI Act gibt es aktuell keine Gründe für US-Unternehem, ethische Modelle zu\\n\\nentwickeln.\\n\\nAuch Open Source Modelle haben Probleme. Sie sollten nicht ungetestet auf die Welt losgelassen werden. Es bringt auch niemand Autos ohne Bremsen auf die Straße und sagt, probiert mal aus. Der Provider muss sicherstellen, dass jedes\\n\\nModell so sicher wie möglich ist.\\n\\nLediglich große (amerikanische) Unternehmen haben die Ressourcen, um\\n\\nFoundation-Modelle zu entwickeln. Selbst bei einer Zusammenarbeit der Open-\\n\\nSource Community, dem Mittelstand und anderen Unternehmen, wird es\\n\\nschwierig, diese Vormachtstellung zu brechen.\\n\\nInterviewprotokoll Höchstleistungsrechenzentrum Stuttgart\\n\\nInterviewter:\\n\\nBastian Koller, Geschäftsführer, HLRS Dennis Hoppe, Leiter Künstliche Intelligenz und Quantum Computing, HLRS Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Patrick Bunk, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. November 2022\\n\\nZusammenfassung\\n\\n1. Governance\\n\\nOrganisationsform\\n\\no Das HLRS wurde 1996 als Bundeshöchstleistungsrechenzentrum\\n\\ngegründet und ist seit mehr als 25 Jahren eine zentrale Einrichtung\\n\\nder Universität Stuttgart. Das HLRS bietet ein umfassendes Paket\\n\\nan Ressourcen und Services für Hochleistungsrechnen (HPC),\\n\\nDatenanalyse, Künstliche Intelligenz, Visualisierung und verwandte\\n\\nTechnologien. Das HLRS unterstützt Wissenschaftler, Ingenieure\\n\\nund Nutzer:innen aus vielen Forschungs- und\\n\\nAnwendungsbereichen durch die Bereitstellung von HPC-Tools und\\n\\nFachwissen, die für Forschung, Entwicklung besserer Produkte,\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n343\\n\\nLösung komplexer und sehr rechenintensiver Probleme oder die\\n\\nUmsetzung neuer Ideen erforderlich sind.\\n\\no Das Gauss Centre for Supercomputing e.V. (GCS) dient dem\\n\\nZusammenschluss der drei Bundeshöchstleistungsrechenzentren\\n\\nin Deutschland: HLRS (Höchstleistungsrechenzentrum der\\n\\nUniversität Stuttgart), FZJ (Forschungszentrum Jülich) und LRZ\\n\\n(Leibniz Rechenzentrum, Garching bei München). Die drei Zentren\\n\\nstimmen sich im Rahmen des GCS ab, agieren aber im operativen Bereich autonom.\\n\\nKunden\\n\\nDas HLRS unterstützt sowohl akademische Nutzer:innen als auch\\n\\nKund:innen aus der Industrie.\\n\\nKunden aus der Industrie sind in Anzahl und Nutzung der Systeme der Bundeshöchstleistungsrechenzentren ein Alleinstellungsmerkmal des HLRS. Hier können, im Schnitt, bis zu 10% der Rechenkapazität durch\\n\\nIndustriekunden, auf Basis kostendeckender Preise, genutzt werden.\\n\\nUm die Nutzung der Rechenressourcen für Produktionsläufe durch die Industrie zu ermöglichen wurde 1995 eine Public Private Partnership mit den Industriepartnern Daimler und Porsche gegründet. Diese PPP,\\n\\ndie HWW GmbH, ist auch heute noch aktiv; aktuelle Gesellschafter sind\\n\\nPorsche, T-Systems, das Land Baden-Württemberg, die Universität\\n\\nStuttgart sowie das Karlsruher Institut für Technologie. Die HWW agiert\\n\\nprimär als Vermittler von Rechenzeit.\\n\\nAktuell rechnen zahlreiche KMUs und Großunternehmen regelmäßig\\n\\nam HLRS.\\n\\nAkquise erfolgt entweder direkt oder bspw. über die SICOS BW GmbH, die potenzielle Kund:innen anspricht. Oft kommt es am Anfang zu Testläufen, Proof-of-Concepts und dann im Idealfall zur Gewinnung\\n\\nneuer Kunden.\\n\\nDas HLRS bietet seine Rechen- und Speicherressourcen seinen Kunden\\n\\nzu kostendeckenden Preisen an.\\n\\nUm Industriekunden zu gewinnen, bietet das HLRS umfangreichen Support, Sicherheit (über ISO-Zertifizierung) und einen Fokus auf Nachhaltigkeit (bspw. Zertifizierung über den Blauen Engel). Dies sind\\n\\nAspekte, die ein gängiger Cloud-Anbieter nicht bietet und Vertrauen\\n\\nschaffen.\\n\\nPersonal\\n\\nDas HLRS hat feste Stellen im Budget der Universität Stuttgart, der\\n\\nRest der Mitarbeitenden sind auf Drittmittel eingestellt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n344\\n\\n\\n\\nIm Rahmen des deutschen SiVeGCS Projektes (das Projekt, das u.a. die\\n\\nRechner der drei Gauss Zentren finanziert), werden u.a. zwei weitere\\n\\nStellen zur Interaktion mit der Industrie auf Landesebene gefördert.\\n\\nDie Förderung von SiVeGCS erfolgt durch den Bund (BMBF), sowie das\\n\\nLand Baden-Württemberg.\\n\\nBei der Beschaffung neuer Flaggschiff-Systeme liegt oft ein Augenmerk\\n\\nauf eine weitergehende Kooperation mit dem Anbieter, der den\\n\\nZuschlag bekommt. Innerhalb solcher Kooperationen ist auch Personal\\n\\ndes Herstellers vor Ort anzustreben, sowie Mitarbeitende, die in der\\n\\nKooperation mit dem HLRS zusammenarbeiten, um die HPC-Services\\n\\nzu stärken. Diese Personen unterstützen dann das HLRS und den\\n\\nProduktionsbetrieb.\\n\\nZugang zu Rechenressourcen\\n\\nDie Wissenschaft bewirbt sich bei Großprojekten zentral über das GCS\\n\\nund deren durch einen Lenkungsausschuss begutachtete peer-\\n\\nreviewte Calls.\\n\\nBeim HLRS sind jederzeit Anfragen für Projekte auf den Systemen möglich, die von einem wissenschaftlichen Lenkungsausschuss begutachtet werden. Ein Test-Zugang kann individuell nach Absprache\\n\\nzügig gewährt werden.\\n\\nFür Entwicklungsprojekte der Wirtschaft gibt es einen klaren\\n\\nZugangsprozess und gegebenenfalls die Möglichkeit der Priorisierung\\n\\nvon Rechenjobs.\\n\\n2. Ausgewählte Projekte mit Industriebezug ● Fortissimo - Factories of the Future Resources, Technology, Infrastructure and Services for Simulation and Modelling\\n\\nDas aktuelle Projekt (FF4EuroHPC) wird über die EuroHPC Joint\\n\\nUndertaking gefördert.\\n\\nEs ermöglicht KMUs den Zugang zu HPC-Systemen in Europa, wie dem\\n\\nHLRS, um sogenannte Businessexperimente durchzuführen.\\n\\nFür die Nutzung müssen Projektanträge geschrieben werden, die von\\n\\nExperten begutachtet werden.\\n\\nErgebnisse sind Case Studies, die veröffentlicht werden.\\n\\nCATALYST\\n\\nDas Projekt läuft nach 5 Jahren Ende des Jahres 2022 aus; es wurde durch\\n\\ndas MWK Baden-Württemberg gefördert.\\n\\nDas Projekt stellt Kontingente (Personal und Rechenbudget) sowohl für\\n\\nWissenschaft und Industrie zur Verfügung, um Technologien und\\n\\nPotentiale zu validieren.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n345\\n\\nEine geringe Einstiegshürde (bspw. gibt es keine lange Antragsphase)\\n\\nermöglicht eine hohe Akzeptanz des Förderprojektes.\\n\\nProjekte haben in der Regel eine Laufzeit zwischen drei und zwölf Monaten. Ergebnis ist eine Success Story oder mindestens eine gemeinsame wissenschaftliche Publikation, die veröffentlicht wird.\\n\\nDas HLRS hat darüber auch neue Kunden gewonnen.\\n\\nSolution Center\\n\\nDie Solution Center sind eigenständige Vereine, die den Mitgliedern als\\n\\nWissensplattform dienen.\\n\\nAnschubfinanzierung erhalten sie vom Land Baden-Württemberg. Mittelfristig müssen sie sich aber über Mitgliedsbeiträge und Förderprojekte finanzieren.\\n\\nFür das HLRS ermöglichen sie den Kontakt in verschiedene Branchen und\\n\\nsind eine wichtige Säule im Wissens- und Technologietransfer.\\n\\nSatellitenprojekte der Solution Center werden unter anderem auf den\\n\\nHLRS-Systemen gerechnet.\\n\\n3. Technik\\n\\nVerlässlichkeit ist für das HLRS wichtig. Daher:\\n\\nbeschafft das HLRS seine neuen Systeme meist basierend auf ausgereiften Konzepten und Technologien, um Stabilität und Sicherheit zu garantieren; dies schließt jedoch den Einsatz neuester\\n\\nTechnologien nicht aus, die als Teil des Gesamtsystems eingebunden\\n\\nwerden können\\n\\nbietet das HLRS einen vollumfänglichen Service für Kunden an. ○ unterzieht sich das HLRS regelmäßig relevanten Zertifizierungen wie EMAS, Blauer Engel oder Sicherheitsstandards wie TISAX oder im ersten Halbjahr 2023 der ISO 27001 Zertifizierung.\\n\\n4. LEAM ● Bei LEAM müssen Wissenschaft und Industrie klar getrennt werden. ● Die 10% Nutzung für die Industrie ist eine Sonderregelung, die andere HPC- Zentren in Deutschland nicht so einfach nutzen können.\\n\\nEine individuelle Kooperation zwischen LEAM und dem HLRS ist zu begrüßen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n346\\n\\nInterviewprotokoll Höchstleistungsrechenzentrum Stuttgart\\n\\nInterviewter:\\n\\nOleksandr Shcherbakov, Wissenschaftlicher Mitarbeiter, HLRS Interviewer: Jörg Bienert, Alexander Thamm Jakob Tesch, Ubermetrics Hauke Timmermann, eco Verband Protokollant: Alex Dickmann, KI Bundesverband Kim Lambers, eco Verband Datum: 02. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Hardware ●\\n\\nIn Stuttgart sind insgesamt 24 KI-Knoten mit jeweils 8 NVIDIA A100 GPUs\\n\\nvorhanden (= 192 GPUs). Die KI-Knoten sind in das HPC-System integriert, um\\n\\nhybride HPC/KI Workflows zu ermöglichen. Neben diesen KI-Knoten existiert\\n\\nnoch ein weiteres KI-System (CS-Storm) mit insgesamt 64 NVIDIA V100 GPUs und\\n\\nlokalen SSDs. Das reicht für LEAM eher nicht aus.\\n\\n\\n\\nIm Jahr 2024 soll ein neues HPC-System installiert werden, welches aller\\n\\nVoraussicht nach ebenfalls über Beschleunigertechnologie verfügen wird.\\n\\nWeitere Informationen können erst Ende 2023/Anfang 2024 bekannt gegeben\\n\\nwerden. Das System wird öffentlich ausgeschrieben.\\n\\nDer HAWK-Cluster wird wassergekühlt; allgemein geht der Trend aufgrund von\\n\\nEnergieeffizienz und Abwärmenutzung in Richtung Wasserkühlung.\\n\\nGekühlt wird direkt auf der CPU und dem Arbeitsspeicherriegel. ● Co-Location ist zunächst am HLRS nicht vorgesehen; über kleinere Systeme kann individuell diskutiert werden.\\n\\nLinkliste:\\n\\nEntgeltordnung ○ https://www.hlrs.de/solutions/systems/hpe-apollo-hawk ○ https://www.hlrs.de/solutions/systems/cray-cs-storm ○ https://kb.hlrs.de/platforms/index.php/Batch_System_PBSPro_(vulcan)#N ode_types\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n347\\n\\nInterviewprotokoll Hubert Burda Media\\n\\nInterviewter:\\n\\nJean-Paul Schmetz, Chief Scientist, Hubert Burda Media Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle\\n\\nBurda sieht eine hohe Bedeutung von Sprachmodellen. ● Derzeit wird u.a. in Vietnam ein eigenes Sprachmodell entwickelt. ● Ein deutsches/europäisches Foundation-Model würde von Burda zum Tuning und Erstellung unterschiedlicher Anwendungen genutzt werden.\\n\\n2. Daten-Problematik\\n\\nProblematisch ist vor allem die Verfügbarkeit von qualitativ hochwertigen Daten.\\n\\nEs reicht nicht aus, einen “Sumpf an Daten” zu haben, diese müssen auch\\n\\ngerankt, kommentiert und überprüft werden. Das kostet viel Arbeitszeit.\\n\\nInvestitionen sind neben HW vor allem in die Aufarbeitung der Daten erforderlich\\n\\n● Burda besitzt qualitativ hochwertige Datensätze, die sie Kunden anbieten.\\n\\n3. Zusammenarbeit mit LEAM ● Burda könnte LEAM prinzipiell einen gut kuratierten Datensatz gegen Lizenzgebühren zur Verfügung stellen..\\n\\nEs ist unbestritten, dass jeder Anwender ein europäisches LEAM Modell benutzen\\n\\nwürde, wenn es kompetitiv zu den amerikanischen Modellen ist.\\n\\nInterviewprotokoll Hugging Face\\n\\nInterviewter:\\n\\nThomas Wolf, Co-Founder Hugging Face Carlos Munoz Ferrandis, Hugging Face Interviewer:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n348\\n\\nJörg Bienert, Alexander Thamm GmbH Alexander Löser, Berliner Hochschule für Technik (BHT) Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\nHinweis: Das Interview wurde auf Englisch geführt.\\n\\nAI Foundation Models\\n\\nThe Project LEAM is a good initiative. There is a need for a data center that\\n\\nspecializes in AI.\\n\\nThe danger is that in the future all foundation models will come from American\\n\\nprivate companies.\\n\\nHugging Face has trained their model on the French computer Jean Zay. Their\\n\\nexperience shows that it is optimized for other workflows and might be\\n\\nchallenging to train an AI foundation model with the standardized workflows of\\n\\nthe Jean Zay.\\n\\nAt Hugging Face, there are already initial considerations to build their own\\n\\ncomputing cluster. However, the goal is not economic profit, but to support the\\n\\nmission of open models.\\n\\n\\n\\nIn addition, high quality data is critical. There needs to be public support for\\n\\nlabeling data.\\n\\nHugging Face is currently investigating what makes a good dataset. The second\\n\\nstep is to produce these datasets on a large scale.\\n\\nCollaboration with LEAM\\n\\nHugging Face would be interested in using the LEAM computing center.\\n\\nInterviewprotokoll Ionos\\n\\nInterviewter:\\n\\nRainer Sträter, SVP Cloud Services and Global Platform Hosting, Ionos Interviewer: Jörg Bienert, Alexander Thamm GmbH Hauke Timmermann, eco Verband Jakob Tesch, Ubermetrics Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n349\\n\\nAlex Dickmann, KI Bundesverband Datum: 14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Verfügbarkeiten Ionos ●\\n\\nIonos betreibt aktuell drei Rechenzentren in Deutschland.\\n\\n\\n\\nIn Frankfurt wird aktuell ein neues Rechenzentrum errichtet. Der Bau beginnt\\n\\n2023. Das Rechenzentrum wird 2024/2025 betriebsbereit sein. Das Ziel ist es,\\n\\ncarbon negative zu sein.\\n\\nEnergiekosten machen aktuell bereits ein Drittel aller Kosten aus. Die Kosten\\n\\nwerden in Zukunft sicherlich weiter steigen. Energieeinsparung ist daher höchst\\n\\nrelevant.\\n\\n2. Zusammenarbeit mit LEAM ● Die Kalkulation für den Aufbau und Betrieb eines Supercomputing RZ für LEAM sieht realistisch aus. Unter den angegebenen Parametern lässt sich das Projekt realisieren.\\n\\nBeschaffung und Aufbau der Infrastruktur stellt für Ionos kein Problem dar. Die Lieferung und der Aufbau der Boxen alleine dauert aber sicherlich ein halbes Jahr.\\n\\nEntscheidend ist für Ionos eine Zusage zur Abnahme von Rechenzeit über 24\\n\\nMonate. Die genaue Höhe muss im operativen Betrieb geklärt werden.\\n\\nV.a. das neue Rechenzentrum in Frankfurt bietet sich für LEAM an. Die anderen\\n\\nRechenzentren in Berlin und Karlsruhe eher weniger.\\n\\nDer Preis richtet sich vor allem nach den Kosten für den Aufbau und Betrieb. ● Eine genauere Ausgestaltung kann im zweiten Schritt diskutiert werden.\\n\\n3. Finanzierung ● Für Ionos ist die Unterstützung von LEAM eine ernsthafte Option. ● Die Finanzierung könnte u.a. erfolgen durch\\n\\nAufbau und Betrieb eines eigenen RZ und Vereinbarung einer garantierten\\n\\nAbnahmemenge (z.B. 60%-70%) der Kapazitäten durch den Bund\\n\\nAufbau und Betrieb eines eigenen RZ mit Anschubfinanzierung durch die\\n\\nöffentliche Hand und Bereitstellung von Rechenkapazität für\\n\\nWissenschaft/Startups/Public.\\n\\nIonos hat umfangreiche Erfahrungen mit öffentlichen Ausschreibungen.\\n\\n● Bei beiden Vorgehen gilt, dass das Konsortium so klein wie möglich gehalten werden sollte. Ansonsten läuft das Projekt Gefahr, in Abstimmungsschleifen zu\\n\\nverlaufen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n350\\n\\n4. Kühlung ● Die A100 besitzen serienmäßig keine Liquidkühlung. Es gibt aber Anbieter, die diese anpassen. Die neue H100 Generation hat eine Standardmäßige\\n\\nLiquidkühlung. Deren Nutzung wird daher empfohlen.\\n\\nDie Power Usage Effectiveness (PUE) liegt in neuen Rechenzentren bei unter 1,1.\\n\\nEs wird also wenig Energie für andere Zwecke als den Betrieb der Server benötigt.\\n\\nWirkliche Effizienz entsteht aber erst, wenn die Abwärme auch großflächig\\n\\ngenutzt und nicht “in die Umwelt gepustet” wird. Hier lassen sich auch neue\\n\\nGeschäfsfelder erschließen.\\n\\nInterviewprotokoll KI Park\\n\\nInterviewter: Olly Salzmann, Stellvertretender Vorstandsvorsitzender Interviewer & Protokollant:\\n\\nVanessa Cann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Warum unterstützt der KI Park eine Initiative wie LEAM? ● KI Park Mission: Wir wollen einen Beitrag leisten, um KI-Innovationen “made in Germany and Europe” zu beschleunigen und damit Deutschland und die EU bis 2030 zu einem global führenden Innovationsstandort für KI zu machen\\n\\nZiel der Aktivitäten des KI Parks ist die Schaffung von Voraussetzungen für die erfolgreiche Erforschung und Entwicklung zukunftweisender KI-Technologien. Der Schwerpunkt des KI Parks liegt auf Deutschland bzw. Europa und in klarer\\n\\nAbgrenzung zu den USA und China. Daraus abgeleitet die Notwendigkeit einer\\n\\ndeutschen bzw. europäischen KI-Souveränität anerkennt und ermöglichen\\n\\nmöchte.\\n\\nLEAM bietet hier eine gute Möglichkeit den Technologiestandort Deutschland\\n\\nbzw. Europa wieder auf die Weltkarte zu bringen. Diese Ambition wird durch das\\n\\nÖkosystem des KI Park in der Hauptstadt Berlin unterstützt.\\n\\nDer KI Park existiert und definiert sich über seine Mitglieder und repräsentiert eine ausgewählte Gruppe an global führenden Unternehmen wie z.B. Deloitte, VW, Schaeffler oder Celonis und Forschungseinrichtungen wie das ZUSE Institute,\\n\\ndie Friedrich-Alexander Universität, Humboldt Innovation oder ISST Fraunhofer.\\n\\nDie Mitglieder des KI Parks können sowohl von den neuesten Technologien und\\n\\nLösungen profitieren, als auch vom kuratierten Zugang zu Wissen, Fähigkeiten\\n\\nund Erfahrungen. Das liefert auch einen Beitrag zum Gelingen von Initiativen wie\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n351\\n\\nzB LEAM.\\n\\n2. Warum eignet sich der KI Park als Standort für LEAM? ● Im September 2022 wurde das neuste und modernste Data Center in\\n\\nDeutschland direkt neben der KI Park-Geschäftsstelle im Marienpark eröffnet\\n\\nund wird in den nächsten Monaten noch kontinuierlich erweitert. Es besteht\\n\\ndaher die Möglichkeit für LEAM auf modernste aber bereits bestehende\\n\\nInfrastruktur zurückzugreifen zu können. Das bestehende Data Center zeichnet\\n\\nsich insbesondere auch durch seine Nachhaltigkeit aus, so wird die Abwärme des\\n\\nData Center zum Heizen des Marienparkcampus verwendet und der Campus hat\\n\\nseinen eigenen Solarpark zur Stromproduktion. Neben der bestehenden bzw. im\\n\\nBau befindlichen (Server) Infrastruktur bietet die Örtlichkeit des Marienpark\\n\\nCampus in Berlin auch die Möglichkeit, weitere Firmen und andere\\n\\nOrganisationen vor Ort anzusiedeln und somit die Infrastruktur und Ergebnisse\\n\\nvon LEAM zu konsumieren bzw. darauf aufzubauen weitere Produkte entwickeln\\n\\nzu können.\\n\\nDer Community Gedanke wird allein durch physische Nähe des KI Park zu den\\n\\nSchaltzentralen des deutschen Staates (u.a. Regierung, Parlament),\\n\\nKompetenzzentren der Industrie (z.B. Siemens City, AWS Research,\\n\\nInnovationshubs, etc.) und Forschungseinrichtungen (drei Universitäten, DFKI,\\n\\nFraunhofer, etc) weiter unterstütz und ausgebaut. Es kommt so also zu einer\\n\\nVerknüpfung von Infrastruktur und Innovations-Community in nächster\\n\\nUmgebung des deutschen Startups und Innovationszentrums Berlin im Rahmen\\n\\ndes Marienpark Technologie- und Innovationscampus.\\n\\nGanz unabhängig vom KI Park in Berlin, bietet sich die Umsetzung eines Data Centers für Projekte wie LEAM allein aus Gesichtspunkten des Energiebedarfs und den damit verbundenen Nachhaltigkeitsaspekten im Norden von\\n\\nDeutschland und insbesondere in Küstennähe an. Da hier zum einen\\n\\nausreichend Strom erzeugt wird und zumindest für den Norden bereits ein\\n\\nausreichend großes Verteilernetz existiert, wohingegen die Leitungen in Richtung\\n\\nSüden noch fehlen.\\n\\n3. Welche Anknüpfungspunkte gibt es zu bestehenden Aktivitäten des KI\\n\\nPark?\\n\\nEs gibt bereits verschiedene Initiativen im Rahmen des KI Park, die sowohl als\\n\\nGrundlage für LEAM dienen können, bzw. sich gut mit LEAM ergänzen. Zum einen\\n\\nist der KI Park physisch Teil des im Aufbau befindlichem Marienparks, ein in\\n\\nBerlin Tempelhof-Schöneberg gelegener, rund 360.000 m2 großer Gewerbepark,\\n\\nin dem ein innovatives Ökosystem mit Unternehmen und Start-ups aus\\n\\nzukunftsweisenden Bereichen wie KI, Critical Infrastructure, Additiver Fertigung\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n352\\n\\noder auch Laboren für die physische Testung neuer Technologie z.B. im Bauhaus\\n\\nder Erde. Es besteht also noch ausreichend Platz und Infrastruktur für die\\n\\nAnsiedlung weiterer Unternehmen, Initiativen für den Aufbau eines Innovations-\\n\\nCampus und einer (physischen) KI-Community.\\n\\nDie Notwendigkeit für viele kleine wie großen Unternehmen einen rechtlichen Rahmen für schnelles Experimentieren und Forschung auch mit externen Partnern zur Verfügung zu haben ist ein Kern des KI Park Ökosystems. Kleine\\n\\nUnternehmen scheuen oft die notwendigen jedoch meist riskanten Investitionen\\n\\nin Technologie und Infrastruktur; bei großen Unternehmen steht meist\\n\\norganisatorische Komplexität und generelle Widerstände, die mit zunehmender\\n\\nUnternehmensgröße entstehen, im Wege neuer Innovationen durch schnelles\\n\\nExperimentieren und Entwicklung Raum zu geben. In Bezug auf LEAM bietet der\\n\\nKI Park eine rechtliche und organisatorische Umgebung für die erfolgreiche\\n\\nRealisierung des LEAM Projektes, sowie eine möglichst geringe Barriere bzw. eine\\n\\nVielzahl von Anknüpfungspunkten für die anschließende (kommerzielle) Nutzung\\n\\nvon riesigen Sprachmodellen.\\n\\nEiner der zentralen Säulen des KI Park ist KI-Ethik oder verantwortungsvolle KI bzw. Wert getriebener Einsatz von Technologie. Da das LEAM Projekt in der Projektbeschreibung auch in Abgrenzung an andere, vergleichbare Initiativen den\\n\\nDatenschutz, europäische Werte und Open-Source in den Vordergrund stellt,\\n\\nbietet sich hier für den KI Park eine Reihe von Anknüpfungspunkten. Ergänzend\\n\\nist hier auch die Unterstützung eines 60 Millionen Euros schweren, EU- weiten\\n\\nKonsortium zur Erforschung von bildgebenden Verfahren und\\n\\nvertrauensspendender KI im Gesundheitswesen durch den KI Park zu erwähnen\\n\\n(TEF Health via FAU).\\n\\nDas LEAM Projekt mit der dazugehörenden Infrastruktur ergänzt sich perfekt mit\\n\\nden bereits angelaufenen Initiativen des KI Parks im Marienpark seinen\\n\\nMitgliedern Zugang zu 5G bzw. 6G Mobilfunknetzwerke und Quanten Computer\\n\\nzu Testzwecken bzw. in Zukunft bei entsprechendem Erfolg auch im Regelbetrieb\\n\\nzur Verfügung zu stellen.\\n\\n4. Wie ließe sich LEAM im Rahmen des KI Park organisatorisch umsetzen und\\n\\nfinanzieren?\\n\\nDie Betreibergesellschaft für LEAM und dessen Kommerzialisierung könnte als Teil des wirtschaftlichen Geschäftsbetriebs den KI Parks realisiert werden. Idee: ohne Profitmaximierung aber kostendeckend bzw. zur Finanzierung weiter\\n\\nForschung bzw. Weiterentwicklung\\n\\nMitglieder des KI Parks können die Umsetzung des LEAM Projektes sowohl finanziell wie auch organisatorisch unterstützen. Der KI Park kann hier als Koordinierungsstelle und erster Ansprechpartner zur Verfügung stehen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n353\\n\\nUnter den an LEAM interessierten Mitgliedern und Freunden des KI Park sind neben DAX 30 Unternehmen, deutschen Unicorns auch die weltweit größte Professional Service Firm (Deloitte), der wichtigste KI-Infrastruktur Anbieter für\\n\\nriesige KI-Modelle schlechthin (NVIDIA), ein hochinnovativer Projektentwickler mit\\n\\nSchwerpunkt Technologie und Innovation (Investa), sowie einer der global\\n\\nführenden Data Center Entwickler und Betreiber (NTT). So besteht hier sowohl\\n\\ndie Bereitschaft mit eigenen Kräften, Technologie und Wissen zur Verfügung zu\\n\\nstellen, als auch ein Interesse sich für die deutsche bzw. europäische KI- Souveränität zu engagieren.\\n\\nInterviewprotokoll Lufthansa\\n\\nInterviewter:\\n\\nChristian Spannbauer, CTO, Lufthansa Group Digital Hangar Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Aktuell arbeitet Lufthansa im Servicebereich mit Foundation-Modellen. ● Generell sind die Bereiche Optimierung, Sustainability, Efficiency u.ä. interessanter. Dort sollen spezifische Anwendungsfälle identifiziert werden\\n\\nBei sensiblen Daten ist die Nutzung amerikanischer Modelle schwierig. Bei\\n\\noperativen Daten (bspw. Flugdaten, Wetterdaten, u.ä.) ist die Problematik nicht\\n\\nso groß.\\n\\nDie Herausforderung ist aktuell nicht die Technologie, sondern die internen Möglichkeiten. Ohne externe Unterstützung ist eine Implementierung nicht möglich.\\n\\n2. Zusammenarbeit mit LEAM ● Bereich zu engagieren.\\n\\n2. Zusammenarbeit mit LEAM ● Bereich zu engagieren.\\n\\nEine Beteiligung wäre im Prinzip möglich, ist aber unter den aktuellen\\n\\ngesamtwirtschaftlichen Rahmenbedingungen zu prüfen und müsste sich für die\\n\\nLufthansa aber betriebswirtschaftlich rechnen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n354\\n\\nInterviewprotokoll Mercedes-Benz\\n\\nInterviewter:\\n\\nJochen Kaiser, Chief Data Officer, Mercedes-Benz Group AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Allgemeines ● Die Entwicklung und Nutzung von KI ist vor allem dann wichtig, wenn sich hieraus ein Wettbewerb differenzierender Faktor ergibt.\\n\\nChina ist derzeit dominierend (56% der Patente) gefolgt von USA (~20%), Europa\\n\\nliegt mit ca. 8% schon sehr weit hinten.\\n\\nEinsatzgebiet von KI sind neben dem autonomen Fahren vor allem\\n\\nFahrzeugsteuerung und Fahrzeugintegration.\\n\\nDerzeit läuft ein Programm, um mehrere hundert Mitarbeiter im Bereich Data&AI\\n\\nauszubilden.\\n\\n2. KI-Foundation-Modelle ● Die Services von OpenAI werden aktuell genutzt. Prinzipiell ist es sinnvoller, große Modelle selbst zu entwickeln oder auf Basis großer Modelle eigene Modelle zu tunen (auch unter Gesichtspunkten von Bias, Ethik und GDPR)\\n\\nFoundation-Modelle sollten idealerweise Open Source und über die gesamte\\n\\nIndustrie angeboten werden.\\n\\nMercedes-Benz betreibt bereits ein eigenes Rechenzentrum in Norwegen. Das\\n\\nThema Nachhaltigkeit ist dabei ein treibender Faktor.\\n\\nNeben der Rechenzeit sind vor allem die Themen Personal und Daten eine große Herausforderung für die Entwicklung von Foundation Modellen. Der Data Act der EU kann dabei helfen, diese frei zugänglich zu machen.\\n\\n3. Zusammenarbeit mit LEAM ● Mercedes-Benz kann die geplanten LEAM Services nutzen. Bei amerikanischen Services behindern juristische Vorgaben oft die Anpassung und Nutzung bzw. verlangsamen den Start von Projekten durch längliche Klärung von juristischen\\n\\nKlauseln.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n355\\n\\nSynergien mit anderen Unternehmen u.a. auch im Rahmen von Catena-X sind\\n\\nsinnvoll.\\n\\nOpendata und der European Data Act können sich mit der Entwicklung von\\n\\nFoundation Modellen gegenseitig positiv beeinflussen.\\n\\nDie Beteiligung in einem Konsortium zur Finanzierung von LEAM ist gut\\n\\nvorstellbar und sinnvoll. Dabei sollte auch auf das Thema Daten geachtet\\n\\nwerden. Die Herausforderung ist die Geschwindigkeit, mit der das Projekt\\n\\nrealisiert wird.\\n\\nDie Initiative ist v.a. auch für kleinere Akteure wie Zulieferer entscheidend. Die werden das Thema Nutzung von Foundation-Modelle nicht alleine umsetzen können.\\n\\nInterviewprotokoll Ministerium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg\\n\\nInterviewter:\\n\\nDr. Peter Mendler, Leitung des Referats „Industrie- und Technologiepolitik, Digitalisierung“, Stv. Abteilungsleiter im Ministerium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg Interviewer: Jörg Bienert, Alexander Thamm Vanessa Cann, KI Bundesverband Protokollant: Alex Dickmann, KI Bundesverband Datum: 30. November 2022\\n\\nZusammenfassung\\n\\n1. Aktivitäten in Baden-Württemberg ● Aleph Alpha als KI Champion 2021 war der erste öffentlich sichtbare Kontaktpunkt mit dem Thema KI-Foundation Modelle.\\n\\nIPAI & KI-Exzellenzzentren sind weitere Anknüpfungspunkte mit dem Thema.\\n\\n● Es gibt verschiedene Förderprojekte und –programme zum Thema KI, aber Foundation Modelle laufen unter anderen Dimensionen.\\n\\nZiel des Landes Baden-Württemberg ist es, Ökosysteme zu schaffen, die auch\\n\\nphysisch angesetzt sind.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n356\\n\\n2. Warum ist LEAM für Baden-Württemberg interessant? ● Es passt in die KI-Strategie der Landesregierung. Generell ist das Thema KI hoch auf der Agenda des Landes angesetzt.\\n\\nBaden-Württemberg hat seine Aktivitäten bei KI in den letzten Jahren mit hohem\\n\\nTempo entwickelt. Es gibt Synergien mit:\\n\\ndem IPAI und den KI-Exzellenzzentren, ○ den Universitäten und Forschungseinrichtungen im Land, insbesondere auch mit dem CyberValley,\\n\\nden zahlreichen Unternehmen mit KI-Aktivitäten einschließlich der\\n\\nzahlreichen KI-Startups\\n\\ndem bestehenden KI-Ökosystem und ○ verschiedenen Wirtschaftsinitiativen wie bspw. die europäische Testing and Experimentation Facility (TEF Manufacturing) oder die Vanguard Initiative.\\n\\nAleph Alpha gilt als Leuchtturm für das Thema. ● LEAM würde dem rasch wachsenden KI-Ökosystem in Baden-Württemberg zusätzliche Dynamik geben und Baden-Württemberg könnte ggf. mit hoher Wahrscheinlichkeit rasch Flächen für LEAM mobilisieren. Dadurch könnten sich\\n\\nzusätzliche nationale und europäische Kooperationen entwickeln.\\n\\n3. LEAM ● Die Initiative wird ein Erfolg, wenn es eine Zusammenarbeit zwischen privaten und öffentlichen Akteuren ist. Dafür müssen beihilferechtliche Fragestellungen geklärt werden.\\n\\nFür die Finanzierung über ein IPCEI-Projekt müssten zuerst die beihilferechtlichen\\n\\nVoraussetzungen geschaffen werden.\\n\\nEs braucht ein tragfähiges Finanzierungskonzept und ein nachhaltiges\\n\\nGeschäftsmodell, damit das Projekt wirtschaftlich erfolgreich sein wird.\\n\\n4. Finanzierung: ● Die Landesregierung Baden-Württemberg möchte bei KI im internationalen Innovationswettbewerb vorne dabei sein.\\n\\nDas Land würde sich dafür einsetzen, dass LEAM nach Baden-Württemberg\\n\\nkommt.\\n\\nPrivate Finanziers müssen in das Projekt eingebunden werden. Hier stellt das\\n\\nLand bei Bedarf gerne Kontakte her.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n357\\n\\nInterviewprotokoll Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen\\n\\nInterviewter:\\n\\nChristian Dinnus, Referatsleiter Digitale Wirtschaft, Digitale Geschäftsmodelle (MWIKE NRW) Sebastian Ley, Digitale Wirtschaft, Digitale Geschäftsmodelle (MWIKE NRW) Dr. Dirk Hecker, Managing Director Fraunhofer Allianz Big Data, Fraunhofer IAIS Interviewer: Vanessa Cann, KI Bundesverband Protokollant: Alex Dickmann, KI Bundesverband Datum: 06.12.2022\\n\\nZusammenfassung\\n\\nWichtiger Hinweis: Die Interviewten äußern ihre fachliche Einschätzung, die im Wesentlichen auf öffentlich verfügbaren Informationen beruhen. Zusagen oder ähnliches sind mit den Äußerungen nicht verbunden.\\n\\n1. Thema KI-Foundation-Modelle ● Die Größe der Modelle und Recheneinheiten spielt eine Rolle. Je größer diese sind, desto mehr Wirkung lässt sich erzielen. Um Basismodelle entstehen Ökosysteme, die darauf Zugriff haben möchten. Diese Ökosysteme sind das, was\\n\\nwir in Deutschland und Europa erreichen sollten. Wichtig ist entsprechend eine\\n\\nAnwendbarkeit der Basismodelle (Anpassen für konkrete Anwendungsfälle).\\n\\n2. Standort NRW ● Das Rheinland liegt ideal zwischen den Welt-Internetknoten in Frankfurt und Amsterdam, bei hoher Stromversorgungssicherheit und eingebettet zwischen\\n\\nden Städten Köln, Düsseldorf, Aachen, Bonn, mit hohem Potenzial an Fachkräften\\n\\n(vgl. Machbarkeitsstudie Dateninfrastrukturen im Rheinischen Revier unter\\n\\nwww.dateninfrastruktur.nrw).\\n\\nDie Region ist auch ein starker Forschungsstandort mit relevanten\\n\\nForschungsfeldern, bspw. RWTH Aachen, Universität Bonn, Universität Köln,\\n\\nFraunhofer IAIS, FZ Jülich (Quanten- und Super-Computing)\\n\\nNordrhein-Westfalen verfügt über ein stark aufstrebendes Start-up Ökosystem, dass gerade im Rheinland (Aachen, Bonn, Köln, Düsseldorf) besonders stark ausgeprägt ist (vgl. www.wirtschaft.nrw/sites/default/files/documents/nrw_start-\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n358\\n\\nup-report_2020.pdf). Im speziellen gibt es im Umfeld von KI und Sprachmodellen\\n\\nbesonders hervorzuhebende Start-ups, wie bspw. DeepL, das erste NRW-Unicorn\\n\\nmit Sitz in Köln oder Cognigy aus Düsseldorf.\\n\\nNordrhein-Westfalen hat die eigene Kompetenzplattform KI.NRW als zentrale\\n\\nAnlaufstelle für Künstliche Intelligenz in Nordrhein-Westfalen. Neben konkreten\\n\\nProjekten zum Transfer aus der Forschung in die Wirtschaft ist das Thema „KI-\\n\\nZertifizierung“ ein wichtiges Thema in NRW.\\n\\n● Das Gaia-X-Projekt Open GPTX für die Entwicklung großer KI-Sprachmodelle hat mehrere starke Teilnehmer und Partner aus NRW, so Westdeutscher Rundfunk (WDR), Forschungszentrum Jülich, Fraunhofer IAIS (und mehr).\\n\\nIm Rheinischen Revier (Hürth) entsteht das Projekt AI Village.\\n\\nAuf Basis der genannten Voraussetzungen werden Maßnahmen umgesetzt, die\\n\\ndas Rheinland zur Digitalregion weiterentwickeln, insbesondere sollen\\n\\nDigitalparks entstehen.\\n\\nEin Digitalpark ist eine für Unternehmen der Digitalwirtschaft optimierte\\n\\nGewerbefläche, in räumlicher Nähe zu großen Rechenzentren und\\n\\nInternetknoten (Grund: sehr schnelle Reaktions- bzw. Latenzzeiten). Die\\n\\nFinanzierung erfolgt über private Investoren.\\n\\n\\n\\nIn einem Digitalpark steht immer auch ein Rechenzentrum. Hier werden\\n\\nSynergien und große Chancen zu dem Vorhaben „KI-Rechenzentrum“ gesehen.\\n\\n3. Governance ● Die Umsetzung im Rahmen eines PPP-Modells ist eine Möglichkeit, die im Detail zu prüfen wäre.\\n\\nDie Höhe des Finanzierungsbedarfs lässt den Bund als natürlichen Partner dieses Projekts erscheinen. Fachlich werden starke Anknüpfungspunkte zu nordrhein- westfälischen Initiativen gesehen (siehe „Standort NRW“, sowie „Housing“).\\n\\n4. Housing ● Standorte für Digitalparks gibt es in der Region, und damit auch für ein Rechenzentrum in der für das KI-Rechenzentrum angedachten Größenordnung.\\n\\nDie Anforderungen (Infrastruktur, Größe, Lage) an einen Standort und die Flächensuche sind sehr wichtig und sollten bei der Machbarkeitsstudie mit bedacht werden. Kompetenzen bei der Flächensuche für Rechenzentren liegen in\\n\\nNRW vor.\\n\\nEine gute Stromversorgung auf der einen Seite und eine gute Nutzung\\n\\nentstehender Abwärme auf der anderen Seite sind für Rechenzentren wesentlich.\\n\\nInsgesamt sollte das Thema „Nachhaltigkeit“ in all seinen Facetten berücksichtigt\\n\\nwerden (Energieeffizienz, erneuerbare Energien, Wassereffizienz, Recycling).\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n359\\n\\nInterviewprotokoll MPI\\n\\nInterviewter:\\n\\nProf. Dietmar Harhoff, Geschäftsführender Direktor, Max-Planck Institut für Innovation und Wettbewerb Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle\\n\\nHerr Prof. Harhoff stimmt der Analyse der LEAM-Initiative definitiv zu. Europa\\n\\ndarf bei diesem Thema nicht in die Abhängigkeit der Amerikaner und Chinesen\\n\\ngeraten.\\n\\nEs ist vor allem eine Frage der Ressourcen. Die Entwicklung von GPT-3 zu ChatGPT war bspw. eine Frage des Aufwands und kein technologischer Durchbruch.\\n\\nDie Frage nach verfügbaren Daten wird entscheidend sein. Zu viele europäische\\n\\nOrganisationen halten ihre Daten noch verdeckt.\\n\\n2. Next-Steps\\n\\nEr empfiehlt der LEAM- Initiative weitere Gespräche.\\n\\nInterviewprotokoll Otto\\n\\nInterviewter:\\n\\nDr. Michael Müller-Wünsch, CIO, Otto Gmbh & Co KG Interviewer & Protokollant: Jörg Bienert, Alexander Thamm GmbH Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n360\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Bis 2030 könnten 50% der IT-Anwendungen durch KI-Modelle geprägt sein. ● Die Gefahr ist, dass alle Daten, die in US-Cloud Services gespeichert und verarbeitet werden, sich nicht mehr im EU-Raum befinden. Letztendlich machen\\n\\nwir Amerika damit stark.\\n\\nOtto evaluiert derzeit KI-Modelle, u.a. neben Sprachmodellen auch im Bereich\\n\\nBild-und Videobearbeitung. Große KI Modelle werden in diesem Umfeld\\n\\nzunehmend an Bedeutung gewinnen.\\n\\nOtto muss prüfen, inwiefern sie US-Services aus Datenschutz und\\n\\nDatensicherheitsaspekten nutzen können.\\n\\n2. Zusammenarbeit mit LEAM ● Ein unabhängiger deutscher Service zur Erstellung und Nutzung von Foundation Modellen ist hilfreich und zu begrüßen. Eine Initiative wie LEAM ist politisch sinnvoll.\\n\\nDas Angebot muss allerdings wettbewerbsfähig im Hinblick auf Servicequalität\\n\\nund Kosten sein.\\n\\nEine Zusammenarbeit mit LEAM und eine Nutzung der Services wird gewünscht. ● Eine mögliche Investition in ein Joint-Venture wäre sinnvoll, ist aber kurz- und mittelfristig wegen der angespannten wirtschaftlichen Situation nicht realistisch.\\n\\nInterviewprotokoll REWE\\n\\nInterviewter:\\n\\nLorenz Determann, Bereichsleiter Analytics, REWE Group Interviewer & Protokollant: Jörg Bienert, Alexander Thamm GmbH Datum 15.12.2022\\n\\nAllgemeine Einschätzung\\n\\nAllgemein wird das LEAM Vorhaben begrüßt, vor allem unter dem Aspekt, alternative Angebote für General Purpose Modelle aus Deutschland zu bekommen\\n\\nEine Nutzung der LEAM Services bzw. Foundation Modelle wird stark abhängig sein, von dem Mehrwert und wirtschaftlichen Nutzen – vor allem hinsichtlich\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n361\\n\\no Den Kosten der bereitgestellten Services/Modelnutzung im\\n\\nWettbewerbsvergleich o Der Qualität der Modelle\\n\\nKI bei Rewe\\n\\nDerzeit werden in unterschiedlichen Bereichen eher kleine / mittelgroße KI-\\n\\nModelle selbst entwickelt (u.a. auch auf Basis von Bert)\\n\\nBei zentralen Modellen ist eine Eigenentwicklung wichtig, um mit eigenen Daten\\n\\nund Berechnungen Wettbewerbsvorteile erzielen zu können\\n\\n\\n\\nIn allgemeineren, unkritischen Bereichen ist auch eine Nutzung von allgemeinen\\n\\nModellen denkbar\\n\\nCloud Infrastruktur\\n\\nRewe greift u.a. auch im Data&Anaytics / AI Bereich auf die Cloud Services von\\n\\nGoogle zurück\\n\\nEine mögliche Abhängigkeit von amerikanischen Providern im Bereich der KI Applicationen / KI Foundation Modellen wird in diesem Kontext nicht als besonders kritisch gesehen.\\n\\nInterviewprotokoll SAP\\n\\nInterviewter:\\n\\nDr. Feiyu Xu, Senior Vice President, Global Head of Artificial Intelligence, SAP Interviewer: Dr. Sven Schmeier, DFKI Dr. Gerhard Paass, Fraunhofer IAIS Protokollant: Alex Dickmann, KI Bundesverband Datum: 13. November 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● SAP nutzt Foundation-Modelle bereits, v.a.in der Dokumentenverarbeitung. Dabei spielen auch multimodale Modelle eine Rolle, wenn bspw. Rechnungen per\\n\\nFoto geschickt werden und weiterverarbeitet werden müssen. Im Bereich\\n\\nProzessplanung machen wir auch Experimente mit Foundation-Modellen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n362\\n\\nEntscheidend für einen erfolgreichen Einsatz von Foundation-Modellen sind\\n\\nfolgende Aspekte:\\n\\n(1) Konzepte für den sicheren Zugang zu den Daten; (2) Eine Infrastruktur zur Datenhaltung und Rechenkapazität, die auf grüner Energie basiert; (3) Realisierung von „Data to Value“: Wie können wir domänenspezifische Modelle trainieren und damit wirtschaftlichen Mehrwert schaffen, z.B. für Geschäftsprozesse, Prozesse in Bereichen wie Manufacturing oder Supply Chain etc. bzw. auch für industriespezifische Anpassungen.\\n\\nWichtig in diesem Kontext ist, dass die deutsche Industrie die KI-Foundation- Modelle nutzen kann. Viele deutsche Firmen haben keinen Zugriff auf die notwendigen menschlichen Ressourcen, wie Data Scientists, und auf die\\n\\nnotwendige KI-Infrastruktur. Deshalb muss der Zugang zu KI-Technologien,\\n\\ninsbesondere zu Foundation-Modellen, auch möglichst einfach gestaltet werden.\\n\\nIn der Planung von LEAM muss man unterschiedliche Personas und Stakeholders\\n\\nfür die Entwicklung und Anwendung der Foundation-Modelle identifizieren und\\n\\nihre Rollen spezifizieren, als Beitrag zur Entwicklung einer holistischen Strategie\\n\\nfür die Deutsche Forschung, Wirtschaft und Industrie.\\n\\n2. Zusammenarbeit mit LEAM ● Ein Ansatz wie LEAM hilft, um ähnliche Modelle parallel an verschiedenen Orten und in verschiedenen Anwendungskontexten berechnen und nutzen zu können.\\n\\nDas Prä-Investment via LEAM ist notwendig, damit KI-Foundation-Modelle auch in\\n\\nder Praxis und mit wirtschaftlichem Mehrwert einsetzbar sind.\\n\\nMan braucht dazu ein Kollaborationsmodell zwischen der Wirtschaft und der\\n\\nWissenschaft.\\n\\nLEAM sollte auf jeden Fall auch die Möglichkeiten für Inferenzen über KI-\\n\\nFoundation-Modelle zur Verfügung stellen. Dadurch bekommen kleinere Akteure\\n\\ndie Ressourcen, um mit Foundation-Modellen gewinnbringend zu arbeiten.\\n\\nInterviewprotokoll Siemens\\n\\nInterviewter:\\n\\nMichael May, Head Company Core Technology Data Analytics & Artificial Intelligence, Siemens AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n363\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Foundation-Modelle sind ein wichtiges Thema. Mit GPT-3 wurde bereits experimentiert, bspw. für den Servicebereich.\\n\\nFür die Siemens-Geschäftsbereiche ist v.a. das Thema Industrial AI wichtig. Hier sind uns im Bereich Foundation-Modelle noch keine echten Anwendungen bekannt. Ein wichtiger Punkt ist, dass die Datensätze tendenziell in diesem\\n\\nBereich kleiner bzw. fragmentiert sind.\\n\\nSiemens arbeitet gemeinsam mit der LMU und TUM an\\n\\nGrundlagenforschungsthemen, z.B. im Rahmen von Doktorarbeiten, und\\n\\nevaluiert parallel konkrete Anwendungsfälle für Foundation Models.\\n\\nSiemens hat derzeit keine eigene hausinterne Infrastruktur zum Trainieren von\\n\\nFoundation Models.\\n\\n2. Zusammenarbeit mit LEAM ● Ein deutsches bzw. europäisches Angebot ist wünschenswert und hätte viele Vorteile.\\n\\nEs besteht ein hohes Interesse, die LEAM-Initiative zu unterstützen ● Projekte zur Erstellung von Foundation-Modellen können dabei unterstützen, das Thema Datenteilung (innerhalb und zwischen Unternehmen) neu zu beleben und ihm eine neue Relevanz zu verleihen.\\n\\nDas wird v.a. kleineren Unternehmen helfen, die aufgrund fehlender finanzieller\\n\\nund personeller Ressourcen weniger Möglichkeiten haben als Großunternehmen.\\n\\nInterviewprotokoll TÜV Süd\\n\\nInterviewter:\\n\\nDirk Schlesinger, Chief Digital Officer, TÜV Süd Interviewer: Jörg Bienert, Alexander Thamm Protokollant: Alex Dickmann, KI Bundesverband Datum: 06.12.2022\\n\\nZusammenfassung\\n\\n1. Thema LEAM ● Wir brauchen eine gemeinsame Aktion!\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n364\\n\\nDas mögliche de-facto Monopol der Marktbegleiter aus den USA verträgt sich\\n\\nnicht mit unserem europäischen Wertekanon.\\n\\nWir müssen Basisfähigkeiten aufbauen, um nicht abhängig und abgehängt zu\\n\\nwerden.\\n\\nEin Gegengewicht aus geostrategischer Sicht ist notwendig – vgl. supply chain für\\n\\nGas, seltene Erden, usw.\\n\\n2. Rechenzentrum ● ● Mit den Gauss Centre for Supercomputing haben wir in Deutschland die Erfahrung, wie das Thema Rechenzentrum geht – wir wissen, wie Großforschung\\n\\ngeht (DESY, Jülich…).\\n\\nCapex ist besser als Opex, weil das Budget nicht jedes Jahr neu verhandelt\\n\\nwerden muss.\\n\\n3. Datenmanagement ● Es gibt offene Fragen zu Themen wie GDPR, homomorpher Verschlüsselung, Federated Learning und weiteren. Die Gefahr ist, dass viele Unternehmen ohne\\n\\nGarantien für ihre IP nicht bereit sind, ihre Daten zu teilen.\\n\\nGDPR-Konformität kann dabei ein Wettbewerbsvorteil ggü. USA und China sein. ● Federated Learning as a Service ist eine großartige Idee. Vor allem, wenn Partnern Rechtsunsicherheiten genommen werden können.\\n\\n4. Was macht der TÜV? ● Der TÜV SÜD nutzt KI selbst in verschiedenen, Projekten (bspw. Visual Analytics). ● Wir (TÜV SÜD) nutzen bspw. NLP für Konsistenzprüfungen in Dokumenten und bauen darauf aktuell ein Document Service Layer auf.\\n\\nDer TÜV verwendet große Modelle bisher nicht! Es ist aktuell auch nicht geplant, diese zu nutzen. Die größte Herausforderung ist dabei sicherlich die Einführung der Modelle innerhalb der Organisation.\\n\\nTÜV nutzt Explainability Verfahren. ● Kuratierte, große Datensätze werden aber ein wichtiges Thema, v.a. in Hinblick auf Marktzulassung von Produkten (benchmarks).\\n\\nCase: Simulation von einem autonom fahrenden Auto braucht Daten über die Straßenbeschaffenheit, das Wetter, die Lichtverhältnisse, Fahrphysik, etc.\\n\\nSobald der AI Act der Europäischen Union umgesetzt ist, wird sicherlich auch der\\n\\nTÜV auf dem Gebiet der Qualitätszertifizierung von KI werden.\\n\\n5. Governance ● AI Lab GmbH:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n365\\n\\nTÜV Gesellschaften arbeiten z.Zt, als Arbeitsgruppe des Verbandes\\n\\nvorwettbewerblich und gemeinschaftlich zusammen, um das Thema KI –\\n\\nQualitätskontrolle voranzutreiben, v.a. auf technischer Ebene.\\n\\nDie Gründung einer AI Lab GmbH ist angestrebt, vorbehaltlich einer\\n\\nGenehmigung durch das Bundeskartellamt. Pilotprojekte werden aber\\n\\nbereits abgehandelt.\\n\\nEin Vertrieb der entwickelten Anwendungen ist hier aber nicht ohne\\n\\nweiteres möglich, bzw. nicht angestrebt. Rolle der AI Lab GmbH ist die\\n\\neines internen R&D-Dienstleisters\\n\\nDie TÜVe haben über viele Projekte dazugelernt. Einen allgemeinverbindlichen\\n\\nBlueprint gibt es nicht.\\n\\nAI Quality und Testing Hub:\\n\\nEine Landesgesellschaft des Landes Hessen und des VDE als erste\\n\\nShareholder.\\n\\nHerausforderung: Zusammenarbeit mit dem Land, das anders plant und\\n\\narbeitet als eine Firma, der VDE oder der TÜV.\\n\\n○ AIQs sind komplementär zu den AI Labs. Die AI Labs sind interne R&D Stellen, die Quality und Testing Hub bieten Kunden Services an.\\n\\nIn NRW ist das AI Quality und Testing Hub ein klassisches Förderprojekt.\\n\\nAktuell gibt es die Hubs nur ‚auf dem Papier‘ sie sind noch nicht final\\n\\ngegründet, obwohl dies die nächsten Wochen / Monate geschehen dürfte.\\n\\nTesting & Experimentation Facility Healthcare:\\n\\nCharite als Konsortialführer. Läuft sehr gut. ○ Europäisches Projekt – 30 plus Partner ○ Ziel: Entwicklung und agile Zertifizierung von ‚echten‘ Medizinprodukten mit KI\\n\\nFSD GmbH als Analogie für AI Lab GmbH:\\n\\nentwickelt Prüfmittel für Hauptuntersuchung der Autos ○ Bund beleiht FSD und ist daher auch beteiligt. Nicht immer sind die Interessen der öffentlichen Hand dieselben wie von\\n\\nWirtschaftsunternehmen – Ausgleich und Absprache vorab wichtig.\\n\\nInterviewprotokoll VDE\\n\\nInterviewter:\\n\\nSebastian Hallensleben, Head of Digitalisation and AI, VDE Verband der Elektrotechnik Elektronik Informationstechnik e. V. Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n366\\n\\nAlex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● KI-Foundation-Modelle sind eine Schlüsseltechnologie. Die europäische Wirtschaft und Gesellschaft braucht eigene Modelle.\\n\\nDie Auswirkungen auf Geschäftsmodelle und Jobs wird sehr groß, z.B.\\n\\nRedenschreiber, Analysten, Standard-Journalismus, Kundenservice, Briefings, etc.\\n\\nDas Training eigener Foundation-Modelle ist eine Frage der europäischen\\n\\ndigitalen Souveränität. Europa braucht die Infrastruktur und die Kompetenz, um\\n\\nbeim Thema nicht abgehängt zu werden.\\n\\nDer Bedarf an den Modellen steigt. ● Das Thema der Qualitätskontrolle dieser Modelle wird wichtiger. Eine Qualitätskontrolle ist aber nur möglich, wenn wir die Technologie selbst\\n\\nbeherrschen.\\n\\nDerjenige, der die Technologie beherrscht, wird auch die Standards setzen und die Regulierung steuern. Wenn wir nicht in der Lage sind, hier mitzuwirken, werden wir die digitale Souveränität auch in diesem Bereich verlieren.\\n\\nDie Verfügbarkeit von Rechenkapazitäten sollte Teil der staatlichen Infrastruktur /\\n\\nDaseinsvorsorge sein.\\n\\n2. AI Quality und Testing Hub ● Gemeinsam mit dem Land Hessen hat die VDE ein AI Quality und Testing Hub gegründet.\\n\\n\\n\\nInhaltlich beschäftigt es sich mit Qualitätsmanagement und KI. Dafür sollen\\n\\nPrüfwerkzeuge zu einem Toolkit zusammengeführt und die Trainingsdaten-\\n\\nQualität überprüft werden.\\n\\nDer Hub soll in den nächsten Jahren organisch wachsen. ● Für eine Zusammenarbeit mit dem Land Hessen mussten einige Punkte im Beihilferecht beachtet werden. Bspw. tritt Hessen als kommerzieller Investor auf,\\n\\nes gab Vorgaben für den Aufsichtsrat und das Finanzministerium muss den\\n\\nBusiness Plan absegnen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n367\\n\\nInterviewprotokoll Volkswagen\\n\\nInterviewter:\\n\\nPatrick van der Smagt, Director of AI Research, Volkswagen Group Machine Learning Research Lab Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband e.V. Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Anwendung von KI-Foundation-Modellen bei VW befindet sich aktuell in der Evaluation. Dabei sind viele Anwendungen, auch im Bereich End-User\\n\\n(Kommunikation im Auto, Chatbots in der Kundenkommunikation, Digitalisierung\\n\\nvon Handbüchern, etc.) denkbar.\\n\\nDabei denkt VW sowohl über die Nutzung und Tuning bestehender Modelle als\\n\\nauch das Training eigener Foundation Modelle nach.\\n\\nVW besitzt kein eigenes Rechenzentrum, um Foundation-Modelle zu trainieren.\\n\\nSie müssten hier auf externe Anbieter zurückgreifen.\\n\\nFür die Entwicklung und Nutzung von Foundation Modellen sind für VW sind auch\\n\\ndie Themen Explainability, Trustworthy AI und die juristischen\\n\\nRahmenbedingungen wichtig.\\n\\nDie Umsetzung und Nutzung von Modellen auf Basis einer durch einen Cloud-\\n\\nService bereitgestellten API ist aus Gründen der Datensicherheit problematisch.\\n\\nVW arbeitet nur mit Modellen, die für sie kontrollierbar sind und idealerweise\\n\\nauch vom Unternehmen gehostet werden.\\n\\nDaneben ist das Thema Datenverfügbarkeit wichtig. VW besitzt eigene, spezielle\\n\\nDatensätze auf denen Modelle trainiert werden müssten.\\n\\n2. Zusammenarbeit mit LEAM ● VW begrüßt die Initiative LEAM und möchte gerne weiter unterstützen. ● LEAM Services würden im Bereich Modell Tuning, aber auch potentiell in der Erstellung von eigenen Foundation Modelle genutzt.\\n\\nDie Beteiligung an einem Joint Venture bzw. einer PPP ist grundsätzlich sinnvoll.\\n\\nVW steht der Idee offen gegenüber.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n368\\n\\nInterviewprotokoll Zalando\\n\\nInterviewter:\\n\\nAlexander Borek, Director of Data Analytics, Zalando SE Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Nachteil der amerikanischen Modelle ist, dass sie nicht öffentlich zugänglich sind und individuell angepasst werden können.\\n\\nDarüber hinaus spielt der Datenschutz eine wichtige Rolle. Vor allem\\n\\nKundendaten sind sehr sensibel und für Zalando auch wettbewerbskritisch.\\n\\nWenn bestimmte Foundation-Modelle aufgrund von Datenschutzbedenken nicht\\n\\ngenutzt werden können und es keine Alternativen gibt, ist das ein großer\\n\\nWettbewerbsnachteil für Zalando.\\n\\nFür Zalando sind vor allem auch die europäischen Sprachen relevant. ● Entscheidend ist, dass die Anwendungen auf Basis der Modelle beim Kunden funktionieren.\\n\\n2. Zusammenarbeit mit LEAM ● Die geplanten LEAM Services sind interessant für Zalando. Eine Nutzung ist wahrscheinlich.\\n\\nEs ist besser, Einfluss auf die Technologie zu haben bzw. die Technologie selber betreiben zu können, als über API auf Fremd-Services aus Übersee zugreifen zu müssen.\\n\\nMögliche Anwendungsfälle sind vor allem Chatbots, Verbesserungen in der\\n\\nSuche, aber (in geringerem Maße) auch die Image-Verarbeitung\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n369\\n\\nAnhang C - Übersicht Cloud-GPU Anbieter\\n\\nDie aufgeführten Kosten haben keinen Anspruch, alle Anbieter abzudecken. Sie dienen lediglich als Orientierung für marktübliche Preise.\\n\\nTabelle 27 gibt einen groben Eindruck über die Preise anderer Cloud Computing Anbieter. Die Preise pro GPU Stunde liegen ungefähr zwischen 1,94 EUR und 3,73 EUR. Somit sind die geplanten 2,21 EUR pro GPU Stunde für das LEAM-Hochleistungsrechenzentrum ein wettbewerbsfähiger Preis. In der Tabelle wurden die Preise für eine Stunde Rechenleistung der NVIDIA A100 Tensor Core GPU 80 GB verglichen, eines der stärksten, marktrelevanten GPUs. Die Preise variieren stark, je nach Anzahl der GPUs, weitere Hardware-Kapazitäten (Anzahl CPUs, RAM etc.) und Dauer der Buchung. Zum Beispiel bieten Amazon Web Services acht A100 GPUs für 1,38EUR pro GPU Stunde an, wenn diese für drei Jahre reserviert werden. Da das Hochleistungsrechenzentrum hauptsächlich für das Training von einzelnen Modellen gebucht wird, sind Buchungszeiträume von mehreren Monaten am ehesten vergleichbar und werden somit als Referenzwert genutzt, falls vorhanden (entsprechende Anbieter sind markiert). Allerdings muss bedacht werden, dass sich die angegebenen Preise auf Kosten für einen GPU belaufen. Es ist möglich, dass manche Anbieter größeren Projekten Rabatte anbieten können. Außerdem bieten manche Anbieter Spot-Preise. Dies sind stark reduzierte Preise für Rechenleistungen, welche zu jeder Zeit abgebrochen werden können, wenn die Nachfrage zu stark steigt. Der Spot-Markt ist ungeeignet für das hochkomplexe Training von Foundation-Modellen und somit sind die Preise nicht vergleichbar.\\n\\nAnbieter\\n\\nPreis\\n\\nGoogle Cloud\\n\\n3,73 EUR\\n\\nMicrosoft Azure\\n\\n3,63 EUR\\n\\nPaperspace\\n\\n2,93 EUR\\n\\nAmazon AWS EC2\\n\\n2,90 EUR\\n\\nNorthern Data\\n\\n2,59 EUR\\n\\nVultr\\n\\n2,31 EUR*\\n\\nCoreweave\\n\\n2,10 EUR\\n\\nDatacrunch\\n\\n2,09 EUR\\n\\nRunPod\\n\\n1,98 EUR\\n\\nFluidStack\\n\\n1,94 EUR*\\n\\nTabelle 27: Kosten für eine GPU-Stunde auf einer NVIDIA A100 Tensor Core GPU 80 GB nach Anbieter. Preise in US-Dollar wurden in Euro umgerechnet zu einem Kurs von $1 = 0,948768EUR (Dollarkurs am 06.12.2022)\\n\\nMonatsraten ansonsten On-Demand-Preise\\n\\nAnhang | Große KI-Modelle für Deutschland', metadata={'source': 'data\\\\LEAM-MBS_KIBV_webversion_mitAnhang_V2_2023 (1).pdf'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./data\")\n",
    "\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Version mit vollständigem Anhang\\n\\n3\\n\\nGrußwort Jörg Bienert\\n\\nIm Jahr 2002 erschien in der New York Times ein Artikel mit der Überschrift „Google\\'s toughest search is for a Business Model“ (Hansell, 2002). Der Autor des Artikels war überzeugt, dass Google sich gegen die damaligen Konkurrenten im Online- Advertising Geschäft nicht behaupten würde und dass das damalige Kerngeschäft, die Lizensierung der Suchmaschine an andere Web-Portale, kein hinreichendes Wachstum bringen wird.\\n\\nSeitdem hat sich viel getan. Innerhalb weniger Jahre beherrschte die Google-Suchmaschine das Internet, den Online-Werbemarkt und spielte eine dominierende Rolle in vielen weiteren Bereichen. Ohne massive Investments in Google-Ads lässt sich heute kein Online-Geschäftsmodell mehr starten, in vielen Autos ist die Navigation von Google-Maps integriert, der meistgenutzte Browser Chrome sammelt vielfältige Daten über unser Surf-Verhalten und mit Online-Diensten wie Gmail, Docs oder Drive vertrauen weltweit Millionen Menschen ihre Daten dem Alphabet Konzern an. Europa hat erfolglos versucht, mit einer eigenen Suchmaschine eine Alternative aufzubauen.\\n\\nJörg Bienert, Präsident KI Bundesverband\\n\\nDas alles sehen wir nur bedingt als problematisch an, weil Google es geschafft hat, einen enormen Vertrauensvorsprung aufzubauen. Was aber, wenn sich dies ändert? Denken wir doch nur einmal an das hypothetische Szenario, Elon Musk würde Alphabet übernehmen. Wären wir in der Lage, das Internet ohne die Google Services sinnvoll zu nutzen? Oder hat Alphabet es durch die Ansammlung von Daten und darauf aufbauenden Diensten bereits geschafft, eine Lock-in-Situation zu erzeugen, aus der wir uns kaum noch befreien können?\\n\\nKünstliche Intelligenz hat das Potential, die Welt ähnlich stark zu verändern, wie es das Internet getan hat. Es wird viele neue Funktionalitäten, Produkte und Geschäftsmodelle geben. Prozessverbesserungen werden zu Effizienzsteigerungen und zu disruptiven Veränderungen führen, die wir heute noch gar nicht absehen können. Im Jahr 2000 hätten wir bei der Vorhersage, dass das Internet zur Insolvenz der größten deutschen Versand- und Warenhäuser führen würde, wahrscheinlich nur den Kopf geschüttelt.\\n\\nWir erleben gerade den Beginn der zweiten Welle der KI-Revolution, die mit der Veröffentlichung von GPT-3 durch OpenAI im Sommer 2020 begonnen hat. Auf Basis riesiger Datenmengen und mit enormem Einsatz von Entwicklerressourcen, Geld und Rechenkapazität hat OpenAI ein Sprachmodell geschaffen, das eine bis dahin unerreichte Performance aufweisen konnte. In dieser Studie beleuchten wir die Entwicklung und den Status Quo genauer.\\n\\nGroße KI-Modelle für Deutschland\\n\\n4\\n\\nWas also wird sich in den nächsten 2-3 Jahren verändern? Wir werden eine Vielzahl von neuen Modellen und Anwendungen sehen. Der Umfang und die Auswirkungen sind derzeit kaum abzuschätzen, aber es werden eine Menge Fragen auftauchen wie z.B.\\n\\nWann und wie werden große Sprachmodelle eine nahezu hundertprozentige inhaltliche Korrektheit in den generierten Texten erreichen?\\n\\nWelche Auswirkungen hat dies für Unternehmen aus allen Branchen und mögliche Anwendungen in den unterschiedlichen Bereichen der Wertschöpfungskette? • Wie gut werden neben Sprachmodellen Bild- und Video-Generatoren sowie die\\n\\nKombination aus diesen? Was bedeutet dies für die Kreativbranche, den Journalismus und die Ausbildung?\\n\\nWie wird sich das Internet mit einer Flut von automatisch generierten Inhalten verändern? Werden die großen Sprachmodelle die Google-Suche ablösen und damit den Online-Werbemarkt auch für andere Akteure öffnen?\\n\\nKönnen wir durch die Demokratisierung von KI den Arbeitsmarkt so umgestalten, dass die demographisch immer kleineren Jahrgänge die Arbeit gesund bewältigen können und gleichzeitig den Fachkräftemangel insbesondere bei “menschlichen” Arbeitsplätzen ausgleichen?\\n\\nInwiefern können die drängendsten Probleme der Menschheit, etwa in Bezug auf die Energiewende, Klimawandel, Gesundheit, mit Hilfe großer Modelle gelöst werden?\\n\\nEine übergeordnete Fragestellung ist dabei von entscheidender Bedeutung. Wer besitzt die Technologie, die Daten und die Ressourcen, um große Modelle zu erstellen und die Entwicklung und revolutionären Durchbrüche zu steuern? Bleibt dies in der Hand weniger großer Konzerne, und werden diese die Nutzung, die Regulierung und auch die Behandlung von ethischen Fragestellungen bestimmen, so wie dies bereits im Internet, bei Suchmaschinen und Sozialen Netzwerken der Fall ist?\\n\\nGenau hier liegt aktuell eine große Herausforderung für Deutschland und Europa. Nur wer die Technologie beherrscht, wird in der Lage sein, deren Nutzung nach eigenem Ermessen zu gestalten und wirtschaftliche und gesellschaftliche Entwicklungen positiv zu beeinflussen. Hier geht es bei weitem nicht nur um die Künstliche Intelligenz als Plattform- Technologie. Große KI-Modelle werden komplett neue Anwendungen ermöglichen und bestehende Geschäftsmodelle und Wertschöpfung disruptiv verändern – in allen Industrien.\\n\\nWenn wir nicht in der Lage sind, diese Basis-Technologie eigenständig zu entwickeln und bereitzustellen, wird die deutsche Industrie auf ausländische Dienste ausweichen müssen, Teile der Wertschöpfungskette verlieren und an Wettbewerbsfähigkeit einbüßen. Wir werden schwierige Diskussionen um Datenschutz, Datensicherheit und die ethische Nutzung von KI-Modellen haben. Die aktuellen Kontroversen um Google und Facebook erscheinen dagegen trivial.\\n\\nGroße KI-Modelle für Deutschland\\n\\n5\\n\\nUm dies zu verhindern und nicht auch in der KI die digitale Souveränität zu verlieren, müssen wir uns in Deutschland in die Lage versetzen, auf internationalem Niveau zu forschen, Daten zu sammeln und zu veredeln, große Modelle zu trainieren und diese offen für die Anwendung durch die Wirtschaft, Konzerne, Mittelstand und Start-ups bereitzustellen.\\n\\nDies ist das Ziel unserer Initiative Large European AI Models, kurz LEAM. Ein Team von 40 Vertretern aus Wissenschaft, Wirtschaft und Gesellschaft hat im vergangenen Jahr zunächst erste Ideen für eine Infrastruktur zur Schaffung von großen Modellen entwickelt. Im Auftrag des Bundesministeriums für Wirtschaft und Klimaschutz wurde nun diese Machbarkeitsstudie erstellt, die wir Ihnen hier vorstellen.\\n\\nKernpunkt des Konzeptes ist der Aufbau einer dedizierten KI-Supercomputing- Infrastruktur. Ein Team von Spezialist:innen betreibt eine dedizierte Hardware- Infrastruktur, die auf große KI-Modelle spezialisiert ist. Es entwickelt diese KI-Modelle weiter und stellt diese anderen zur Verfügung. Darüber hinaus sammelt und veredelt das Team die zum Betrieb und den Anwendungen notwendigen Daten und implementiert Software und Services rund um diese KI-Modelle, die das Training und Tuning von großen Modellen vereinfachen und diese für unterschiedliche Zielgruppen einfach nutzbar machen.\\n\\nMit LEAM planen wir ein zentrales KI-Leuchtturmprojekt, um das sich ein leistungsfähiges Ökosystem aus Wissenschaft, Wirtschaft und Start-ups bilden wird – in enger Zusammenarbeit auch mit bestehenden Aktivitäten wie Open GPT-X, Aleph Alpha oder Bloom und als wichtiger Player im europäischen Kontext. Ein Schwerpunkt liegt dabei auf der Berücksichtigung europäischer Werte und kommender Standards und Regulierungen.\\n\\nWir sind sehr froh über den breiten Zuspruch aus Wissenschaft, Wirtschaft und Politik. Denn nur gemeinsam mit allen Beteiligten können wir die Herausforderung meistern, die Möglichkeiten der Künstlichen Intelligenz zum Wohle der Menschen einzusetzen, durch leistungsfähige Forschung und Produkte international wettbewerbsfähig zu bleiben und den Wohlstand in Deutschland zu sichern.\\n\\nGroße KI-Modelle für Deutschland\\n\\nKernergebnisse\\n\\n6\\n\\nGroße KI-Modelle für Deutschland\\n\\n7\\n\\nGroße KI-Modelle für Deutschland\\n\\n8\\n\\nInhaltsverzeichnis\\n\\nGrußwort Jörg Bienert .............................................................................................................. 3 Kernergebnisse ......................................................................................................................... 6 Inhaltsverzeichnis ..................................................................................................................... 8 Einleitung ................................................................................................................................. 11 Ziele der Machbarkeitsstudie ................................................................................................ 13 Autor:innen der Machbarkeitsstudie .................................................................................... 13 Leseanleitung und Dokumentenstruktur ............................................................................. 16 Das Paradigma der KI-Foundation-Modelle .............................................................. 19 1. Technologische Grundlagen ....................................................................................... 23 2. Die Bedeutung der Größe von KI-Foundation-Modellen ...................................... 31 Anwendungsgebiete von KI-Foundation-Modellen............................................... 33 Vertrauenswürdige KI-Foundation-Modelle .......................................................... 45 Offene Forschungsfragen, neueste Entwicklungen und Erwartungen................ 51 KI-Foundation-Modelle im internationalen Vergleich .............................................. 55 Bedarf der Wirtschaft an KI-Foundation-Modellen .................................................. 67 Unterstützung bei der Entwicklung durch Forschung und Wissenschaft ............... 76 Chancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle .... 85 Erste europäische multilinguale Foundation-Sprachmodelle .............................. 88 Vermeidung von Falschaussagen, Bias und Toxizität ........................................... 97 Verbindung von Foundation-Modellen mit großen Wissensbeständen ............. 99 Kombination von Sprache mit anderen Modi und Medien ................................ 100 Fragestellungen und Weiterentwicklungen ......................................................... 105 Foundation-Modelle in anderen Datendomänen ............................................... 106 Zusammenfassung ................................................................................................ 107 Voraussetzungen bei Software und Personal ......................................................... 109 Applikations-Layer: Trainings- & Inference-Technologien .................................. 112 Data-Storage & -Loading-Layer ............................................................................. 118 System-Layer .......................................................................................................... 120 Framework- & Service-Layer ................................................................................. 121 LEAM als Leuchtturmprojekt für die Zukunft des KI-Ökosystems ..................... 123 Zusammenfassung ................................................................................................ 125 Aufbau eines KI-Hochleistungsrechenzentrums ..................................................... 127 Definition Rechenzentrum .................................................................................... 127 Anforderungen an ein KI-Hochleistungsrechenzentrum ................................... 130 Nachhaltigkeitsaspekte ......................................................................................... 136 Infrastrukturanforderungen im Detail ................................................................. 137 Standortauswahl .................................................................................................... 139 Betrieb eines KI-Rechenzentrums ........................................................................ 156 Zusammenfassung und Empfehlung ................................................................... 164 Die organisatorische Struktur von LEAM ................................................................. 167 Zielgruppen des LEAM KI-Servicezentrums ......................................................... 167 Organisationseinheiten des LEAM KI-Servicezentrums ...................................... 168 Das LEAM-Board .................................................................................................... 181 Zusammenfassung ................................................................................................ 182\\n\\n2.1 2.2 2.3 2.4\\n\\n3. 4. 5. 6.\\n\\n6.1 6.2 6.3 6.4 6.5 6.6 6.7\\n\\n7.\\n\\n7.1 7.2 7.3 7.4 7.5 7.6\\n\\n8.\\n\\n8.1 8.2 8.3 8.4 8.5 8.6 8.7\\n\\n9.\\n\\n9.1 9.2 9.3 9.4\\n\\nGroße KI-Modelle für Deutschland\\n\\n9\\n\\nBetriebswirtschaftliche Aspekte ............................................................................... 184 Kosten ..................................................................................................................... 184 Einnahmen ............................................................................................................. 189 Finanzierungsmodelle von LEAM ............................................................................. 193 Öffentliche Finanzierung ....................................................................................... 194 Private Finanzierung .............................................................................................. 195 Public-Private-Partnership .................................................................................... 197 Rechtliche Rahmenbedingungen.......................................................................... 199 Auswirkungen der Rechtsmaterien auf die Finanzierungsmodelle ................... 205 Abschließende Übersicht ...................................................................................... 218 Gesellschaftsstruktur von LEAM............................................................................... 220 Öffentliche Finanzierung ....................................................................................... 220 Private Finanzierung .............................................................................................. 221 Public-Private-Partnership .................................................................................... 221 Szenario für ein LEAM KI-Servicezentrum ............................................................... 223 Fazit ............................................................................................................................. 227 Beurteilung der Machbarkeit ................................................................................ 227 Ausblick ................................................................................................................... 229 Quellenverzeichnis .................................................................................................... 231 I. Abbildungsverzeichnis .............................................................................................. 240 II. Tabellenverzeichnis ................................................................................................... 242 III. Abkürzungsverzeichnis ............................................................................................. 243 IV. V. Methodik der Machbarkeitsstudie ........................................................................... 245 Anhang ………………………………………………………………………………………………………………………251\\n\\n10.\\n\\n10.1 10.2\\n\\n11.\\n\\n11.1 11.2 11.3 11.4 11.5 11.6\\n\\n12.\\n\\n12.1 12.2 12.3\\n\\n13. 14.\\n\\n14.1 14.2\\n\\nGroße KI-Modelle für Deutschland\\n\\n10\\n\\nMACHBARKEITSSTUDIE\\n\\nzum Aufbau und Betrieb eines dedizierten KI-Hochleistungsrechenzentrums für das Trainieren großer KI-Modelle in Deutschland\\n\\nGroße KI-Modelle für Deutschland\\n\\nEinleitung\\n\\n11\\n\\nGroße KI-Modelle für Deutschland\\n\\n12\\n\\nWie leistungsstark große KI-Modelle bzw. KI-Foundation-Modelle 1 bereits sind, zeigen diese einleitenden Worte. Sie wurden nicht von einem der vielen Forscher:innen und Expert:innen geschrieben, die an dieser Studie mitgewirkt haben, sondern von dem auf GPT-3 basierenden Chatbot ChatGPT.\\n\\nDie Antworten von ChatGPT zeigen eindrucksvoll, wozu KI-Foundation-Modelle bereits heute fähig sind. Dabei steht die Entwicklung noch ganz Anfang und findet bisher vor allem in den USA und China statt. Um eine mittelfristige Abhängigkeit ausländischer Technologiekonzerne zu verhindern und dem Wettbewerb standzuhalten, müssen Deutschland und Europa in die Lage versetzt werden, diese nächste Generation innovativer KI-Technologien mitzugestalten.\\n\\nZu diesem Zweck hat der KI Bundesverband 2021 die Initiative LEAM - Large European Language Models - ins Leben gerufen, die von über 40 namhaften Institutionen aus Forschung und Wirtschaft sowie weiteren europäischen KI-Verbänden unterstützt wird. Eine zentrale Forderung der Initiative ist die Förderung einer europäischen KI- Recheninfrastruktur, die von Wissenschaft, Industrie und Start-ups gleichermaßen genutzt werden soll, sowie der Aufbau eines dedizierten KI-Hochleistungsrechenzentrums in Deutschland.\\n\\nUm die Umsetzbarkeit dieser Ziele zu überprüfen, hat das Bundesministerium für (BMWK) den KI Bundesverband beauftragt, eine Wirtschaft und Klimaschutz Machbarkeitsstudie zu LEAM durchzuführen. In dieser Studie werden die Herausforderungen und Potentiale von KI-Foundation-Modellen für die deutsche KI- Entwicklung kritisch untersucht, Strategien und Instrumente für die Umsetzung der LEAM- Ziele benannt und inhaltlich konkretisiert sowie Handlungsoptionen ausgearbeitet und miteinander verglichen. Die Erkenntnisse der Machbarkeitsstudie sind wegweisend für die Entwicklung der Künstlichen Intelligenz in Deutschland und entscheidend für die Innovationskraft des europäischen KI-Ökosystems und der digitalen Souveränität von Deutschland in der Zukunft.\\n\\n1 Die Begriffe “Große KI-Modelle” und “KI-Foundation-Modelle” sind synonym. Für diese Studie wird der Begriff “KI-Foundation-Modelle” benutzt, der international für diese Art der KI anerkannt ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n13\\n\\nZiele der Machbarkeitsstudie\\n\\nIn kürzester Zeit hat Künstliche Intelligenz gigantische Entwicklungssprünge gezeigt und damit selbst Technologieexpert:innen ins Staunen versetzt. Zurückzuführen ist dies insbesondere auf die großen Fortschritte im Bereich der großen KI-Modelle. Seit OpenAI GPT-3 eingeführt hat, wurden auf Basis der großen KI-Modelle, die auch Foundation- Modelle genannt werden, viele Anwendungen entwickelt.\\n\\nNeben den enormen Chancen, die sich daraus für die Arbeit und das Leben eröffnen, ergeben sich daraus auch einige Herausforderungen für die deutsche Wirtschaft, Wissenschaft und Gesellschaft, denn Europa ist im Wettbewerb um KI-Foundation- Modelle abgeschlagen. Für Deutschland heißt das konkret, Datenschutz und Datensicherheit verfolgen geringere Standards, Verzerrungen und mangelnde Datenqualität können aufgrund identifiziert und entgegengewirkt werden, und deutsche Unternehmen werden lediglich Nutznießer und keine Gestalter von Foundation-Modelle. Technologisch rutscht Deutschland damit in die Abhängigkeit.\\n\\nfehlender Transparenz nicht\\n\\nEine grundlegende Herausforderung dabei: Für die Erstellung von KI-Foundation- Modellen sind enorme Rechenkapazitäten und Ressourcen nötig, die im Vergleich zu den USA nicht ausreichend in Deutschland bereitstehen.\\n\\nZiel dieser Studie ist es ein Konzept zu erarbeiten, wie in Deutschland eine dezidierte KI- Recheninfrastruktur aufgebaut werden kann, die es ermöglicht KI-Foundation-Modelle zu trainieren und der Wirtschaft bereitzustellen. Dabei sollen vor allem die Bedürfnisse deutscher Unternehmen berücksichtigt werden.\\n\\nIndem die Studie Empfehlungen für die Ausgestaltung eines KI-Rechenzentrums und einer entsprechenden Servicegesellschaft gibt, möchte sie der privaten und öffentlichen Hand als eine Entscheidungsgrundlage für die Umsetzung von LEAM in Deutschland dienen.\\n\\nAutor:innen der Machbarkeitsstudie\\n\\nDie LEAM Machbarkeitsstudie wurde in Zusammenarbeit mit Alexander Thamm GmbH, Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), eco – Verband der Internetwirtschaft e. V., Fieldfisher LLP, Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS), Merantix Momentum GmbH, Simmons & Simmons und Ubermetrics Technologies GmbH durchgeführt. Die Gesamtprojektleitung oblag dem KI Bundesverband e.V..\\n\\nGroße KI-Modelle für Deutschland\\n\\n14\\n\\nGroße KI-Modelle für Deutschland\\n\\n15\\n\\nUnser Dank gilt auch dem gesamten Team des KI Bundesverbandes für die Umsetzung der LEAM-Konferenz und ihr Mitwirken im Rahmen der Studie: Daniel Abbou, Stefanie Baade, Dr. Sandra Bütow, Katharina Fischer, Franziska Fink, Phillip Handy, Benjamin Rodatz, Valentin Roth, Julia Sartisson und Esther Schragmann.\\n\\nGroße KI-Modelle für Deutschland\\n\\n16\\n\\nLeseanleitung und Dokumentenstruktur\\n\\nKapitel 1 bis 5 sind der Bestandsaufnahme gewidmet. Kapitel 1 beginnt mit einem Abschnitt über die wichtigsten Eigenschaften des neuen Paradigmas, gefolgt von einem Abriss des Standes der Technologie in der internationalen Forschung und Entwicklung (Kapitel 2). Dieser Abriss schildert auch vielfältige Anwendungen, die bereits realisiert und evaluiert wurden (Kapitel 2.2). Ein spezieller Abschnitt widmet sich den Technologien, die die Vertrauenswürdigkeit der KI-Foundation-Modelle sichern und kritische Fälle inadäquater Performanz verhindern sollen (Kapitel 2.3). Dem schließt sich eine Analyse des internationalen Wettbewerbs an, die auf einer Zusammenstellung aller bisher veröffentlichten KI-Foundation-Modelle beruht (Kapitel 3). In dieser Analyse werden auch die Ursachen des festgestellten Ungleichgewichts zwischen den USA, China und Deutschland bzw. Europa diskutiert.\\n\\ninternationalen KI-Entwicklung Nach dem Überblick zum aktuellen Stand der konzentrieren wir uns in den Kapiteln 4 und 5 auf die Situation in Deutschland. Dazu beleuchten wir insbesondere die Bedarfe der deutschen Wirtschaft und legen dar, wie die Forschung die Entwicklung der benötigten europäischen KI-Foundation-Modelle unterstützen kann. Zu diesem Zweck wurden zahlreiche Expert:innen in Wissenschaft und Wirtschaft befragt.\\n\\nDas sechste Kapitel zieht die Konsequenzen aus den Ergebnissen der vorangegangenen Kapitel. Die Untersuchung hat das große Potenzial der Technologie, gleichzeitig aber auch einen immensen Bedarf an Foundation-Modellen ermittelt. Dieses Kapitel beschriebt, welche Modelle vordringlich entwickelt werden sollten und könnten, ob und wie man die Daten für diese Modelle bekommen kann und welche Optionen es für künftige Modellentwicklungen gibt.\\n\\nDie folgenden zwei Kapitel untersuchen die Anforderungen, die an High-Performance- Computing-Systeme (HPC) und speziell an ein benötigtes KI-Hochleistungsrechenzentrum gestellt werden. In Kapitel 7 wird erklärt, wie die dafür notwendige Software-Architektur aussehen kann und wie daraus Anforderungen an Hardware und Personal abgeleitet werden können. Dabei zeigen wir auf, wie sich diese Architektur mittels Open-Source- Software (OSS) aufbauen lässt, um die Wichtigkeit von OSS für ein wettbewerbsfähiges KI- in Ökosystem zu betonen. Dabei wird die technische Machbarkeit unmittelbar Zusammenhang mit den wissenschaftlichen und infrastrukturellen Kapiteln dieser Machbarkeitsstudie gestellt, um so den gesellschaftlichen Mehrwert des Betriebs eines kompetitiven KI-Hochleistungsrechenzentrums zu erläutern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n17\\n\\nKapitel 8 betrachtet insbesondere die technischen Voraussetzungen, die von KI- Foundation-Modellen an die Infrastruktur eines entsprechenden Rechenzentrums gestellt werden. Außerdem wird eine Übersicht über die HPC-Kapazitäten im Bereich der Forschung und der kommerziellen Anbieter gegeben, die bereits für KI-Anwendungen geeignete HPC-Cloudlösungen anbieten. Es werden außerdem die nötigen Schritte für die Standortermittlung und den Aufbau eines KI-Hochleistungsrechenzentrums mit passender Gebäudeinfrastruktur beleuchtet. Weitere Möglichkeiten wie Collocation- Betrieb oder der mögliche Bezug von HPC-Rechenleistungen aus der Cloud werden anhand von Beispielen untersucht. Beim Betrieb eines HPC-Rechenzentrums sind mit Blick auf die Entwicklung der Strompreise und Anforderungen des Klimaschutzes insbesondere die Energieeffizienz der Systeme sowie der Nachhaltigkeitsaspekt beim Betrieb eines KI-Rechenzentrums wichtige zu untersuchende Faktoren.\\n\\nKapitel 9 bis 13 beleuchten die organisatorische, wirtschaftliche und juristische Machbarkeit eines KI-Hochleistungsrechenzentrums. Wir schlagen die Einrichtung eines LEAM-KI-Servicezentrums vor, das seine Dienste der Wirtschaft und Wissenschaft anbietet. Dafür werden verschiedene Organisationsmodelle, Gesellschaftsstrukturen und Finanzierungsmöglichkeiten näher beleuchtet und diskutiert. Darüber hinaus findet eine rechtliche Bewertung der Optionen statt. Das Kapitel schließt mit einem Szenario zur Realisierung eines LEAM-KI-Servicezentrums.\\n\\nDie Ergebnisse werden in Kapitel 14 zusammengefasst. Das Kapitel gibt außerdem die Empfehlung ab, eine Projektentwicklungsgesellschaft zu gründen, die das Thema weiter vorantreibt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n18\\n\\nDas Paradigma der KI-Foundation-Modelle\\n\\nGroße KI-Modelle für Deutschland\\n\\n19\\n\\n1. Das Paradigma der KI-Foundation-Modelle KI-Foundation-Modelle schreiben das neueste Kapitel in der Erfolgsgeschichte der Künstlichen Intelligenz.\\n\\nKI-Foundation-Modelle sind große neuronale KI-Modelle, die auf gigantischen Mengen generischer Daten vortrainiert wurden. Das Besondere an diesen Modellen ist, dass sich das im sogenannten Vortraining (pre-training) erworbene implizite Wissen als Grundlage für viele verschiedene Anwendungen eignet. Für manche Anwendungen ist sogar kein oder nur minimales zusätzliches Training erforderlich. Andere Anwendungen erfordern zwar weiterhin ein Training auf anwendungsspezifischen Lerndaten, wobei aber oft viel weniger Lerndaten benötigt werden oder aber eine Leistungsfähigkeit erreicht wird, die ohne das generische Vortrainieren nicht möglich war.\\n\\nDer breiten Öffentlichkeit bekannt geworden ist das neue Paradigma spätestens seit dem großen Medienecho auf GPT-3, einem großen Sprachmodell des kalifornischen KI-Labors OpenAI. GPT-3 wurde 2020 veröffentlicht und gilt als spektakulärer Durchbruch auf dem Gebiet der intelligenten Sprachtechnologie. Das KI-Modell kann anspruchsvolle Texte verfassen, Fragen beantworten, Sätze ergänzen und nebenbei auch noch ganz passabel übersetzen. Derzeit gilt die mediale Aufmerksamkeit dem neuen KI-System ChatGPT, Journalist:innen und Testbenutzer:innen einer Variante des GPT-3 Modells, das gleichermaßen fasziniert aber manchmal auch verschreckt, weil es so eloquent parliert und meist informative, bedachte und ausgewogene Antworten auch auf schwierige Fragen gibt und nach Benutzervorgaben mitunter sogar druckreife Texte für die verschiedensten Zwecke formuliert. Und all das in mehreren Sprachen.\\n\\nAbb. 1: Anzahl der Parameter großer KI-Sprachmodelle seit GPT-3 (Open Source Modelle rot markiert). Quelle: state of ai Report 2022 (Benaic & Hogarth, 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n20\\n\\nMöglich wurde diese erfolgreiche Innovation durch eine neue Architektur für neuronale Netze, Transformer genannt, die 2017 in der Google-Forschung entwickelt und zuerst erfolgreich in der automatischen Textübersetzung getestet wurde. 2018 folgten zwei KI- Foundation-Modelle, die jeweils nur Teile der Transformer-Architektur realisieren: GPT, der Vorläufer von GPT-3 von OpenAI und BERT, ein einflussreiches bidirektionales Sprachmodell aus der Google-Forschung.\\n\\nDie Grundidee des Transferlernens ist die Nutzung von bereits trainierten Netzen für neue Aufgabenstellungen. Anstatt für eine Anwendung ein neues Netzwerk zu trainieren, verwendet man ein Netzwerk, das bereits für eine andere Aufgabenstellung vortrainiert wurde. So kann durch den Einsatz des vortrainierten Netzes der Bedarf an annotierten (labeled) anwendungsspezifischen Lerndaten reduziert werden. Gelingt es nun, Aufgaben für das Vortraining so zu wählen, dass entweder hinreichend große Mengen an bereits annotierten Lerndaten vorhanden sind oder aber die annotierten Daten vollautomatisch hergestellt werden können, dann verringert sich der Aufwand für die Datenannotierung immens.\\n\\nDie ersten KI-Foundation-Modelle waren große Sprachmodelle. Sie unterscheiden sich von früheren KI-Modellen durch ihre vielseitige Verwendbarkeit. Möglich geworden ist diese Vielseitigkeit durch eine zentrale inhärente Eigenschaft der neuen KI- Kerntechnologie, die als Emergenz bezeichnet wird. Damit ist eine neue Stufe in der Evolution der Künstlichen Intelligenz erreicht. Bisherige neuronale KI-Modelle waren immer auf eine bestimmte Anwendung ausgerichtet. Sie beruhten in der Regel auf Training durch überwachtes (supervised) oder semi-überwachtes (semi-supervised) Lernen. Aber im Gegensatz zur menschlichen Intelligenz konnten die erworbenen Fähigkeiten nicht für weitere Aufgabenstellungen genutzt werden.\\n\\nDeshalb wurde diese Künstliche Intelligenz auch treffend als schmale KI (Narrow AI) bezeichnet und als solche der sogenannten Künstlichen Allgemeinen Intelligenz (Artificial General Intelligence - AGI) gegenübergestellt.\\n\\nNun gibt es aber viele Anwendungen, welche die Kombination von Aufgaben erfordern. So kann ein Voice-Chatbot aus der Kombination von Spracherkennung, semantischer Analyse, Fragenbeantwortung und akustischer Sprachausgabe bestehen, die jeweils durch eigene Modelle realisiert sind. Hier kann es zu Inkonsistenzen kommen, indem zum Beispiel eins dieser Modelle Wörter oder Begriffe kennt, die ein anderes nicht gelernt hat. Die Homogenisierung der Modelle hat diese Inkonsistenzen reduziert. Die KI-Foundation- Sprachmodelle können das vortrainierte implizite Sprachwissen bereits für mehrere Anwendungen einsetzen. Es kommt nicht mehr vor, dass eine Anwendung oder Komponente, Begriffe oder Satzstrukturen nicht kennt, die von einer anderen beherrscht werden. Das Paradigma der versatilen Foundation-Modelle stellt so den nächsten Schritt in Richtung einer breiteren KI dar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n21\\n\\nDie Bezeichnung Foundation-Modelle wurde erst im August 2021 durch das neue Center for Research on Foundation Models (CRFM) an der Stanford Universität vorgeschlagen (Bommasani et al., 2021), das sich als Teil des Stanford Institute for Human-Centered Artificial Intelligence\\'s (HAI) ganz dediziert dem neuen KI-Paradigma widmet. Der Terminus ist treffender als der Begriff große Sprachmodelle, denn Foundation-Modelle müssen nicht immer Sprachmodelle sein, sondern können z.B. auch auf Bildern, Videos oder DNA-Sequenzen trainiert und dann jeweils für eine Vielzahl von Anwendungen eingesetzt werden. Zudem gibt es auch große Sprachmodelle, die mit entsprechend ausgewählten und annotierten Daten nur für eine spezielle Anwendung, zum Beispiel maschinelle Übersetzung, trainiert wurden und sich somit nicht als Grundlage (Foundation) für viele verschiedene Anwendungen eignen.\\n\\nWeil die vortrainierten, vielseitigen Foundation-Modelle gegenwärtig die Diskussion um die nächsten Durchbrüche der KI dominieren, wurde der intuitive neue Begriff sehr schnell von der internationalen Forschungsgemeinschaft aufgegriffen. In ihrem initialen Positionspapier zu dem Forschungsthema schildern die Stanford-Wissenschaftler nicht nur das Anwendungspotenzial der bereits existierenden Modelle, sondern sie argumentieren auch überzeugend, dass das Paradigma der Foundation-Modelle die nächste Entwicklungsstufe der KI bestimmen wird, in der die Modelle Fähigkeiten aufweisen werden, die bis vor Kurzem noch undenkbar schienen und die den Menschen bei vielen Aufgaben übertreffen.\\n\\nIm gleichen Artikel schildern sie aber auch die Risiken, die entstehen können, wenn diese mächtige Technologie die Konzentration von technologischer und wirtschaftlicher Macht in der Hand einiger weniger IT-Konzerne verschärft. Die bloße Verfügbarmachung von fertigen Modellen genügt nicht, um die wirtschaftlichen und sozialen Interessen der Gesellschaft zu sichern und den Missbrauch der Technologie wirksam zu verhindern.\\n\\nGroße KI-Modelle für Deutschland\\n\\nKapitel2\\n\\n22\\n\\nTechnologische Grundlagen\\n\\nGroße KI-Modelle für Deutschland\\n\\n23\\n\\n2. Technologische Grundlagen Sprachmodelle gehören zu den Grundwerkzeugen der maschinellen Sprachverarbeitung (Natural Language Processing). Das sind mathematische Modelle, die bestimmen können, ob gewisse Sätze oder Äußerungen zur Sprache gehören oder nicht, beziehungsweise mit welcher Wahrscheinlichkeit sie das tun. Bereits die ersten Versionen von Siri, Alexa oder Google Translate nutzten stochastische Sprachmodelle, die für jede Abfolge von drei, vier oder fünf Wörtern die Wahrscheinlichkeit gelernt hatten, in genau dieser Reihenfolge in Texten oder gesprochenen Äußerungen vorzukommen. Mit solchen Modellen, die damals noch nicht als neuronale Netze realisiert wurden, konnte die Korrektheit oder Natürlichkeit verbessert werden. Man hat die Wahrscheinlichkeiten aber auch verwendet, um bei der Analyse gesprochener Eingaben Unsicherheiten in der akustischen Erkennung von Wörtern aufzulösen. Diese Wahrscheinlichkeiten waren durch syntaktische und semantische Faktoren bestimmt, insbesondere durch grammatische Regularitäten und Wortbedeutungen. Schon früh wurde in der Sprachverarbeitung deshalb die Idee entwickelt, die Bedeutung von Wörtern durch die Wörter in der Nachbarschaft zu erklären. Leider gibt die Schreibweise von Wörtern nur wenig Aufschluss über ihre Bedeutung. Daher entstand schon vor längerer Zeit die Idee, die Bedeutung eines jeden Wortes durch einen langen Vektor, eine Einbettung, zu repräsentieren. Allerdings stellte sich heraus, dass viele Wörter je nach Kontext unterschiedliche Bedeutungen haben. Beispielsweise kann „Bank“ ein Sitzmöbel oder ein Finanzinstitut sein. Vor fünf Jahren wurde von Google- Wissenschaftler:innen diese Bedeutungsunterschiede mit kontextsensitiven Einbettungen erfassen kann (Vaswani et al., 2017). Transformer sind eine Variante der tiefen Neuronalen Netze, die seit 2012 weite Teile der KI revolutioniert haben. Das wirklich Neue an diesen Transformern ist, dass sie die Einbettung des Kontextes in vergleichbar effizienter Weise berechnen können. Dies war zuvor nicht möglich, die Kontexte beschränkten sich vorher nur auf wenige Worte. Ähnlich wie wir Menschen, erkennen die Transformer dabei die relevanten Worte in weiteren Kontexten, auch über Satzgrenzen hinweg, und können dadurch den semantischen Bezug herstellen.\\n\\nder\\n\\nTransformer\\n\\nentwickelt,\\n\\nwelcher\\n\\nDa die Anzahl der unterschiedlichen Wörter der Sprache durch Verbindung von einfachen Wörtern zu zusammengesetzten Wörtern nahezu unbeschränkt ist, verwendet neuere Sprachmodelle statt der Wörter ein beschränktes Vokabular von Token (Teilwörtern und häufigen Wörtern), aus denen man jedes Wort zusammensetzen kann. Die Tokenisierung wird, genauso wie das KI-Modell, auf Trainingsdaten trainiert und ist somit Bestandteil des Modells.\\n\\nHerzstück des Transformers ist der Self-Attention Block, der in sehr flexibler Weise die Bedeutungsrelationen zwischen verschiedenen Token durch Korrelationen der jeweiligen Einbettungen ermittelt und damit neue Einbettungen konstruiert. Mit ihm wurden Sprachmodelle mit Milliarden von Parametern trainiert. Paradebeispiel ist GPT-3 (Brown et al., 2020), das syntaktisch und inhaltlich stimmige Texte von bisher unerreichter Qualität produzieren kann.\\n\\nGroße KI-Modelle für Deutschland\\n\\n24\\n\\nDie Ableitung von kontextsensitiven Einbettungen lässt sich am besten mit dem BERT- Modell (Devlin et al., 2019) erläutern: Jedem Token des Eingabetextes wird ein Einbettungsvektor zugeordnet, der die semantische Bedeutung des Tokens repräsentiert und ein weiterer Vektor, der die Position des Tokens im Text markiert. Diese Einbettungsvektoren sind Parameter und werden im Laufe des Trainings angepasst.\\n\\nDas Verfahren der Self-Attention kann nun in dem Satz „Die Bank verleiht Geld“ die Einbettung von „Bank“ durch die Berücksichtigung der Einbettung von „Geld“ so abändern, dass die Bedeutung „Finanzinstitut“ betont wird. Dazu berechnet es die „Korrelation\" (Skalarprodukt) der Einbettung von Bank mit denen sämtlicher anderen Token (Abb. 2). Dies geschieht für komplementäre „Aspekte\" der Einbettungen, die durch Parameter ausgedrückt werden (Attention-Heads). Schließlich werden die mit den Korrelationen gewichteten Einbettungen aller Token addiert, um eine neue Einbettung für das Token „Bank\" zu erzeugen, die der Bedeutung von „Geld\" Rechnung trägt. Jeweils eine Self-Attention Schicht mit mehreren Attention-Heads sowie eine anschließende nichtlineare voll verbundene Schicht von Neuronen bilden einen Encoderblock, welcher grundlegender Bestandteil fast aller Foundation-Modelle ist.\\n\\nAbb. 2: Verdeutlichung der Self-Attention am Satz \"Die Bank verleiht Geld\". Das Token Bank (unten) hat eine hohe Korrelation mit dem Token Geld (oben), wobei die Korrelation zu den anderen Token geringer ausfällt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n25\\n\\nPrimäres Ziel des BERT-Modells ist es, kontextsensitive Einbettungsvektoren zu bestimmen, die möglichst viele Informationen über die jeweiligen Token eines Textes enthalten. Zunächst werden die Parameter mit Zufallszahlen initialisiert. Weiter werden im Eingabetext ein Teil der Eingabetoken durch „MASK“ ersetzt. Für jeden Eingabetoken berechnet BERT als Autoencoder mit Hilfe einer Reihe von Encoderblöcken die kontextsensitiven Einbettungen, welche die semantische Bedeutung des Tokens erfassen und mit jeder Schicht besser werden. Trainingsziel ist es, aus den besonders aussagekräftigen Einbettungen in der obersten Schicht an der Position eines maskierten Tokens eine möglichst hohe Wahrscheinlichkeit für das maskierte Token zu prognostizieren. Dies geschieht durch Optimierung der Parameter. Da Teile der beobachteten Daten prognostiziert werden müssen und keine menschlichen Annotationen benötigt werden, spricht man hier von selbstüberwachtem Lernen.\\n\\n(Devlin et al., 2019)\\n\\nDas erworbene Wissen über Sprache kann man in einem zweiten Schritt auswerten, indem man BERT für eine weitere Klassifikationsaufgabe trainiert. Im einfachsten Fall wird für einen zusätzlichen „CLS“-Token eine kontextsensitive Einbettung berechnet, aus der mit Hilfe eines logistischen Regressionsmodells die Wahrscheinlichkeit der beobachteten Klasse prognostiziert werden kann. Ein Beispiel ist die Sentimentanalyse, bei der das Modell entscheiden muss, ob der Eingabetext eine negative oder positive Bewertung enthält. Diese zweite Trainingsaufgabe, auch Finetuning genannt, benötigt einen von Menschen annotierten Trainingsdatensatz. Obwohl das Finetuning alle Parameter des Modells anpasst, benötigt es nur einen kleinen Bruchteil des Trainingsaufwandes für das Basismodell, sodass die annotierten Trainingsdaten für das Finetuning meist relativ wenige Beispiele umfassen muss. Zur Unterscheidung wird die erste Trainingsaufgabe mit einem großen allgemeinen Textkorpus ohne Annotationen auch als Vortraining bezeichnet. BERT kann für viele semantische Klassifikationsaufgaben die Genauigkeit bisherige Modell wesentlich verbessern. Die Übertragung von erworbenem Wissen von einem Lernproblem auf ein anderes, aber verwandtes Problem nennt man Transferlernen.\\n\\nSprachmodelle verwenden ebenfalls Schichten von Self-Attention Modulen. Sie werden aber nicht darauf trainiert, maskierte Token innerhalb eines Textes zu prognostizieren, sondern sollen für einen bestehenden Anfangstext das nächste Token vorhersagen. Hierbei werden mehrere Schichten von Encoderblöcken auf die bisher bekannten Worte des Textes angewendet. Die kontextsensitive Einbettung des letzten bekannten Wortes in der obersten Schicht bildet dann die Eingabe für ein logistisches Regressionsmodell, das die Wahrscheinlichkeit der unterschiedlichen Token an der nächsten Position prognostiziert. Während des Trainings wird das Modell so angepasst, dass diese letzte Einbettung möglichst viel Informationen über das nächste Token enthält und die Token der Trainingsmenge eine hohe Wahrscheinlichkeit erhalten. Sprachmodelle wie GPT-3 (Brown et al., 2020) und PaLM (Chowdhery et al., 2022) sind in der Lage, Anfangstexte syntaktisch fehlerfrei und inhaltlich überwiegend stimmig fortzusetzen, indem sie einen Token nach dem anderen generieren. Die Auswahl des nächsten Tokens findet dabei zufällig gemäß den abgeschätzten Wahrscheinlichkeiten statt, so dass bei einer Wiederholung immer ein neuer Text entsteht.\\n\\nGroße KI-Modelle für Deutschland\\n\\n26\\n\\nDarüber hinaus kann ein Sprachmodell auch Anweisungen ausführen. Beispielsweise antwortet GPT-3 auf den Starttext “Create an outline for an essay about Walt Disney: I: Introduction” mit einem detaillierten Text über Walt Disney. Man kann also ohne Zusatztraining erreichen, dass GPT-3 eine bisher unbekannte Aufgabe löst. Häufig kann man die Qualität der Antworten noch durch zusätzliche Beispiele verbessern, z.B. durch die Instruktion „English: I do not speak French. French: Je ne parle pas français. English: Where is the restroom? French:“. GPT-3 erkennt die Eingabe als Aufforderung zur Übersetzung und liefert die französische Übersetzung. Dieses „k-shot Learning” eröffnet völlig neue Möglichkeiten zur Nutzung von Sprachmodellen, ohne zusätzliches Finetuning. Allerdings ist die Genauigkeit oft höher, wenn das Sprachmodell durch Finetuning für die neue Aufgabe trainiert wird.\\n\\nSequence-to-Sequence-Modelle (seq2seq) übersetzen eine Sequenz von Token in eine andere Sequenz. Wichtigstes Anwendungsgebiet ist die Übersetzung eines Textes in eine andere Sprache. Der Prototyp dieser Architektur wurde von (Vaswani et al., 2017) vorgestellt:\\n\\nDer Encoder ist ein BERT-Modell, welches kontextsensitive Einbettungen der Eingabetoken berechnet.\\n\\nDer Decoder arbeitet wie ein Sprachmodell und wird auf die bisher erzeugten Token der Übersetzung angewendet. Jeder Decoderblock enthält mehrere Self-Attentions, die die Korrelation mit den schon generierten Token der Übersetzung auswerten. Zum anderen werden über sogenannte Cross-Attentions die Informationen in den Einbettungsvektoren der Eingabetoken berücksichtigt. Aus den Einbettungen des obersten Decoderblocks wird die Wahrscheinlichkeit der Token für die nächste Wortposition der Übersetzung berechnet.\\n\\nDie Trainingsmenge enthält Paare von Eingabetexten und deren Übersetzungen. Beim Training werden die Parameter von Encoder und Decoder gleichzeitig so angepasst, dass die Wahrscheinlichkeiten der korrekten Ausgabetoken möglichst hoch werden.\\n\\nEs ist bemerkenswert, dass der ursprüngliche Encoderblock mit Multi-Head-Self-Attention immer noch von fast allen Foundation-Modellen verwendet wird. Abbildung 3 zeigt die Gemeinsamkeiten in der Struktur von BERT-Encoder, Sprachmodell und Transformer Encoder-Decoder.\\n\\nGroße KI-Modelle für Deutschland\\n\\n27\\n\\nAbb. 3: Zentraler Bestandteil der Foundation-Modelle sind Schichten mit Self-Attention Blöcken (blau), die kontextsensitive Einbettungsvektoren (violett) von Eingabetoken (grün) berechnen. Die logistische Schicht L prognostiziert die Wahrscheinlichkeit der Ausgabetoken. Beim Training werden die Parameter so optimiert, dass die Wahrscheinlichkeiten der korrekten fehlenden bzw. nächsten Token (gelb) möglichst hoch sind.\\n\\nIn den letzten Jahren wurde eine Reihe von Verbesserungen für die ursprüngliche Architektur gesucht. Bei der Self-Attention wächst der Rechenaufwand und der Speicherbedarf quadratisch mit der Länge der Eingabesequenz. Daher wurden Varianten entwickelt, bei denen der Aufwand nur noch linear ansteigt. Dies ist eine signifikante Verbesserung, die es ermöglicht, sehr viel längere Texte als Eingabesequenz zu geben. Dadurch können Sprachmodelle auf Textstellen weiter vorne im Text Bezug nehmen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n28\\n\\nFoundation-Sprachmodelle können gleichzeitig auf Dokumenten unterschiedlicher Sprachen trainiert werden, wenn ein gemeinsames Tokenvokabular verfügbar ist. Dies ermöglicht z.B. Frage-Antwort Systeme, die Informationen aus unterschiedlichen Sprachen in eine Antwort integrieren können. Dabei stellte sich heraus, dass die Modelle selbst dann die Bedeutung der Wörter in unterschiedlichen Sprachen in Beziehung setzen können, wenn jedes Trainingsdokument nur in einer Sprache verfasst ist (Liu et al., 2020).\\n\\nAuch große Foundation-Modelle können nur begrenzt Informationen in ihren Parametern speichern. Daher wurde mit unterschiedlichen Ansätzen versucht, zusätzliche Informationen in Foundation-Modelle einzubeziehen. Dabei werden hauptsächlich folgende Ansätze verfolgt:\\n\\nDie zusätzlichen Informationen werden sprachlich formuliert. Dies funktioniert sowohl für Tabelleninhalte (Yin et al., 2020) als auch für Wissensbasen. Beispielsweise verbalisiert TekGen (Agarwal et al., 2021) die komplette Wikidata Wissensbasis mit Hilfe des T5 Seq2seq-Modells. Dieser Datenbestand kann dann zum Beispiel als zusätzliche Trainingsdaten für ein Sprachmodell verwendet werden.\\n\\nDer vielversprechendste Ansatz ist die Verwendung von Retrievaltechniken. Viele aktuelle Sprachmodelle nutzen ein Retriever-Reader-Modul, um die gefundenen Dokumente zu berücksichtigen (Izacard and Grave, 2021).\\n\\nGroße KI-Modelle für Deutschland\\n\\n29\\n\\nAbb. 4: Ein Foundation-Modell kann Informationen aus verschiedenen Datenquellen verschiedener Modalitäten berücksichtigen. Dieses eine Modell kann dann eine Vielzahl von nachgelagerten Aufgaben lösen (Bommasani et al., 2021).\\n\\nEs ist nun möglich, auch andere Medieninhalte durch Token zu repräsentieren. Ein Bild kann zum Beispiel in kleine quadratische Pixelbereiche unterteilt und so durch Bildtoken dargestellt werden. Die Sprachmodelle lassen sich in nahezu unveränderter Form auf diese alternativen Tokensequenzen anwenden und sind in der Lage, die Leistung bestehender\\n\\nGroße KI-Modelle für Deutschland\\n\\n30\\n\\nModelle zu verbessern. Besonders beeindruckend ist die Mischung verschiedener Modalitäten mit denen beispielsweise aus einem Text ein Bild erzeugt werden kann. Abbildung 4 verdeutlicht das Vortraining dieser Modelle mit gleicher Architektur auf verschiedenen Medien und die Anwendung auf unterschiedlichste Aufgabenbereiche. Diese Modelle haben für eine extrem große Anzahl von Fragestellungen der Künstlichen Intelligenz den Stand der Kunst verbessert und werden daher als Foundation-Modelle bezeichnet (Bommasani et al., 2021). Sie bilden die Grundlage für eine Vielzahl weiterer KI-Anwendungen. Einen aktuellen Überblick über Foundation-Modelle gibt die Monographie von Paaß et al. (2023).\\n\\nDie hervorstechenden Merkmale des neuen Paradigmas der Foundation-Modelle sind Emergenz und Homogenisierung:\\n\\nEmergenz bezeichnet den Umstand, dass ein Foundation-Modell Fähigkeiten aufweist, die nicht explizit konstruiert, sondern implizit gelernt werden. Ein Beispiel ist das GPT-3-Modell, welches eine neue Aufgabe lösen kann, indem man es durch eine natürlichsprachliche Beschreibung der Aufgabe, den so genannten Prompt, instruiert. Das Modell kann also eine Aufgabe lösen, obwohl es nie dafür trainiert wurde.\\n\\nHomogenisierung rührt daher, dass nahezu alle aktuellen Modelle die Architektur von wenigen Foundation-Modellen (insb. BERT, RoBERTa, T5, GPT-3) implementieren. Dies bewirkt eine Vereinheitlichung von der Modellierung für ein breites Spektrum von Anwendungen. Folglich kann ein Modell durch Anpassungen für viele verschiedene Aufgaben verwendet werden. Ähnliche Ansätze zur Sequenzmodellierung können für Texte, Bilder, gesprochene Sprache, Video, DNA- Sequenzen u.ä. verwendet werden. Dies hat aber auch den möglichen Nachteil, dass diese KI-Systeme die gleichen problematischen Vorurteile oder Fehler einiger weniger Grundmodelle erben können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n31\\n\\n2.1 Die Bedeutung der Größe von KI-Foundation-Modellen\\n\\nDie Größe eines Modells, insbesondere die Anzahl der Parameter, hat einen entscheidenden Einfluss auf die Leistung des Modells, seinen Speicherbedarf und die Rechenressourcen für das Training. Kaplan et al. (2020) untersuchten empirisch die Abhängigkeit zwischen der Anzahl der Modellparameter, dem Umfang der Trainingsdaten und dem Rechenaufwand für das Training. Sie bewerteten eine große Anzahl von Modellen und zogen die folgenden Schlussfolgerungen:\\n\\nDie Leistung der Modelle hängt weitgehend von diesen drei Größen ab. Andere Architekturmerkmale wie Breite oder Tiefe haben nur einen schwachen Einfluss.\\n\\nWerden Modellgröße und Trainingsdaten in gleichem Maße erhöht, wächst die Modellgenauigkeit zuverlässig über einen großen Bereich des Rechenaufwandes. Wenn einer dieser Faktoren konstant gehalten wird, fällt die Verbesserung geringer aus und nähert sich einer Schranke.\\n\\nDies erklärt auch den Erfolg von großen Foundation-Modellen wie T5, GPT-3 oder PaLM. Allerdings erfordert das Training großer Modelle eine extrem leistungsfähige Infrastruktur.\\n\\nAbb. 5: Die Genauigkeit des „few-shot\"-Lernens von GPT-3 wird durch die Erweiterung der Modellgröße und der Anzahl der präsentierten Beispiele erhöht (Brown et al., 2020).\\n\\nAbbildung 5 zeigt, dass wichtige Eigenschaften von Foundation-Modellen erst für große Modelle beobachtet werden können (Emergenz). Während das GPT-3-Modell mit 13 Mrd. Parametern natürlichsprachige Anweisungen mit einer Genauigkeit von etwa 20 % beantworten kann, steigt dieser Anteil bei 175 Mrd. Parametern auf über 60 % (Brown et al., 2020). Offenbar benötigen die Modelle ein großes Geflecht von Beziehungen zwischen Begriffen, um korrekt auf natürlichsprachige Prompts zu reagieren. Eine mögliche Folge der Emergenz ist, dass es eine Reihe von Aufgaben gibt, die für die derzeitigen Foundation-Modelle unerreichbar sind, die aber bald erfolgreich bewältigt werden könnten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n32\\n\\nEine einfache Möglichkeit, die Anzahl der Parameter ohne höheren Trainingsaufwand zu erhöhen, ist eine Mixture-of-Experts-Architektur. Sie besteht aus einem einzigen Gating- Modul und einer Reihe von Expertenmodulen mit identischer Architektur, aber unterschiedlichen Parametern. Jedes Expertenmodul ist nur auf eine Teilmenge der Daten jede Eingabe den wenigen (z.B. 2) spezialisiert, und das Gating-Modul ordnet Expertenmodulen zu. Diese Zuordnung wird automatisch optimiert, so dass das gesamte Modell die optimale Leistung erbringt. Eine Verringerung des Rechenaufwands kann erreicht werden, da nur wenige Expertenmodule für eine Eingabe tatsächlich verwendet werden. Die Architektur ermöglicht massive Modelle und ist besonders effizient für verteilte Systeme, bei denen die Experten auf verschiedene Recheneinheiten verteilt sind. Beispiele sind Switch, GLaM und WuDao-2.0.\\n\\nDa die ersten Versionen erfolgreicher Modelle oft extrem groß sind, wurden verschiedene Techniken zur Komprimierung und Beschleunigung der Modelle entwickelt. Wissensdestillation (Hinton et al., 2015) überträgt das Wissen von einem großen Lehrermodell auf ein kleineres Schülermodell. Der Vorteil dieses Ansatzes ist, dass das Schülermodell beim Training die internen Aktivierungen des Lehrermodells nutzen kann. Für eine Reihe von Modellen ergab sich eine deutliche Reduzierung des Speicher- und Rechenaufwands bei nahezu identischer Leistung.\\n\\nFoundation-Modelle benötigen enorm viele Daten für das Training. Beispielsweise verwendet das PaLM-Modell 780 Milliarden Token (25-mal mehr als alle Texte in Wikipedia), die einen großen Bereich natürlicher Sprache abdecken (Chowdhery et al., 2022). Es ist wichtig, dass die Texte eine hohe syntaktische und inhaltliche Qualität besitzen, da sonst grammatikalische und fachliche Fehler sowie Vorurteile von dem Modell reproduziert werden. Texte mit niedriger Qualität findet man oft in sozialen Medien oder Nutzer:innenkommentaren im Internet. Sie sind nicht zum Training geeignet. Die Kuratierung der Datenqualität ist daher einer der aufwändigsten und kostspieligsten Aspekte des Trainings von Foundation-Modellen. Diese Aspekte werden im Folgenden noch ausführlich diskutiert.\\n\\nDie Leistung der ersten Foundation-Modelle wurde mit wenigen Benchmarks (z.B. GLUE) überprüft. Mittlerweile sind KI-Foundation-Modelle so leistungsfähig, dass sie Weltwissen aus vielen Bereichen abdecken und sogar Schlussfolgerungen durch die Kombination von Fakten ziehen können (Zhang et al., 2022). Es hat sich herausgestellt, dass viele Benchmarks nach einer gewissen Zeit saturiert sind und die Fortschritte aktueller Modelle nicht mehr erfassen können. Daher werden nun Batterien von mehreren hundert Benchmarks verwendet (z.B. BIG-Bench), die eine Vielzahl von Aufgaben abdecken, darunter logisches Denken, Übersetzung, Beantwortung von Fragen, Mathematik und andere. Mittlerweile übertrifft PaLM mit 5-shot Prompts die Leistung durchschnittlicher menschlicher Bearbeiter (Chowdhery, et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n33\\n\\n2.2 Anwendungsgebiete von KI-Foundation-Modellen\\n\\nIn diesem Abschnitt stellen wir die vielfältigen Anwendungsmöglichkeiten der Foundation-Modelle vor.\\n\\nDabei gehen wir erst auf Anwendungen im Sprachbereich ein, und beschreiben dann Anwendungen im Bereich Multimedia, Biowissenschaften und Robotersteuerung. Die hier aufgeführten Anwendungsgebiete stellen lediglich eine Übersicht dar und sind nicht vollständig. In vielen Bereichen lassen sie die möglichen Anwendungen aktuell noch gar nicht abschätzen.\\n\\nDie folgenden Abschnitte diskutieren exemplarisch relevante Anwendungsmöglichkeiten aus ganz unterschiedlichen Bereichen.\\n\\n2.2.1 Anwendungen im Sprachbereich\\n\\nAnwendung\\n\\nBeschreibung\\n\\nInformationsextraktion\\n\\nDokumentensuche\\n\\nBeantwortung von Fragen\\n\\nMaschinelle Übersetzung\\n\\nErzeugung von Computercode\\n\\nextrahiert Konzepte und Namen aus dem Text und gegebenenfalls deren Relationen findet passende Texte zu einer Anfrage. Dabei werden auch sinnverwandte Formulierungen berücksichtigt. erzeugt eine Antwort zu einer Frage. Grundlage sind die Informationen aus dem Vortraining und die Ergebnisse einer Dokumentensuche. Die Antwort kann ggf. erklärt werden. übersetzt einen Text aus einer Sprache in eine andere. Dies ist mit dem gleichen Modell für mehr als 100 Sprachen möglich. erzeugt aus einer natürlichsprachlichen Beschreibung lauffähigen Source-Code in ganz unterschiedlichen Programmiersprachen\\n\\nZusammenfassung und Vereinfachung von Dokumenten\\n\\nerfasst die wichtigsten Aussagen eines oder mehrerer Dokumente und verfasst eine vereinfachte Version\\n\\nGenerierung neuer Texte\\n\\nTextkorrektur\\n\\nDialogsysteme\\n\\nMeinungs- und Sentimentanalyse\\n\\nerzeugt eine inhaltliche kohärente Fortsetzung eines Textes. Dabei können inhaltliche Vorgaben berücksichtigt werden. verbessert und kontrolliert Texte in Bezug auf Rechtschreibung, Grammatik, Stil, Formatierung, Wirksamkeit oder Terminologie führen ein längeres Gespräch mit einem menschlichen Dialogpartner. Dabei werden Informationen über den Dialogverlauf gespeichert und wiederverwendet. erkennt und klassifiziert Meinungen und emotionale Einstellungen zu Produkten, Personen, Organisationen, Ereignissen usw.\\n\\nEntdeckung von Fake News und Bot Texten\\n\\nerkennt Falschaussagen und automatische hergestellte Nachrichten\\n\\nTabelle 1: Eine Auswahl möglicher Anwendungen auf Basis von Sprachmodellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n34\\n\\n[Sprachmodell GPT-3]\\n\\nDem privatwirtschaftlichen Forschungsunternehmen OpenAI gelang mit dem Sprachmodell GPT-3 ein wissenschaftlicher Durchbruch: Das Modell ist in der Lage, das nächste Wort in einem Satz vorherzusagen. Schnell bildete sich eine Community aus Entwickler:innen und Nutzer:innen um das Modell und Applikationen wurden entwickelt, die zeigen, dass GPT-3 überzeugende Aufsätze schreiben, Diagramme und Websites aus Textbeschreibungen erstellen, Computercode generieren und vieles mehr kann (Tamkin & Ganguli, 2021).\\n\\nSeit der Veröffentlichung von GPT-3 im Juni 2020 werden immer mehr konkurrierende, teilweise sogar als Open Source Lösung angebotene Modelle publiziert. Dabei bietet OpenAI GPT-3 als API an und ermöglicht Nutzer:innen so, das Modell über die OpenAI-Plattform zu nutzen oder GPT-3 in eigenen Anwendungen einzubauen. Daraus hat sich ein wachsender Markt an Tools und Anwendungen entwickelt, der heute viele Industrien und Geschäftsbereiche als auch Kreative beeinflusst.\\n\\nDas KI-Foundation-Modell macht es möglich, Text in Sekunden zusammenzufassen oder zu ergänzen. Mittels Chatbots können Kundeninteraktionen einfacher gesteuert werden. Weiterführend ermöglicht GPT-3 die Generierung von Programmcode durch die Eingabe von Sprachbefehlen. Realisiert werden Anwendungen dieser Art durch Start-ups, welche die API von OpenAI gegen ein Entgelt nutzen. Gerade im Bereich Gesundheitswesen, eCommerce sowie im Medien- und Communications-Bereich profitieren Unternehmen, die Chatbots sowie Anwendungen für Natural Language Understanding (NLU) entwickeln, massiv von der Weiterentwicklung großer Sprachmodelle.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n35\\n\\nInformationsextraktion Informationsextraktion ist die Aufgabe, automatisch für die Nutzer:innen relevante strukturierte Informationen aus unstrukturierten und/oder halbstrukturierten maschinenlesbaren Dokumenten zu extrahieren. Diese Funktionen sind von großer und Bedeutung Verwaltungsvorgängen. Hierbei sind insbesondere die Klassifikation von Dokumenten nach inhaltlichen Kriterien, die Eigennamenerkennung und die Relationsextraktion zu nennen, wobei letztere Relationen zwischen Eigennamen und Begriffen aus einem Text extrahiert und in einer Datenbank speichert. Foundation-Modelle haben bei diesen Aufgaben die Genauigkeit stark verbessert und übertreffen oft die Performanz von Menschen.\\n\\nfür\\n\\ndie\\n\\nautomatische\\n\\nErfassung\\n\\nvon\\n\\nTextdokumenten\\n\\nDokumentensuche Eine extrem wichtige Anwendung ist die Dokumentensuche. Dabei werden sowohl die Dokumente einer Textsammlung als auch die Anfrage durch eine Einbettung kodiert und nach Berechnung die zur Anfrage ähnlichsten Dokumente zurückgegeben. Vorteil ist, dass auch Synonyme und alternative Formulierungen des gleichen Sachverhaltes berücksichtigt werden. Diese einbettungsbasierten Retrievalverfahren übertreffen die klassische Stichwortsuche und werden mittlerweile bei allen Internetsuchmaschinen genutzt.\\n\\nAbb. 6: Von einem Foundation-Modell mit Hilfe von Retrieval gefundene Antwort auf eine Frage im Natural Question Benchmark. Aktuelle Modelle erreichen eine Genauigkeit (F1) von 80% (Zhanag et al., 2021).\\n\\nGroße KI-Modelle für Deutschland\\n\\n36\\n\\nBeantwortung von Fragen Bei der Beantwortung von Fragen (Question Answering, QA) erhält ein System eine natürlichsprachliche Anfrage und generiert automatisch eine Antwort in natürlicher Sprache. Fortschrittliche Systeme arbeiten in der Regel in zwei Stufen (Abb. 6): Für eine Frage findet ein einbettungsbasiertes Retriever-Modul eine Reihe von passenden Dokumenten aus einer Textsammlung. Hierbei werden auch Dokumente mit ähnlichen Inhalten gefunden, die unterschiedlich ausgedrückt wurden. Anschließend prozessiert ein Reader die Frage und die gefundenen Dokumente und generiert eine natürlichsprachliche Antwort. Retriever-Reader Module werden von vielen fortgeschrittenen Sprach- und Dialogmodellen eingesetzt und produzieren wesentlich bessere Antworten als sehr große Sprachmodelle ohne diese Erweiterung. Die Antwort kann durch die gefundenen Dokumente erklärt und begründet werden.\\n\\nMaschinelle Übersetzung Zur maschinellen Übersetzung gibt es mittlerweile Modelle, z.B. M2M von Facebook AI (Fan et al., 2022), die die Übersetzung zwischen beliebigen Paaren von über 100 Sprachen gestatten. Durch das gleichzeitige Training des Encoder-Decoders mit vielen Sprachen wird die Übersetzungsqualität für fast alle Sprachpaare wesentlich verbessert, insbesondere für regionale Sprachen mit wenigen Trainingsdaten.\\n\\nErzeugung von Computercode Die Erzeugung von Computercode aus einer textuellen Beschreibung ist eine spezielle Übersetzungsaufgabe (M. Chen et al., 2021), die mittlerweile relativ zuverlässig laufenden Code erzeugen kann und ein großes Anwendungspotential besitzt.\\n\\nDokumentenzusammenfassung Die automatische Zusammenfassung von Dokumenten kann helfen, die wichtigsten Informationen in Dokumenten zu erfassen. Modelle zur Zusammenfassung nutzen meist ein Seq2seq-Modell, welche als Eingabe ein Dokument erhalten und die Zusammenfassung ausgeben. Dabei werden insbesondere Modelle mit einer langen Eingabesequenz benötigt. Die Qualität der Zusammenfassung ließ sich mit Foundation- Sprachmodellen sehr stark erhöhen.\\n\\nAbb. 7: Zusammenfassung eines Textes von 800 Wörtern durch das Modell BRIO im Vergleich zu der von Experten erstellten Zusammenfassung (Liu et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n37\\n\\nTextgenerierung Die Generierung neuer Texte ist die zentrale Aufgabe von Sprachmodellen. Hierbei wird ein vorgegebener Starttext syntaktisch fehlerfrei und inhaltlich stimmig fortgesetzt. GPT- 3 ist hier das bekannteste Modell. Durch einen Starttext mit Anweisungen (Prompt) kann man GPT-3 beauftragen, eine Reihe von Punkten in den erzeugten Text aufzunehmen. Diese Anweisungen können auch Lösungsbeispiele enthalten, wodurch das Modell bei der Konstruktion der Ausgabe unterstützt wird (k-shot prompts). Da die Worte des neuen Textes entsprechend ihrer Wahrscheinlichkeit generiert werden, entsteht bei einer Wiederholung immer ein anderer Text.\\n\\nMittlerweile gibt es auch interaktive Verfahren zur Generierung von Texten (A. Yuan et al., 2022), in denen die Nutzer:innen die Ausgestaltung des Textes steuern können. PaLM ist ein mehrsprachiges, fortgeschrittenes Sprachmodell mit 540 Milliarden Parametern, welches auf mehr als 150 Benchmarks die Leistung durchschnittlicher menschlicher Bearbeiter:innen übertraf. Zudem konnte PaLM nach entsprechender Anleitung komplexe Aufgaben für seine Schlußfolgerungen liefern (Chowdhery, et al., 2022). Starttexte können Sprachmodelle im Prinzip dazu bringen, beleidigende Äußerungen und Fake News zu produzieren. Durch nachträgliche Filtertechniken, Finetuning und Retrieval können allerdings verletzende Äußerungen und Falschinformationen weitgehend vermieden werden.\\n\\nin einfache Schritte zerlegen und Erklärungen\\n\\nDialogsysteme Dialogsysteme (Chatbots) generieren automatisch adäquate Antworten auf die Äußerungen menschlicher Gesprächspartner:innen im Laufe eines längeren Gesprächs. Sie kombinieren Techniken zur Beantwortung von Fragen mit der Generierung von Geschichten und dem Retrieval zusätzlicher Informationen. Dabei wird auch der Stand der Diskussion und Informationen über die „Persönlichkeit“ des Chatbots in einer Datenbank gespeichert und durch Retrieval weiterverwendet. Eine Bewertung durch menschliche Prüfer:innen zeigt, dass z.B. das LaMDA System (Thoppilan et al., 2022) in Bezug auf Sensibilität, Sicherheit und Faktentreue der menschlichen Leistung nahekommt. Im Bereich des Kundenkontakts gibt es sehr viele Anwendungsfälle für Chatbots. Das Modell ChatGPT (OpenAI, 2022b) hat eine ähnliche Architektur, ist aber frei im Internet nutzbar. Es kann Fragen beantworten, Code generieren, Texte zusammenfassen, aber auch einen kohärenten Dialog führen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n38\\n\\n[ChatGPT]\\n\\nChatGPT ist ein am 30. November 2022 innerhalb einer Open-Beta veröffentlichtes Sprachmodell, welches ist, sinnvolle Konversationen mittels Text zu führen. Trainiert mit großen Datensätzen von Konversationen, ist es in der Lage, realistische Gespräche zu führen, Sachverhalte zu erklären oder Programmiercode zu erstellen. Die wichtigste Eigenschaft von ChatGPT ist dabei, dass es in der Lage ist, kontextbezogen zu antworten. Dabei können weiterführende Fragen innerhalb einer Konversation gedeutet und interpretiert werden.\\n\\nin der Lage\\n\\nChatGPT hat eine Vielzahl potenzieller Anwendungen, da es in der Lage ist, angemessene Antworten auf eine breite Palette von Aufforderungen zu generieren und Gespräche in verschiedenen Kontexten zu führen. Einige Beispiele für diese Anwendungen sind die Entwicklung von Chatbots für den Kundendienst oder die Bereitstellung von Informationen, die Entwicklung virtueller Assistenten, die den Nutzern bei Aufgaben und der interaktiver Beantwortung von Fragen helfen, und die Entwicklung Tutorensysteme. Dies sind nur einige der möglichen Einsatzgebiete von ChatGPT zur Entwicklung intelligenter und interaktiver Systeme, die mit Menschen auf natürliche und intuitive Weise kommunizieren können.\\n\\nDa mittels Fragestellungen direkt beantwortet werden können, stellt ein Modell wie ChatGPT sowie eventuelle zukünftige Entwicklungen dieser Art eine ernstzunehmende Herausforderung für konventionelle Suchanbieter wie Google dar. Mittels konversationeller KI-Modelle lassen sich Fragestellungen von Nutzern direkt beantworten sowie weiter spezifizieren, statt nur auf weiterführende Webseiten zu verweisen.\\n\\nkonversationeller KI-Modelle\\n\\nObwohl sich ChatGPT noch in der Beta-Version befindet, existieren bereits Suchmaschinen auf Basis des KI-Modells. Damit wird dem Nutzer ein viel spezifischeres Suchergebnis geboten. Gerade miteinander verknüpfte Inhalte lassen sich deutlich leichter finden und in Zusammenhang bringen. Auch als unternehmensinternes Tool, im Sinne einer “Single Source of Truth” sind KI-Modelle wie ChatGPT denkbar, da sie dem/der Nutzer:in ein sehr authentisches und simples Suchergebnis bieten - ähnlich einem Gespräch mit einem/einer Mitarbeitenden.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n39\\n\\n2.2.2 Anwendungen von multimodalen Foundation-Modellen\\n\\nKI-Foundation-Modelle sind in der Lage, nicht nur Token der natürlichen Sprache zu modellieren, sondern auch Token-Elemente beliebiger anderer Sequenzen. Im Folgenden werden Anwendungen des Paradigmas auf unterschiedlichen Datendomänen wie gesprochene Sprache, Bilder, Videos, DNA und Proteine vorgestellt, die teilweise völlig neue Problemlösungen ermöglichen.\\n\\nDabei erschließt insbesondere die Verknüpfung von Daten aus zwei oder mehr Domänen zusätzliche anspruchsvolle Anwendungsfelder. Hierbei gibt es Modelle, die nur auf eine Anwendung spezialisiert sind, aber auch Foundation-Modelle wie NÜWA, OFA und GATO, die viele Aufgaben gleichzeitig lösen können. Da bei dieser Art der Modelle verschiedene Datendomänen kombiniert werden, bezeichnet man sie als multimodale Modelle.\\n\\nAnwendung\\n\\nBeschreibung\\n\\nSprache zu Text\\n\\nübersetzt gesprochene Sprache in Text für viele unterschiedliche Sprecher:innen\\n\\nText zu Sprache\\n\\ntransformiert Text in gesprochene Sprache unter Berücksichtigung von Sprachmelodie und Sprecher:innenstimme\\n\\nErkennung von Objekten\\n\\nidentifiziert die wichtigsten Objekte eines Bildes und deren Positionen\\n\\nErkennung von Aktionen\\n\\nidentifiziert Aktionen in Videos und deren Positionen\\n\\nBild nach Text\\n\\ngeneriert zu einem Bild eine Bildbeschreibung\\n\\nText in Bild\\n\\nerzeugt zu einer Bildbeschreibung ein passendes Bild\\n\\nVideo nach Text\\n\\nbeschreibt die Objekte und Abläufe in einem Video durch Text\\n\\nText nach Video\\n\\nerzeugt zu einer Bildbeschreibung ein kompatibles Video\\n\\nGenomik\\n\\nDNA-Sequenzen werden analysiert und die daraus erzeugten Proteine prognostiziert\\n\\nProteomik\\n\\nDie 2D- und 3D-Struktur der Proteine wird vorhergesagt\\n\\nVerstärkungslern en\\n\\nSequenzen von Aktionen und Zuständen für Steuerungsprobleme werden prognostiziert\\n\\nLivesynchronisat ione\\n\\nübersetzen der Sprache eines Videos\\n\\nTabelle 2: Eine Auswahl möglicher Anwendungen auf Basis von multimodalen Modellen\\n\\nDie Anwendungsbereiche multimedialer Foundation-Modelle stellen wir in diesem Kapitel genauer vor.\\n\\nGroße KI-Modelle für Deutschland\\n\\n40\\n\\nGesprochene Sprache Ein Anwendungsbereich ist gesprochene Sprache. Das Audiosignal wird dabei oft durch sein Frequenzspektrum (MFCC) für jedes 10 msec Zeitintervall repräsentiert. Wav2vec 2.0 (Baevski et al., 2020) führt unüberwachtes Lernen auf Sprachdaten ohne Transkription mit Convolution- und Self-Attention-Schichten durch. Derartige Convolution-Schichten wurden ursprünglich für die Bilderkennung mit Convolutional-Neural-Networks (CNNs) entwickelt. Ähnlich wie das BERT-Modell für Text lernt es, maskierte „Sound-Tokens“ vorherzusagen. Die Transkription von Sprache in Text kann dann durch eine Kombination von Convolution- und Self-Attention-Schichten erfolgen (Zhang et al., 2020). Zur Transkription von Text in gesprochene Sprache erzeugt beispielsweise FastSpeech 2 (Ren et al., 2022) aus den eingegebenen Phonemen mit einem Seq2seq-Modell das Frequenzspektrum für kleine Zeitintervalle, aus dem direkt die Sprachausgabe produziert werden kann. Dabei werden verschiedene Informationen wie Dauer, Tonhöhe und Energie berücksichtigt. FastSpeech 2 wird von menschlichen Juroren besser beurteilt als konkurrierende Systeme.\\n\\nBilder Bilder können in eine Sequenz von Pixelbereichen zerlegt werden, die als Bild-Token genutzt werden können. Der Vision-Transformer (Dosovitskiy et al., 2020) verwendet Pixelbereiche der Größe 14x14 als Token und führt ein Vortraining mit einem sehr großen Datenbestand von 300 Mio. nicht-annotierten Bildern durch. Dabei sind maskierte Bild- Token zu prognostizieren. Anschließend erfolgt ein Finetuning des Modells auf den ImageNet-Daten zur Klassifikation von Bildern in 1000 Klassen. Der Vision-Transformer erzielte eine höhere Genauigkeit als alle CNNs bei gleichzeitig wesentlich geringerem Trainingsaufwand. Eines der ersten Modelle zur Kombination von Bildern und deren textueller Beschreibung ist das CLIP-Modell (Radford et al., 2021). Mit separaten Encodern erzeugt es aus einem Bild und dem zugehörigen Text je eine Einbettung. Die Differenz zwischen den beiden Einbettungen wird dann durch Training minimiert. Damit lässt sich zu einer Bildunterschrift das am besten passende Bild finden und umgekehrt.\\n\\nAbb. 8: Zu unterschiedlichen Texten von DALL-E 2 erzeugte Bilder (Ramesh et al., 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n41\\n\\nDie Erzeugung von Bildern aus Text verwendet oft CLIP, um zu einem Text passende Bild- Einbettung zu finden. Ein Diffusionsmodell kann den Prozess der Degradierung eines Bildes durch Zufallsänderungen modellieren. Hierbei wird sukzessive die Farbe einzelner Pixel zufällig geändert, so dass sich mit der Zeit eine graue Fläche ergibt. Dieser Prozess kann umgekehrt und zur Rekonstruktion von Bildern in hoher Auflösung aus den Einbettungen genutzt werden. Bekanntestes Modell ist DALL-E 2 (Ramesh et al., 2022), welches zusätzlich noch Bereiche eines Bildes modifizieren kann. Das OFA-Modell (Wang et al., 2022) kann gleichzeitig viele Aufgabe erledigen: die Position von Objekten in Bildern bestimmen, Bildunterschriften erzeugen, Fragen zu einem Bild beantworten, Objekte in einem Bild erkennen, fehlende Bereiche in einem Bild ausfüllen und Bilder zu einem Text erzeugen. Darunter lassen sich viele Anwendungen heute schon produktiv einsetzen, beispielsweise in der Bearbeitung von Fotos. Mittels des Modells Stable Diffusion (Rombach et al., 2022) ist es etwa möglich, unerwünschte Objekte aus Bildern zu entfernen, mehrere Bilder zu verbinden oder auch ausgehend von einer textuellen Bildbeschreibung ganz neue Bilder zu Werbezwecken bzw. als Ersatz für sogenannte „Stock Photos” zu generieren. Bei den so erzeugten Bildern handelt es sich um Unikate. Zwar spiegeln sie den Datensatz wider, mit dem das Modell trainiert wurde, sind aber keine Kopien. Dadurch, dass es sich bei den generierten Bildern nicht um Kopien handelt, können diese potentiell kommerziell genutzt werden. Allerdings sind die Copyright- Fragen noch nicht abschließend geklärt.\\n\\nVideo Um Videos mit Foundation-Modellen verarbeiten zu können, verwendet man meist Video- Token, die einen Pixelbereich in mehreren hintereinander folgenden Videobildern beschreiben. Flamingo (Alayrac et al., 2022) ist ein visuelles Sprachmodell, das Sequenzen von beliebig hintereinander folgenden Bildern, Videos und Texten verarbeiten kann. Es nutzt im Hintergrund ein großes Sprachmodell für Text. Das Modell kann einerseits Fragen zu Bildern beantworten oder Bilder beschreiben. Zum anderen kann es Aktionen in Videos beschreiben oder klassifizieren. Schließlich kann es durch gemischte Few-shot- Prompts eine neue Beschreibungsaufgabe zu erledigen.\\n\\naus\\n\\nTexten und Bildern/Videos\\n\\ninstruiert werden,\\n\\nAbb. 9: Zu unterschiedlichen Texten von CogVideo erzeugte Videos (Hong et al., 2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\n42\\n\\nNÜWA (Wu et al., 2021) ist ein Encoder-Decoder-Modell, das ein Video zu einem Text erzeugen kann. Es verwendet einen speziellen Attention-Mechanismus, um die Relation der Token sowohl für räumliche als auch für zeitliche Achsen zu erfassen. Das Modell kann einerseits eine Reihe von Bild-Aufgaben lösen, etwa Erzeugung eines Bildes zu einem gegebenen Text. Weiter kann es ein Video zu einem Text generieren, die Fortsetzung eines Videos prognostizieren oder Videos manipulieren.\\n\\nImagen-Video (Ho et al., 2022) wurde mit 60 Millionen Bild-Video-Paaren und 14 Millionen Text-Video-Paaren jeweils aus dem öffentlich zugänglichen LAION 400M-Datensatz (Schuhmann, 2021) trainiert. Mit Hilfe der Bilder können auch bestimmte Kunststile imitiert werden und z.B. ein Video im Stil von Monet erzeugt werden. Das Modell Make-a- Video (Singer et al., 2021) bietet die zusätzliche Möglichkeit zum unüberwachten Lernen auf Videodaten ohne textuelle Beschreibung, um realistische Bewegungen von Objekten und Szenen zu lernen. Zudem kann es zwischen einem Paar von Bildern einen dynamischen Übergang in Form eines Videos erzeugen. Beide Modelle sind nicht nur in der Lage, Videos mit hoher Wiedergabetreue zu generieren, sondern die Modelle umfassen ein hohes Maß an Weltwissen und sind im Detail kontrollierbar mit der Fähigkeit, verschiedene Videos und Textanimationen in verschiedenen künstlerischen Stilen und mit 3D-Objekt- verständnis zu erzeugen. Leider sind Make-a-Video und Imagen Video proprietär und der Modell-Code ist nicht frei verfügbar. Die beschränkte Länge von hochauflösenden Videos von derzeit fünf Sekunden ist offenbar der limitierende Faktor. Insgesamt sind die erzeugten Videos noch nicht perfekt.\\n\\nGenomik und Proteomik Die Entschlüsselung der Sprache der DNA ist eines der wichtigsten Ziele der biologischen Forschung. Der genetische Code ist universell und erklärt, wie die DNA in Proteine übersetzt wird. Im Gegensatz dazu variiert der regulatorische Code, der bestimmt, wann und wie die Gene exprimiert werden, zwischen verschiedenen Zelltypen und Organismen. ist ähnlich zur Polysemie und entfernten semantischen Beziehungen bei Dies natürlichsprachigen Texten. DNABERT wurde auf einer großen Menge von DNA- Sequenzen vortrainiert und kann durch Finetuning den Stand der Technik für viele von spezifische Prognoseaufgaben Sequenzmotiven (DNA-Abschnitten mit biologischer Relevanz) und die Prognose der Promotor-Regionen (Nukleotid-Sequenz, die die regulierte Expression eines Gens ermöglicht). MoDNA (An et al., 2022) und GeneBERT (Mo et al., 2021) haben eine ähnliche Funktionalität.\\n\\nverbessern. Darunter\\n\\nsind die Analyse\\n\\nProteine sind lineare Ketten von Aminosäuren, die durch kovalente Bindungen verbunden sind. Aminosäuren lassen sich durch ein Alphabet mit 25 Zeichen repräsentieren. Die Zeichenketten eignen sich hervorragend für viele Natural-Language-Processing (NLP) Methoden (Ofer et al., 2021). AminoBERT ist ein Sprachmodell (Chowdhury et al., 2022), welches aus einer Proteinsequenz als Eingabe die 3D-Proteinstruktur prognostiziert. Dabei wird auch eine natürliche Methode zur Beschreibung der Polypeptidgeometrie verwendet, die auf der Ebene des Polypeptids als Ganzes rotations- und translationsinvariant ist. Im Durchschnitt übertrifft das Modell AlphaFold2 (Jumper et al., 2021) und RoseTTAFold (Baek et al., 2021) bei verwaisten Proteinen und Klassen von konstruierten Proteinen und erreicht dabei eine bis zu 106-fache Reduzierung der Rechenzeit. Es gibt eine Reihe weiterer Modelle mit ähnlichen Ergebnissen, z.B. das\\n\\nGroße KI-Modelle für Deutschland\\n\\n43\\n\\nProtein-Sprachmodell ESMFold (Lin et al., 2022). Es erzeugt Einbettungen, die in nachgelagerten Aufgaben eingesetzt werden können, zum Beispiel zur Erfassung der strukturellen Eigenschaften von Proteinen.\\n\\n[AlphaFold]\\n\\nMit der Software AlphaFold gelang dem privaten Forschungsunternehmen DeepMind 2021 ein großer Durchbruch in der Forschung an der Faltung von Proteinen. Mithilfe eines KI-Modells erreichte das Team, die Faltung von Proteinen vorherzusagen und dadurch deren Form und Funktion innerhalb des Organismus genauer zu erforschen. Über 200 Millionen Strukturen von Proteinen konnten so gefunden und in einer Protein-Datenbank gespeichert werden - im Gegensatz zu den vorher nur circa 1 Millionen verfügbaren Proteinstrukturen.\\n\\nAlphaFold hat bereits heute einen bedeutenden, direkten Einfluss auf die menschliche Gesundheit. Bei einem Treffen mit Forscher:innen der European Society of Human Genetics wurde deutlich, wie wichtig die AlphaFold-Strukturen für Biolog:innen und Kliniker:innen sind, die versuchen, die Ursachen zu entschlüsseln. Darüber hinaus beschleunigt AlphaFold die Entdeckung von Medikamenten, indem es ein besseres Verständnis neu identifizierter Proteine ermöglicht, die als Angriffspunkte für Medikamente in Frage kommen. Es hilft Wissenschaftler:innen, schneller potenzielle Medikamente zu finden, die sich an diese Proteine binden.\\n\\nseltener genetischer Krankheiten\\n\\nEnde Infobox Für das Wirkstoffdesign ist die Vorhersage der Interaktion zwischen einem Arzneimittel und dem Zielorgan wichtig. Sie ist für die Entdeckung neuer Medikamente und die Umwidmung bestehender Medikamente von entscheidender Bedeutung. Yazdani-Jahromi et al. (2021) beschreiben ein Sprachmodell für derartige Anwendungen.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n44\\n\\nSteuerung dynamischer Systeme Verstärkungslernen (auch als Reinforcement Learning bekannt) betrachtet ein System mit Zuständen, Aktionen und Belohnungen zu bestimmten Zeitpunkten. Der Agent wählt zu den gegebenen Zuständen eine neue Aktion, während die nächsten Zustände und die Belohnung durch die Umgebung bestimmt wird. Ziel ist, eine Strategie zu erlernen, die jedem Zustand eine Aktion zuordnet und die Summe der Belohnungen maximiert. Mit derartigen Systemen lassen sich Brett- und Videospiele beschreiben, aber auch Robotersteuerungen und selbstfahrende Autos. Der Decision-Transformer (L. Chen et al., 2021) ist ein Sprachmodell, welches die Aktionen nacheinander prognostiziert. Dabei schätzt er die optimale Summe aller zukünftigen Belohnungen. Das Modell wird auf einer großen Menge von beobachteten Zeitreihen trainiert. Anschließend kann der Agent zu einem gegebenen Zustand die Aktion auswählen, welche zur höchsten prognostizierten Belohnungssumme führt. GATO (Reed et al., 2022) ist ein multimodales Modell, welches Text, Bilder und Sequenzen von Werten verarbeiten und daraus Steuerungsstrategien ableiten kann. Es erzielte auf mehr als 600 Benchmarks gute Resultate.\\n\\nAbb. 10: Das Gato-Modell generiert aus den aktuellen Zuständen (hellgelb) neue Aktionen (dunkelgelb). Die Umgebung produziert daraus neue Zustände, usw. Das Modell kann Texte, Messwerte, Bilder, usw. verarbeiten (Reed et al. 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n45\\n\\n2.3 Vertrauenswürdige KI-Foundation-Modelle\\n\\nDer großflächige Einsatz von KI-Systemen und das damit verbundene technische Innovationspotential wird erwartungsgemäß Auswirkungen von breiter wirtschaftlicher, aber auch gesellschaftlicher Relevanz haben. Bei ChatGPT haben sich innerhalb von fünf Tagen über eine Millionen Nutzer:innen angemeldet, um das System selbst auszuprobieren. Dieses große Interesse der breiten Öffentlichkeit zeigt, wie schnell und stark große KI-Systeme die öffentliche Diskussion prägen können. Aus dem Black-Box ähnlichen Charakter der eingesetzten trainierten Modelle ergeben sich Risiken, die besondere Maßnahmen im Hinblick auf den vertrauenswürdigen Einsatz von KI notwendig machen. Die vielfältigen Anwendungsmöglichkeiten von KI-Foundation- Modellen erfordern einen systematischen Ansatz zur Bewertung bzw. Abschwächung der entstehenden Risiken. Sie müssen parallel zur Modellkonstruktion im LEAM-Projekt angegangen werden.\\n\\nDie folgende Darstellung orientiert sich an dem im Kontext des KI.NRW-Flagship-Projekts „Zertifizierte KI (Poretschkin, 2022) erarbeiteten risikobasierten Ansatz zur vertrauenswürdigen KI und den vorhergehenden Arbeiten (Cremers et al., 2019; Poretschkin et al., 2021). Dieser Ansatz zielt darauf ab, KI-spezifische Risiken systematisch zu erfassen, messbar zu machen und unter Berücksichtigung von verschiedenen Dimensionen der Vertrauenswürdigkeit hinreichend zu reduzieren.\\n\\n”\\n\\nWir stellen im Folgenden sechs Dimensionen der Vertrauenswürdigkeit dar und skizzieren aktuelle Maßnahmen zu deren Identifikation und Mitigation.\\n\\nFairness Die Dimension Fairness soll sicherstellen, dass die KI-Anwendung nicht zu ungerechtfertigter Diskriminierung führt. Typische Ursachen hierfür sind unausgewogene (mit Bias behaftete) Trainingsdaten oder auch die statistische Unterrepräsentation von Personengruppen, welche zu einer verringerten Qualität der KI-Anwendung in Bezug auf diese Gruppen führen kann. Da die Foundation-Modelle mit Texten trainiert weprrden, die von Menschen verfasst wurden, spiegeln diese Texte häufig die in der Gesellschaft vorhandenen Stereotype wider. Hierbei kann es insbesondere zu einer Bevorzugung oder Benachteiligung von geschlechtsspezifischen oder ethnischen Gruppen kommen. Die Entwickler:innen großer KI-Foundation-Modelle sind sich vieler der immanenten Risiken bewusst und implementieren korrektive Maßnahmen, die darauf ausgerichtet sind, unerwünschte Effekte zu vermeiden. So berichten die Entwickler:innen von DALL-E (OpenAI, 2022a), dass spezielle Techniken eingesetzt werden, um einem in genutzten Trainingsdaten tatsächlich vorhandenen Bias (z.B. „heroic firefighter” wird zumeist als männliche Person dargestellt) entgegenzuwirken. Große Sprachmodelle wie PaLM und LaMDA verwenden mit gutem Erfolg filter-basierte Techniken, um unerwünschte Ausgaben („toxic language”) zu vermeiden. Diese Methoden sind aber sehr spezifischer Natur und mit großer Wahrscheinlichkeit im breiten Einsatz nicht ausreichend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n46\\n\\nAutonomie und Kontrolle Diese Dimension zielt auf zwei Dinge ab: zum einen die Autonomie der KI-Anwendung und zum anderen die Autonomie des Menschen. Einerseits ist hier zu beurteilen, welcher Grad an Autonomie (Gehman et al., 2020) für die Anwendung angemessen ist. Andererseits wird untersucht, ob der Mensch durch die KI-Anwendung angemessen unterstützt wird und ausreichend Handlungsspielraum in der Interaktion mit der KI- Anwendung erhält. Die spezifische Herausforderung liegt darin, dass die Möglichkeiten der Interaktion mit dem Menschen meist erst im Design der konkreten nachgelagerten KI-Anwendung festgelegt werden und nicht durch das Foundation-Modell an sich kontrolliert werden kann, das in dem Prozess aber eine zentrale Rolle spielt. Hier ist noch substantielle Forschungsarbeit notwendig, um die Querbeziehungen zwischen den Anwendungen und den Foundation-Modellen kontrolliert abdecken zu können.\\n\\nÄhnlich zur zwischenmenschlichen Kommunikation, können „toxische“ Modellausgaben (Gehman et al., 2020) (etwa Beleidigungen oder Mobbing) zu psychologisch-emotionalen Beeinträchtigungen der Nutzer:innen führen. Jenseits von verletzender Sprache können (manipulative) Kommunikationsstrategien oder die Vorspiegelung falscher Tatsachen emotionale Abhängigkeiten schaffen und damit die menschliche Autonomie potentiell einschränken. Einen frühen Ansatz, diesem Risiko zu begegnen, schlagen Glaese et al. (2022) vor. Sie optimieren ihre Conversational AI dergestalt, dass jene ihre maschinelle Natur, wenn immer nötig, offen kommuniziert. Auch hier stehen wir erst ganz am Anfang und die deutsche KI-Forschung könnte wesentliche Beiträge liefern.\\n\\nTransparenz Unter diesem Oberbegriff sind Aspekte der Nachvollziehbarkeit, Reproduzierbarkeit und Erklärbarkeit subsumiert. Die Dimension Transparenz untersucht insbesondere, ob die grundlegende Funktionsweise der KI-Anwendung für Nutzer:innen und Experten:innen angemessen nachvollziehbar ist und ob Ergebnisse der KI-Anwendung reproduziert und ggf. begründet werden können. Die Transparenz-Dimension wird in Foundation-Modellen zwar bereits auf der Ebene der Dokumentation und Beschreibung der Daten/Modelle (z.B. durch Modelcards (Gehman et al., 2020) adressiert, bedarf aber noch einer systematischen Herangehensweise in Bezug auf den tatsächlichen Einsatz der KI- Foundation-Modelle in konkreten Anwendungen.\\n\\nWie andere soziale Medien, kann ein Chatbot durch Fine-tuning oder Prompts dazu gebracht werden, dem/der Nutzer:in nur bestimmte Aspekte zu kommunizieren. Sie befinden sich dann in einer „Filterblase\", in der Nachrichten, die nicht der geäußerten Meinung entsprechen, ausgeblendet werden. Für diese Problematik gibt es mittlerweile Audit-Verfahren (Cen & Shah, 2021), mit denen überprüft werden kann, ob die Plattform unerwünschte inhaltliche Filter verwendet, wobei nur ein Black-Box-Zugriff auf den Filteralgorithmus erforderlich ist. In allen diesen Bereichen haben wir bisher wenig Kontrolle speziell bei Foundation-Modellen und es sind weitere Anstrengungen im Bereich der Forschung und dann wahrscheinlich auch der Regulierung erforderlich.\\n\\nGroße KI-Modelle für Deutschland\\n\\n47\\n\\nVerlässlichkeit Diese Dimension bezieht sich vornehmlich auf die Qualität der KI-Komponente und beurteilt u.a. deren Robustheit, das heißt die Konsistenz ihrer Ausgaben. Ein erschwerender Umstand dabei ist, dass die Erzeugung von Text meist zufallsgesteuert ist. Ein wichtiger Aspekt ist auch die Faktentreue der Ausgaben, da Foundation-Modelle nicht zwischen plausiblen und korrekten Zusammenhängen unterscheiden können (Lin et al., 2021). Beispielsweise hat sich der Wahrheitsgehalt von „Angela Merkel ist Bundeskanzlerin“ im Laufe der Zeit geändert. Im Bereich von Foundation-Modellen wird hier bereits mit verschiedenen Ansätzen (z.B. „safety fine-tuning”) untersucht, wie unerwünschte („unsafe”) Ausgaben möglichst vermieden werden können.\\n\\nDie „factfulness“ von Sprachmodellen zu erhöhen, ist zurzeit eine stark bearbeitete Forschungsrichtung im Bereich NLU (Natural Language Understanding, siehe z.B. Glaese et al. (2022), Nakano et al. (2021)). Retrieval-basierte Foundation-Modelle wie WebGPT, Retro, und LaMDA können auf eine große Sammlung von aktuellen Textdokumenten zugreifen, um den zu erstellenden Text durch relevante abgerufenen Informationen zu verbessern. Shuster et al. (2021) haben gezeigt, dass der Einsatz von Retrieval die Rate der ,,Halluzinationen” reduziert. Insgesamt erlaubt Retrieval die gezielte Verwendung aktuellen Wissens, um die Antwortqualität zu verbessern.\\n\\nEin weiteres Problem sind (mutwillige) Zweckentfremdungen, etwa zur Generierung von „Fake News“ oder zur Erstellung gewaltverherrlichender Texte, für die Brundage et al. (2022) einen Maßnahmenkatalog vorschlagen. Allgemeiner ist es schwierig, die Fähigkeiten und damit das Missbrauchspotential von großen Modellen einzuschätzen, da deren „abilities“ (Fähigkeiten) oftmals hochgradig sensitiv auf die Details des jeweiligen Modellinputs (Prompting) sind - z.B. Chain-of-Thought Prompting (Suzgun et al., 2022) und Prompt Injection Attacks (Branch et al., 2022) - und sich ex-ante nicht vorhersagen lassen. Die HELM-Initiative (Liang et al., 2022) nähert sich diesem Problem, indem sie eine Vielzahl an Sprachmodellen standardisiert evaluiert und vergleicht und dabei neben Performanz auch Aspekte wie Kalibrierung, Robustheit und Fairness berücksichtigt.\\n\\nEin wichtiger Aspekt in diesem Kontext ist “Trusted-AI”: Die Möglichkeiten, Garantien über die Korrektheit der Ergebnisse der Foundation-Modelle geben zu können, wie sie insbesondere in sicherheitskritischen Kontexten wichtig sind. Hier gibt es vor allem zwei Ansätze: Deduktive Verifikation, die auf Basis von grundlegenden Theorien über die KI- Algorithmen formale Beweise über die Korrektheit bestimmter Aspekte führt, und induktive Validierung, die durch systematisches Testen Aussagen über die Eigenschaften von KI-Systemen ableiten. Beide Bereiche werden in der KI teilweise schon erfolgreich angewendet, müssen aber insbesondere für die hier betrachteten, sehr komplexen Modelle noch deutlich weiterentwickelt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n48\\n\\nSicherheit Diese Dimension adressiert sowohl Eigenschaften der funktionalen Sicherheit als auch die Absicherung gegenüber Angriffen und Manipulationen der KI-Anwendung. Da sich die Maßnahmen in dieser Dimension primär auf die Einbettung der KI-Komponente beziehen, können Foundation-Modelle hier unter anderem durch klassische Methoden der IT- Sicherheit geschützt werden. Dennoch bleiben auch Fragen offen, die sich insbesondere aus dem Zielkonflikt ergeben, einerseits immer mehr auch öffentlich zugängliche Trainingsdaten zu nutzen, diese aber gleichzeitig vor Angriffen, die sich auf das Modellverhalten auswirken könnten, zu schützen.\\n\\nDialogsysteme wie BlenderBot 3 verwenden fine-tuning auf „korrekten” Dialogen, um z.B. das System darin zu verbessern, auf kritische Anmerkungen auch angemessen zu reagieren (Ung et al., 2022). Dennoch erfordern viele der bekannten Herausforderungen der vertrauenswürdigen KI für Foundation-Modellen bessere Lösungen. Als ein Beispiel sei hier nur das Problem der „visuellen Synonyme” genannt (Mishkin et al., 2022), mit denen sprachbasierte Filtertechniken umgangen werden können (z.B. „rote Flüssigkeit” statt „Blut”).\\n\\nDatenschutz Diese Dimension bezieht sich auf den Schutz sensibler Daten im Kontext von Entwicklung und Betrieb einer KI-Anwendung. Dabei wird sowohl der Schutz personenbezogener Daten als auch von Geschäftsgeheimnissen adressiert. In Zusammenhang mit sind hier insbesondere Risiken zu beachten, die sich im Umfeld der Techniken um „Model- Inversion” ergeben. So konnten bereits durch gezielte und systematische Abfragen von Modellen sensible Daten wie Sozialversicherungsnummern oder auch realistische Abbildungen (vorher unbekannter) Personen erzeugt werden.\\n\\nFerner sind für die oftmals generativ verwendeten Sprachmodelle sogenannte „training data extractions“ relevant, bei denen die erzeugten Modellausgaben Fragmente der Trainingsdaten enthalten. Sofern letztere nicht vollständig von personenbeziehbaren oder urheberrechtlich geschützten Informationen bereinigt wurden, können solche „Extraktionen“ Datenschutz- oder Eigentumsrechte von Dritten verletzen (Carlini et al., 2021). Nichol (2022) schlägt hierzu eine Ad-hoc-Mitigationsstrategie vor.\\n\\nEs gibt mehrere Möglichkeiten, Datenschutzprobleme bei Foundation-Modellen zu entschärfen. Ein Ansatz wäre, Sequenzen aus den generierten Daten herauszufiltern, die bereits in den Trainingsdaten vorkommen, etwa durch einen Bloom Filter. Ein anderer Ansatz ist das Training mit differential privacy Ansätzen. Hier gibt es einen hohen Forschungsbedarf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n49\\n\\nForschungsrichtungen und -ansätze Es bedarf weiterer intensiver Forschung und Entwicklung, um die Anforderungen aus allen beschriebenen sechs Dimensionen der vertrauenswürdigen KI für Foundation- Modelle systematisch abzudecken. Die bestehenden Ansätze zur Adressierung spezieller Risiken von Foundation-Modellen müssen erweitert und systematisch zusammengeführt werden. Dabei stellt neben der Komplexität und der schieren Größe der Foundation- Modelle auch die Vielfalt möglicher Anwendungen eine besondere Herausforderung dar. Ohne diese Ansätze ist es meist nicht möglich, die entwickelten Foundation-Modelle in der Praxis verantwortungsvoll zu nutzen.\\n\\nZusätzlich zu den technischen Maßnahmen während der Entwicklungs- und Testphase benötigt man einen kontinuierlichen Prozess und Regeln zur Governance, um den Einsatz eines Foundation-Modells zu begleiten. Diese sind bei den bisherigen Modellen aus den USA und China nicht gegeben bzw. wenig kontrollierbar. Während des Modellbetriebs ist sicherzustellen, dass die Prinzipien einer weiterhin vertrauenswürdigen KI erfüllt bleiben. Grundsätzlich sind geeignete organisatorische Maßnahmen zu ergreifen, um in Situationen, in denen z.B. ein mögliches Fehlverhalten eines Modells auftritt, reagieren zu können. Hierbei ist auch das Wechselspiel zwischen möglicherweise verschiedenen beteiligten Organisationen, wie dem Entwickler des Foundation-Modells und dem Anbieter einer darauf aufbauenden Anwendung, zu berücksichtigen. Dabei ist insbesondere darauf zu achten, dass zur Behebung gefundene Fehler auch erneute technische Maßnahmen, wie z.B. Modellverbesserungen oder Einführung weiterer Filter, angestoßen werden können.\\n\\nzu überwachen und\\n\\nMögliche und nötige Schritte umfassen:\\n\\nTieferes Verständnis der Strukturen und Funktionsweise von Foundation-Modelle\\n\\nVerfahren zur Risikobewertung und Tests entlang spezifischer Proxy-Aufgaben\\n\\nEtablierung von Benchmarks zur Vertrauenswürdigkeit von Foundation-Modellen\\n\\nUntersuchung und Test von semantischen Eigenschaften des latenten Raums\\n\\nValidierung von Modellen zur Input- oder Output-Überwachung\\n\\nDefinition geeigneter organisatorischer Maßnahmen zur Überwachung des\\n\\nlaufenden Betriebs\\n\\nGroße KI-Modelle für Deutschland\\n\\n50\\n\\nSPOTLIGHT 2txt NLG GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\n2txt ist Anbieter einer Software zur automatischen Textgenerierung auf Basis von KI-Sprachmodellen. Die Software ist für Anwendungsfälle im Travel-Bereich, in der Finanzbranche und im E-Commerce optimiert. 2txt zeichnet sich durch besonders einfaches und schnelles Setup, leichte Integrierbarkeit in Enterprise- Anwendungen und vor allem durch eine konstant zuverlässige und sehr hohe Textqualität aus.\\n\\nJohannes Bubenzer, Founder und CEO von 2txt\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir setzen Foundation-Modelle mit linguistischen Modellen ein, um beschreibende Texte für diverse\\n\\nThemengebiete im B2B zu generieren. So erzeugen wir z.B. Produktbeschreibungen, Ortsbeschreibungen oder Finanzberichte komplett automatisiert. Wir kombinieren Foundation-Modelle mit klassischen Sprachmodellen, um das beste aus beiden Welten zu erhalten: Foundation-Modelle macht unser Produkt skalierbar und kreativ, während linguistischen Modelle unsere Produkte kontrollierbar und zuverlässig machen.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle ermöglichen es uns, unser Produkt in kürzester Zeit zu skalieren. Arbeitsschritte, die früher Monate gedauert haben, können mit Hilfe von Foundation-Modellen in Sekunden erledigt werden. Das spart Ressourcen, Kosten und Zeit und ermöglicht uns, einen wesentlich größeren Markt zu adressieren.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Heutzutage: Die Anbieter von KI-Modellen betreiben diese als digitale Services. Das bedeutet, dass sie Zugriff auf alle eingehenden Daten und ausgehende Texte haben. Das ist ein Albtraum für Datenschutz und Geschäftsgeheimnisse der nutzenden Unternehmen. Für die nahe Zukunft: Es ist abzusehen, dass die Entwicklungen im Bereich der KI- Foundation-Modelle eine der zentralen technologischen Revolutionen der Menschheit auslösen wird. Es werden intelligente Maschinen entstehen und es wäre ein unermesslicher sozialer und wirtschaftlicher Fehler, den Wettlauf um diese Technologien privatwirtschaftlichen Firmen in den USA oder China zu überlassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n51\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Unternehmen können nur dann nachhaltige KI-Geschäftsmodelle aufbauen, wenn sie einen technologischen Vorsprung aufbauen und verteidigen können. Deswegen ist es essentiell, dass wir Downstream Modelle auf freien und offenen Foundation-Modellen mit eigenen Daten trainieren können. Ansonsten wäre jeglicher technologische Fortschritt in diesem Feld zu leicht replizierbar.\\n\\nZusätzlich sind einige der interessantesten Anwendungen für KI-Foundation-Modelle solche, involviert sind, z.B., wenn es um Kundenkommunikation oder Business Intelligence geht. Solche Services sind völlig undenkbar, wenn die Daten unkontrollierbar in die USA oder nach China transferiert werden müssen.\\n\\nin denen persönliche Daten\\n\\n2.4 Offene Forschungsfragen, neueste Entwicklungen und\\n\\nErwartungen\\n\\nDie Entwicklung von Foundation-Modellen befindet sich noch in ihren Anfängen und es bestehen viele offene Fragen und potenzielle Risiken. Gleichzeitig sind die Chancen insbesondere für die Wirtschaft sehr vielversprechend und Foundation-Modelle prägen bereits heute die Geschäftsmodelle und zugrundeliegenden KI-Lösungen von Unternehmen mit steigender Tendenz.\\n\\nDie folgenden Entwicklungen sind abzusehen:\\n\\nAusweitung auf weitere Domänen Die starke Entwicklung und Nutzung von Foundation-Modellen hat im NLP-Bereich seinen Anfang genommen und wird sich voraussichtlich auf alle anderen KI-Bereiche ausbreiten und weitere Datentypen abdecken: Bilder, Ton, Videos, Genom- und Proteinsequenzen, Sensordaten und so weiter.\\n\\nMultimodalität und das Limit von Textdaten Multimodalität ist ein wichtiger Faktor. Es ist bereits absehbar, dass multimodale Foundation-Modelle höhere Genauigkeiten als unimodale Modelle erreichen werden und eine noch größere Vielfalt von Anwendungsfällen abdecken können. Darüber hinaus ist festzustellen, dass die ausschließliche Verwendung von Textdaten zum Training von Foundation-Modellen bereits heute an ihre Grenzen stößt, da sehr große Teile der verwendbaren digitalen Texte des Internets schon genutzt werden. Hier könnten multimodale Datensammlungen eine Lösung darstellen, bspw. die enorme Menge von Videos auf YouTube.\\n\\nGroße KI-Modelle für Deutschland\\n\\n52\\n\\nHürde zum Training von Foundation-Modellen Die existierende Kluft zwischen einer überschaubaren Anzahl von Technologiekonzernen mit den notwendigen Ressourcen, um Foundation-Modelle zu trainieren, und all jenen, denen es an den finanziellen Mitteln und entsprechenden Rechenressourcen mangelt, könnte in Zukunft weiter wachsen. Diesem Trend wirken Initiativen wie LEAM entgegen. Weiteren Einfluss nehmen Bestrebungen, die eine KI-Demokratisierung durch verteiltes Lernen vorantreiben oder Open Source Kollektive wie Hugging Face, die mit der Big Science Initiative das Modell BLOOM entwickelt und öffentlich gemacht haben.\\n\\nKeine Monopolbildung, aber verzögerte Entwicklung Trotz des eingeschränkten Zugangs zu Rechenressourcen ist eine Monopolbildung zur jedoch zum aktuellen Zeitpunkt eher Entwicklung von Foundation-Modellen unwahrscheinlich. Im Fall von GPT-3 wurden vergleichbare Modelle veröffentlicht: Jurassic-1-Modelle von A21 Labs, OPT von Meta, die Modelle GPT-Neo und GPT-J von Eleuther AI, bis hin zu nicht-englischen Modellen wie dem russischen ruGPT-3 von Sber, dem koreanischen HyperCLOVA von Naver, den chinesischen CPM-1/CPM-2-Modellen der Tsinghua-Universität, PanGu-α von Huawei und Wu Dao 2.0 von der Beijing Academy of Artificial Intelligence. Dennoch ist darauf hinzuweisen, dass die Entwicklung der Wissenschaft dem Stand der amerikanischen Wirtschaft um bis zu zwei Jahre hinterher ist. Ein solcher Umstand ist höchst ungewöhnlich.\\n\\nEnge Zusammenarbeit von Entwicklung und Anwendung Die Grenze zwischen KI-Entwickler:innen und Anwender:innen wird in den kommenden Jahren vermutlich unschärfer. Grund dafür ist, dass immer mehr Menschen ohne KI- Expertise in der Lage sein werden, Foundation-Modelle erfolgreich für ihre eigenen Fälle anzupassen. Gleichzeitig ist somit ein exponentieller Anstieg neuer KI-basierter Produkte zu erwarten.\\n\\nGrounding Foundation-Sprachmodelle lernen alleine Korrelationen zwischen Begriffen und sprachlichen Konzepten. Dabei ist z.B. ein Hund mit den Begriffen Leine, Ohren, Katze, Säugetier, Bein, Fell, Schwanz, Spielzeug, Bellen usw. verbunden. Was fehlt sind Aspekte wie z.B. die dreidimensionale Gestalt des Hundes, seine Art, sich zu bewegen, der Klang seines Bellens, seine dynamische Reaktion auf Katzen oder Menschen. Damit zusammen hängen Gesetzmäßigkeiten der Physik, wie z.B. die Permanenz und Verformbarkeit von Objekten, die Wirkung der Schwerkraft. Daher lässt sich das Konzept des Hundes am besten lernen, wenn es in mehreren Medien auftaucht, zum Beispiel als Bild, in Worten oder in einem Film, in dem er eine Katze jagt. Die Verwendung von multimodalen Foundation-Modellen bietet die Möglichkeit für ein solches integriertes Lernen von Konzepten in der Welt. Yann LeCun sagt: „Anstelle von Sprache oder Bildern wird die nächste KI-Generation jedoch direkt aus Videos lernen. Meta unternimmt derzeit große Anstrengungen, um Videodaten aus der Ich-Perspektive für diese neue KI-Generation zu sammeln, aber auch YouTube-Videos sind als Trainingsmaterial geeignet\\'\\' (Schreiner, 2022; Jawahar, 2021). Das kürzlich vorgeschlagene Foundation-Modell PLATO ist ein erster Versuch, um intuitive Physik aus Videos zu lernen (Piloto et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n53\\n\\nPlanung und Simulation Daniel Kahneman hat auf der Grundlage langjähriger Studien des menschlichen Verhaltens eine Hypothese über zwei verschiedene Denksysteme entwickelt (Kahneman, 2011). Fast Thinking ist schnell, instinktiv und emotional. Beispiele hierfür sind das Verstehen eines einfachen gesprochenen Satzes oder das Erkennen eines Objekts auf einem Bild. Fast Thinking läuft ständig und erzeugt Eindrücke, Intuitionen und schnelle Urteile auf der Grundlage unserer unmittelbaren Wahrnehmungen. Fast Thinking kann zum großen Teil schon heute mit den existierenden Foundation-Modellen geleistet werden. Slow Thinking ist langsamer, überlegter und logischer. Es ist z.B. dafür verantwortlich in einer engen Parklücke zu parken oder die Rechenaufgabe 16*34 zu lösen. Slow Thinking kommt nur dann zum Einsatz, wenn Probleme mit Fast Thinking auftreten, das heißt, wenn es die Wahrnehmungen nicht gut erklären kann. Slow Thinking ist in der Lage, Probleme mental durchzuspielen und Ergebnisse für verschiedene Randbedingungen zu simulieren. Es entspricht daher weitgehend dem Konzept der Steuerung durch Prognosemodelle (model predictive control). Diese Fähigkeiten können ansatzweise von Foundation-Modellen wie GATO (Reed et al., 2022) realisiert werden. Notwendig Integration dieser Modelle mit den Perzeptionsmodellen für unterschiedliche Medien und die flexible Anwendbarkeit auf neue Planungsprobleme. Yann LeCun zufolge ist die „Fähigkeit, Modelle der Welt zu konstruieren, im Grunde das Wesen der Intelligenz”. Diese Modelle werden nicht nur benötigt, um physische Bewegungen vorherzusagen, sondern auch das Verhalten von Menschen, wirtschaftliche Aktivitäten usw. Die große Herausforderung der Künstlichen Intelligenz im nächsten Jahrzehnt besteht darin, prädiktive Modelle der Welt zu erlernen, um mit Unsicherheiten umzugehen (Fridman, 2022).\\n\\nist eine\\n\\nGroße KI-Modelle für Deutschland\\n\\n54\\n\\nKI-Foundation-Modelle im internationalen Vergleich\\n\\nGroße KI-Modelle für Deutschland\\n\\n55\\n\\n3. KI-Foundation-Modelle im internationalen Vergleich\\n\\nDieses Kapitel beleuchtet die Entwicklung von KI-Foundation-Modellen im internationalen Vergleich. Es zeigt, dass die USA und China aktuell führend sind und erläutert die Gründe, warum Europa bislang keine Vorreiterrolle in der Entwicklung großer KI-Modelle einnimmt.\\n\\nAbb. 11: Um 2016 tauchte ein neuer Trend zu sehr großen Modellen auf, die von großen Internetfirmen trainiert wurden (rot). Diese waren in der Lage, die notwendigen Investitionen zu finanzieren. Die untere blaue Linie veranschaulicht den Berechnungsaufwand der anderen Modelle, z.B. von Universitäten (Sevilla et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n56\\n\\nIm internationalen Vergleich zeichnen sich folgende Trends deutlich ab: 2\\n\\nVon der Wissenschaft oder KI-Community entwickelte Modelle zur öffentlichen Verwendung im Text- und Bildbereich liegen etwa 14 bzw. 15 Monate nach der Erstentwicklung aus dem wirtschaftlichen Sektor (Benaic & Hogarth, 2022).\\n\\nDer Großteil der veröffentlichten Foundation-Modelle stammt aus den USA und China: Seit 2017 stammen 73 % aus den USA, 15 % aus China.\\n\\nDie meisten der entwickelten Modelle stammen aus dem wirtschaftlichen, nicht wissenschaftlichen Umfeld: 86 % der Modelle wurden seit 2017 aus dem wirtschaftlichen Sektor heraus entwickelt, 13 % im wissenschaftlichen Sektor.\\n\\nAbb. 12: Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) 3 unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach Ländern\\n\\n2 Für den internationalen Vergleich wurden 125 Foundation-Modelle ab der initialen Entwicklung des Transformers in 2017 betrachtet. Dazu wurde der Datensatz von Sevilla et al. (2022) nach Foundation-Modellen gefiltert und um aktuelle Veröffentlichungen erweitert. 3 Gleitkommaoperationen, Plural von FLOP (floating-point operation) - hier eine Maßeinheit zum Vergleich von Trainingsaufwänden gemessen in den kleinsten Operationen, nicht zu verwechseln mit FLOPS (floating-point operations per second).\\n\\nGroße KI-Modelle für Deutschland\\n\\n57\\n\\nAbb. 13: Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach vier Kategorien von Organisationen\\n\\nDie Übersichtsgrafiken (Abb. 12 und 13) zeigen ganz eindeutig, dass das Technologiegebiet von zwei Ländern, den USA und China, dominiert wird, und dass Europa einschließlich Deutschland im Verhältnis zu seiner Größe und Rolle in der globalen Gesellschaft stark unterrepräsentiert ist. 73 % der Modelle stammen aus den USA und 15 % aus China.\\n\\nAus Deutschland und anderen EU-Ländern wurden tendenziell kleinere Foundation- Modelle mit niedrigerem Trainingsaufwand veröffentlicht. Das BLOOM Modell stellt eine Ausnahme dar und wurde von einem Wissenschaftskollektiv bestehend aus über 250 Institutionen auf dem Jean Zay Supercomputer in Frankreich trainiert. Als einziges Modell aus Deutschland taucht in den Abbildungen 12 und 13 das Modell Luminous des deutschen KI-Unternehmens Aleph Alpha auf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n58\\n\\n[Aleph Alpha]\\n\\nAleph Alpha aus Heidelberg ist ein unabhängiges, deutsches KI- Forschungsunternehmen, gegründet von Jonas Andrulis und Samuel Weinbach. Jonas ist erfolgreicher KI-Serienunternehmer und ehemaliger Senior Manager aus Apples KI-Forschungsabteilung für geheime Innovationsprojekte in Kalifornien. Samuel hat 10 Jahren Erfahrung für KI-Innovation und ist aktuell einer der führenden Köpfe für das Engineering von großen Sprachmodellen.\\n\\nAleph Alpha hat auf europäischen Daten und in fünf Sprachen ein GPT3-Äquivalent entwickelt, das in der größten Ausbaustufe die doppelte Anzahl von Parametern verglichen zu OpenAIs bestem Angebot bietet. Zusätzlich entwickelte das Team um Jonas und Samuel eine multimodale Erweiterung, die nicht nur Text, sondern auch Bilder im Kontext versteht. Damit hat Aleph Alpha nach eigenen Aussagen Anfang 2023 das weltweit einzige multimodale Angebot für große Sprachmodelle. Diese und weitere Innovationen werden in zahlreichen akademischen Publikationen und Open-Source Veröffentlichungen mit der Community geteilt. Mit vielen der wissenschaftlichen Spitzenforschern besteht eine enge Zusammenarbeit.\\n\\nAleph Alpha gelang der Aufbau eines Teams aus 50 internationalen Experten von den besten Unternehmen aus den USA und dem Rest der Welt. In zwei Finanzierungsrunden konnte das Team mit der Unterstützung von einigen der Spitzeninvestoren unabhängige Rekordfinanzierung in Höhe von 28 Mio. EUR sichern.\\n\\naus Deutschland\\n\\nund\\n\\nEuropa\\n\\neine\\n\\nZum souveränen Betrieb auch für sicherheitskritische Anwendungsszenarien hat Aleph Alpha ein spezialisiertes Rechenzentrum aufgebaut und betriebt damit leistungsstärkste kommerzielle KI-Rechenzentrum Europas. Die aktuell das Technologie von Aleph Alpha wird aktuell in Unternehmen verschiedenster Größe und Branche im Finanzsektor, bei Gesundheit, Recht und in Verwaltung und Sicherheit eingesetzt.\\n\\nDurch die angedachte LEAM-Infrastruktur könnte Aleph Alpha in den Forschungs- und Open Source-Projekten auch für moderne Foundation-Modelle mit hohen Anforderungen Unterstützung anbieten. Auch für Aleph Alphas KMU-Kunden ohne eigene Rechenzentren sind diese Möglichkeiten entscheidende Zutaten für den Eintritt in ein neues Technologiezeitalter.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n59\\n\\nAlle erheblichen Entwicklungssprünge im Bereich der Foundation-Modelle kommen aus den USA und zu einem geringeren Grad auch aus China. Das betrifft sowohl die Arten der Modelle und neue Funktionalitäten, als auch Sprünge in der Modellgröße und die Erschließung neuer Datendomänen.\\n\\nIn den USA sind die wichtigen Durchbrüche in wenigen Industrielaboren erzielt worden oder Investitionen über gigantische Infrastrukturen und großzügige Personalmittel für Spitzenforscher:innen verfügen. Dazu gehören die Labs von Google, Meta, Microsoft, Amazon sowie OpenAI und zu einem geringeren Anteil auch AllenAI.\\n\\nin Labs, die durch privatwirtschaftliche\\n\\nIn China wurde mit dem Modell Wu Dao 2 an der Beijing Academy of AI der erste große Durchbruch aus mehreren Spitzenuniversitäten und arbeitet eng mit industriellen Partnern (u.a. Xiaomi, Meituan und Kuaishou) zusammen. Ein großer Teil der Investition stammt vom chinesischen Staat. Weitere große Modelle kommen aus den Forschungslaboren von Alibaba, Baidu, Tencent, Huawei, Inspur und anderen chinesischen Hightech-Konzernen.\\n\\nerreicht. Dieses\\n\\nLabor\\n\\nvereint\\n\\nExpert:innen\\n\\nUnter dem Aspekt der Innovationskraft können wir grob drei Klassen von Modellen unterscheiden. Als diese gelten:\\n\\ni. Die bekanntesten Modelle, die jeweils bahnbrechende Durchbrüche\\n\\nrepräsentieren,\\n\\nii. Modelle, die bestehende Modelltypen in der Größe, den Daten oder ihren\\n\\nFunktionalitäten verbessern oder erweitern sowie\\n\\niii. Modelle, die bestehende Modelle mehr oder weniger genau kopieren und sich technologisch nur unwesentlich von ihren Vorbildern unterscheiden. Dazu gehören auch verkleinerte oder vereinfachte Nachahmungen.\\n\\nWährend Modelle der Klassen (i) und (ii) wesentlich zur Evolution der Technologie beitragen, sorgen Modelle der Klasse (iii) für eine Verfügbarkeit der Modelle in Organisationen und Geschäftsmodellen, in denen die Nutzung zur aus wirtschaftlichen, technischen oder regulatorischen Gründen nicht möglich war.\\n\\nDie Entwicklung von Modellen der Klassen (ii) und (iii) hat einen wichtigen Seiteneffekt: Sie bewirkt den Aufbau oder die Verstärkung der technologischen Kompetenz für zukünftige Technologieentwicklungen\\n\\nAlle Modelle der Klasse (i) und die meisten Modelle der Klasse (ii) wurden in den USA und China geschaffen. Die meisten Modelle aus anderen Ländern kopieren die GPT- Architektur, manchmal mit geringen Modifikationen, und erweitern die Lerndaten, um gewisse Sprachen (mitunter auch Anwendungen) besser abzudecken. Dazu gehören die Modelle aus Israel, Russland, Schweden, Frankreich und Deutschland. Es gibt derzeit einige Modelle der Klasse (ii), u.a. das Modell GPT-SW3 von Sweden AI, das Modell Luminous der deutschen Firma Aleph Alpha und das Modell BLOOM, das von der französischen Initiative OpenScience gemeinsam mit Hugging Face geschaffen wurde. Alle\\n\\nGroße KI-Modelle für Deutschland\\n\\n60\\n\\ndiese Modelle wurden allerdings erst 15-24 Monate nach der Veröffentlichung von GPT-3 fertiggestellt.\\n\\nUm Foundation-Sprachmodelle entwickeln zu können, müssen drei essentielle Voraussetzungen erfüllt sein: Kompetenz, Infrastruktur und Daten.\\n\\nVerfügbarkeit von intellektueller Kompetenz: Gibt es genügend Expert:innen, die das notwendige Wissen und u.U. auch Erfahrung für die Modellentwicklung mitbringen, und sind diese Personen verfügbar?\\n\\nVerfügbarkeit von Infrastruktur: Reicht die Recheninfrastruktur für die Modellentwicklung, das heißt minimal für Datenaufbereitung, Training und Evaluation?\\n\\nVerfügbarkeit von Daten: Gibt es hinreichende Mengen von Daten in einer oder mehreren Sprachen, um damit die emergenten Fähigkeiten zu erzeugen? Das ist nicht alleine eine Frage der Menge an Daten, denn durch hohe Diversität und Qualität kann ein Mangel an Masse bis zu einem gewissen Grad kompensiert werden.\\n\\nDie Auswertung der Expert:inneninterviews und Umfragen (s. Kapitel 4 und 5) hat gezeigt, dass sowohl Deutschland als auch Europa in allen drei Voraussetzungen vor erheblichen Herausforderungen stehen. Im Rahmen der Studie wird ein Fokus auf die verfügbare Infrastruktur gelegt. Dennoch muss betont werden, dass für erfolgreiche europäische KI- Foundation-Modelle alle drei Voraussetzungen erfüllt werden müssen.\\n\\nDie Verfügbarkeit von hinreichenden Infrastrukturen ist derzeit zum Flaschenhals für die erfolgreiche Beteiligung bei der Technologieentwicklung und -kommerzialisierung geworden. Infrastrukturen, die neue Modelle der Klasse (i) ermöglichen, sind weitaus größer als die Mindestvoraussetzungen für die Entwicklung von Modellen der Klasse (ii). Die Forschung an der Spitze der Technologieentwicklung erfordert das kreative Experimentieren mit vielen Kandidaten für neue Architekturen und Lernmethoden und deren zahlreichen Varianten. Das erschwert die Teilnahme von Universitäten an dieser Forschung. 86 % der Modelle wurden aus dem wirtschaftlichen Sektor heraus entwickelt und nur 13 % im wissenschaftlichen Sektor. Von der Wissenschaft oder KI-Community entwickelte Modelle zur öffentlichen Verwendung im Text- und Bildbereich liegen etwa 14 bzw. 15 Monate nach der Erstentwicklung in den Industrielaboren.\\n\\nSelbst die großen US-Universitäten können sich ohne Kooperationen mit den Forschungslaboren der Industrie nicht mehr am Wettbewerb beteiligen. Die US-Regierung hat daher die National Artificial Intelligence Research Resource Task Force (NAIRRTF) eingerichtet, um die Infrastrukturen für die KI-Forschung deutlich zu verbessern (THE NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH RESOURCE TASK FORCE (NAIRRTF), o.D.).\\n\\nGroße KI-Modelle für Deutschland\\n\\n61\\n\\nVersuche, die durchaus beachtlichen Hochleistungscomputer der wissenschaftlichen Hochleistungsrechenzentren für das Trainieren von Foundation-Modellen zu nutzen, haben nur begrenzten Erfolg. Es lassen sich zwar mit guter Planung neue GPT-Modelle trainieren und das auch mit zusätzlichen Daten und kleinen Modifikationen. Für die für das systematische Optimierung und Weiterentwicklung der Modelle und Experimentieren mit neuen Modelltypen fehlt jedoch die kontinuierliche Verfügbarkeit von hinreichend großen Computerressourcen. Auch für die Entwicklung und Evaluation neuer Anwendungsklassen können Hochleistungscomputer der Wissenschaft nicht genutzt werden.\\n\\nUm die Entwicklung von Modellen der Klasse (iii) muss man sich weniger Sorgen machen. Durch die kontinuierliche Zunahme von Rechenleistung in Industrie und Wissenschaft und durch die ständige Verbesserung der Lernverfahren wird das Kopieren von Modelltypen leichter werden.\\n\\nWährend die Anforderungen an Rechenkapazität für das Trainieren von Modellen, selbst mit Erweiterungen der Lerndaten, eher abnehmen, nimmt die erforderliche Rechenleistung für die Entwicklung von Modellen der nächsten Generationen noch stark zu. Der Grund dafür ist die bevorstehende Fusion von Modalitäten zum Erwerb von Weltwissen und zusätzlichen Funktionalitäten, die die Nutzung von großen Volumina an Filmdaten und analogen Daten aus der realen Welt erfordern. Das Kapitel 8 beleuchtet das Thema der Infrastruktur näher.\\n\\nGroße KI-Modelle für Deutschland\\n\\n62\\n\\n[OpenGPT-X]\\n\\nOpenGPT-X ist ein Kooperationsprojekt mit Partnern aus Wissenschaft, Wirtschaft und Technologie. Das Ziel des Projektes ist die Schaffung großer KI-Sprachmodelle, um innovative Sprachanwendungen für Europa und nach europäischen Werten voranzutreiben. Mittels Gaia-X, der sicheren Dateninfrastruktur zur Förderung der Innovation in Europa, wird OpenGPT-X KI-Sprachmodelle und Sprachdienste europaweit offen und in mehreren Sprachen zur Verfügung stellen.\\n\\nAls Anwendungsbeispiele zukünftige Produktentwicklungen werden beispielsweise im Bereich Medien KI-Sprachmodelle zur Fragenbeantwortung bei interaktiven Medienformaten entwickelt. In der Domäne Finanzwesen werden die der Modelle Schadensabwicklung durch Versicherungen eingesetzt und in der Mobilität sollen sie als persönliche Assistenten beim Autofahren zu mehr Sicherheit und Fahrkomfort beitragen.\\n\\nfür\\n\\nfür\\n\\neine\\n\\neffizientere\\n\\nDokumentenverarbeitung\\n\\nbei\\n\\nAktuell trainieren die Projektpartner:innen ein erstes Modell mit zwölf Milliarden Parametern. Dabei liegt der Fokus darauf, möglichst inhaltlich korrekte Antworten von dem Sprachmodell zu erhalten.\\n\\nDas Vorhaben wird vom Bundesministerium für Wirtschaft und Klimaschutz von Januar 2022 bis Dezember 2024 im Rahmen des Förderprogramms Innovative und im digitalen Ökosystem Gaia-X praxisnahe Anwendungen und Datenräume gefördert. Beteiligt sind das Fraunhofer-Institut für intelligente Analyse- und Informationssysteme (IAIS), das Fraunhofer-Institut für Intelligente Schaltungen (IS), 1&1 IONOS SE, das Forschungszentrum Jülich, die Technische Universität Dresden, die Alexander Thamm GmbH, das Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Aleph Alpha, Control Expert, Westdeutscher Rundfunk (WDR) und der KI Bundesverband.\\n\\nZwischen OpenGPT-X und LEAM gibt es einen engen Austausch. Die Ergebnisse und Erfahrungen des Projektes sind in die Ausarbeitung der Machbarkeitsstudie eingeflossen und werden auch weiterhin in die Planung von LEAM einfließen.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n63\\n\\nBedeutung für die technologische Souveränität Deutschlands Im internationalen Vergleich wird deutlich: Deutsche Unternehmen sind aus Mangel an leistungsfähigen europäischen Alternativen maßgeblich auf die Nutzung amerikanischer und chinesischer Foundation-Modelle angewiesen. Daraus ergeben sich eine Reihe an Herausforderungen, denn die bestehenden Modelle erfüllen in vielerlei Hinsicht nicht die europäischen Qualitätsstandards.\\n\\nDiese Defizite fassen wir in fünf Punkten zusammen:\\n\\n(1) Inhaltliche Funktionalität: Deutsche und europäische Inhalte sind unterrepräsentiert. Das gilt insbesondere für wirtschaftliche, gesellschaftspolitische, rechtliche, regionale und kulturelle Themen. Die großen Modelle sind zudem eher auf den Massenmarkt hin ausgerichtet als auf die Anforderungen der Anwendungen in Wirtschaft, Politik, Bildung.\\n\\n(2) Sprachliche Funktionalität: Die bestehenden Modelle sind durch die verwendeten Lerndaten, aber auch durch die Evaluationsdaten und Benchmarks viel stärker auf das Englische, im Falle der chinesischen Modelle auch auf das Chinesische, hin ausgerichtet als auf andere Sprachen. Sprachen, die im Internet stark vertreten sind, wie das Deutsche, Französische und Spanische, sind zwar auch gut repräsentiert, aber in der messbaren Funktionalität längst nicht so wirksam abgedeckt. Die meisten europäischen Sprachen sind nur sehr unzureichend repräsentiert. Ein wichtiges Ergebnis der linguistischen Forschung ist, dass Sprache das Denken beeinflusst und sogar die grundlegenden Aspekte menschlicher Erfahrung verändert: Raum, Zeit, Kausalität und die Beziehung zu anderen (Boroditsky, 2012). Um die deutsche Kultur adäquat zu erfassen, sind daher Foundation-Modelle für die deutsche Sprache erforderlich.\\n\\n(3) Verfügbarkeit: Die\\n\\nihrer Eigentumsverhältnisse und Lizenzmodelle nur eingeschränkt für kommerzielle Anwendungen einsetzbar. Das gilt für die Anpassung durch modifizierte (erweiterte, korrigierte, gefilterte) zusätzliches Pretraining durch andere Trainingsaufgaben, Inferenz (also praktischen Einsatz), few-shot prompting, large- scale fine-tuning und Integration in umfangreichere Anwendungen.\\n\\ngroßen\\n\\ninternationalen Modelle\\n\\nsind wegen\\n\\nLerndaten,\\n\\n(4) Sicherheit, Verlässlichkeit: Für viele europäische Anwendungen wären zusätzliche Maßnahmen zur Gewährleistung besserer Performanz in Hinblick auf Sicherheit und Verlässlichkeit erforderlich, in Korrektheit, Konsistenz und Datenschutz. Zu den Sicherheitsanforderungen gehört aber auch die Vertraulichkeit der Eingabedaten im Test- und Inferenzgebrauch. Eine weitere Anforderung ist die Verlässlichkeit im Hinblick auf Persistenz, das heißt die langfristige ständige Verfügbarkeit der eingesetzten vortrainierten und insbesondere der durch aufwendiges Nachtraining angepassten Modelle. Dies gilt vor allem dann, wenn diese oder in Anwendungen Sicherheitsinteressen ausfallsicher und unterbrechungsfrei betrieben werden müssen.\\n\\ninsbesondere\\n\\neingesetzt werden,\\n\\ndie\\n\\naus wirtschaftlichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n64\\n\\n(5) Ethische Akzeptabilität: Die Ausgaben der Modelle verletzen mitunter durch Bias (Aussagen/Entscheidungen basierend auf falschen Vorurteilen) und Toxizität (Verwendung von ethisch oder stilistisch-ästhetisch nicht akzeptablen sprachlichen Ausdrucksweisen) die de-facto Standards für den Einsatz in Wirtschaft, Politik und Bildung. Daher muss es für die Anwendung möglich sein, Korrektur- und Filtermaßnahmen eigenständig zu definieren, in die Modelle zu integrieren und anzupassen.\\n\\nDie europäische Forschung kann mit Recht stolz sein auf ihre ersten europäischen Foundation-Sprachmodelle wie Aleph Alpha, BLOOM oder GPT-SW3. Weitere Modelle wie Open GPT-X sind in der Vorbereitung. Diese europäischen Modelle sind ermutigende Beispiele dafür, dass Europa, wenn auch mit etwas Verspätung, Foundation- Sprachmodelle entwickeln kann. Sie sind aber noch kein Indiz für ein Aufschließen der europäischen Forschung in die Avant-Garde der internationalen Forschung zu diesem Thema. Zudem decken diese Modelle trotz großer Fortschritte bisher weder die deutsche Sprache noch die Bandbreite der anderen europäischen Sprachen in dem Maße ab, wie es heute bereits für das Englische erreicht ist.\\n\\nEin Grund dafür sind die materiellen Forschungsbedingungen. In der Künstlichen Intelligenz, wie auch in der gesamten Informatik gilt, dass nur Forschungsgruppen, die für eine neue Technologie spielerisch viele Möglichkeiten der Realisierung, Adaption und Optimierung ausprobieren können, längerfristig erfolgreich sein können. Wenn im Falle der Foundation-Modelle die benötigten materiellen Infrastrukturen für Computation und Speicherung so groß sind, dass die universitäre Forschung von der Spitzenforschung ausgeschlossen ist, werden die neuen Durchbrüche aus den Spitzenlaboren der Industrieunternehmen kommen, die sich zusätzlich zur Infrastruktur auch personell und kulturell einen echten Forschungsbiotop aufbauen und leisten können. Ein Kennzeichen solch erfolgreicher Strukturen ist der scheinbare Überfluss, gekennzeichnet durch Redundanz in Infrastruktur und personeller Kompetenz.\\n\\nDer Begriff AI CERN für die von CLAIRE 4 und anderen Akteuren geforderte und dringend benötigte Infrastruktur für die europäische KI-Forschung ist eine wirksame Metapher, aber im gewissen Sinne auch irreführend. Der infrastrukturelle Bedarf der Teilchenphysik unterscheidet vom Bedarf der KI und der gesamten von Softwaretechnologieforschung: Während Grundlagenforschungsprojekten gebucht werden kann, um in wenigen sehr großen Experimenten grundlegende Forschungsfragen zu beantworten, nutzen die KI- Forschungsgruppen der großen industriellen Labore die Computerressourcen fast durchgängig für große Zahlen von Experimenten für das schrittweise Trainieren großer Modelle und für deren Evaluation und Modifikation. Die Evolutionszyklen bestehend aus Modifikation der Technologie und Selektion sind sehr kurz, weil sie nicht wie in der Physik oder in den klassischen Ingenieurwissenschaften in der physikalischen Welt realisiert und\\n\\nsich\\n\\nsich\\n\\nstark\\n\\ndie\\n\\nInfrastruktur\\n\\ndes CERN\\n\\n4 Die Confederation of Laboratories for Artificial Intelligence Research in Europe (CLAIRE) ist ein Verein mit dem Ziel, Forschung, Innovation und Zusammenarbeit im Bereich der KI zu verstärken. Mehr als 1000 KI-Expert:innen aus ganz Europa unterstützen die Bestrebungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n65\\n\\ngetestet werden müssen. Auch die Wege vom Labor in die Anwendungen können in der KI sehr kurz sein. So konnte Google die großen Sprachmodelle zeitnah nach ihrer Fertigstellung bereits im Produktionsbetrieb für die Suche und für die Platzierung von Werbung testen.\\n\\nWegen der hohen infrastrukturellen Anforderungen gibt es bisher weltweit nur eine kleine Zahl von Forschungslaboren und Entwicklungs\\xadzentren, die den heutigen Stand der KI-Technologie in ihrer vollen Komplexität beherrschen. Das heißt, es gibt nur ganz wenige Organisationen, die Training und Evaluation von großen multilingualen, multimodalen und multimedialen Foundation-Modellen selbst ausführen können. Mit der multimodalen und multimedialen Verknüpfung der Daten und der Kombination der assoziativen Modelle mit expliziten Daten- und Wissensrepositorien nimmt diese Komplexität noch weiter zu.\\n\\nMit dieser Komplexität wachsen natürlich auch die Einstiegsbarrieren für neue Akteure, denn zusätzlich zu gut ausgebildeten Expert:innen wird deren extensive individuelle und kollektive Erfahrung im Trainieren, Evaluieren und Einsatz großer Foundation-Modelle benötigt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n66\\n\\nBedarf der Wirtschaft an KI- Foundation-Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n67\\n\\n4. Bedarf der Wirtschaft an KI-Foundation- Modellen\\n\\nEine grundlegende Frage, die diese Studie klären möchte, ist, welche Bedeutung Foundation-Modelle für die Wirtschaft haben und welche Anforderungen Unternehmen an die Modelle stellen.\\n\\nUm dieser Frage auf den Grund zu gehen wurden zwei Methoden angewandt 5. Diese sind:\\n\\neine Umfrage an kleine und mittlere Unternehmen, die sich auf die Entwicklung von KI spezialisiert haben sowie\\n\\n\\n\\nleitfadengestützte Interviews mit den Leiter:innen KI, Data, o. ä. deutscher Großunternehmen.\\n\\nDie an der Umfrage teilnehmenden Unternehmen lassen sich wie folgt zusammenfassen:\\n\\nAnzahl Empfänger\\n\\n373 Mitglieder des KI Bundesverbandes 25 Kontakte aus dem Netzwerk der Merantix Momentum GmbH\\n\\nAnzahl Antworten\\n\\n71 Teilnehmer (18 % Rückmeldequote)\\n\\nGröße der Unternehmen 90 % weniger als 100 Mitarbeiter\\n\\nBranche der Unternehmen (Mehrfachnennung möglich)\\n\\n80 % Informationstechnologie 25 % Dienstleistungen je 11 % Industrie & Medien\\n\\nEinsatz von KI\\n\\n90 % setzen KI ein\\n\\nTabelle 3: Übersicht der wichtigsten Information zu der Umfrage für die Wirtschaft\\n\\n5 Eine genauere Betrachtung der Methodik findet sich im Kapitel V.\\n\\nGroße KI-Modelle für Deutschland\\n\\n68\\n\\nInterviews wurden mit folgenden Personen durchgeführt. Ergebnisprotokolle der Interviews befinden sich im Anhang B.3:\\n\\nTitel\\n\\nVorname\\n\\nName\\n\\nUnternehmen\\n\\nDr. Dr. Dr. Dr.\\n\\nDr. Dr.\\n\\nDr. Dr.\\n\\nDr.\\n\\nDr.\\n\\nDr. Dr. Dr. Dr. Dr. Dr. Dr.\\n\\nDr.\\n\\nWolfgang Maik Marion Hans-Jörg Michael Jean-Paul Mario Corina Matthias Stephan Frank Sebastian Thomas Sabine Nico Rainer Christian Stephan Jochen Michael Armin Lorenz Feiyu Andreas Michael Dirk\\n\\nSebastian\\n\\nHauner Friedel Legler Vögel Fausten Schmetz Deng Apachiţe Dorner Kaulbach Säuberlich Kaiser Wolf Donauer Kelling Sträter Spannbauer Meyer Kaiser Müller-Wünsch Kurrle Determann Xu Wierse May Schlesinger\\n\\nHallensleben\\n\\nAllianz SE BASF SE Bayer AG BMW Group Robert Bosch GmbH Burda Media BWI GmbH Continental AG DATEV eG Deutsche Bahn AG EnBW Energie Baden-Württemberg AG Ergo Group AG Hugging Face, Inc. Infineon Technologies AG Infineon Technologies AG Ionos SE Lufthansa Group Munich RE Mercedes-Benz Group AG Otto GmbH & Co KG Porsche AG Rewe Group SAP SE sicos BW GmbH Siemens AG TÜV Süd AG VDE Elektronik Informationstechnik e. V. Volkswagen AG Zalando SE\\n\\nVerband\\n\\nder\\n\\nElektrotechnik\\n\\nDr. Dr.\\n\\nPatrick Alexander\\n\\nvan der Smagt Borek\\n\\nTabelle 4: Befragte Expert:innen aus der Wirtschaft\\n\\nin die LEAM- Alle Erkenntnisse aus der Umfrage und den Machbarkeitsstudie eingeflossen. Im Folgenden werden wir zusammenfassend einige Aspekte und Übereinstimmungen illustrieren.\\n\\nInterviews sind\\n\\nGroße KI-Modelle für Deutschland\\n\\n69\\n\\nAuswertung der Interviews und Umfrage Ungefähr 66 % der befragten KI-Unternehmen setzen bereits Foundation-Modelle ein oder beabsichtigen, diese in der Zukunft einzusetzen. Dies ist ein beachtlicher Anteil, wenn man bedenkt, dass Foundation-Modelle eine relativ neue Entwicklung der interviewten Intelligenz sind. Daneben bestätigen auch viele der Künstlichen Großunternehmen, bereits KI-Foundation-Modelle im produktiven Einsatz zu haben oder aktuell an Anwendungen zu arbeiten. Dadurch wird eindeutig, wie essentiell Foundation- Modelle für die gesamte Wirtschaft bereits sind. Die Interviewten geben außerdem an, dass die Bedeutung in den nächsten Jahren weiter steigen wird. Es sei aktuell noch nicht abzusehen, welche Disruption und neuen Geschäftsmodelle KI-Foundation-Modelle in den nächsten Jahren ermöglichen werden.\\n\\nAbb. 14: Ergebnisse der Umfrage mit KMUs zu deren Einsatz von Foundation-Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n70\\n\\nKI-Unternehmen, die aktuell noch nicht mit KI-Foundation-Modellen arbeiten, geben verschiedene Gründe an, die erfüllt werden müssten, damit sie Foundation-Modelle nutzen würden. Genannt wurden hier insbesondere niedrigere Kosten, die Bereitstellung von Open Source-Modellen sowie die Verfügbarkeit von Daten, die jeweils rund 58 % der Befragten angaben. Weitere Hürden, die Unternehmen als Gründe äußerten, Foundation- Modelle nicht zu nutzen, sind ein Mangel an Recheninfrastruktur (38 %), datenschutzrechtliche Hürden (33 %), ein Mangel an qualifizierten Mitarbeiter:innen (25 %) sowie verfügbaren europäischen Modellen (25 %).\\n\\nAbb. 15: Ergebnisse der Umfrage mit KMUs zu Hindernissen beim Einsatz von Foundation- Modellen\\n\\nGroße KI-Modelle für Deutschland\\n\\n71\\n\\nAuch Großunternehmen teilten diese Bedenken in den geführten Interviews. Dabei wurde deutlich: Großunternehmen verfolgen nicht das Ziel, eigene Foundation-Modelle zu entwickeln. Stattdessen wollen sie bestehende Modelle für ausgewählte Anwendungen anpassen. In der aktuellen Lage ist dies häufig aber aus Compliance-Gründen nicht möglich, da die Modelle nicht frei, sondern nur über Programmierschnittstellen zur Verfügung stehen und Daten für das Tuning aus dem europäischen Wirtschaftsraum heraus gesendet werden müssen. Vor allem mit sensiblen Datensätzen ist daher ein Anpassen der Modelle für Großunternehmen nicht möglich. Die befragten Großunternehmen sehen darin einen klaren Nachteil im internationalen Wettbewerb. Selbst im stark reglementierten und national ausgeprägten Versicherungsumfeld berichten Unternehmen, dass neue, digitale Geschäftsmodelle, die außerhalb Europas entstehen, eine Gefahr darstellen.\\n\\nDarüber hinaus zeigten die Interviews mit Großunternehmen, dass generelle Foundation- Modelle häufig nicht ausreichen, um den hohen Qualitätsstandards der Unternehmen zu entsprechen. Stattdessen brauche es Foundation-Modelle, die auf die Bedürfnisse einzelner Branchen abgestimmt sind. Das Ziel sollte es also sein, bspw. ein Gesundheitsmodell, ein Industriemodell und ein Versicherungsmodell zu entwickeln, auf deren Basis die Unternehmen dann einzelne Anwendungen entwickeln können. Trotz bestehender Datensätze findet diese Entwicklung aktuell nicht statt, da die deutschen Unternehmen ihre Patienten-, Maschinen- und Versicherungsdaten nicht in die USA übertragen möchten. Dieser Wunsch nach speziellen Modellen spiele auch eine Rolle, da auf dem Datensatz einer bestimmten Population trainierte Modelle nicht direkt auf andere Populationen übertragbar seien. So berichtet beispielsweise das Chemie- und Pharmaunternehmen Bayer davon, dass sich amerikanische Patient:innendaten von europäischen, asiatischen oder afrikanischen unterscheiden.\\n\\nDaneben fehle es in vielen Unternehmen an gut ausgebildeten Mitarbeiter:innen und es sei schwierig, KI-Modelle in den laufenden Betrieb zu integrieren.\\n\\nInsgesamt wird die Bedeutung von Foundation-Modellen für die gesamtwirtschaftliche Entwicklung Deutschlands mit 73 % als sehr hoch eingeschätzt. Besonders der Aufbau eines europäischen KI-Ökosystems wird von 82 % der Befragten, die Berücksichtigung von Werten wie Transparenz, Reduktion von Bias und Nachhaltigkeit von 85 % der Befragten als relevant erachtet. Dabei wird die direkte Zusammenarbeit mit der Forschung etwa von der Hälfte (54 %) der Befragten als bedeutend eingeschätzt. Die Zusammenarbeit mit KMUs und Start-ups hingegen von 67 %. Neben der bereits hohen Nutzung von Foundation-Modellen zeigen diese Ergebnisse die gesamtwirtschaftliche Relevanz, die ihnen in der Industrie zugesprochen wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n72\\n\\nAbb. 16: Ergebnisse der Umfrage mit KMUs zur Bedeutung von unterschiedlichen Aspekten der Foundation-Modell-Entwicklung\\n\\nDie Interviewten sehen in KI-Foundation-Modellen eine strategische Bedeutung für den Wirtschaftsstandort Europa. So betonen sie die Notwendigkeit eigener europäischer Modelle, um die Wettbewerbsvorteile der Technologie vollständig zu nutzen, die Qualität der Modelle umfänglich zu kontrollieren und Sicherheitsrisiken zu minimieren. Der letzte Punkt sei mit Hinblick auf die geostrategische Situation Europas und existierende Diskussionen rund um 5G und Huawei besonders wichtig. Aktuell haben für die befragten Unternehmen besonders Sprachmodelle eine hohe Relevanz bei der Nutzung und Entwicklung von Foundation-Modellen (genannt von 71 % der Unternehmen). Dies reflektiert den hohen Erfolg und die Prominenz von Sprachmodellen, wie GTP-3. Ebenfalls als wichtig eingeschätzt werden multilinguale Sprachmodelle (52 %) und multimodale Modelle (38 %). Beide Arten von Modellen stellen logische nächste Schritte von Sprachmodellen dar und werden von Unternehmen wie OpenAI mit Dall-E 2 und ChatGPT bereits erfolgreich entwickelt. Wie oben angedeutet, gehen die Einschätzungen der Großunternehmen noch einen Schritt weiter. In den Interviews wurden zwar auch vor allem Sprach- und Multimodale Modelle genannt, mit denen viele Unternehmen bereits experimentieren, daneben brauche es aber speziellere Modelle für einzelne Anwendungsbereiche. In der aktuellen Situation seien nur inkrementelle Fortschritte und keine Disruption möglich.\\n\\nGroße KI-Modelle für Deutschland\\n\\n73\\n\\nMit jeweils circa 34 % der Befragten finden auch Geschäfts- und Fertigungsprozesse sowie Robotik als bedeutsame Bereiche Beachtung. Diese werden in der aktuellen Foundation- Modell Entwicklung wenig fokussiert behandelt. Hier kann für LEAM eine Chance liegen, diese Nischen in der Modellentwicklung zu besetzen.\\n\\nAbb. 17: Ergebnisse der Umfrage mit KMUs zur Relevanz von unterschiedlichen Arten von KI- Modellen bei der Foundation-Modell-Entwicklung (Antworten mit einer Antwortrate von weniger als 20 % wurden ausgelassen. Die vollständigen Antworten befinden sich in Anhang A.2)\\n\\nInsgesamt zeigt die Auswertung den Bedarf an europäischen Foundation-Modellen. Die befragten Unternehmen hatten am Ende der Umfrage die Möglichkeit, weitere Kommentare zu hinterlassen. Ähnlich wie in den Expert:inneninterviews mit der Wissenschaft (s. Kapitel 5) wurde vermehrt angemerkt, dass „alle außer [den] große[n] Internet-Konzerne[n] darauf beschränkt sind, existierende Foundation-Modelle zu benutzen” und somit in massive Abhängigkeit geraten. Biases können so bei down-stream Anwendungen nur schwer vermieden werden. Ein weiteres häufiges Thema in den Antworten war, dass die europäische Wirtschaft bereits jetzt im internationalen Vergleich zurückliegt. Diese Aussagen bestätigen auch die Interviews mit den Großunternehmen. Es brauche jetzt eine gemeinsame Aktion im Bereich Infrastruktur und Daten und es sei unbestritten, dass kompetitive europäische Modelle, amerikanischen vorzuziehen seien. Besonders im Bereich Datenschutz und -sicherheit würden europäische Modelle die Implementierung enorm vereinfachen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n74\\n\\nErfreulicherweise geben viele Interviewte an, die Initiative LEAM unterstützen zu wollen und einem Engagement im Rahmen eines Joint Ventures bzw. einer PPP generell offen gegenüberzustehen.\\n\\nDie Auswertung der Umfrage sowie der Interviews lässt sich in drei Kernaussagen zusammenfassen:\\n\\n(1) KI-Foundation-Modelle werden elementarer Bestandteil der IT-Anwendungs- Architektur. Branchenübergreifend implementieren bzw. planen Unternehmen die Implementierung von Anwendungen auf Basis der Modelle.\\n\\n(2) Aktuelle Modelle haben häufig den Nachteil, dass sie nicht Open Source verfügbar und damit frei anpassbar sind. Es gibt erhebliche datenschutzrechtliche Bedenken bzgl. der Nutzung außereuropäischer Modelle. Europäische Open-Source-Modelle werden daher als Alternative gebraucht.\\n\\n(3) Die Aufgabe kann nicht von einem Akteur alleine bewältigt werden. Es braucht eine gemeinsame Aktion der Unternehmen in Deutschland. Die Wirtschaft erkennt diesen Bedarf und steht einem Engagement offen gegenüber.\\n\\nGroße KI-Modelle für Deutschland\\n\\n75\\n\\nUnterstützung bei der Entwicklung durch Forschung und Wissenschaft\\n\\nGroße KI-Modelle für Deutschland\\n\\n76\\n\\n5. Unterstützung bei der Entwicklung durch Forschung und Wissenschaft\\n\\nAus den Interviews mit führenden Industrie- und KI-Unternehmen wird deutlich: Die Nachfrage nach europäischen Foundation-Modellen ist groß. Eines der primären Ziele, die sich LEAM daher gesetzt hat, ist es Foundation-Modelle für die Wirtschaft bereitzustellen.\\n\\nDie Entwicklung von Foundation-Modellen erfordert jedoch umfangreiche Maßnahmen im Bereich der Forschung und Entwicklung. Aus diesem Grund ist es wichtig, die spezifischen Anforderungen von Forschung und Wissenschaft zu kennen und mitzudenken, damit die Entwicklung von leistungsfähigen Foundation-Modellen gelingt.\\n\\nHierfür wurden im Rahmen dieser Studie Interviews mit führenden Wissenschaftler:innen aus Deutschland durchgeführt. Darin wird herausgestellt, wie der aktuelle Forschungsstand von KI-Foundation-Modellen in der Wissenschaft ist und welche Rahmenbedingungen nötig sind, um Wirtschaft und Wissenschaft bei der Entwicklung dieser entsprechend zu unterstützen.\\n\\nMethodik der Interviews Die Interviews wurden durch fünf Leitfragen strukturiert:\\n\\nLeitfrage 1 „Relevanz von Foundation-Modellen in der Wissenschaft“\\n\\nLeitfrage 2 „International führende Arbeitsgruppen“\\n\\nLeitfrage 3 „Wissenschaftliche und wirtschaftliche Defizite der Foundation- Modelle“\\n\\nLeitfrage 4 „Maßnahmen zur Förderung der Forschung und wirtschaftlichen Nutzung von Foundation-Modellen in Deutschland“\\n\\nLeitfrage 5 „Sonderstellung der Foundation-Modelle und zukünftige Entwicklungen“\\n\\nDie Interviews wurden mit 21 Expert:innen zwischen Anfang Oktober und Mitte Dezember 2022 geführt (s. Tabelle im Anhang B.1).\\n\\nDie Erkenntnisse aus den Interviews sind in die Planung von LEAM und an den passenden Stellen in den gesamten Text dieses Kapitels eingeflossen. Im Folgenden werden wir lediglich einige ausgewählte Aspekte der Interviews entlang der fünf Leitfragen illustrieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n77\\n\\nAuswertung der Interviews In der Leitfrage 1 „Relevanz von Foundation-Modellen in der Wissenschaft“ beschreiben sich die Expert:innen als Nutzer:innen der Modelle und erkennen die große Bedeutung der Modelle – auch für die Wirtschaft – als Forschungsparadigma der kommenden Jahre an. Für viele Expert:innen bestehen Hürden, um auf einer internationalen Bühne wissenschaftlich mitwirken zu können, da der Zugriff zu den notwendigen Technologien fehlt und sie somit in ihrer Forschung eingeschränkt sind. Foundation-Modelle können klimaverträglich gestaltet werden, wenn zentral verwaltete Modelle der KI-Community zur Verfügung gestellt werden können. Die Reproduktion der Modelle würde damit hinfällig.\\n\\nAls Antwort auf Leitfrage 2 „international führende Arbeitsgruppen“ wurden US- amerikanische Technologie-Konzerne wie Microsoft, Open AI, Google, Deepmind, und Meta genannt. In einzelnen Fällen wurden Universitäten wie Stanford oder die Carnegie Mellon University erwähnt, die bei den kleineren Sprachmodellen kompetitiv sind. Die chinesischen Modelle wurden hingegen selten erwähnt. Als Erfolgsfaktoren wurden der Zugang zu Infrastruktur, Daten und Talenten hervorgehoben.\\n\\nManche Interviewten betonten Deutschlands Stärke in anderen Gebieten wie dem Vorhandensein von gut ausgebildeten Wissenschaftler:innen, die allerdings aufgrund der internationalen Angebote oftmals nicht In diesem Zusammenhang wurde auch von der Abhängigkeit von den überwiegend kommerziellen Anbietern der Modelle gewarnt.\\n\\nin Deutschland bleiben.\\n\\nDie Leitfrage 3 „Wissenschaftliche und wirtschaftliche Defizite der Foundation-Modelle“ wurde mit der mangelnden Anpassbarkeit der Foundation-Modelle beantwortet. Es fehlt an deutschem und auch mehrsprachigem Vokabular, Fachwissen, Robustheit/Invarianz, sowie Erklärungen der Ergebnisse. Diese Hürden können nicht durch ein Nachtraining abgebaut werden, bzw. nur zu finanziellen Bedingungen der Hyperscaler. Probleme wie Intransparenz bezüglich der Trainingsdaten und -Prozeduren Bias, Privacy und erschweren die Nutzung. Die Vertrauenswürdigkeit der existierenden Modelle ist damit infrage gestellt und rechtliche Fragen bleiben unbeantwortet.\\n\\nDas kontinuierliche Lernen und auch die Verbindung mit Domänenwissen, Unternehmenswissen oder Applikationswissen, wie etwa Faktenwissen aus Wissensgraphen ist ausbaufähig. Die Frage nach der Kontrolle und Validierung der fehlende ist eine offene Forschungsfrage. Weitere Defizite sind Ergebnisse Geschäftsmodelle und die geringe Effizienz der Modelle. An letzterer wird aktiv geforscht, aber der Vergleich mit sehr großen Foundation-Modellen (z.B. GPT-3) kann nicht gezogen werden, was im Gegensatz zur wissenschaftlichen Praxis steht.\\n\\nDie Generalisierungsfähigkeit der Modelle wurde als unzureichend eingestuft. Derzeit werden zum größten Teil (Sprach-)Daten als einzige Wissensquellen genutzt, welche in zukünftiger Entwicklung ein limitierender Faktor sein können. Somit ist eine Verbesserung der Kuratierung von Trainingsdaten, die Anreicherung mit Wissen sowie mehr komplementäre Daten für die Modellentwicklung wie Ontologien, Sequenzdaten oder Bilder nötig.\\n\\nGroße KI-Modelle für Deutschland\\n\\n78\\n\\nFoundation-Modelle können als Kulturgut, bzw. ein öffentliches Gut für die Grundlagenforschung verstanden werden, die einem zentralem und transparenten Entwicklungsprozess unterliegen sollten, um Vertrauen zu schaffen. Andernfalls können Machtkonzentration und sinkende digitale Souveränität die Folge sein.\\n\\nAls Antwort zur Leitfrage 4 „Maßnahmen zur Förderung der Forschung und wirtschaftlichen Nutzung von Foundation-Modellen in Deutschland“ wurde hauptsächlich die Förderung einer Infrastruktur und der leichte Zugang zu jener für Wirtschaft und Wissenschaft genannt. Komplizierte Antragsverfahren und zu lange Wartezeit auf Rechenkapazität stellen eine erhebliche Hürde für beide Sektoren dar. Darüber hinaus ist die Kuratierung von geeigneten Trainingsdaten essentiell, wobei die europäische Sprachenvielfalt und Multimodalität fokussiert betrachtet werden sollten. Zusätzlich spielen Zeitreihendaten und Ontologien eine wichtige Rolle. In allen Fällen ist die Rechtssicherheit zu berücksichtigen und sicherzustellen, dass die Datenstrategie ethischen Prinzipien genügt.\\n\\nAttraktive Forschungsbedingungen als starkes Ökosystem sind nötig, in dem Forscher:innen kollaborieren können. Es sollen Anreize geschaffen werden, noch nicht stark digitalisierte, alte Industrien interessanter für junge Forscher:innen zu machen.\\n\\nIm Themenkomplex der Leitfrage 5 „Sonderstellung der Foundation-Modelle und implizite Wissen der Foundation-Modelle zukünftige Entwicklungen“ wurde das hervorgehoben. Die Modelle sind damit u.a. in der Lage, Programmiersprachen zu erlernen, was bis vor einigen Jahren technologisch nicht möglich war.\\n\\nPotential wird in Deutschland und Europa darin gesehen, Sprache als wichtigstes menschliches Kommunikationsmittel in vielen verschiedenen wirtschaftlichen B2B- Anwendungen durch Assistenzsysteme zu unterstützen (z.B. beim Einkauf, bei der Ansprache der Kund:innen, bei der Kommunikation mit den Kund:innen, bei der schnelleren Abarbeitung von Dokumenten, Rechnungen, Verkauf, Service, Ermittlung von Stimmungen etc.).\\n\\nWissenschaftliche Durchbrüche werden in der Effizienzsteigerung (mobile Anwendungen und Edge-Computing) und bspw. in der Verarbeitung längerer Texte gesehen, wozu auch das Erkennen dokumentübergreifender Beziehungen gehört. Weiteres Potential wird auch in einer kontrollierteren Generierung von Texten gesehen.\\n\\nEinige Expert:innen betonten das Potential durch die Einbindung von Wissen bis hin zu (symbolischen) Subsystemen, die vom neuronalen System (assoziativ) angesprochen werden. Hier wird Modularität zur Anpassung führen, ohne Gelerntes „vergessen\" zu müssen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n79\\n\\nSPOTLIGHT Alexander Thamm GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie Alexander Thamm GmbH [at] ist ein führendes deutsches Beratungsunternehmen für Data & AI. Mit 350 Mitarbeiten unterstützt [at] seit mehr als 10 Jahren DAX-Konzerne sowie mittelständische Unternehmen und setzt innovative KI-Projekte um.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Der Einsatz von Künstlicher Intelligenz ist bei unseren Kunden ein wichtiges Thema. Wir entwickeln AI- Strategien, Konzepte und implementieren Projekte auf Basis der neuesten wissenschaftlichen Erkenntnisse. Dabei spielen Sprachverarbeitung und Foundation-Modelle eine immer wichtigere Rolle und wir investieren in die Nutzung der Technologie in Deutschland, unter anderem in der aktiven Mitwirkung am Projekt OpenGPT-X Unsere Teams sind auf vielfältige Bereiche der KI-Entwicklung spezialisiert und setzen unter anderem Projekte um in den Bereichen Bildverarbeitung, Natural Language Processing, Forecasting, Anomalie-Detection. Beispiele hierfür sind ein KI- gesteuertes System zur Unterstützung der Zug-Disposition bei der DB, Robotersystem zur Unterstützung der Altenpflege und neuartige Verfahren für das autonome Fahren.\\n\\nAlexander Thamm, Founder und CEO der Alexander Thamm GmbH.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle werden zentraler Bestandteil der KI-Anwendungen und der Infrastruktur in vielen Bereichen. Derzeit entwickeln wir individuelle KI-Applikationen oft von Grund auf unter Nutzung spezifischer Kundendaten. In der Zukunft wird es hier eine Verlagerung hin zum Transfer-Learning bzw. Tuning von existierenden, leistungsfähigen Foundation_Modellen geben.\\n\\nGleichzeitig werden durch die Nutzung von Foundation-Modellen neue Anwendungsgebiete erschlossen und wir werden für unsere Kunden Applikationen entwickeln, die derzeit noch schwer umsetzbar sind – vor allem im Bereich NLP. Der Markt wird wachsen und wir sehen hier eine große Chance für uns, aber vor allem auch für die Wettbewerbsfähigkeit der deutschen Wirtschaft.\\n\\nDas hat intensive Auswirkungen auf unser Geschäftsmodell, vor allem, wenn wir auf die Nutzung und Lizenzierung von Foundation-Modellen angewiesen wären, auf die wir nur über APIs zugreifen können und auf die wir keinen direkten Einfluss haben. Wenn wir diese Modelle dann nur von nicht-europäischen Anbietern beziehen können, müssen wir uns zusätzlich noch intensiv mit Datenschutz- und Datensicherheitsaspekten auseinandersetzen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n80\\n\\nDamit unsere Kunden und unser Unternehmen nicht in eine einseitige Abhängigkeit geraten, ist es enorm wichtig, dass wir auch auf Foundation-Modelle zugreifen können, die in Deutschland oder Europa entwickelt wurden, und wir diese nicht nur über APIs nutzen können. Gleichzeitig hat die Berücksichtigung von europäischen Werten, z.B. beim Thema Bias, für uns und unsere Kunden eine enorme Bedeutung.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? US-amerikanische Internet-Unternehmen investieren derzeit intensiv in die Entwicklung und Verbreitung von Foundation-Modellen. Durch die Bereitstellung über APIs können die ersten am Markt befindlichen Services gleichzeitig eine Menge Daten z.B. über die Nutzungsschwerpunkte sammeln. Damit besteht die Gefahr, dass sich hier wieder – wie bei den Suchmaschinen – Monopole bilden und eine zunehmende technologische Abhängigkeit entsteht. Wenn die zentralen KI-Anwendungen nur aus Übersee kommen, werden sich langfristig unsere Aktivitäten auf die Gestaltung von Frontends- und Workflows beschränken. Wir haben keinen oder nur noch geringen Einfluss auf die Modelle, was vor allem hinsichtlich Qualität und Bias problematisch ist. Damit könnte diese Entwicklung auch zu einer potenziellen Bedrohung unseres derzeitigen Geschäftsmodells werden – und zu unserer Unternehmens-Mission, die Wettbewerbsfähigkeit der europäischen Wirtschaft in diesem Bereich sicherzustellen.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Europäische Foundation-Modelle, die wir als Basis für unsere KI-Entwicklungen nutzen könnten, würden uns befähigen, auch in Zukunft innovative Anwendungen zu entwickeln. Da wir nicht nur über APIs zugreifen, sondern die Modelle als Open Source zur Verfügung hätten, könnten wir in vielen Bereichen auch unsere eigenen Forschungsaktivitäten intensivieren und für unsere Kunden State-of-the Art KI- Systeme bauen. Damit wäre sichergestellt, dass wir auch in Zukunft bei unseren Kunden Prozesse optimieren und neue Produkte und Geschäftsmodelle ermöglichen können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n81\\n\\nMethodik der Umfrage Zusätzlich zu den 20 interviewten Expert:innen aus der industriellen und universitären Forschung wurden aus Kapazitätsgründen, und um das Meinungsbild noch detaillierter darzustellen, weitere vertiefende Stellungnahmen von 21 führenden KI-Forscher:innen in Form einer Umfrage eingeholt. Die Umfrage wurde zwischen Anfang Oktober und Mitte Dezember 2022 durchgeführt und umfasste 23 Fragen (siehe Tabelle im Anhang A.1). Hinzugekommen sind einige detailliertere Fragen, die aufgrund der begrenzten Interviewzeit in den Expert:inneninterviews nicht gestellt werden konnten.\\n\\nAlle Erkenntnisse aus der Umfrage sind in die LEAM-Machbarkeitsstudie und in den Text des gesamten Kapitels eingeflossen. Im Folgenden werden wir in Anlehnung an die geführten Expert:inneninterviews einige Aspekte und Übereinstimmungen entlang ausgewählter Leitfragen illustrieren.\\n\\nAuswertung der Umfrage In der Umfrage haben sich überwiegend die Aussagen der Interviewten aus den Expert:inneninterviews bestätigt.\\n\\nDie große Mehrheit aller befragten KI-Forscher:innen arbeiten bereits mit KI-Foundation- Modellen oder beabsichtigen, die Modelle zukünftig einzusetzen. Alle schätzen ihre Bedeutung für die Wissenschaft und Wirtschaft als hoch ein.\\n\\nDie Art der verwendeten Daten ist durchaus unterschiedlich. So wurden u.a. öffentlich verfügbare Texte und wissenschaftliche Publikationen sowie medizinische und technische Daten aufgeführt. Das Thema fehlender Daten ist allerdings auch einer der Gründe, warum Foundation-Modelle noch nicht überall eingesetzt werden. Weitere Gründe waren eine mangelnde Transparenz im Datenschutz bei existierenden Modellen, fehlende Infrastruktur, technische Limitierungen, Fachkräftemangel, hohe Investitionskosten und Unklarheiten bei der Regulation. Dennoch besteht der Wille, Foundation-Modelle künftig einzusetzen.\\n\\nDie Diversität der genannten möglichen Einsatzgebiete spiegelt dies ebenso wider, wobei auch die Art der als bedeutsam eingeschätzten Foundation-Modelle eine Rolle spielt – Bilder, Audio und Mobilitätsdaten wurden hier genannt. Keine größere Rolle spielen die vermeintlichen Gefahren, die von den Modellen ausgehen könnten. Lediglich potentieller militärischer Missbrauch, sowie die derzeit noch fehlende Erklärbarkeit wurden hier genannt. Außerdem wurden die Dominanz einzelner Big-Tech-Unternehmen und die fehlenden Antworten seitens der akademischen Forschung bemängelt. Auf dieses Ungleichgewicht wurde zudem häufiger in den Expert:inneninterviews hingewiesen.\\n\\nIm internationalen Vergleich wird Deutschland lediglich in den Bereichen Kompetenz und Kreativität sowie Ausbildungsmöglichkeiten als wettbewerbsfähig angesehen. In anderen Punkten wie Datenlage, Hardware und Förderung fällt Deutschland hinter den USA und China zurück. Die schwache Digitalisierung und Kommerzialisierung werden zudem als fehlende Voraussetzungen gesehen. Daraus folgt auch die zunehmende Abwanderung von Talenten, die in Deutschland einfach nicht die gleichen Voraussetzungen finden, wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n82\\n\\nanderswo. Dies betrifft nicht nur fehlende Investitionen, sondern auch die starke Regulierung neuer Technologien, ausgeprägte gesellschaftliche Ängste und die generelle Abneigung gegenüber dem Fortschritt, der aus der KI-Forschung entstehen kann.\\n\\nsowie wissenschaftlicher Die Fragestellungen bzgl. Daten und Datenverarbeitung wurde von den Befragten durchschnittlich als „eher hoch” eingestuft. Gleiches gilt für die domänenagnostischen Pre-Trainings und das nachfolgende domänenspezifische Fine-Tuning. Auch hier sahen die Befragten die Relevanz als „eher hoch” an. Für diese grundlegenden Themen besteht demnach größerer Forschungsbedarf innerhalb Deutschlands.\\n\\nRelevanz\\n\\nverschiedener Modellarchitekturen\\n\\nBzgl. der inhaltlichen Einsatzgebiete besteht wie erwartet keine einheitliche Meinung, da die Befragten aus unterschiedlichen wissenschaftlichen Richtungen kommen. Die Notwendigkeit von Foundation-Modellen in den Anwendungen der verschiedenen Disziplinen wurde generell als „mittel” bis „eher hoch” eingestuft. Erklärbarkeit, Aufbau von Common Sense Wissen, Einbezug weiterer Datenquellen (außer Text), hybride Verarbeitung (symbolisch und subsymbolisch), Kausalität, uvm. wurden ebenso genannt.\\n\\nDie erwarteten Kosten der Umfrageteilnehmer:innen stehen im Einklang mit den tatsächlich bekannten Kosten für große Foundation-Modelle. Die Befragten waren in dieser Hinsicht bereits gut informiert. Bzgl. der Regulation von Foundation-Modellen fühlte sich jedoch der überwiegende Teil der Befragten als nicht ausreichend informiert, wenngleich sie dies als Voraussetzung für ein erfolgreiches Einsetzen der Technologie sahen.\\n\\nBezüglich möglicher Mängel der Daten wie Bias, Diskriminierung und Misrepresentation fielen die Antworten dichotom aus. Nur eine kleine Mehrheit der Befragten gab an, sich mit diesen zu beschäftigen und sah sich in der Lage, diese angemessen zu adressieren. Diese Ergebnisse stehen im Einklang mit den Ergebnissen der Interviews, in denen die Hälfte aller Befragten auf diese Problematik hinwies. Die Mehrzahl der Befragten empfand ihr Wissen über die Regulation der Entwicklung von Foundation-Modellen als Voraussetzung für einen möglichen Einsatz der Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n83\\n\\nZusammenfassend zeigten sich die Erkenntnisse der Interviews großteils kongruent mit den Ergebnissen der Umfrage. Demnach sind die momentan führenden Nationen auf dem Gebiet der Foundation-Modelle die USA und China. Faktoren, die diese Entwicklung begünstigten, sind die Datenlage, Hardware, Ressourcen und Förderungen. Im internationalen Vergleich wird Deutschland lediglich in den Bereichen Kompetenz und Kreativität sowie Ausbildungsmöglichkeiten als vergleichbar angesehen. Faktoren, welche in Deutschland den Einsatz von Foundation-Modellen noch hindern, seien fehlende Infrastruktur und Zugang, technische Limitierungen, Fachkräftemangel, Bedenken bzgl. Datenschutzes und mangelnde Information über die Regulation der Entwicklung von Foundation-Modellen.\\n\\nZudem wurden auch mögliche Gefahren der Foundation-Modelle genannt, diese bezogen sich hauptsächlich auf die Monopolisierung der Technologie sowie mögliche Mängel der Modelle wie Bias, Diskriminierung, Toxizität, Misrepresentation und Erklärbarkeit. Die meistgenannten Interessen für Foundation-Modelle bezogen sich auf Multimodalität, die europäische Sprachenvielfalt, Erklärbarkeit und den Aufbau von allgemeinem Wissen.\\n\\nAbschließend lässt sich festhalten, dass die Mehrheit aller befragten KI-Forscher:innen bereits mit Foundation-Modellen arbeitet oder beabsichtigt, die Modelle zukünftig einzusetzen und ihre Bedeutung für die Wissenschaft und Wirtschaft als hoch eingeschätzt werden.\\n\\nDie deutsche Wissenschaft und Forschung sind also bestens in der Lage leistungsfähige Foundation-Modelle umzusetzen, um der Nachfrage der Wirtschaft gerecht zu werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n84\\n\\nChancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle\\n\\nGroße KI-Modelle für Deutschland\\n\\n85\\n\\n6. Chancen und Pläne bei der Entwicklung europäischer KI-Foundation-Modelle Bei der Frage welche Modelle am dringendsten von der Wirtschaft gebraucht werden, zeichnet sich ein deutliches Bild ab. Multilinguale Sprachmodelle wurden in den Befragungen der Expert:innen am häufigsten genannt. An zweiter Stelle folgten multimodale Modelle, die ebenfalls Sprache beinhalten.\\n\\nObwohl die wirtschaftliche Bedeutung von Foundation-Modellen in der Biomedizin, in der autonomen Steuerung von Fahrzeugen und Robotern und nicht zuletzt auch im Management von Produktions- und Geschäftsprozessen als sehr hoch eingeschätzt wurde, wurden Sprachmodelle durchweg als wichtigster Anwendungsbereich genannt.\\n\\nEs ist wenig überraschend, dass großen multilingualen Sprachmodellen die höchste Priorität gegeben wurde. Die meisten der bereits erfolgreichen Foundation-Modelle sind entweder Sprachmodelle oder multimodale Modelle mit einem hohen Anteil an Sprachdaten und sprachlicher Funktionalität. Durch die zentrale Rolle der Sprache in der in nahezu allen Teilbereichen der menschlichen menschlichen Gesellschaft lassen sich die Sprachmodelle für eine Vielzahl von Aufgaben in fast allen Sektoren der Wirtschaft und Gesellschaft einsetzen.\\n\\nIntelligenz und\\n\\nBei der Entwicklung von eigenen europäischen Foundation-Modellen empfehlen wir daher, mit multilingualen Sprachmodellen zu beginnen. Dafür gibt es mehrere Gründe, die wir in diesem Kapitel näher erläutern möchten.\\n\\nUnmittelbarer Bedarf Sprachtechnologien werden bereits heute in unzähligen Anwendungen genutzt: Dazu gehören Chatbots und Voice Assistants, automatische Übersetzung, Texterzeugung, Textkorrektur, Text\\xadzusammen\\xadfassung, Textvereinfachung, semantische Suche, Verschlagwortung, Tutorensysteme, Informationsextraktion, Entdeckung von Fake News, automatische Klassifikation und Beantwortung von Emails, Sprachlernsoftware, und forensische Textanalyse. ist die heutige Sprachtechnologie noch stark verbesserungsfähig. Die bestehenden Defizite in der Verlässlichkeit schränken die Märkte für die Sprachtechnologien noch stark ein. Mit den immensen Leistungssteigerungen durch Foundation-Modelle können in allernächster Zukunft größere Märkte erschlossen werden, denn Basisanwendungen mit Produkten, Dienstleistungen und Vertriebskanälen existieren bereits. Deutschland hat Hunderte von Unternehmen, die von dem Technologiefortschritt profitieren würden. Andererseits wären die Produkte und Dienstleistungen dieser Firmen bedroht, wenn stattdessen nur ausländische, insbesondere US-amerikanische Anbieter mithilfe der neuen Technologie leistungsfähigere Produkte auf den Markt bringen. Die betroffenen deutschen Firmen sind als KMUs nicht in der Lage, selbst große Foundation-Modelle zu trainieren.\\n\\nIn allen diesen Anwendungsbereichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n86\\n\\nWirtschaftliches Potential Das wirtschaftliche Potential liegt nicht nur in der Verbesserung der Qualität, der Funktionalität und der Marktchancen von bestehenden Anwendungen. Durch die zusätzliche Leistungsfähigkeit werden viele neue Anwendungen möglich. Dazu gehören z.B. leicht zu bedienende Zugänge für Bürger:innen, Patientin:innen, Mitarbeiter:innen zu praktischer Information, zu Wissen und zu Vorgängen, die sie betreffen. Das gilt für öffentliche Verwaltungen, Gesundheitssysteme, Unternehmen und jedes andere Teilsystem der Gesellschaft. Als Folge der Digitalisierung wird der direkte passive und aktive Zugriff auf all diese Daten und Prozesse zwar möglich, ist jedoch oft zu schwer zu bedienen. Erst wenn der digitale Zugang so einfach ist, wie zu der Zeit, als man mit kooperativen menschlichen Ansprechpartner:innen kommunizierte, wird die Digitalisierung ihr ganzes Potential entfalten können und allseits akzeptiert werden. Neueste Chatbots wie Googles LaMDA und OpenAIs ChatGPT demonstrieren, dass Foundation-Sprachmodelle mächtig genug sind, um solche Schnittstellen zu digitalen Diensten zu realisieren. Schon sehr bald werden wir alle in der Lage sein, in unserer Muttersprache, gesprochen oder geschrieben, ohne Behördentermine, Warteschleifen oder komplexe Eingabemasken mit allen Diensten zu kommunizieren, die unseren Alltag bestimmen. In der Wirtschaft werden solche Systeme die Kommunikation mit anderen Unternehmen und mit Endkunden:innen revolutionieren. Multilinguale Modelle gestatten es Organisationen mit wenig Aufwand, Akquise und Kundenkommunikation auf andere Länder auszudehnen. Im Bildungswesen werden natürlichsprachliche Lern- oder Tutorsysteme den Wissensstand der Lernenden ermitteln, Defizite erkennen und diese gezielt durch geeignetes Lernmaterial oder personalisierte Wissenselemente und Erklärungen überwinden.\\n\\nGesellschaftliche Relevanz Unsere Sprache ist ein so wichtiges Element der menschlichen Kultur, dass wir die Technologien, die den Gebrauch der Sprache erleichtern, beeinflussen und für verschiedenste Zwecke einsetzen, selbst beherrschen, und sie für unsere Ziele anpassen müssen. Die Sprachtechnologie wird schließlich in der Zukunft eine noch wichtigere Rolle in der Kommunikation zwischen Menschen und zwischen Mensch und Technik einnehmen. Alle gesellschaftlichen Entwicklungen spiegeln sich auch in der Sprache wider, das zeigt sich z.B. in den aktuellen Bemühungen um eine Sprache, die unseren ethischen Werten, unserem Geschichtsverständnis und unserem wissenschaftlichen Weltbild entspricht.\\n\\nVon hoher gesellschaftlicher Relevanz ist auch die Multilingualität. Deutschland sieht seine eigene Zukunft als Teil einer multikulturellen und multilingualen europäischen Gesellschaft. Nur so können wir im geopolitischen Kräftespiel unsere Werte bewahren und wirtschaftlich eine Rolle in der Welt einnehmen, die es uns ermöglicht, unseren Lebensstandard zu erhalten. Eine zentrale Komponente der europäischen Integration ist das Prinzip der Gleichheit unter den beteiligten Sprachen. Selbst wenn das Englische in der wissenschaftlichen, technischen, wirtschaftlichen Kommunikation weltweit eine besondere Rolle als Lingua Franca einnimmt, haben wir allen Mitgliedern der Europäischen Union garantiert, dass ihre Sprachen erhalten und geschützt werden, unabhängig von der Zahl ihrer muttersprachlichen Sprecher:innen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n87\\n\\nEine große Barriere für den gemeinsamen digitalen Binnenmarkt der europäischen Gemeinschaft ist die Mehrsprachigkeit dieses Marktes. Im Unterschied zu unseren größten globalen Wettbewerbern, den USA und China, müssen die Endkunden der Produkte und Dienstleistungen in vielen Sprachen erreicht werden. Während US- amerikanische und chinesische Anbieter kleinere europäische Sprachen gefahrlos vernachlässigen können, dürfen europäische Anbieter das nicht. Das gilt natürlich nicht nur für digitale Produkte, ein Beispiel sind die Beipackzettel der Arzneimittelhersteller.\\n\\n2018 verabschiedete das europäische Parlament eine Resolution mit dem Titel „Gleichstellung von Sprachen im digitalen Zeitalter” (Parliament, 2018), die nicht nur die Gleichheit der Sprachen betont, sondern darüber hinaus auch Forderungen an die Sprachtechnologie richtet, um diese Gleichstellung auch in der Praxis zu erreichen.\\n\\nWissenserwerb anderen sich Foundation-Sprachmodelle Datendomänen, wie Proteinen oder DNA-Sequenzen, dadurch aus, dass über die probabilistische Modellierung sprachlicher Texte Wissen zu vielen, wenn nicht sogar zu allen Sachgebieten gelernt wird. Ein Sachgebiet könnte nur dann völlig unberücksichtigt bleiben, wenn es überhaupt nicht in digitalen Texten repräsentiert ist. Durch diese Form des Wissenserwerbs wird auch ein ungelöstes Problem der Wissensmodellierung über Knowledge Engineering in der symbolischen KI angegangen, nämlich die Schwierigkeit, die Verbindungen zwischen den Sachgebieten herzustellen, z.B. zwischen Werkstoffen und Verfahren, zwischen Werkstoffen und Marktpreisen oder zwischen synthetisierbaren Proteinen und deren Beschaffungsquellen. Durch die Vielseitigkeit des erworbenen ist das in klassischen oder auch neu definierten Wissensgebieten Wissens Anwendungspotential der Foundation-Sprachmodelle besonders hoch.\\n\\nzeichnen\\n\\ngegenüber Modellen\\n\\nin\\n\\nEntwicklungsstand und Vergleichbarkeit Für die Wahl von Foundation-Sprachmodellen spricht auch der Stand der Entwicklung. Durch das Vorhandensein von mehrjähriger gut dokumentierter Erfahrung im Training der Sprachmodelle und umfangreichen und vielseitigen Benchmarks ist es leichter, zur Spitzenforschung aufzuschließen und den eigenen Fortschritt zu kontrollieren als bei neueren Arten von Foundation-Modellen.\\n\\nVorhandene Kompetenz Auf den Gebieten der neuronalen NLP und der multilingualen Sprachtechnologie gibt es in Deutschland eine anerkannte Forschungstradition und eine starke wissenschaftliche Community mit weltweit anerkannten Spitzenforscher:innen und exzellent ausgebildeten Nachwuchskräften. Es gibt auch eine Vielzahl von älteren Sprachtechnologie Unternehmen und neuen KI-Start-ups, die die NLP-Komponenten in marktfähige Produkte integrieren können und die Anforderungen der bestehenden Märkte kennen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n88\\n\\nVerfügbare Datenbestände Durch mehrere erfolgreiche EU-Projekte zur Sammlung von Sprachdaten für europäische Sprachen haben wir Zugriff auf große Mengen von geeigneten Daten für europäische Foundation-Sprachmodelle. Auch sogenannte kleinere Sprachen sind in den Datensammlungen vertreten. Viele der Daten haben eine hohe Qualität, weil sie von öffentlichen Verwaltungen und gemeinnützigen Organisationen der EU-Mitgliedstaaten eingebracht wurden. Unter den Daten sind auch viele parallele bilinguale und multilinguale Textkorpora. Die dadurch verfügbaren Sprachdaten erhöhen die Erfolgswahrscheinlichkeit für Sprachmodelle, die den Anspruch haben, den speziellen europäischen Anforderungen zu genügen, und sie lassen hoffen, dass diese Foundation- Modelle zumindest nach diesem Kriterium deutliche Vorteile gegenüber den großen bestehenden Modellen bieten.\\n\\nPotential für künftige Entwicklungen Neuere Entwicklungen von multimodalen und multimedialen Sprachmodellen verbinden die Daten der repräsentierten Datendomänen untereinander. Das bewirkt, dass man sich mit Hilfe von textueller Eingabe auch thematisch passende Bilder oder Videos generieren lassen kann. Diese Funktionalität wird als cross-modal (transmodal) bezeichnet. Der Sprache kommt hierbei jeweils eine besondere Bedeutung zu, denn sie ist geeignet um die Konzepte und Kriterien der Benutzer:innen auszudrücken. In der Zukunft wird die Zahl der Datendomänen zunehmen, wobei die Sprache voraussichtlich immer die semantische Basis für die Kombinationen darstellen wird. Daher können neue Kombinationen durch die Erweiterung von Sprachmodellen hergestellt werden.\\n\\nAus diesen Gründen empfehlen wir, die Entwicklung von Foundation-Modellen mit großen europäischen Sprachmodellen zu beginnen, denn hier liegt das größte Potential für die deutsche Wirtschaft und die Nachfrage ist besonders hoch. Schrittweise sollen die Foundation-Modelle dann auch um weitere Datenarten erweitert werden.\\n\\n6.1 Erste europäische multilinguale Foundation-Sprachmodelle\\n\\nMit Blick auf Sprachmodelle ergeben sich eine Vielzahl an Fragen für die konkrete Umsetzung und Weiterentwicklung. Im Folgenden werden wir ausführen, welche Architekturen und Daten den Sprachmodellen zugrunde gelegt werden sollen und welche Prioritäten die Entwicklung und die Evaluation der Modelle bestimmen sollen.\\n\\nArchitektur der Basis Foundation-Modelle Für die Basisarchitektur gibt es derzeit keine ernsthafte Alternative zum Transformer- Ansatz. Fast alle großen Foundation-Sprachmodelle folgen diesem Ansatz, der sich auch für Bild-, Video- und Proteindaten als tragfähig erwiesen hat und sich somit auch für die geplanten Erweiterungen um multimodale und multimediale Daten eignet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n89\\n\\nNun gibt es allerdings verschiedene Ausformungen des ursprünglichen Transformer Modells, die grob in drei Klassen fallen: (i) Encoder-Decoder Modelle, die mehr oder weniger dem Ausgangsmodell entsprechen, (ii) Encoder Modelle und (iii) Decoder Modelle.\\n\\nAm erfolgreichsten sind bisher Modelle der Klassen (i) und (iii), wobei die allergrößten Modelle häufiger in die Klasse (iii) fallen, was aber sicher daran liegt, dass der Trainingsaufwand für große Decoder Modelle sehr viel geringer ist.\\n\\nDie Fachwelt geht aber mehrheitlich davon aus, dass Modelle der Klasse (i), also volle Transformer Modelle, Vorteile für die bestimmte anspruchsvolle Aufgabenstellungen bieten, die die tiefere Analyse der Eingabe und die dadurch erzeugte abstrakte Repräsentation der semantischen Beziehungen nutzen können. Dazu gehören die Beantwortung von Fragen und andere Aufgaben, die Schlussfolgerungen erfordern, sowie die automatische Übersetzung.\\n\\nBisher konnten Decoder-Modelle die Nachteile der einfacheren Architektur durch Größe, eine größere Menge von Lerndaten und eine höhere Zahl von Parametern ausgleichen. Besonders die Performanz von ChatGPT, des neuesten Modells der GPT-3 Klasse, demonstriert auf beeindruckende Weise, dass mit dem Decoder-Modell (zusammen mit geeigneten Verfahren des Nachtrainierens) Funktionalitäten möglich sind, die einen hohen Grad an semantischer Abstraktion erfordern.\\n\\nFür die erste Generation an Modellen wird daher empfohlen, die Decoder Architektur der GPT-Modelle zu verwenden. Das bietet die folgenden Vorteile:\\n\\nschneller Kompetenzaufbau durch die Verwendung der einfacheren Architektur ● bessere Vergleichbarkeit mit den neuesten GPT-Modellen und mit europäischen Modellen (BLOOM, Luminous, GPT-SW3)\\n\\ndurch die Vergleichbarkeit eine leichtere Evaluation der Beiträge der zusätzlichen Daten\\n\\nkürzere Zeit bis zu ersten verwendbaren Ergebnissen\\n\\nVersionen der ersten Modelle sollen sich von den bestehenden Vorbildern aber auch durch die Auswahl der Lerndaten unterscheiden. Zusätzlich zu den bisher eingesetzten gecrawlten Webdaten werden spezielle europäische Korpusdaten verwendet, die sowohl die europäischen Sprachen als auch die relevanten Gegenstandsbereiche besser abdecken. Einen besonderen Effekt für die angestrebte Multilingualität erwarten wir vom Einsatz paralleler bilingualer und multilingualer Korpora.\\n\\nfür die Wirtschaft\\n\\nDaneben werden die Trainingsdaten auch um Wissensdaten erweitert, die aus großen Wissensgraphen stammen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n90\\n\\n6.1.1 Lerndaten\\n\\nLerndaten für Foundation-Sprachmodelle sind große Mengen von digitalen Textdaten, in denen die europäischen Sprachen bestmöglich repräsentiert sind.\\n\\nDie zentrale Komponente der Trainingsdaten für die größten Sprachmodelle sind sehr große Textvolumina, die durch Webcrawling kuratiert wurden. Die gemeinnützige Organisation Common Crawl sammelt und archiviert seit 2011 große Teile des World Wide Webs. Diese werden kostenlos für Forschung, Entwicklung und andere Nutzungen zur Verfügung gestellt, seit 2013 im Web-Archivformat WARC.\\n\\nAufbauend auf den Datenbeständen von Common Crawl wurden die Web-Crawl Corpora zusammengestellt und gereinigt, die den größten Foundation-Sprachmodellen zugrunde liegen. Eine Auswahl dieser Datensätze soll hier vorgestellt werden.\\n\\nC4 - Colossal Clean Crawled Corpus Das ist insbesondere der Korpus C4 (Colossal Clean Crawled Corpus), der sowohl exklusiv für das Englische, aber auch in multilingualen Varianten existiert (Raffel et al., 2020) Das Attribute „Clean“ bezieht sich auf mehrere Verfahren zur Datensäuberung, die eingesetzt wurden, um echte monolinguale Texte von Mischdaten zu trennen.\\n\\nZu den Säuberungsverfahren gehörte auch der Einsatz von Blockierlisten (Blocklists), Listen von Wörtern, an denen man obszöne, rassistische und anderweitig anstößige Texte zu erkennen hoffte. Beispiele sind die Lists of Dirty, Naughty, Obscene, and Otherwise Bad Words (LDNOOBW), die auf GitHub für das Englische und ca. 25 weitere Sprachen angeboten werden. Im C4 Korpus wurden nun alle Webseiten ausgefiltert, auf denen sich mindestens eines der anstößigen Wörter in der jeweiligen Sprache fand. Man nahm an, dass es bei der großen Menge an Daten weniger Probleme geben würde, wenn nach diesem groben Kriterium mitunter auch ungerechtfertigt gefiltert würde, als durch die Aufnahme anstößiger Inhalte in die KI-Modelle.\\n\\nEs konnte dann aber von Kritiker:innen gezeigt werden, dass die Löschung von Texten mit sexuellen Bezügen oder mit Slangausdrücken dazu führte, dass wichtige Bereiche der Gesellschaft in den so „gereinigten“ Texten unterrepräsentiert waren, unter anderem Teile der LGBTQ-Gemeinschaft oder ethnische Minderheiten (Dodge et al., 2021). Zum Glück gibt es aber auch „noblocklist“ Versionen des C4 Korpus, auf die diese lexikalischen Filter nicht angewandt wurden.\\n\\nmC4 Für multilinguale Foundation-Sprachmodelle wurde das Webkorpus mC4 geschaffen, eine spezielle multilinguale Fassung des C4 Korpus, dessen Vorteile durch die Performanz des mit mC4 trainierten Modells mT5 demonstriert werden konnten (Xue et al., 2021).\\n\\nDas Korpus mC4 enthält 27 TB Textdaten für 101 Sprachen. Die best repräsentierte Sprache ist natürlich Englisch mit 10401 GB, während das westafrikanische Yoruba mit nur mehr 0,158 GB in diesem Korpus das Schlusslicht bildet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n91\\n\\nDas Englische macht mit 2733 Mrd. Token 5,67 % der Daten aus. Deutsch liegt mit 1404 GB oder 347 Mrd. Token nach Russisch und Spanisch an vierter Stelle, was 3,05 % entspricht. Kleinere europäische Sprachen wie Litauisch, Estnisch und Lettisch sind noch mit jeweils 11, 6,9 und 7,9 Mrd. Tokens vertreten. Die kleinsten im Korpus präsenten europäischen Sprachen sind Luxemburgisch mit 1 Mrd. und Irisch mit 0,5 Mrd. Tokens. Im Gegensatz zum Luxemburgischen ist das Irische seit dem 1. Januar 2022 sogar eine der Arbeitssprachen der Europäischen Union.\\n\\n6.1.2 Europäische Projekte\\n\\nGlücklicherweise gab es in Europa seit 2010 eine Reihe von Projekten, die europäische Sprachdaten für die Forschung und Wirtschaft kuratieren und verfügbar machen. Diese von der EU in verschiedenen Programmen geförderte Vorhaben sollten die europäische KI, in die Lage versetzen, die europäische Sprachenvielfalt technologisch beherrschbar zu machen und somit für die europäische Wirtschaft und den gemeinsamen digitalen Binnenmarkt Wettbewerbsschranken abzubauen.\\n\\ninsbesondere die Sprachtechnologie,\\n\\nDazu gehören insbesondere die Vorhaben ParaCrawl und European Language Resource Coordination (ELRC), gefördert im Programm Connecting Europe Facility (CEF), und das European Language Grid (ELG), gefördert im Programm Horizon 2020.\\n\\nParaCrawl ParaCrawl hat von 2017 bis 2021 in drei Phasen parallele Texte in den europäischen Sprachen durch Web-Crawling kuratiert. Das Ergebnis ist ein paralleler Datensatz mit 41 Sprachpaaren, die hauptsächlich Englisch mit einer anderen Sprache verbinden (39 von 41). Neben den europäischen Sprachen enthält ParaCrawl auch Daten für neun ressourcenarme, außereuropäische Sprachen.\\n\\nELRC - European Language Resource Coordination Der Schwerpunkt des Infrastrukturvorhabens ELRC, das im Januar 2023 abgeschlossen wird, lag auf der Stärkung der europäischen Übersetzungstechnologie. Die europäische Wirtschaft und Gesellschaft soll auch unabhängig von Google Translate und den anderen großen Übersetzungsdiensten der amerikanischen Hyperscaler in der Lage sein, Sprachbarrieren zu überwinden, ohne dass dabei die Stellung von europäischen Sprachen, auch nicht der kleineren und kleinsten Sprachen, leidet. Daher lag der Schwerpunkt von ELRC auf der Sammlung oder Erzeugung von bilingualen und multilingualen parallelen Korpora, die dann zum überwachten Trainieren von maschinellen Übersetzungssystemen eingesetzt werden können. Eine Herausforderung bestand darin, hochqualitative und wirtschaftlich bzw. gesellschaftlich relevante Korpora in den Staaten der EU zu kuratieren, und das von Organisationen in allen EU- Mitgliedsländern.\\n\\nELRC hat insgesamt über 200 Milliarden Wörter an hochqualitativen Sprachdaten in Europa gesammelt und aufgearbeitet und ein europaweites ELRC Netzwerk in allen EU- Mitgliedsstaaten etabliert. Die Daten enthalten 5600 Ressourcen: bilinguale und\\n\\nGroße KI-Modelle für Deutschland\\n\\n92\\n\\nmultilinguale parallele Korpora sowie monolinguale Textkorpora. Es gibt mehr als 880 parallele Korpora mit mehr als 1 Mio. Wörtern, darunter mehr als 230 mit mehr als 10 Mio. Wörtern und mehr als 50 mit über 100 Mio. Wörtern. Die europäischen Nationalsprachen sowie die Sprachen der größten Minderheiten sind in den Daten vertreten. Über 950 Ressourcen enthalten deutsche Daten und selbst kleine Sprachen wie das Irische und das Maltesische sind noch mit über 300 bzw. 200 Ressourcen recht gut vertreten.\\n\\nELG - European Language Grid Das Projekt ELG European Language Grid hat unter seinem Namen eine Plattform geschaffen, auf der zehntausende sprachtechnologische Ressourcen für die Nutzung bereitgestellt werden, die meisten davon unter nichtkommerziellen Lizenzbedingungen. Unter diesen Ressourcen finden sich auch tausende von Textkorpora, darunter auch sehr viele für europäische Sprachen, die in den großen Webkorpora zu wenig repräsentiert sind.\\n\\nEFNIL - European Federation of National Institutions for Language Im Dachverband EFNIL (European Federation of National Institutions for Language) haben sich nationalen Sprachinstitutionen der europäischen Staaten zusammengeschlossen. Zu diesen Institutionen gehören Sprachinstitute wie das Institut für Deutsche Sprache in Mannheim oder das Institut für die Tschechische Sprache in Prag, aber auch Einrichtungen der Regierungen wie der Dänische Sprachrat oder die Generaldelegation für die Französische Sprache. Viele dieser Einrichtungen verwalten und pflegen nationale Korpora, große, gut gepflegte und mehr oder weniger repräsentative Textsammlungen für ihre jeweilige Sprache oder Sprachen. In der Vergangenheit waren diese Daten für die Sprachtechnologie meist nicht zugänglich, weil die jeweils geltenden Eigentums- oder Urheberrechte dieser Nutzung im Wege standen.\\n\\nOPUS Neben diesen EU-Projekten gibt es bereits seit 2004 die Initiative OPUS des Nordic Natural Language Processing Lab, die jetzt über drei Millionen parallele Texte mit über 100 Millionen Sätzen frei zur Verfügung stellt.\\n\\neTranslation Die meisten europäischen Organisationen in Wirtschaft, Politik und Zivilgesellschaft schrecken aus Sicherheitsgründen davor zurück, die Übersetzungsdienste großer multinationaler Unternehmen für Übersetzung, Suche und Dolmetschen zu verwenden. Für die reine Textübersetzung hat die Europäische Kommission bereits 2017 den Übersetzungsdienst eTranslation geschaffen, der europäischen Organisationen die kostenlose Übersetzung zwischen allen Arbeitssprachen der EU anbietet.\\n\\nDieser Dienst der Generaldirektion Übersetzung der Europäischen Kommission setzt bereits neuronale Übersetzungssysteme für viele Sprachpaare ein, wobei in der Regel immer zwischen Englisch und einer weiteren europäischen Sprache übersetzt wird und indirekt über das Englische als Zwischensprache alle anderen Übersetzungen vorgenommen werden. Die Europäische Kommission verfügt über keine Foundation- Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n93\\n\\nObwohl eTranslation in der Qualität nicht an Google Translate heranreicht, wird der Dienst für Informationsübersetzungen verwendet, was nicht nur an den Sicherheitsanforderungen der Nutzerorganisationen liegt, sondern auch an den Stärken des europäischen Systems im Hinblick auf europaspezifische Sprache und Inhalte. Pro Jahr werden ca. 300 Mio Seiten übersetzt.\\n\\nvon\\n\\nvielen Organisationen\\n\\nfür\\n\\neine\\n\\nRohübersetzung\\n\\nbzw.\\n\\n6.1.3 Die Bedeutung paralleler Sprachdaten\\n\\nEs ist eine offene Forschungsfrage, welche Rolle parallele Sprachdaten längerfristig in der Welt der Foundation-Modelle spielen werden.\\n\\nWenn parallele Sprachdaten bereits vorhanden sind, kann man diese als Übersetzungspaare direkt in die Daten für das Pretraining übernehmen, wobei das Modell dann ganz alleine lernt, dass es sich um Paare von bedeutungsgleichen Sätzen in den jeweiligen zwei Sprachen handelt. Man kann aber auch das überwachte Trainieren der Übersetzungsfähigkeit in das Pretraining integrieren, ohne dass zusätzlich Kosten für die Datenproduktion oder -annotation anfallen. Die parallelen Daten können dann natürlich auch noch als monolinguale Daten für die Ergänzungs- oder Ersetzungsaufgaben des selbstüberwachten Pretrainings verwendet werden.\\n\\nEs wurde aber beobachtet, dass sich die Übersetzungsfähigkeit als eine emergente Funktionalität einstellt, sobald nur hinreichend große Textvolumina für die Einzelsprachen in den Pretrainingsdaten vorhanden sind. Textübersetzung wird somit zu einer Zero-Shot oder höchstens zu einer Few-Shot Anwendung des multilingualen Foundation-Modells. Für Sprachen, die nicht hinreichend in den Lerndaten repräsentiert sind, müsste das Modell dann weiterhin durch überwachtes Lernen nachtrainiert werden. Somit verringert sich der Bedarf an parallelen Korpora, die in ihrer bestehenden Menge begrenzt und teuer zu produzieren sind.\\n\\nIn ihrem Forschungsbericht nmT5 - Is parallel data still relevant for pre-training massively multilingual language models? zeigen Kale et al. (2021) jedoch, dass parallele Korpora wegen der Knappheit der Daten für viele Sprachen immer noch eine wesentliche Bedeutung für die Anwendbarkeit der Modelle in diesen Sprachen und für die Qualität der Übersetzungen haben.\\n\\nDieses Alleinstellungsmerkmal hat großes wirtschaftliches Potential, das über Anwendungen für Textübersetzung und multilinguale parallele Texterzeugung weit hinausgeht. Ein weiteres Anwendungsgebiet ist die translinguale Suche (crosslingual search), und zwar sowohl die Suche nach Dokumenten als auch die Suche nach Informationen und Wissensinhalten. Wenn die Bürger:innen oder Kund:innen, insbesondere Menschen, die nicht Englisch als Muttersprache haben, in ihrer eigenen Sprache in vielsprachigen Inhalten suchen können, erleichtert das ihr Leben. Eine andere Anwendung ist die Unterstützung von bilingualer und multilingualer Konversation, wie z.B. durch eine simultane Dolmetscherfunktion für Beratungen, Besprechungen und Verhandlungen. Zoom bietet, aufbauend auf deutscher Übersetzungstechnologie, bereits\\n\\nGroße KI-Modelle für Deutschland\\n\\n94\\n\\ndie fast-simultane Übersetzung zwischen neun großen Sprachen an, darunter fünf EU- Sprachen, aber eine Erweiterung auf die Breite der europäischen Sprachen ist schon alleine wegen des Mangels an geeigneten Trainingsdaten nicht in Sicht.\\n\\n6.1.4 Empfehlung\\n\\nTrotz der Tatsache, dass die europäischen Sprachen, insbesondere die sogenannten kleineren Sprachen, in den Korpora noch nicht hinreichend repräsentiert sind, sollten zuerst die bestehenden Korpora C4 bzw. mC4 für das Training der multilingualen europäischen Foundation-Modelle eingesetzt werden. Dies erlaubt zum einen eine Vergleichbarkeit mit anderen großen Sprachmodellen und verhindert, dass Daten fehlen, die essentiell zur Performanz der bekannten Modelle beigetragen haben. Zum anderen erlaubt es, die Beiträge der speziellen europäischen Daten für relevante Anwendungen besser beurteilen zu können.\\n\\nEs gibt bisher keine Untersuchungen darüber, zu welchen Anteilen sich die Daten in den durch die europäischen Projekte bereitgestellten Korpora mit den Webdaten der großen amerikanischen Foundation-Modelle überlappen. Dadurch, dass Common Crawl aber keine Daten einsammelt, die nur nach Registrierung zugänglich sind und zudem die Crawling-Bestimmungen (nofollow, robots.txt) der originären Websites respektiert, ist anzunehmen, dass über die europäischen Projekte große Volumina an zusätzlichen Daten für das Training verwendet werden können.\\n\\nELRC und ELG werden vom DFKI koordiniert. Die Datenhaltung und -bereitstellung für beide Projekte wird vom Projektpartner in Athen verantwortet. Die Projektkoordinatoren und der Direktor des ILSP haben ihre Bereitschaft bekundet, die Entwicklung europäischer Modelle bei der Datenkuratierung aus ihren Beständen zu unterstützen. Jahrestagung den nationalen Im September 2021 wurden auf der EFNIL Sprachinstitutionen die Möglichkeiten und Absichten bezüglich europäischer Foundation- Modelle vorgestellt. Die erste Kommunikation mit EFNIL Mitgliedsorganisationen ergab deren grundsätzliche Bereitwilligkeit, ihre Korpusdaten unter kontrollierten Bedingungen für das Training von Foundation-Modellen zur Verfügung zu stellen.\\n\\nILSP\\n\\nDas Institut für Deutsche Sprache hat seine Bereitschaft erklärt, die hochqualitativen Textkorpora des Instituts von rund 40 Milliarden Wörtern für das Training europäischer Modelle verfügbar zu machen und das Vorhaben zu Aspekten der Repräsentanz der deutschen Sprache in den europäischen Foundation-Modellen bei Bedarf auch wissenschaftlich zu unterstützen.\\n\\nDarüber hinaus sammelt das Projekt OpenGPT-X Daten für das Training eines europäischen Sprachmodells. Die dort gemachten Erfahrungen und verwendeten Daten können auch über das Projekt hinaus Anwendung finden.\\n\\nDiese Daten, zusammen mit dem Vorhandensein der parallelen Sprachdaten für europäische Sprachen, ermöglichen das Training erster europäischer Foundation- Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n95\\n\\nDurch die Verwendung der parallelen Daten im Pretraining gibt es die Möglichkeit, mehrere Ziele zu erreichen:\\n\\n(1) die Erfüllung der besonderen europäischen Anforderungen auf Überwindung der\\n\\nSprachgrenzen\\n\\n(2) eine bessere Berücksichtigung der kleineren europäischen Sprachen\\n\\n(3) die Schaffung eines besonderen Alleinstellungsmerkmals der Modelle gegenüber\\n\\nden bestehenden Foundation-Modellen\\n\\nSPOTLIGHT Bayer AG An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nBayer ist ein weltweit tätiges Unternehmen mit Kernkompetenzen auf den Life-Science-Gebieten Gesundheit und Ernährung. Mit seinen Produkten und Dienstleistungen will das Unternehmen Menschen nützen und die Umwelt schonen, indem es zur Lösung grundlegender Herausforderungen einer stetig wachsenden und alternden Weltbevölkerung beiträgt.\\n\\nDr. Marion Legler, Head of Decision Science & Advanced Analytics, Bayer Pharma.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir verwenden KI-Foundation-Modelle, um große Textmengen automatisch zu verarbeiten und unseren Experten zu helfen, schnell relevante Informationen zu finden. Zum Beispiel, die Modelle:\\n\\nSie konvertieren die von den Ärzten während der klinischen Studien verfassten Texte in standardisierte medizinische Diagnosecodes, was die anschließende manuelle Überprüfung durch unsere Experten vereinfacht. ● Lesen Sie die gesamte medizinische Literatur und die Berichte über klinische Studien, um diejenigen zu identifizieren, die für die Therapiegebiete und Behandlungen von Bayer besonders relevant sind.\\n\\nTausende von Dokumenten der Zulassungsbehörden (EMA, FDA usw.) werden durchgesehen und die Themen in jedem Teil der Dokumente automatisch klassifiziert, so dass die Informationen leicht auffindbar sind. Scannen Sie die von Patienten erhaltenen Mitteilungen auf unerwünschte Ereignisse. ● Gruppierung der von Bayer-Vertretern nach Besprechungen mit Ärzten verfassten Erkenntnisse, um aufkommende Diskussionsthemen zu entdecken.\\n\\nWir verwenden auch vortrainierte Computer-Vision-Modelle für Anwendungsfälle, in denen nur begrenzt kommentierte Bilder zur Verfügung stehen, wie z. B. im Zusammenhang mit bestimmten Krebstumoren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n96\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Die meisten Anwendungsfälle, die durch diese Modelle ermöglicht werden, waren vorher nicht möglich, insbesondere solche, bei denen es darum geht, große Mengen von Dokumenten zu erkennen. In anderen Fällen, wie der Kodierung in klinischen Studien oder der Erkennung von unerwünschten Ereignissen in Texten, unterstützen die KI-Modelle die manuelle Arbeit der menschlichen Experten und sparen etwa 50 % der Zeit, die diese für die sich wiederholenden Aufgaben aufwenden.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Da die meisten großen Sprachmodelle nur in englischer Sprache verfügbar sind, müssen wir oft erst aus anderen Sprachen ins Englische übersetzen und dann die Modelle verwenden. Der Inhalt und die Feinheiten des Textes gehen bei der Übersetzung sicherlich verloren. Open-Source-Modelle, die alle europäischen Sprachen abdecken, könnten dazu beitragen, dass Patient:innen, Ärzt:innen oder Behörden, die sich in verschiedenen Sprachen äußern, gleichermaßen berücksichtigt werden. Zwar gibt es solche Modelle bereits (BLOOM), doch werden sie derzeit kaum genutzt, da sie auf allgemeine, nicht- medizinische Texte trainiert sind. Die Herausforderung, jemals ein brauchbares mehrsprachiges Modell für Gesundheitsanwendungen zu haben, besteht darin, dass die wissenschaftliche Literatur und die Dokumente, die zum Trainieren medizinischer Sprachmodelle verwendet werden, ursprünglich nur auf Englisch geschrieben sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n97\\n\\n6.2 Vermeidung von Falschaussagen, Bias und Toxizität\\n\\nWie in Kapitel 2.3 beschrieben, wurden bei neuronalen Modellen, die auf großen Mengen von Webdaten trainiert wurden, mehrere Formen von inadäquatem Verhalten beobachtet. Um die Foundation-Modelle so zu gestalten, dass sie den Kriterien für in Wirtschaft und Gesellschaft genügen, muss solches Verhalten Anwendungen verhindert oder minimiert werden. Deshalb sollte die Verhinderung des Fehlverhaltens eine zentrale Priorität bei Entwurf, Training und Evaluation der Modelle sein.\\n\\nFalschaussagen Die deutlichste Form von Falschverhalten sind fehlerhafte Aussagen. Das Modell wird solche Aussagen tätigen, wenn trotz des Pretrainings Wissenslücken verbleiben oder Wissensinkonsistenzen durch widersprüchliche Trainingsdaten erst entstehen. Zudem können Foundation-Modelle nicht zwischen faktisch korrekten Aussagen und plausiblen inkorrekten Aussagen unterscheiden, die ja auch im Bereich der Literatur oft zu finden sind. Die Methoden zur Behebung sind eine bessere Datenauswahl und die Hinzunahme von Wissen aus nichtsprachlichen Wissensbeständen (s. Kapitel 2.3).\\n\\nBias Schwieriger zu entdecken und auch schwerer zu verhindern sind Unausgewogenheiten bis hin zu Voreingenommenheiten, falschen Verallgemeinerungen und Fehlurteilen mit ethisch verwerflichen Konsequenzen. Diese werden oft unter dem Sammelbegriff Bias subsumiert. Häufig kommentierte Formen von Bias sind Voreingenommenheiten in Bezug auf Geschlecht und ethnische oder soziale Herkunft, die wir auch bei Menschen finden. (ref)\\n\\nAllerdings muss nicht jeder Bias negativ sein, es kann z.B. vorkommen, dass im Zusammenhang mit einem Produkt die häufigere Erwähnung von seltenen Gefahren im Vergleich zur Erwähnung der Vorteile des Produkts zu Aussagen führt, die einen vorsichtigen Gebrauch nahelegen. Es ist auch nicht möglich, alle Formen von negativen Bias vorherzusehen, weil diese von von Problembewusstsein und Entwicklungen bestimmten zeitabhängigen Sensibilisierungen bestimmt werden.\\n\\ngesellschaftlichen\\n\\nVersuche, präventiv alle Quellen von negativem Bias aus den Trainingsdaten zu tilgen, sind daher unrealistisch. Stattdessen versuchen Entwickler:innen, durch gezieltes Nachtrainieren bereits bekannte Formen von Bias durch adäquates Antwortverhalten zu überschreiben. Einen ähnlichen Ansatz haben die Entwickler von ChatGPT gewählt, die damit wirksam auf Kritiken bezüglich der früheren GPT-Modelle reagiert haben.\\n\\nZum adäquaten Antwortverhalten gehört auch eine Pluralität und Ausgewogenheit in der Nennung von alternativen Antworten. Die Ergänzungsaufgaben des Pretrainings reichen nicht aus, um ein solches Verhalten zu erreichen. Wenn ein Modell bei mehreren möglichen Antworten nicht einfach die wählen soll, die statistisch durch die Lerndaten präferiert ist, muss ein Sprachmodell das angemessene Antwortverhalten durch dediziertes Training erlernen. Auch hier hat ChatGPT vorgemacht, wie dieses Ziel durch Nachtrainieren erreicht werden kann. OpenAI hat diese Verbesserungen im Wesentlichen\\n\\nGroße KI-Modelle für Deutschland\\n\\n98\\n\\ndurch Bestärkungslernen (Reinforcement Learning) erreicht, das heißt in diesem Fall die systematische Korrektur der Gewichte durch die Reaktion von Testbenutzern.\\n\\nAus den Testeingaben zusammen mit den Antworten und Reaktionen der Testbenutzer:innen lassen sich annotierte Lerndaten generieren, so dass man spätere Modelle dann überwacht nachtrainieren kann und so die Kosten für die manuelle Leistung spart.\\n\\nZusätzlich zum Nachtrainieren bietet sich auch eine bewährte einfachere Methode für die Verbesserung von Antwortverhalten an: die Einbettung der Benutzer:inneneingaben in Prompts, die in einem oder mehreren Sätzen spezifizieren, welche Form der Antwort erwartet wird. Diese Methode wird auch vielfach eingesetzt, um Inhalt und Form der erwarteten Antworten auf die Anforderungen spezifischer Anwendungen anzupassen.\\n\\nToxizität Das Phänomen der Toxizität reicht von der Verwendung obszöner oder ethisch anstößiger Ausdrücke bis hin zu Äußerungen, die von Menschen als Ausdruck von Hass oder Verachtung interpretiert werden können oder auf andere Weise als beleidigend oder verletzend empfunden werden.\\n\\nVersuche, solche Ausgaben durch Zensur der Lerndaten zu erreichen, also durch das automatische Ausfiltern von Texten, die bestimmte anstößige Wörter oder Ausdrücke enthalten, sind nicht das geeignete Mittel, um Toxizität zu verhindern. Viele Wörter, die auf die Listen der anstößigen Wörter (Blocklists) gelangt sind, haben auch Verwendungen, die durchaus akzeptabel sind. Das Ausfiltern der harmlosen Verwendungen würde auf der Datenseite ganze Themenbereiche schwächen. Zum anderen können nicht alle Beleidigungen oder Verächtlichmachungen an den verwendeten Wörtern alleine erkannt werden. Es zeigt sich aber, dass die neueren Sprachmodelle selbst eine abstrakte Zuordnung von sprachlichen Ausdrücken zu sprachlichen Stilen und Register erlernen. Viele offensichtliche Formen der Toxizität kann das Modell nach Bestärkungslernen eigenständig vermeiden. Der Sprachgebrauch von ChatGPT erscheint im Vergleich zu früheren Foundation-Modellen sehr vorsichtig, ja fast schon konservativ.\\n\\nOb sich aber verlässlich alle Äußerungen vermeiden lassen, deren Aussagen oder Präsuppositionen von sensibilisierten Benutzer:innengruppen als beleidigend oder verletzend empfunden werden können, ist eine offene Frage. Dieses Thema ist ja auch eine Herausforderung für menschliche Textproduzent:innen. Das Problem wird sich aber ohnehin auch nur bei ganz speziellen Anwendungen stellen.\\n\\nEmpfehlung Für alle Formen von inadäquatem Antwortverhalten gilt: Selbst wenn es nicht sofort möglich ist, alle Formen dieses Fehlverhaltens für alle Anwendungen zu 100 % auszuschließen, so ist es doch für den Erfolg der Foundation-Modelle und deren Akzeptanz in Wirtschaft und Gesellschaft essentiell, Art, Grad und Häufigkeit von potentiellen Fehlverhalten empirisch zu bestimmen bzw. vorherzusagen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n99\\n\\nEs sollten deshalb schon während der Entwicklungszeit der ersten Modelle eine Testbatterie von Eingabeprompts für alle bekannten Arten von Bias und Toxizität zusammenstellen, die geeignet sind, Ausdrücke des Fehlverhaltens hervorzurufen und in ihrer Häufigkeit zu messen. Die ständige Evaluation und Verbesserung der Modelle in Bezug auf diese Probleme sollte ein zentrales Forschungsthema sein.\\n\\n6.3 Verbindung\\n\\nvon\\n\\nFoundation-Modellen\\n\\nmit\\n\\ngroßen\\n\\nWissensbeständen\\n\\nNeben dem aus Texten gewonnenen Wissen können auch bereits explizit formalisiertes Wissen für Foundation-Modelle verfügbar gemacht werden. Explizites Wissen kann in Datenbanken, Ontologien oder Wissensgraphen kodiert sein. Heutzutage werden für die Repräsentation großer Wissensbestände hauptsächlich Wissensgraphen (engl. Knowledge-Graphen, kurz KG) eingesetzt, weil sie die Vorteile von Ontologien und Datenbanken verbinden und sich im großvolumigen Einsatz bewährt haben.\\n\\nDie Nutzung von KGs soll mehrere Probleme der Foundation-Modelle lösen oder reduzieren:\\n\\nFalsche Antworten bei unzureichendem Wissen oder unzureichender Konfidenz: Neuronale Modelle geben in solchen Situationen manchmal ganz falsche Antworten. Auch ohne direkt nach ihnen gefragt zu werden, können Modelle mitunter in Ausgaben auch Fakten behaupten, die nicht der Wahrheit entsprechen. Das gehört in den Bereich der sogenannten Halluzinationen.\\n\\nDynamik des Wissens: Änderungen im Wissen oder gänzlich neue Fakten sind oft in den Trainingsdaten noch nicht repräsentiert oder im Vergleich zum überholten Wissen statistisch unterrepräsentiert.\\n\\nLücken im Detailwissen: KGs enthalten auch Details, die wegen mangelnder allgemeiner Relevanz im frei zugreifbaren Internet nicht zu finden sind, z.B. gewisse Teile von Produktspezifikationen, Mitgliederlisten, Messwerte usw.\\n\\nZur Nutzung der Wissensrepositorien gibt es drei vielversprechende Ansätze:\\n\\n(1) Die Aufnahme der Wissensbestände in die Trainingsdaten (z.B. KELM) (2) Der Zugriff des Modells auf die Wissensbestände als Teil der Inferenz (3) Die Berücksichtigung von Wissen in großen Textkorpora durch Retrieval\\n\\nBeim ersten Ansatz kann man die Wissenselemente des KG, sogenannte RDF Triple, automatisch in Sätze einer natürlichen Sprache umwandeln, wobei sich wegen der Benamung in den KGs das Englische anbietet. Alternativ kann man die RDF-Triples aber auch in der RDF-Syntax wie Sätze einer eigenen Sprache zu den multilingualen Trainingsdaten hinzufügen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n100\\n\\nDie Hinzunahme des Wissens zu den Trainingsdaten hat den Vorteil, dass sie bereits beim Pretraining das Weltwissen des Models verbessern. Außerdem besteht die Hoffnung, dass die Projekte erlernen können, zwischen einfachen sprachlichen Aussagen und den entsprechenden Wissensgraph-Fragmenten zu übersetzen ähnlich der Übersetzung zwischen natürlichen Sprachen.\\n\\nBeim zweiten Ansatz erlernt das Modell, durch SPARQL Queries auf benötigtes Wissen zuzugreifen und dieses für die Berechnung der endgültigen Beantwortung zu berücksichtigen. Dieser Ansatz hat den Vorteil, dass alles neue Wissen im KG unmittelbar ohne Nachtraining verfügbar wird.\\n\\nBeide Ansätze werden gegenwärtig bereits im Projekt Open GPT-X getestet. Folgeprojekte werden in der Realisierung dieses Ziels auf Erkenntnissen und praktischen Resultaten dieses Vorläufer-Vorhabens aufbauen können.\\n\\nGeeignete Kandidaten für Wissensrepositorien sind dabei DBPedia und WikiData zwei große generische Wissensgraphen, die von großen Wissensgemeinschaften (Knowledge Communities) gepflegt werden und kostenfrei nutzbar sind. In diesen Ressourcen ist allerdings das Wissen über die europäischen Wirtschaftsunternehmen nicht vollständig und auch nicht immer für alle Firmen aktuell. Ins Auge gefasst sollte daher auch die Einbeziehung von Spezialwissensquellen wie OpenCorporates oder alternative kommerzielle Angebote.\\n\\nEin dritter Ansatz sind Retrievalverfahren zur Nutzung zusätzlicher großer Textdatenbestände. Hierbei (z.B. es Suchmaschinenergebnisse) handeln, die noch nicht in die Trainingsdaten eingeflossen sind. Zum anderen können das auch interne Daten (z.B. Servicereports) sein, die als Zusatzinformation für das Modell wertvoll sein können. Über ein embedding-basiertes Retriever-Reader-Modell können sie in die Antworterzeugung einfließen und so Fehler in den erzeugten Texten reduzieren und aktuelle Informationen verwenden. Multilinguale Retriever-Reader-Modelle können dabei die Information in unterschiedlichen Sprachen nutzen.\\n\\nkann\\n\\nsich\\n\\num\\n\\naktuelle\\n\\nTexte\\n\\n6.4 Kombination von Sprache mit anderen Modi und Medien\\n\\nAuf die Frage nach dem Faszinierenden an Foundation-Modelle bemerkte Prof. Andreas Dengel: „Von Forschungsseite ist natürlich Multimodalität sehr spannend.\" Als zusätzliche Medien kommen insbesondere Bilder, Videos, gesprochene Sprache, Audio, und 3D- Modelle in Betracht, die mit Text kombiniert werden können. Yann LeCun, Forschungschef von Meta, geht noch einen Schritt weiter: „Anstelle von Sprache oder Bildern wird die nächste KI-Generation jedoch direkt aus Videos lernen. Meta unternimmt derzeit große Anstrengungen, um Videodaten aus der Ich-Perspektive für diese neue KI-Generation zu sammeln, aber auch YouTube-Videos sind als Trainingsmaterial geeignet\\'\\' (Schreiner, 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n101\\n\\nDie europäische Wirtschaft und Wissenschaft sollte diese Entwicklung ernst nehmen. Eine Implementierung und Training dieser Modelle für europäische Sprachen und Bildinhalte sowie ihre Erweiterung auf größere Dauer sind ein lohnendes Ziel für europäische Modelle.\\n\\nDie erfolgreiche Nutzung der Modelle erfordert die Lösung einer Reihe von Problemen:\\n\\nVerlängerung der zeitlichen Dauer von Videos um etwa eine Größenordnung. • Mögliche Verwendung einer autoregressiven Architektur. • Effiziente Nutzung von bestehenden Modellen zur Einzelbilderzeugung (z.B. Stable Diffusion).\\n\\nIn der Regel reicht ein einfacher Satz nicht mehr zur Spezifikation der Inhalte aus, sondern es muss eine Storyline mit mehreren Punkten angegeben und berücksichtigt werden.\\n\\nNeue Strategien um längere Eingabesequenzen verarbeiten zu können, z.B. nach dem Muster von S4.\\n\\nErweiterte Ansätze zur temporalen und räumlichen Disaggregation bei der Videogenerierung durch Diffusions-Modelle.\\n\\nEinbeziehung weiterer Modalitäten, wie etwa gesprochene Sprache und Geräusche.\\n\\nEinbindung von existierenden Bildern und 3D-Modellen, welche mit den Video- Techniken animiert werden können.\\n\\nGleichzeitige Behandlung verschiedener Objekte der gleichen Art. • Berücksichtigung von Kamerabewegungen, Morphing und Szenenwechsel.\\n\\nFür synthetisch generierte Videos gibt es einen riesigen Anwendungsbereich:\\n\\nBei Ausbildung und Lehre können Zusammenhänge direkt visualisiert werden. Ein Beispiel ist: „Zeige wie der indische Subkontinent das Himalayagebirge auffaltete.”\\n\\nIm Sprachunterricht können Szenen und Abläufe visualisiert werden, die dann die/der Schüler:in beschreiben muss.\\n\\nBei der personalisierten Werbung für ein neues Produkt: „Zeige wie Karl Müller mit seinem neuen Elektromobil über den Gotthardpass fährt”.\\n\\nAnleitungen zum Gebrauch eines Produktes können „on the fly” für eine neue Umgebung erstellt werden.\\n\\nAnimationsfilme lassen sich auf kostengünstige Art und Weise produzieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n102\\n\\nDas Erlernen von Text-Video-Modellen erfordert in der Regel einen Datensatz von Videoclips mit manuell eingegebenen Untertiteln. Die Erstellung solcher Datensätze ist jedoch teuer und zeitaufwendig und daher nur schwer in großem Umfang möglich. Stattdessen lassen sich diese Modelle mit Hilfe von Videos mit natürlichsprachlichen Annotationen in Form von automatisch transkribierten Sprachdaten trainieren. Ein Beispiel ist der HowTo100M Datenbestand mit 136 Millionen Videoclips aus 1,22 Millionen kommentierten Lehrvideos im Internet (Miech et al., 2019), die Menschen bei der Ausführung und Beschreibung von über 23.000 verschiedenen visuellen Aufgaben zeigen. Allerdings hat dieses Vorgehen auch einige potentielle Nachteile. Einerseits sind die durch Spracherkennung erzeugten Texte nicht fehlerfrei und die zeitliche Zuordnung ist nicht perfekt. Zudem handelt es sich um die eingeschränkte Domäne von Lehrvideos. Die sind umfangreicher und erfassen einen größeren neueren Datenbestände Themenbereich. HD-VILA-100M enthält 100 Millionen Videos in 720p-Auflösung, welche gleichzeitig Audiodaten, Untertitel und Video-Frames enthalten und durchschnittlich 13.4 Sekunden dauern (Zellers et al., 2022). Die Autor:innen zeigen, dass Audiodaten den Trainingserfolg signifikant verbessern. (Nagrani et al., 2022) übertragen Untertitel aus Bild-Text-Daten auf Videoclips ohne zusätzlichen manuellen Aufwand. Mit dieser Pipeline erstellen sie unscharf annotierte Audio-Video-Daten mit Millionen von gepaarten Clips und Beschriftungen. Sie zeigen, dass mit diesen Daten sehr leistungsfähige Modelle zur Videosuche und Videountertitelung trainiert werden können.\\n\\nInsgesamt gibt es auf Youtube, Shutterstock, Dreamstime und Reddit viele Millionen Videos, die mit Audio und teilweise mit Untertiteln verfügbar sind. Eine weitere wertvolle Ressource sind die Archive der Rundfunkanstalten, die für den barrierefreien Zugang zu ihren Sendungen routinemäßig Transkripte und Untertitel produzieren müssen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n103\\n\\nSPOTLIGHT Continental Automotive Technologies An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nContinental entwickelt wegweisende Technologien und Dienste für die nachhaltige und vernetzte Mobilität der Menschen und ihrer Güter. Das 1871 gegründete Technologieunternehmen bietet sichere, effiziente, intelligente und erschwingliche Lösungen für Fahrzeuge, Maschinen, Verkehr und Transport. Continental erzielte 2021 einen Umsatz von 33,8 Milliarden Euro und beschäftigt aktuell mehr als 190.000 Mitarbeiterinnen und Mitarbeiter in 58 Ländern und Märkten.\\n\\nDr. Corina Apachiţe, Head of AI, Continental Automotive Technologies\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case?\\n\\nWir setzen Foundation-Modelle insbesondere im Bereich „Natural Language Processing\", also der Analyse von Texten ein. Dort sind diese Modelle momentan das Maß aller Dinge. Unsere Anwendungen sind dabei die Analyse von „Requirements- Dokumenten zur Unterstützung unserer Entwickler, oder im Bereich \"Conversational AI\", also generell bei Mensch-Maschine-Schnittstellen. Eine weitere Anwendung, die zukünftig eine Rolle im automatisierten Fahren spielen kann, ist die Analyse und Formalisierung von Verkehrsregeln mit Hilfe von Sprachmodellen. Im Bereich Bildverstehen oder Bildgenerierung können Foundation-Modelle auch für das Erstellen und Analysieren von Bild-Datensätzen verwendet werden, um beispielsweise Trainingsdaten für AI-Modelle automatisch zu annotieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Für qualitativ hochwertige Anwendungen im Bereich Sprachverstehen sind Foundation-Modelle unerlässlich. Somit ermöglichen uns erst die KI-Foundation- Modelle unsere Anwendungen. Im Bereich Textanalyse von Requirements- Dokumenten spart uns die Technologie erheblich Zeit, macht die Analyse einfacher und effizienter und entlastet damit unsere Mitarbeiter:innen. Die Nutzbarkeit von Chat-Bots hängt primär von der Qualität der KI-Modelle ab. Zukünftige Foundation- Modelle, wie beispielsweise multi-modale Varianten, die sowohl Text als auch Bilddaten verstehen, werden auch für eine Vielzahl weiterer Aufgaben interessant sein, die bisher nur schwer umzusetzen sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n104\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Generell haben KI-Modelle eine direkte Abhängigkeit von den Daten, mit denen sie trainiert wurden. Ihre Qualität und Funktion der Modelle hängen primär von diesen Daten ab. Somit müssen Modelle primär auf Daten der entsprechenden Regionen trainiert werden. Europäische Besonderheiten, wie beispielsweise die Sprachdiversität sowie regulatorische oder kulturelle Unterschiede im Straßenverkehr, werden von nicht-europäischen KI-Modellen möglicherweise vernachlässigt. Qualität und Funktionen verlangen also nach regionalen Lösungen.\\n\\nZudem sorgen die hohen datenschutzrechtlichen Standards in Europa dafür, dass sensible Daten kaum an amerikanische oder chinesische KI-Modelle gesendet werden können. Des Weiteren bleiben die Modelle geschlossene Systeme („Black Box\"). Detailliertere Analysen oder Weiterentwicklungen der Modelle werden nicht ermöglicht.\\n\\nEine weitere Schwierigkeit ist die Abhängigkeit von den Besitzern der KI-Modelle in kommerzieller Hinsicht. Zugangshürden könnten aufgebaut oder spezielle Eigenschaften der Modelle designt werden, die nicht im Interesse europäischer Nutzer:innen sind. Und nicht zuletzt geht es auch um Standortnachteile durch Abwanderung von Talenten dorthin, wo die „besten\" Foundation-Modelle erstellt werden.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Jeder dieser Aspekte wäre ein großer Vorteil für die Verwendung von KI-Foundation- Modellen. Die Veröffentlichung als Open Source würde die Untersuchung der Inhalte von Foundation-Modellen vollumfänglich ermöglichen. Es gäbe mehr Gestaltungsmöglichkeiten, gerade auch im Hinblick auf die hohe Diversität in Europa. Foundation-Modelle können angepasst und beliebig modifiziert werden, wie z.B. das Zuschneiden von Modellen auf regionale Besonderheiten, auf die Bedürfnisse verschiedener Industrien oder Anwendungsdomänen.\\n\\nIn hohen Datenschutzstandards sehen wir einen kompetitiven Vorteil, der zusammen mit Transparenz die Akzeptanz von Foundation-Modellen erheblich erhöhen wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n105\\n\\n6.5 Fragestellungen und Weiterentwicklungen\\n\\nGerade auch für multimodaler Modelle gilt die Interviewaussage von Prof. Dr. Wrobel: „Das größte Defizit ist das mangelnde Verständnis der Fähigkeiten und Grenzen.” Dies wird von Prof. Schütze in seinem Interview unterstützt und konkretisiert: „Es wäre nochmal ein Paradigmenwechsel, wenn man andere Modalitäten vollumfänglich in die Modelle integrieren könnte und wenn man auch tatsächlich echtes Grounding berücksichtigen könnte.”\\n\\nGegenüber reinen Sprachmodellen weisen Foundation-Modelle, die Texte mit Bildern bzw. Videos kombinieren, noch eine zusätzliche Komplexitätsdimension auf. Daher sollten Ansätze entwickelt werden, um die Modellergebnisse zu erklären und die Zuverlässigkeit der Resultate für bestimmte Eingaben abzuschätzen.\\n\\nVideos enthalten sehr viele unterschiedliche Informationstypen, die bei der Analyse und Generierung integriert werden können:\\n\\nOCR kann verwendet werden, um im Video sichtbare Schriftzüge zu erkennen. Dazu kann man z.B. ein Texterkennungsmodell integrieren, welches Texte oder Tokeneinbettungen liefert.\\n\\nGesprochene Sprache lässt sich über eine Spracherkennung gewinnen, die Texte oder Tokeneinbettungen produziert.\\n\\nGesichter und die zugehörigen Personen sind besonders wichtige Merkmale zum Verständnis eines Videos. Hier lassen sich Modelle zur Gesichtserkennung in die Video-Pipeline einbeziehen.\\n\\nAudio, zum Beispiel Motorengeräusche, können bei der Interpretation von Videos verwendet werden. Hier lässt sich beispielsweise ein Audio-Erkennungssystem nutzen, welches auf den YouTube-8M Daten trainiert wurde, bei denen Objekte in den Videos annotiert wurden.\\n\\nSzenen (z.B. Bäume, Berge, Friedhof) können über ein Szenen-Modell erkannt werden, welches mit dem PLACE365 trainiert wurde.\\n\\nObjekte, wie sie etwa im ImageNet annotiert sind.\\n\\nBewegungen, die durch Modelle zur Aktionenerkennung erkannt werden können, welche mit den Kinetics-Daten trainiert werden können.\\n\\nEin erstes Modell in diese Richtung ist Merlot Reserve, das Audio, Untertitel und Video einbezieht (Zellers et al., 2022).\\n\\nDie gleiche Szene kann auch durch mehrere Videos erfasst werden, z.B. binokulare Kameras oder mehrere Kameras mit unterschiedlichem Gesichtsfeld. Hier besteht die Aufgabe, diese Abläufe zu integrieren und daraus eine einheitliche Interpretation zu gewinnen. Eine derartige Anordnung erleichtert die Rekonstruktion der 3D-Szene.\\n\\nModelle zur Erzeugung von Videos können jedoch auch missbraucht werden, um beispielsweise gefälschte, hetzende, herabsetzende oder bösartige Inhalte zu erzeugen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n106\\n\\nDiese Gefahren können z.B. durch interne Filter für die Eingabetexte und für die Prüfung der erzeugten Videoinhalte reduziert werden. Hier gibt es aber einen großen Forschungsbedarf, um die Modelle abzusichern und schwer erkennbare soziale Vorurteile und Stereotypen herauszufiltern. Google und Meta nennen diese Gefahren als Grund, warum sie ihre Modelle noch nicht freigeben.\\n\\nAuch in diesem Bereich der KI-Foundation-Modelle zeichnet sich bereits eine amerikanische Dominanz ab. Europäische Entwickler:innen muss die Möglichkeit gegeben werden, Videodaten zu nutzen und auf deren Basis fortschrittliche KI- Foundation-Modelle zu entwickeln. Nur so kann sichergestellt werden, dass europäische Werte und Standards auch in diesen Modellen Beachtung finden. Dafür müssen Entwickler:innen und Forscher:innen bestmögliche Voraussetzungen haben.\\n\\n6.6 Foundation-Modelle in anderen Datendomänen\\n\\nDie vorgeschlagene Strategie der Modellentwicklung, die multilingualen Foundation- Sprachmodellen schrittweise um Wissensbestände und Multimodalität zu erweitern, bietet auch eine gute Ausgangsbasis für die Entwicklung von Foundation-Modellen in ganz anderen Daten- oder Wissenschaftsdomänen. Erfolgreiche Anwendungen von Modellen in Genetik und Proteomik zum Beispiel beruhen auf der Einsicht, dass auch in diesen Bereichen Muster in Symbolsequenzen gelernt werden können, die neue Erkenntnisse zu Eigenschaften, Wirkung oder Veränderung der Moleküle ermöglichen. Es ist sehr wahrscheinlich, dass sich in Chemie, Werkstoffwissenschaften, Biowissenschaften oder anderen Natur- und Ingenieurwissenschaften weitere solche Einsatzgebiete finden werden.\\n\\nGroße zukünftige Anwendungsbereiche der Technologie liegen in der Betriebswirtschaft und in allen anderen Bereichen der Gesellschaft, in denen Prozesse gestaltet, gesteuert, optimiert, automatisiert und überprüft werden müssen. Weil Prozesse sequentielle Abfolgen von Teilprozessen und Einzelhandlungen sind, liegt der Einsatz von Transformermodellen zum Entwurf, zur Verbesserung und zum Monitoring solcher Prozesse nahe. Durch die Digitalisierung der Unternehmen und Verwaltungen wird es hinreichend Daten zu den tatsächlichen Abläufen in Geschäfts-, Produktions- und Verwaltungsprozessen geben, aus denen Foundation-Modelle ein generelles Wissen über die Eigenschaften und Elemente dieser Prozesse erwerben können. Solch ein Ansatz zur Gewinnung von neuronalen Prozessmodellen wird wahrscheinlich sehr schnell multimodal werden, indem er die sprachlichen Benamungen und Beschreibungen von Prozesselementen sowie die Informationsobjekte der Prozesse wie Formulare, Datentransaktionen, Geolokationen und Transportwege in die Lerndaten einbezieht. GATO (Reed et al., 2022) ist ein erstes multimodales Modell, welches Sequenzen von Texten, Bildern und Messwerten verarbeiten und daraus Steuerungsstrategien ableiten kann. Es erzielte auf mehr als 600 Benchmarks gute Steuerungsergebnisse.\\n\\nDas in dieser Studie diskutierte KI-Rechenzentrum würde den Einstieg der deutschen Wirtschaft und Forschung zu solchen neuen Modelltypen sehr erleichtern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n107\\n\\n6.7 Zusammenfassung\\n\\nBis hierhin hat die Studie gezeigt, dass das wirtschaftliche Potential und die gesellschaftliche Relevanz von KI-Foundation-Modellen immens sind. Um aber das volle wirtschaftliche Potential auszuschöpfen, müssen europäische Entwickler:innen dazu befähigt werden, eigene Foundation-Modelle zu entwickeln. Ansonsten besteht die Gefahr, von amerikanischen Modellen abhängig zu werden oder die Modelle gar nicht zu nutzen. Beide Fälle bedeuten einen erheblichen Wettbewerbsnachteil für die Wirtschaft.\\n\\nUm qualitativ hochwertige Foundation-Modelle zu trainieren, müssen Bias, Toxizität und Falschaussagen reduziert oder bestenfalls komplett ausgeschlossen werden. Auch wenn es hier bereits große Fortschritte gibt, hilft nur die Arbeit mit und die Forschung an KI- Foundation-Modellen, diesen Themenkomplex anzugehen. Die deutsche und die europäische Gesellschaft sollten alles daransetzen, diese Entwicklungen nicht den Amerikanern oder Chinesen zu überlassen.\\n\\nDabei sollte der Fokus zunächst auf Sprachmodellen liegen. Die Multilingualität Europas ist eine Herausforderung, aber zugleich auch eine große Chance für europäische Sprachmodelle. Eine adäquate Abbildung dieser Multilingualität in der Funktionalität der Foundation-Modelle erhöht deren Akzeptanz und den wirtschaftlichen Nutzen. Sie ist aber auch von hoher Relevanz für die gesamte Technologieentwicklung, denn der größte Teil der Welt ähnelt in Bezug auf Sprachenvielfalt eher Europa als den USA oder China.\\n\\nVerschiedene Projekte bereiten bereits vielsprachige Datensätze auf und stellen diese zur Verfügung. Dahinter steht bereits die nächste Generation der Foundation-Modelle in den Startlöchern, welche z.B. Videos erzeugen könnten. Hier ist der Entwicklungsbedarf noch größer, das Potential immens und viele Fragestellungen weiter ungelöst.\\n\\nDeutsche und europäische Entwickler:innen und Forscher:innen müssen jetzt befähigt werden, nach besten Standards an KI-Foundation-Modellen zu arbeiten und zu forschen. Der erste Schritt dafür sollte der Aufbau einer kompetitiven Infrastruktur für das Training an KI-Foundation-Modellen sein.\\n\\nWie das technisch gelingen kann, darin gibt nun das nächste Kapitel einen Einblick.\\n\\nGroße KI-Modelle für Deutschland\\n\\n108\\n\\nVoraussetzungen bei Software und Personal\\n\\nGroße KI-Modelle für Deutschland\\n\\n109\\n\\n7. Voraussetzungen bei Software und Personal Die in LEAM angedachte Entwicklung und das Training von KI-Foundation-Modellen (Bommasani et al., 2021) setzt eine integrierte und leistungsfähige Hard- und Software- Infrastruktur voraus, wie sie es bisher und in öffentlichen Investitionsplänen in Deutschland und der EU noch nicht gibt. Während sich diese Infrastruktur in ihrer Grundstruktur von etablierten Strukturen in High-Performance-Computing-Systemen lässt, setzt die Arbeit mit KI-Foundation-Modellen besondere (HPC) ableiten Voraussetzungen an deren Organisation und der dabei eingesetzten Software. Dieses Kapitel erklärt sowohl diese besonderen KI-Elemente und deren Zusammenwirken mit der HPC-Grundstruktur, als auch die dafür notwendigen Voraussetzungen und Investitionen, um LEAM möglich zu machen.\\n\\nHierfür ist es notwendig, gleichzeitig die technischen Voraussetzungen und Möglichkeiten der LEAM-Initiative wie auch ihrer Chance für das KI-Ökosystem und die deutsche und europäische Gesellschaft zu betrachten. Während in diesem Kapitel gezeigt werden kann, dass die Herausforderungen an LEAM auf der Software-Seite lösbar sind, werden auch ihre sozialen und ökonomischen Mehrwerte aufgezeigt. Ein Ziel von LEAM sollte es sein, das monolithisch simplifizierte Berufsbild der „IT-Fachkraft” aufzubrechen, um die vielfältigen, spezialisierten und voneinander abhängigen Rollen, die für ein wettbewerbsfähiges KI-Ökosystem notwendig sind, zu differenzieren und zu stärken. Als technologisches, soziales und ökonomisches Leuchtturmprojekt kann LEAM dafür den richtigen Impuls setzen, damit Deutschland und Europa zu einem globalen KI- Wettbewerber avanciert, wenn die Chancen des KI-Ökosystems richtig eingeschätzt und genutzt werden.\\n\\nIn diesem Kapitel wird eine Variante für den Software-Stack eines KI-HPC-Systems und die dafür benötigten hochspezialisierten Berufe und Fähigkeiten skizziert. Hierbei ist es möglich, strukturell auf existierenden und praxisbewährten HPC-Systemen aufzubauen und um KI-spezifische Komponenten zu ergänzen. 6 Besonderes Augenmaß wird darauf gelegt, den Stack mit Hilfe von Open-Source-Software (OSS) zu konstruieren, da das globale KI-Ökosystem einerseits auf nicht-proprietärer Software aufgebaut ist und andererseits so eine Abhängigkeit von Software-Konzernen vermieden werden kann. Dieser Aspekt der Unabhängigkeit fördert auch die Souveränität und Resilienz des KI- Ökosystems, da auf Zulieferer im Software-Bereich weitestgehend verzichtet werden kann, wie es z.B. auch die Digitalstrategie der Bundesregierung vorsieht (Bundesministerium tatsächlichen Implementierung können die Details abweichen, für diese Machbarkeitsstudie ist jedoch\\n\\nfür Digitales und Verkehr, 2022).\\n\\nIn der\\n\\n6 Wir betrachten in diesem Kapitel bevorzugt zentralisierte und homogen organisierte Infrastrukturen, obwohl B. Yuan et al., (2022) prototypisch gezeigt haben, dass KI-Modelle auch dezentralisiert und heterogen auf vernetzten Rechnern trainiert werden können. Zentralisierte Infrastrukturen haben den Vorteil, dass der eingesetzte Software-Stack einfacher und weniger fehleranfällig gehalten werden kann, die Datenquellen zuverlässiger und schneller verfügbar sind sowie Nutzerrechte und Sicherheitsvorkehrungen (DMZ, VPN) schlichter zu handhaben sind. Darüber hinaus können mit einem zentralisierten KI-Hochleistungszentrum zuverlässigere Statistiken über Energieverbrauch und Effizienz erhoben werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n110\\n\\nentscheidend, dass LEAM auf der Software-Ebene mit bereits existierenden Technologien umgesetzt werden kann.\\n\\nBei der Betrachtung dieser Infrastruktur lassen sich vier aufeinander aufbauende Ebenen (Layer) unterscheiden, die mit einzelnen Modulen interagieren, um die Entwicklung und das Training von KI-Modellen möglich zu machen (Abb. 18). Diese Layer setzen jeweils verschiedene spezialisierte Berufe voraus, um deren komplexe Funktionalität bereitstellen zu können. Diese einzelnen Hard- und Software-Layer sind dabei jeweils für bestimmte Aufgaben optimiert, um dedizierte, rechenintensive Anwendungen in einem effizienten Rechenprozess zu organisieren. In der Grafik sind die für LEAM notwendigen und für die Arbeit mit KI-Foundation-Modellen besonderen KI-Elemente im Trainings- und Applikations-Layer gebündelt, welches auf den System- und Framework-Layern der High- Performance-Computing-Infrastruktur aufsitzt und von den Nutzer:innen durch ein Service-Layer angesteuert werden kann.\\n\\nAbb. 18: Simplifizierte Darstellung der Hard- und Software-Infrastruktur von HPCs\\n\\nDas System-Layer bildet dabei mit seinen Recheneinheiten ein Hardware-Fundament, welches durch das Framework-Layer in betriebsfähige Software-Systeme gebündelt wird. Das Trainings- und Applikations-Layer nutzt diese Systeme, um KI-Anwendungen durchzuführen, während es vom Data-Storage & -Loading-Modul mit Daten beliefert wird. Der zentrale Unterschied zum Aufbau von traditionellen HPC-Systemen liegt hier darin, dass für die Entwicklung von KI-Modellen anwendungsspezifische KI-Beschleuniger wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n111\\n\\nz.B. GPUs, FPGAs, Cerebras-Chips und andere (siehe System-Layer) statt CPUs eingesetzt werden. Diese sind notwendig, damit die auf den System- und Framework-Layern aufbauenden, rechenintensiven Trainings- und innerhalb akzeptabler Zeiträume abgeschlossen werden können. 7 Während CPUs für die Datenaufbereitung über eine Cloud-Lösung gemietet werden könnten, sollten die mehreren tausend GPUs und unterstützenden CPUs, die für den Betrieb von LEAM notwendig wären, lokal verfügbar sein. In Kapitel 8 wird für diese Machbarkeitsstudie mit einer Anzahl von 4480 GPUs kalkuliert.\\n\\nInferenzanwendungen\\n\\nTabelle 5: Im Betrieb von LEAM werden für Training, Tuning und Inference Tausende GPUs benötigt.\\n\\nLEAM ist für den Auf- und Ausbau des Innovationsstandorts Deutschland und Europa daher unerlässlich, um dieses komplexe und aufeinander abgestimmte Geflecht an Hard- und Software aufzubauen und der Wissenschaft und Wirtschaft zur Verfügung zu stellen. Diese massive Recheninfrastruktur ist für die Entwicklung von KI-Foundation-Modellen notwendig. So erfordert das Training großer KI-Modelle zum Beispiel eine Vielzahl leistungsfähiger und optimierter KI-Beschleuniger, auf denen sowohl Daten als auch Modelle parallel geschaltet werden, was eine deutliche Leistungssteigerung hiesiger Rechenzentren erfordert. Dabei ist die Wiederverwendbarkeit dieser rechenintensiven Modelle von zentraler Bedeutung, um die dabei verbrauchten Ressourcen und investierten Kosten amortisieren zu können.\\n\\nDiese Leistungssteigerung ist durch die Erfüllung technologischer Voraussetzungen erreichbar, wie z.B. durch das effiziente und schnelle Laden und Speichern von Daten. Gleichzeitig ist sie aber auch von sozialen Veränderungen abhängig, wie z.B. die gezielte Anziehung von Fachkräften, um das Wachstum des KI-Ökosystems aufrechtzuerhalten. Hier sollte LEAM sowohl ein Anstoß und eine Inspiration sein, in Technologien zu investieren, die den Innovations- und den Wirtschaftsstandort Deutschland und Europa stärken sowie Wege zu finden, um Fachkräfte differenzierter und erfolgreicher zu umwerben und auszubilden.\\n\\n7 Mit einer einzelnen NVIDIA V100 GPU braucht man 355 Jahre, um GPT-3 zu trainieren (Li, 2020). CPU-basierte Frameworks sind mindestens 5-10 mal langsamer als GPU-basierte Frameworks. Ohne den Einsatz von GPUs braucht es also mehrere tausend Jahre Rechenzeit, um GPT-3 zu trainieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n112\\n\\nAus einer für diese Machbarkeitsstudie durchgeführten Umfrage mit 71 Teilnehmer:innen aus dem KI-Ökosystem Deutschlands ging hervor, dass die Berufe DevOps, MLOps, Machine Learning Engineer und Machine Learning Researcher für die Entwicklung und das Training von KI-Foundation-Modellen besonders wichtig sind. Des Weiteren braucht es für den Betrieb von LEAM Software Engineers und System-Administrator:innen, während Site Reliability Engineers unter den Befragten als weniger wichtig erachtet wurden. 56 % der Befragten gaben an, dass die Entwicklung eines KI-Foundation-Modells mehr als 25 Mio. Euro kosten würde, während 37 % davon ausgehen, dass sich diese Kosten innerhalb von fünf bis zehn Jahren amortisieren.\\n\\nDieses Kapitel der Machbarkeitsstudien zeigt also auf, dass die Herausforderungen an Software für die Entwicklung von KI-Foundation-Modellen in LEAM mit den heute zur Verfügung stehenden Technologien und Mitteln bereits lösbar sind, wenn sich Deutschland und Europa bereit zeigen, in die Integration von LEAM zu investieren. Durch den Einsatz von Open-Source-Software in Verbindung mit wenigen, ausgewählten proprietären Applikationen lassen sich auch hierzulande große KI-Modelle entwickeln und trainieren, obgleich die dafür notwendige Hardware zum größten Teil aus den USA und dem nicht-europäischen Ausland stammt.\\n\\nEine der größten Herausforderungen und Chancen für LEAM stellen die dafür notwendigen spezialisierten und umworbenen Fachkräfte dar. Für das Training der in LEAM vorgesehenen KI-Foundation-Modelle wird ein Team von etwa 20 dedizierten Expert:innen entlang der oben beschriebenen Rollen vorausgesetzt. Während das KI- Ökosystem in Deutschland und Europa bereits heute von hochqualifizierten Expert:innen betrieben wird, übersteigt die Nachfrage das Angebot noch deutlich (Streim, 2022). LEAM kann dazu beitragen, die Ausbildungsqualität und die Attraktivität des hiesigen KI- Ökosystems zu verbessern, und vor allem bewirken, dass KI-Fachkräfte in Deutschland und Europa bleiben wollen, weil sie hier mit LEAM Voraussetzungen wiederfinden, wie sie es derzeit nur außerhalb Europas gibt.\\n\\n7.1 Applikations-Layer: Trainings- & Inference-Technologien\\n\\nDas Training und die Entwicklung von großen KI-Foundation-Modellen bergen im Vergleich zu klassischen, verteilten Systemen ohne naiven Parallelismus einige Besonderheiten: Deep-Learning-basierte KI-Entwicklungen sind für die Konzeption und das Training von KI-Foundation-Modellen einzigartig befähigt, weil deren Algorithmen parallel geschaltet werden können. Das bedeutet, dass eine Vielzahl von Datenpunkten gleichzeitig verarbeitet werden, anstatt sie nacheinander, also sequentiell, zu bearbeiten. Diese Eigenschaft bildet einen wesentlichen Vorteil gegenüber klassischen HPC- Anwendungen und erlaubt es, spezifische algorithmische Vorteile auszunutzen, um das Training großer KI-Foundation-Modelle überhaupt erst in endlicher Zeit zu ermöglichen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n113\\n\\nAbb. 19: Die Architektur des Trainings- & Applikations-Layers im Detail\\n\\nDaten- & Modell-Parallelismus Diese Eigenschaften sind im „Daten-” bzw. „Modell-Parallelismus” zusammengefasst und erklären, warum dafür eine massive Recheninfrastruktur notwendig ist (Hugging Face, o.D.). Während im Daten-Parallelismus Trainingsdaten zu gleichen Anteilen auf die KI- Beschleuniger zwar aufgeteilt werden kann, ist die Anzahl der zu berechnenden Parameter in KI-Foundation-Modellen so groß, dass die Modelle selbst im Modell- Parallelismus 8 auf mehrere Beschleuniger aufgeteilt werden müssen. Das bedeutet, dass nicht jeder Beschleuniger eine identische Kopie des Modells parallel berechnet und deren Ergebnisse iterativ an alle parallelen Beschleuniger übermittelt, sondern die zu berechnenden Daten sequentiell von einem Beschleuniger zum nächsten weitergegeben werden müssen. Damit man dabei die Modellparameter aktualisieren kann, überträgt der letzte Beschleuniger in einer Reihe das Fehlersignal (Loss) wieder an den ersten Beschleuniger zurück. Daten- und Modell-Parallelismus 9 können gleichzeitig verwendet werden, was die Komplexität der Entwicklung weiter erhöht.\\n\\nBei solch einer zirkulären Sequenzierung bleiben allerdings die meisten KI-Beschleuniger ungenutzt, wenn diese gerade keine Operation in der Reihenschaltung durchführen. Diese Stillzeit kann im Modell-Parallelismus über raffiniertes Programmieren (Kosson et al., 2021) und durch Optimierungssysteme minimiert werden, die eine höchstmögliche Auslastung aller Beschleuniger gewährleisten (s. Kapitel 8.3). Diese Voraussetzungen machen das Programmieren und das Betreuen des Trainings von KI-Foundation-Modellen kompliziert und herausfordernd (Bommasani et al., 2021).\\n\\n8 Narayanan et al. (2021) beschreiben Modell- bzw. Pipeline-Parallelismus im Detail. 9 Mudigere et al. (2022) beschreiben Best Practices im Training großer Recommender-Modelle.\\n\\nGroße KI-Modelle für Deutschland\\n\\n114\\n\\nTrainingsmanagement, Evaluation & Benchmarking Im Applikations-Layer wird auch das zuverlässige Management von einzelnen Trainingsjobs gesteuert, um bei einem Hardware-Versagen keine Resultate zu verlieren. Anders als bei dem Training von kleineren KI-Modellen kommt es beim Training von KI- Foundation-Modellen häufig zu einem Ausfall einzelner KI-Beschleuniger. 10 Ein zuverlässiges Launch- und Relaunch-System beugt dabei durch fortlaufendes Monitoring Ergebnis-Verluste vor, um von derselben Stelle aus weiter zu trainieren, bei der der Fehler aufgetreten ist.\\n\\nDes Weiteren werden auch die Evaluation und das Benchmarking von KI-Modellen in diesem Layer implementiert. Dazu gehören sowohl das Monitoring während des Trainingsprozesses, als auch das Testen der Modelle an anwendungsrelevanten Datensätzen, um deren Performanz festzustellen und diese später in Data & Model Card- Dokumentationen überführen zu können (Pushkarna et al., 2022). Für das Training und die Evaluation ist auch eine Anbindung an die ETL-Infrastruktur 11 notwendig, die hier implementiert, getestet und versioniert wird.\\n\\nWiederverwendbarkeit in Deployment-Infrastruktur Aufgrund des rechenintensiven Trainings von KI-Foundation-Modellen steht die Wiederverwendbarkeit von kostspielig trainierten KI-Modellen im Mittelpunkt, um die dafür notwendigen Ressourcen und Kosten zu amortisieren. Dabei werden u.a. Destillierungsmechanismen, Adaptions- oder Finetuning-Verfahren eingesetzt (s. Kapitel 2), welche wiederum anwendungsrelevante Benchmarks voraussetzen.\\n\\nDabei spielt die Entwicklung und Betreuung einer optimalen Deployment-Infrastruktur eine zentrale Rolle, da so große Effizienzsteigerungen erreicht werden können. Hier muss auf eine Implementierung Wert gelegt werden, um die KI-Foundation-Modelle bestmöglich mit der darunterliegenden Hardware betreiben zu können und externe Anfragen per API schnell und zeitnah zu beantworten. 12 Es muss hierbei auf komplexe Produktionsaspekte geachtet werden, um einen reibungslosen Prozess auch unter Stress zu gewährleisten. So sollte die Deployment-Infrastruktur z.B. resilient gegenüber einer Häufung von Anfragen sein, mit einer stabilen API ausgestattet sein sowie robuste Zugangsbeschränkungen und Sicherheitschecks beinhalten.\\n\\n10 Dies ist ein bekanntes Phänomen aus der Datacenter-Branche: Je mehr Festplatten betrieben werden, desto häufiger fallen diese aus. Gleiches gilt dementsprechend auch für HPC- Beschleuniger. 11 ETL steht für Extract-Transform-Load und beschreibt die Aggregation von Rohdaten aus einer Produktionsdatenbank in ein Format, das zur Analyse der Daten verwendet werden kann. 12 Beispielhaft können hier Freeware-Bibliotheken wie TensorRT und Triton genannt werden, welche von NVIDIA zur Verfügung gestellt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n115\\n\\nRessourceneffizienz des Trainings Das ressourcenintensive Training von KI-Foundation-Modellen kann zu einem Problem werden, wenn der Trainingsvorgang nicht kontinuierlich kontrolliert wird und dabei der Energieverbrauch mit geeigneten Maßnahmen verringert wird. Die Nachhaltigkeit muss also bei der Konzeption eines LEAM-KI-Hochleistungsrechenzentrum und beim Entwickeln und Trainieren von KI-Modellen von Beginn an mitgedacht werden. 13\\n\\nDie in der KI-Entwicklung verbrauchten Ressourcen können auf dem Software-Level durch die regelmäßige und fortlaufende Messung von CO2-Äquivalenten festgehalten werden. Dabei werden die gemessenen CO2-Äquivalenten an drei verschiedenen Komponenten kontrolliert, um sie der richtigen Quelle zuzuordnen (Hintemann, 2020).\\n\\nWährend des Trainings und der Inferenz benötigen die KI-Beschleuniger Energie zum Ausführen der Rechenoperationen, sogenannter MACs. 14 Diese lassen sich auf dem Applikations- und Framework-Level feststellen (Bannour et al., 2021; CodeCarbon, 2020). Aufgrund der hohen Datenmenge verbraucht der Data-Storage & -Loading-Layer für das Bereitstellen, die Verarbeitung und den Transport von Daten ebenfalls einen signifikanten Anteil des Energieverbrauchs. Die zuverlässige Kühlung des Gesamtsystems und andere, kleinere Komponenten benötigen darüber hinaus eine ununterbrochene Stromzufuhr. Wie das Hochleistungszentrum an sich ressourcenarm/klimaneutral konzipiert werden kann wird im Kapitel 8.6 erläutert.\\n\\nDie verschiedenen Messungen können direkt an den Service-Layer übergeben werden, um den Nutzer:innen die Einsicht und Kontrolle über den Energieverbrauchs des Gesamtsystems und des Trainingsprozesses zu gewährleisten. Über Warnsignale kann ein unkontrollierter Mehrverbrauch verhindert werden. Solche Kontroll-Angaben werden teilweise bereits heute von Hyperscalern zur Verfügung gestellt.\\n\\nVoraussetzungen Die Lingua Franca für die Entwicklung moderner KI-Systeme ist Python (van Rossum, 1995), wobei KI-Anwendungen zunehmend auch in anderen Programmiersprachen entwickelt werden. Für die Entwicklung des Service-Layers kommen viele Sprachen in Frage, die hier nicht aufgelistet werden sollen. Die Softwarebibliotheken, die beim Trainieren und der Inferenz zum Einsatz kommen, sind großteils, wenn nicht gänzlich, durch Open-Source-Software abdeckbar. Beispielhaft sind hier auf dem Framework-Level PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2016), Keras (Keras, 2015/2022), und Jax (Frostig et al., 2018) zu nennen.\\n\\nBei der Orchestrierung nutzt der Applikations-Layer teilweise dieselbe Software wie der Framework-Layer, bspw. Ray (Moritz et al., 2018), Slurm (Yoo et al., 2003), kubeflow (Kubeflow, 2017/2022), hydra (Hydra, 2019/2022), Abseil (Abseil Python Common Libraries, 2017/2022) und andere (s. Kapitel 7.4). Weiterhin werden auch numerische Bibliotheken\\n\\n13 Vgl. bspw. (CSTB Releases Report Fostering Responsible Computing Research, 2022; Patterson et al., 2022) und die darin enthaltenen Referenzen. 14 MAC steht für Multiply-Add-Compute, der zentralen Rechenoperation von Computerprozessoren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n116\\n\\nwie BLAS („An Updated Set of Basic Linear Algebra Subprograms (BLAS)”, 2002), NumPy (Harris et al., 2020) und andere Verwendung finden. Beim ML-spezifischen Monitoring kann auf Bibliotheken wie AimStack (Arakelyan et al., 2020) oder MLFlow (MLflow, 2018/2022) zurückgegriffen werden. Diese Liste ist ausschließlich exemplarisch und dient lediglich der Darstellung der Wichtigkeit von Open-Source-Software für die Entwicklung moderner KI-Applikationen.\\n\\nDie Aufgaben im Applikations-Layer erfordern eine hohe Spezialisierung, welche üblicherweise von Informatiker:innen und Software-Entwickler:innen in geteilten Rollen übernommen wird. Diese kann man in vier Rollenprofile unterscheiden: Machine Learning Researcher konzipieren und entwickeln neue Algorithmen, während sich Machine Learning Engineers mit der Optimierung des Trainings- und Inferenzcodes und der Implementierung des verteilten Lernens beschäftigen. Zusammen bilden sie ein Team, um KI-Modelle zu skalieren.\\n\\nFull-Stack und Backend-Ingenieur:innen entwickeln den Service-Layer und arbeiten u.A. mit Systemadministrator:innen und Dev-Ops-Expert:innen zusammen, um die Benutzeroberfläche mit den verschiedenen Funktionalitäten des Gesamtsystems zu koppeln. Data Engineers und Data Scientists arbeiten an dem Data-Storage & Data- Loading-Layer und seiner Infrastruktur. Das beinhaltet das Design der Datenbanken oder anderer Speichereinheiten, die Entwicklung der ETL-Pipelines sowie das Kontrollieren der Daten-Lade-Pipelines und das korrekte Design der Trainings-Datensätze.\\n\\nGroße KI-Modelle für Deutschland\\n\\n117\\n\\nSPOTLIGHT Fyrfeed GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nFyrfeed erstellt Content für B2B-Unternehmen durch eine Kombination aus Fachautoren und Künstlicher Intelligenz. Kunden müssen dadurch bloß 5 Minuten pro Monat investieren und sparen 80% der Kosten gegenüber Agenturen.\\n\\nFyrfeed-Gründerteam: Ehud Alexander Avner, Dr. Thomas Lindemann, Benjamin Zengler\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Wir setzen, neben anderen Technologien und Tools, auch große, allgemeine Sprachmodelle ein, um Fachautoren bei der Erstellung von hochwertigem Content – z. B. Beiträgen für soziale Medien, Blogartikeln oder Whitepapers – zu unterstützen.\\n\\nDieser Ansatz, bei dem Mensch und KI zusammenarbeiten, nennt sich Human-in-the- Loop.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? KI-Foundation-Modelle ermöglichen uns, qualitativ hochwertige Texte schnell und kostengünstig anzubieten. Statt Modelle von Grund auf selbst neu trainieren zu müssen, was mit erheblichem Kosten- und Zeitaufwand verbunden ist, können wir allgemeine, vortrainierte Modelle sofort einsetzen. Dadurch werden Weiterentwicklungen des Produkts sowie das Testen neuer Anwendungsmöglichkeiten um ein Vielfaches einfacher.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Da wir Kunden aus der ganzen Welt bedienen, ist es denkbar, dass wir für verschiedene Sprachen verschiedene Modelle einsetzen. Obwohl bestehende State- of-the-Art-Sprachmodelle multilingual einsetzbar sind (und dies tun wir auch sehr erfolgreich), kann die Qualität zwischen Sprachen (und Fachbereichen) variieren. Modelle, die auf europäische (und außereuropäische) Sprachen spezialisiert sind – und gleichzeitig auch alle Vorteile von Open Source anbieten – wären ein großer Gewinn.\\n\\nGroße KI-Modelle für Deutschland\\n\\n118\\n\\n7.2 Data-Storage & -Loading-Layer\\n\\nDer Erfolg vieler KI-Foundation-Modelle hängt wesentlich von einem effizienten und schnellen Aufbereiten der zu ladenden Daten ab. Sowohl in der ETL als auch während des Trainings der Modelle ist Effizienz ausschlaggebend. Damit kommt dem Data-Storage & Loading-Layer eine besondere Bedeutung zu, wie sie es so in anderen HPC-Systemen oft nicht gibt.\\n\\nBeim Training von KI-Foundation-Modellen wird das Laden der Daten mithilfe eines mehrschichtigen Prozesses beschleunigt: Der ETL-Vorgang sowie das Filtern und Aufbereiten der Rohdaten werden sequentiell ausgeführt, sodass diese möglichst latenzarm geladen und im Daten-Parallelismus verteilt werden können. Für ETL werden dabei häufig große, klassische CPU-Server eingesetzt. Diese müssen in der Lage sein, Peta- und Exabyte an Daten zu verarbeiten, z.B. beim Scrapen von Webseiten und bei Map- Reduce-Jobs (Dean & Ghemawat, 2004) und ähnlichen, massiven Vorgängen. Dabei kommen spezialisierte Tools 15 in der Daten-Infrastruktur zum Einsatz, wie z.B. Spark (Apache Spark, 2014/2022), Flink (Apache Flink, 2014/2022), oder Dask (Dask, 2015/2022).\\n\\nDie per ETL bereitgestellten Rohdaten werden dann entweder in einer Datenbank oder einem Cloud-basierten Storage-Layer abgelegt. Dabei ist es möglich, aus diesen Rohdaten einen vorgefilterten Datensatz zu generieren und ebenfalls lokal oder in der Cloud abzulegen. Für das Vorfiltern werden ähnliche Tools und Ressourcen wie auch für ETL benötigt, diese sind also bereits vorhanden. Diese Art der Datenverarbeitung beschleunigt später den Trainingsprozess, setzt aber einen erhöhten Speicheraufwand voraus.\\n\\nWährend des Trainings von KI-Foundation-Modellen ist es wichtig, die Daten effizient und möglichst ohne Redundanzen zum KI-Beschleuniger zu transportieren. Dazu ist nicht nur eine entsprechende Netzwerk-Architektur notwendig (s. Kapitel 8), sondern auch ein dedizierter Software-Stack, welcher für das Laden von Daten in verteilten Systemen optimiert wurde. Dabei ist essentiell, dass Daten aufgrund ihrer Größe nicht auf einzelne Rechner geladen und gespeichert werden können, sondern durch ein effizientes Streaming-System bereitgestellt werden müssen, was die Entwicklung deutlich erschwert.\\n\\nDabei werden die Daten periodisch immer wieder in zufälliger Reihenfolge gestreamt (Nguyen et al., 2022), wenn sie von variierenden KI-Beschleunigern angefragt werden. Der Software-Stack muss in der Lage sein, sowohl von lokalen Datenbanken oder Festplatten, als auch von unterschiedlichen Cloud-Storage-Systemen lesen zu können, um etwaigen Benutzeranforderungen entsprechen zu können. 16 Solche Systeme werden z.B. von Amazon, Google und Microsoft vertrieben oder werden von Konzernen privat betrieben. - Ebenso müssen beim Laden Filterfunktionen, Datenaugmentierungen und\\n\\n15 Diese und folgende Listen haben keinen Anspruch auf Vollständigkeit und es werden nur exemplarische Elemente jeder Kategorie erwähnt. Die genaue Wahl der Tools hängt am Ende von den Implementierungsdetails ab und soll zu diesem Zeitpunkt nicht festgelegt werden 16 Erwartungen an die Datenhoheit sind erfahrungsgemäß sehr heterogen. Um eine breite Akzeptanz und Benutzung zu erreichen, muss das System also mit möglichst vielen Szenarien kompatibel sein.\\n\\nGroße KI-Modelle für Deutschland\\n\\n119\\n\\ntransformationen unterstützt werden, wie es beim Training von Deep Neural Networks üblich ist. Dies ist erforderlich, um Modelle robust und generalisierbar zu trainieren. Der Software-Stack muss auch dazu fähig sein, mehrere Datensätze miteinander zu kombinieren, um neue ETL-Jobs zu vermeiden und somit Ressourcen zu sparen.\\n\\nDer Data-Storage- & Loading-Layer muss auch mit lokalen Benutzerrechten sowie mit geltendem Datenrecht kompatibel sein. Als Bibliothek muss die Daten-Infrastruktur Access Control Levels (ACL 17) berücksichtigen, um den Zugang zu Daten abzusichern und so (un-)beabsichtigte Zugriffe auf Daten von Dritten zu vermeiden. Dies wird über eine Schnittstelle zum Service-Layer gesteuert, in dem die Benutzerverwaltung organisiert ist.\\n\\nInternationales Datenrecht sieht vor, dass das Speichern und Verarbeiten von Daten streng regulierten Praktiken zugrunde liegen muss, wenn es die Verarbeitung von personenbezogenen Daten betrifft, wie es z.B. in der Datenschutzgrundverordnung geregelt ist (Data Protection in the EU, o.D.). Darüber hinaus sind in verschiedenen geographischen Räumen auch bestimmte Zertifikate verpflichtend oder werden vom Markt erwartet, wie etwa SOC-2/SOC-3 (System and Organization Controls, o.D.) in den USA und Nordamerika oder ISO 27001 (ISO - ISO/IEC 27001 and Related Standards — Information Security Management, o.D.) bzw. ISO 27017/27018 (ISO 27017 and ISO 27018 Certification | DEKRA, o.D.) im europäischen Raum. Daten mit bestimmten Eigenschaften, bspw. Informationen über Gesundheit oder Kreditwürdigkeit unterliegen weiteren, lokal regulierten Bestimmungen, wie etwa die in den USA geltenden Regulierungen HIPAA oder FCRA.\\n\\nDarüber hinaus kann es auch notwendig sein, dass der Daten-Layer innerhalb einer Demilitarized Zone (DMZ) oder in einem Virtual Private Network (VPN) auf die Daten zugreifen kann, falls die Daten nicht innerhalb des Hochleistungszentrums temporär abgelegt werden können. Dies sollte allerdings in der Praxis vermieden werden, da die damit verbundene Latenzzeit die Geschwindigkeit beim Laden stark beeinträchtigen kann.\\n\\nVoraussetzungen Für die Entwicklung und Betreuung der Daten-Systeme sind besondere Fähigkeiten von Data Engineers und Machine Learning Engineers notwendig, um das ETL und das Stream- basierte, verteilte Laden der Daten auf die Beschleunigerknoten zu gewährleisten. Eingesetzte Software sind z.B. Spark, Flink, oder Dask für ETL oder torchdatasets (Maszke, 2019/2022), Squirrel (Sohofi et al., 2022/2022), Deep Lake (Deep Lake, 2019/2022), ffcv (FFCV, 2021/2022) und andere (Ofeidis et al., 2022) für das verteilte Laden der Daten. Es handelt sich hier in der Regel um frei verfügbare Open-Source-Software, deren Beschaffung keine Herausforderung darstellt. Zum Speichern der Daten können klassische verteilte Datenbanken wie HDFS (Apache Hadoop, 2014/2022) oder GlusterFS (GlusterFS, 2011/2022), Cloud-Strorage Systeme, ähnlich Google Cloud Storage, Azure Blob Storage oder auch NAS-Systeme verwendet werden.\\n\\n17 Man unterscheidet hierbei zwischen Read, Write, Execute und Discoverable. Also die Erlaubnis, Daten zu lesen, zu schreiben, Anwendungen auszuführen oder - weniger bekannt - die Fähigkeit, Informationen über das Vorhandensein von Daten zu finden, ohne diese jedoch lesen zu dürfen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n120\\n\\n7.3 System-Layer\\n\\nIm System-Layer werden die tatsächlichen KI-Beschleuniger und die für deren Einsatz notwendige Hardware-Struktur organisiert, um hochleistungsfähige und verlustarme Rechenleistung zur Verfügung zu stellen. KI-Beschleuniger („AI Accelerator”, 2022) sind typischerweise spezialisierte Prozessoren, welche für die Entwicklung und das Training von KI-Modellen optimiert wurden (Reuther et al., 2022). Der Einsatz dieser Prozessoren ist unerlässlich, um den zeitlichen Rechenaufwand für KI-Foundation-Modelle zu beherrschen sowie wettbewerbsfähig zu sein (Khan & Mann, 2020). Weltweit führend in der Herstellung von High-Performance-Prozessoren sind die US-amerikanischen Firmen NVIDIA (Campa et al., 2020), AMD (AMD, 2021) und Intel (Intel, 2022), aber auch Start-ups wie z.B. Tachyum (Tachyum, 2022). Edge- und Embedded-Systems, wie z.B. NVIDIA Jetsons oder FPGAs, kommen hier nicht in Betracht, da sie nicht für das Training von KI- Foundation-Modellen geeignet sind.\\n\\nDiese Prozessoren werden mithilfe spezialisierter Bibliotheken gesteuert, die oft von Hardware-Herstellern selbst entwickelt werden. Beispielhaft zu nennen sind hier CUDA (NVIDIA Developer, 2013) und ROCm (AMD, o.D.), welche in der Regel als Freeware 18 mit liegende Server- der Hardware zur Verfügung gestellt werden. Das darüber Betriebssystem steuert die physikalische Hardware mittels dieser Bibliotheken an, wobei fast immer Linux-Betriebssysteme, wie Distributionen der Debian- (Ubuntu), Redhat- (Fedora, CentOS) oder Arch-Familien, eingesetzt werden (Joseph et al., 2022). Die einzelnen Server-Einheiten, welche mehrere KI-Beschleuniger bündeln, werden dann mittels entsprechender Middleware in das Kommunikations-Substrat eingebunden, wie es in Abbildung 20 näher beschrieben ist.\\n\\nAbb. 20: Die Architektur des System- und Data-Storage & Loading-Layers im Detail\\n\\n18 Im Vergleich zu OSS stehen bei Freeware-Bibliotheken nur die kompilierten Binärdateien zur Verfügung und nicht der gesamte, menschenlesbare Quellcode.\\n\\nGroße KI-Modelle für Deutschland\\n\\n121\\n\\nKommunikations-Substrat Das Kommunikations-Substrat dient einerseits der nahtlosen Verbindung von System- und Framework-Layer, aber auch dazu den KI-Beschleunigern Datenbanken und Datenspeicher zur Verfügung zu stellen. Dabei kann das Kommunikations-Substrat mehrere Server-Einheiten miteinander verknüpfen, um große Datenmengen effizient verarbeiten zu können. Die dabei für KI-Anwendungen typischerweise eingesetzten Kommunikationsbibliotheken basieren auf spezialisierten Technologien, wie MPI („Message Passing Interface”, 2022), OpenMPI („Open MPI”, 2022) oder NCCL (NVIDIA, o.D.).\\n\\nDie genaue Topologie des Substrates ist durch die tatsächliche, physikalische Verkabelung der Hardware gegeben und muss beim Bau eines Hochleistungszentrums mitgedacht werden. Dabei muss bereits zu Anfang klargestellt werden, welche Anwendungen die KI- Beschleuniger berechnen werden.\\n\\nVoraussetzungen Für die Konzeption, den Aufbau und die Betreuung des System-Layers und des Kommunikations-Substrats bedarf es als Systemadministrator:innen ausgebildete Fachkräfte. Für die Betreuung wird spezialisierte Kontroll-Software eingesetzt, welche aus proprietärer und Open-Source-Software (OSS) bestehen kann. Generell besteht die Hardware- und Software-Infrastruktur für KI-Hochleistungsrechenanwendungen überwiegend aus OSS, was deren Bedeutung für das KI-Ökosystem unterstreicht (Sonnenburg et al., 2007).\\n\\nDarüber hinaus übernehmen die Systemadministrator:innen auch die Rechtevergabe und Kontrolle der Nutzer:innen, bspw. über entsprechende Access Control Levels mittels LDAP- oder AD-Systeme, aber auch die Serveradministration und das -monitoring, um die dauerhafte Gesundheit des Systems zu gewährleisten. Des Weiteren fällt auch die Administration von DMZs und VPNs sowie Cyber-Sicherheits-Vorkehrungen in den Aufgabenbereich der Systemadministrator:innen.\\n\\n7.4 Framework- & Service-Layer\\n\\nFramework-Layer In dem über dem Kommunikations-Substrat liegenden Framework-Layer werden die spezialisierten KI-Technologien eingesetzt (Abb. 21). Dazu werden die im System-Layer als Server-Einheiten durch Containerlösungen wie Docker (Merkel, 2014) abstrahiert, um die Umgebung auf die entsprechenden KI-Systeme zu normalisieren (Carpintero, 2021) und zu homogenisieren. Damit lassen sich unterschiedlichste KI-Systeme entwickeln, obwohl die darunterliegende Hardware dieselbe bleibt.\\n\\nzusammengefassten\\n\\nKI-Beschleuniger\\n\\nnochmals\\n\\nIn der Container-Umgebung wird ein weiteres, eigenes Linux-basiertes Betriebssystem eingesetzt, welches mit Hochleistungsbibliotheken wie BLAS oder cuBLAS („Basic Linear Algebra Subprograms”, 2022) sowie für die Entwicklung von KI-Modellen notwendigen Programmiersprachen wie Python, C und C++ samt ihrer Compiler ausgestattet sind. Weiterhin befinden sich hier auch die für Deep Learning-Anwendungen spezifischen\\n\\nGroße KI-Modelle für Deutschland\\n\\n122\\n\\nFrameworks, mit welchen sich KI-Anwendungen programmieren lassen. Darunter zählen z.B. PyTorch, TensorFlow, Keras oder Jax (Gopani, 2021).\\n\\nAbb. 21: Die Architektur des Framework- & Service-Layers im Detail\\n\\nService-Layer Über die für die operative Entwicklung und das Training von KI-Foundation-Modellen notwendige Hard- und Software-Infrastruktur werden manche der darin enthaltenen Services im Service-Layer über Benutzeroberflächen abgebildet. Zum Beispiel werden hier unveränderte KI-Modelle für eigene Use-Cases eingesetzt oder KI-Modelle automatisch an bereitgestellte Datensätze adaptiert. Im Service-Layer können aber auch administrative Prozesse wie die Kontrolle von Nutzern oder die Anfragen von weiteren Ressourcen abgewickelt werden.\\n\\nVoraussetzungen Im Framework-Layer wird die Cluster-Orchestrierung gesteuert, um gezielt Aufträge im System starten zu können und verfügbare System-Ressourcen effizient einsetzen zu können. Hierfür wird ein Job-Management-System eingesetzt, welches typischerweise auf SLURM, Kubernetes (Kubernetes (K8s), 2014/2022), Terraform (Terraform, 2014/2022), DockerHub oder anderen Komponenten basiert (Mujkanovic et al., 2020). Damit lassen sich Aufträge aneinanderreihen und individuell priorisieren. Darüber hinaus gehört zum Framework-Layer auch eine für Deep Learning-Anwendungen spezifische Komponente, um Daten passgenau und effektiv aus den Datenspeicher-Systemen in die Rechnerumgebung zu laden (s. Kapitel 7.2).\\n\\nFür diese Fülle an spezialisierten Aufgaben werden Fachkräfte benötigt, die in der teilweise automatisierten Operationalisierung der Entwicklungs- (DevOps) und Machine- Learning-Umgebung (MLOps) ausgebildet sind. Dabei ist es notwendig, sowohl die dauerhafte Gesundheit der Umgebungen zu gewährleisten, als auch über Weiter- und Neuentwicklungen von Software-Lösungen für das Framework-Layer informiert zu bleiben, um bei eingehender Prüfung Verbesserungen am System vorzunehmen. Für die Orchestrierung und Priorisierung der Cluster werden vor allem Software Engineers und DevOps-Expert:innen gebraucht.\\n\\nGroße KI-Modelle für Deutschland\\n\\n123\\n\\n7.5 LEAM als Leuchtturmprojekt für die Zukunft des KI-Ökosystems\\n\\nEine für die Konzeption, das Training und die Unterhaltung von KI-Foundation-Modellen notwendige und leistungsfähige KI-Software-Umgebung setzt aber nicht nur technische und infrastrukturelle Bedürfnisse voraus, sondern stellt auch vielfältige und zahlreiche Voraussetzungen an gut ausgebildetes Personal, welche derzeit in Deutschland und der EU noch kaum oder gar nicht erfüllt werden. So belegen Studien zum Fachkräftemangel in der IT-Dienstleistungsbranche einen Mangel an spezialisiert ausgebildeten Fachkräften, aber auch eine fehlende Differenzierung zwischen einzelnen hochspezialisierten Rollen, die für die Entwicklung und das Training von KI-Foundation-Modellen notwendigerweise zusammenwirken müssen (Hickmann & Koneberg, 2022). Dies erschwert eine passgenaue Ansprache und Werbung für den Nachwuchs.\\n\\nIn der Praxis wird die Arbeit an KI im Allgemeinen und insbesondere an KI-Foundation- Modellen in Teams organisiert, die verschiedene Fähigkeiten und Ausbildungen einbringen. Spezialisierte Teams müssen verschiedene Programmiersprachen sowie Mathematik, Datenwissenschaft und Datentechnik beherrschen, aber auch vertiefte Kenntnisse in Informatik, Statistik und Wissen über Software-, Hardwarekomponenten und -architekturen vereinen: „Machine Learning-Spezialisten arbeiten in Teams – mit ML- Spezialisten an der Spitze, Softwareentwicklern in großer Zahl an der Basis und Datenwissenschaftlern und typische Personalstruktur fortschrittlicher Technologiecluster wie dem Silicon Valley” (Philippe Lorenz & Kate Saslow, 2019, eigene Übersetzung).\\n\\ningenieuren dazwischen – das\\n\\nist die\\n\\nHeute sind diese vielfältigen Teams in Deutschland allerdings unterbesetzt, sodass es „für neun von zehn offenen Stellen [in der Informatik] zuletzt keine passend qualifizierten Arbeitslosen” gab (Hickmann & Koneberg, 2022). Bitkom-Präsident Achim Berg sagte dazu: „Der sich verschärfende Mangel an IT-Spezialistinnen und -Spezialisten wächst sich zu einer ganz realen Bedrohung für Deutschlands große Transformationsaufgaben aus.” (Bitkom e.V., 2022a).\\n\\nIn einer dreiteiligen Studie hat das Center for Security and Emerging Technology (CSET) an der Georgetown University festgestellt, wie schnell der Bedarf an KI-Expert:innen wächst: Zwischen 2015 und 2019 ist der Anteil der gesamten Arbeitnehmer:innen, die in den USA direkt am KI-Ökosystem beteiligt sind, von sechs auf neun Prozent gewachsen – zu 14 Millionen Arbeitnehmer:innen (Gehlhaus et al., 2021). Innerhalb der nächsten zehn Jahre soll diese Berufsgruppe dabei sogar doppelt so schnell wachsen wie der Bundesdurchschnitt (Gehlhaus et al., 2021). Doch anders als in den USA, können bürokratische Hürden, ausländische Fachkräfte einzustellen und an den Arbeitsmarkt zu binden, hierzulande deutlich höher sein.\\n\\nIn der schulischen und beruflichen Bildung ist es dabei notwendig, stärker zu differenzieren und die unterschiedlichen Rollen im KI-Ökosystem gezielter auszubilden. Hier können neben Universitäten insbesondere Fachhochschulen und Hochschulen für für spezialisierte Angewandte Wissenschaften mehr Aufgabenfelder investieren, während Universitäten mehr Mittel für die Ausbildung von ML-Spezialisten zur Verfügung haben sollten (Wannemacher & Bodmann, 2021).\\n\\nin attraktive Angebote\\n\\nGroße KI-Modelle für Deutschland\\n\\n124\\n\\nDas Bildungsangebot und der Arbeitsmarkt der USA sind uns dabei mehrere Schritte voraus. Dazu stellte CSET weiterhin fest, dass viele Berufswege im KI-Ökosystem keine mehrjährige Ausbildung an einer Universität oder gar einen Doktortitel benötigen. In der Studie, „Training Tomorrow’s AI Workforce,” plädiert das CSET für eine neue Strategie, zu befähigen, KI-Expert:innen nicht-universitäre Bildungseinrichtungen besser auszubilden, in dem diese auf föderaler und regionaler Ebene gefördert werden sowie besser mit den lokal verorteten Unternehmen verknüpft werden (Gehlhaus & Koslosky, 2022). So sollen Student:innen und Arbeitgeber:innen bessere Voraussetzungen vorfinden, regional integrierte KI-Ökosysteme zu betreiben.\\n\\nDarüber hinaus plädiert CSET für eine Abkehr von weitreichenden, langjährigen Studiengängen in der Informatik hin zu einer größeren Vielfalt an modular aufgebauten Weiterbildungskursen, um sich schneller zu spezialisieren und so für den Arbeitsmarkt verfügbar zu sein. Dieser Strategiewechsel muss laut CSET aus der US-Regierung heraus gesteuert werden, indem eine Stabstelle für die Ausbildung von KI-Expert:innen im Bundeskabinett geschaffen wird (Gehlhaus & Koslosky, 2022). Eine ebenso zentralisierte, befähigte Stabsstelle wäre auch in Deutschland und der EU ein wichtiges Signal, um den Fachkräftemangel im Detail richtig beurteilen zu können und das Angebot an KI- Weiterbildungen in der Masse und in der Tiefe zu verbessern.\\n\\nGerade diese wirtschaftliche Entwicklung des KI-Ökosystems hin zu einer zentralen, alle anderen Wirtschaftsbereiche durchdringenden Querschnittsbranche untermauert die Wichtigkeit von LEAM als Befähiger der deutschen und europäischen Wissenschaft und Wirtschaft. LEAM kann neben seinen wirtschaftlichen Möglichkeiten auch ein wirtschaftspolitischer Leuchtturm sein, um Expert:innen auszubilden, anzuwerben und langfristig zu binden, wie auch junge Menschen dazu zu inspirieren, IT-Berufe zu erlernen. LEAM kann also nicht nur ein starkes Zeichen für die Wettbewerbsfähigkeit des Innovationsstandorts Deutschland und Europa setzen, sondern auch über die konkrete Anwendung hinaus einen für die Zukunftsfähigkeit unserer Wirtschaft notwendigen Bildungsauftrag in der Gesellschaft erfüllen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n125\\n\\n7.6 Zusammenfassung\\n\\nDie Herausforderungen des von LEAM vorgeschlagenen KI-Hochleistungsrechenzentrum zur Entwicklung und des Trainings von KI-Foundation-Modellen sind auf der Software- Ebene technisch lösbar. Die dafür notwendigen Technologien wurden bereits entwickelt und stehen größtenteils als Open-Source-Software zur Verfügung. Die wesentliche Herausforderung liegt also nicht in der Entwicklung der Software, sondern im Betrieb eines solchen KI-Hochleistungsrechenzentrum, wofür eine Vielzahl an hochspezialisierten Expert:innen in Teams zusammenarbeiten muss.\\n\\nFalls eine Umsetzung gelingt, kann LEAM wissenschaftliche Durchbrüche aus Deutschland und Europa lancieren, indem mit komplexer, heute bereits verfügbarer Software auf moderner, massiver Hardware KI-Modelle robust skaliert werden. So könnte LEAM die deutsche und europäische Wissenschaft und Wirtschaft nicht nur zukunfts- und wettbewerbsfähig machen, sondern auch für unsere Gesellschaft über die tatsächliche, technische Umsetzung Mehrwerte in der Bindung von Fachkräften und der Lösung von gesellschaftlichen Aufgaben bieten\\n\\n.\\n\\nGroße KI-Modelle für Deutschland\\n\\n126\\n\\nAufbau eines KI-Hochleistungsrechenzentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n127\\n\\n8. Aufbau eines KI-Hochleistungsrechenzentrums Als einer der Hauptgründe, warum in Deutschland bzw. Europa kein regelmäßiges Training großer KI-Foundation-Modelle à la GPT-3 möglich ist, nennen die Expert:innen, die für diese Studie interviewt wurden, die fehlende High Performance Computing (HPC) Infrastruktur für die Berechnung solcher Modelle. Denn KI-Foundation-Modelle stellen insbesondere an die Rechenkapazitäten besondere Anforderungen. Anders als bei herkömmlichen HPC-Systemen, die Central Processing Unit (CPU)-basiert arbeiten, werden im Bereich Künstlicher Intelligenz und speziell bei der Berechnung großer KI- Foundation-Modelle außergewöhnlich hohe Rechenkapazitäten von bis zu 4500 Graphics Processing Unit (GPU) benötigt, die trotz dieser hohen Kapazität sehr lange Laufzeiten benötigen, um die nötigen Rechenaufgaben zu bewältigen. Diese Art von konzentrierten Rechenkapazitäten sucht in Deutschland und EU bisher noch ihresgleichen. Indem Industrie und Wissenschaft die Zugänge zu diesen Rechenkapazitäten fehlen, werden Sprunginnovationen im Bereich KI-Foundation-Modelle stark erschwert. Ein Rechenzentrum (RZ) ist eine Infrastruktureinrichtung, in der Computer, Server, Speichersysteme und andere Technologiekomponenten zusammengefasst sind, um eine große Menge an Daten und Anwendungen zu verarbeiten und zu speichern (Hintemann & Clausen, 2018). Rechenzentren dienen in der Regel als zentrale Ressource für die Verarbeitung und Speicherung von Daten und Anwendungen in Unternehmen, Organisationen und Institutionen. Sie können auch für verschiedene Zwecke eingesetzt werden, von der Verarbeitung von Transaktionen und der Bereitstellung von IT-Diensten bis hin zur Ausführung von KI-Anwendungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n128\\n\\nEs gibt verschiedene Arten von Rechenzentren, die sich in ihrer Größe, ihrem Zweck und ihrer technischen Ausstattung unterscheiden (vgl. Tabelle 6).\\n\\nRechenzentrum\\n\\nBeschreibung\\n\\nEnterprise Rechenzentren\\n\\nEnterprise Rechenzentren sind Rechenzentren, die von Unternehmen und Organisationen betrieben werden, um die Verarbeitung und Speicherung von Daten und Anwendungen für ihre internen Zwecke zu ermöglichen. Enterprise Rechenzentren können in verschiedenen Größen und Formen auftreten, von kleinen Räumen mit wenigen Servern bis hin zu großen Anlagen mit tausenden Computern und Servern.\\n\\nCollocation Rechenzentren\\n\\nCollocation Rechenzentren sind Rechenzentren, die von Dritten betrieben werden und an Unternehmen und Organisationen vermietet werden. Collocation Rechenzentren bieten den Mietern Platz für ihre eigenen Server und andere Computerkomponenten und stellen die erforderliche Infrastruktur wie Strom, Kühlung und Netzwerkverbindungen zur Verfügung.\\n\\nHyperscaler Rechenzentren\\n\\nHyperscaler Rechenzentren sind Rechenzentren, die von Hyperscaler-Unternehmen betrieben werden, um die Verarbeitung und Speicherung von Daten und Anwendungen für ihre Kunden zu ermöglichen. Hyperscaler Rechenzentren sind oft sehr groß und bieten eine hohe Rechenleistung und Speicherkapazität, um große Mengen an Daten schnell und effizient zu verarbeiten und zu speichern.\\n\\nTabelle 6: Beispiele für Rechenzentren\\n\\nGroße KI-Modelle für Deutschland\\n\\n129\\n\\nNeben dem Geschäftsmodell können Rechenzentren auch nach ihrer Größe unterschieden werden. Folgende Größenangaben können hier als Referenz dienen (The role of data centers in an interconnected world, o.D.):\\n\\nArt\\n\\nGrößen\\n\\nLeistungs- aufnahme\\n\\nSchwerpunkt\\n\\nMicro Data Center\\n\\nAb 1 Server-Rack aufwärts; passende Konfigurationen für einen Container\\n\\n100 kW\\n\\nEDGE-Anwendungen, die von der Nähe zur IoT- Quelle profitieren\\n\\nKleine Rechenzentren (auch EDGE- Rechenzentren)\\n\\nca. 500 m2\\n\\n1 MW\\n\\nOftmals ein Unternehmens-eigenes Rechenzentrum für kritischen Datenbestand\\n\\nMittlere Collocation/Hostin g Rechenzentren\\n\\nca. 10.000 m2\\n\\n10 MW\\n\\nMulti Tenant Collocation RZs sowie Ausrichtung auf Hosting und Managed Services\\n\\nGroße Collocation Rechenzentren\\n\\nca. 50.000 m2\\n\\n50 MW\\n\\nMulti und Single Tenant Collocation sowie große Hosting/Managed Service Anbieter mit internationaler Ausrichtung\\n\\nHyperscaler\\n\\n100.000 m2 und mehr\\n\\n100 MW und mehr\\n\\nGroße, global operierende Cloud-Anbieter, die oftmals an mehreren Standorten (10-200) weltweit tätig sind\\n\\nTabelle 7: Größen von Rechenzentren\\n\\nAbgrenzung von KI-Hochleistungsrechenzentren LEAM plant die Berechnung von großen KI-Foundation-Modellen, hierzu ist spezielle Hardware notwendig. Die benötigten KI-Hochleistungsrechenzentren unterscheiden sich in erster Linie durch ihren Fokus auf die Verarbeitung von KI-Anwendungen. Andere Rechenzentren sind meist auf die Verarbeitung allgemeiner Daten und Anwendungen ausgerichtet und können für eine Vielzahl von Zwecken verwendet werden. Ein KI- Hochleistungsrechenzentrum hingegen ist speziell für die Verarbeitung von KI- Anwendungen entwickelt und ausgestattet und bietet die erforderliche Rechenleistung und -umgebung, um KI-Modelle und -Algorithmen schnell und effizient zu trainieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n130\\n\\nKI-Hochleistungsrechenzentren grenzen sich von anderen Rechenzentren auch in ihrer technischen Ausstattung und Funktionsweise ab. KI-Hochleistungsrechenzentren können zum Beispiel über eine höhere Rechenleistung und spezielle Hardware wie Grafikprozessoren (GPUs) verfügen, die für die Verarbeitung von KI-Anwendungen besonders geeignet sind. Sie können auch über spezielle Software-Tools und - Umgebungen verfügen, die es ermöglichen, KI-Modelle und -Algorithmen zu entwickeln und zu trainieren.\\n\\n8.2 Anforderungen an ein KI-Hochleistungsrechenzentrum\\n\\nInsgesamt sind KI-Hochleistungsrechenzentren also auf die speziellen Anforderungen von KI-Anwendungen ausgerichtet und bieten die erforderliche Infrastruktur und Ressourcen, um KI-Anwendungen effektiv zu verarbeiten. Im Folgenden werden die konkreten Anforderungen an ein KI-Hochleistungsrechenzentrum zur Berechnung großer KI- Foundation-Modelle beschrieben.\\n\\nProzessoren Für die Berechnung von KI-Anwendungen sind Grafikprozessoren (Graphic Processing Units/GPUs) geeignet. GPUs sind speziell für die Bearbeitung von Graphikaufgaben konzipiert. Sie verfügen über eine Vielzahl von Rechenkernen, wodurch sie große Datenmengen schnell und parallel verarbeiten können. Dadurch können sie Aufgaben wie das Rendern von 3D-Grafiken oder das Trainieren von Modellen für maschinelles Lernen und Künstliche Intelligenz schnell bewältigen. In vielen Rechenzentren werden jedoch anstatt von GPUs, CPUs (Central Processing Units) verwendet. CPUs haben eine geringere Anzahl an Rechenkernen (6-12) und können ein breites Spektrum an Aufgaben bewältigen, sind aber nicht besonders gut für Aufgaben geeignet, die ein großes Maß an paralleler Verarbeitung erfordern.\\n\\nNetzwerkanforderungen Neben der Leistung der Prozessoren ist es für die Berechnung von großen KI-Foundation- Modellen von entscheidender Bedeutung, welche Verbindungstechnologie und welche Bandbreite zwischen den GPUs eingesetzt wird und mit welcher Bandbreite die einzelnen Knoten verbunden sind. Es ist zwingend notwendig, die Arbeitslast auf sehr viele GPUs zu verteilen. Dafür sind laut der befragten Expert:innen eine InfiniBand oder schnelle Ethernet Verschaltung und eine starke Bandbreite innerhalb des Clusters, aber auch zwischen Cluster und Storage notwendig. InfiniBand ist besonders gut für den Einsatz im HPC-Bereich geeignet und bis zu 10-mal leistungsfähiger als das gängige Interconnect PCIe. Außerdem sollten hier Storage-Umgebungen mit geringer Latenz eingebunden werden (Hensel & Ostler, 2020).\\n\\nGroße KI-Modelle für Deutschland\\n\\n131\\n\\nLeistungsdichten Als Leistungsdichte wird die elektrische Aufnahme der IT-Komponenten pro Server-Rack bzw. pro m² Whitespace (Flächenbedarf pro Rack) verstanden. Laut der befragten Expert:innen steigen die Leistungsdichten pro Rack im High Performance Computing Bereich stetig und liegen aktuell zwischen 20-30 kW pro Rack und vereinzelt auch höher. Die für die Berechnung von Foundation-Models benötigte Leistungsdichte liegt zwischen 30 und 45 kW (z.B. NVIDIA Superpod) pro Rack und somit im High Performance Computing-Bereich. Die Leistungsdichte hat ebenfalls Einfluss auf die Wahl der eingesetzten Kühlung der Systeme.\\n\\nKühlung Die von der Server-Hardware aufgenommene elektrische Leistung wird zu fast 100 % in Wärme umgewandelt. Je höher die Leistungsdichte eines Server-Racks, desto höher die abgegebene Wärmemenge. Computer dürfen die zugelassenen Betriebstemperaturen jedoch nicht überschreiten, um einen sicheren Betrieb zu gewährleisten. Hersteller von in der Regel die einzuhaltenden Temperaturbereiche und Luftfeuchtebedingungen vor, an die wiederum die Garantieleistungen geknüpft sind. Viele klimatechnische Vorgaben für den Betrieb von IT-Equipment in Serverräumen finden sich in den „Data Center Power Equipment Thermal Guidelines and Best Practices” des amerikanischen Verbandes ASHRAE wieder, an denen sich die IT-Hersteller ausrichten (ASHRAE, 2016). Um die Temperaturen im zulässigen Bereich zu halten, gibt es verschiedene Möglichkeiten der Kühlung, die in Rechenzentren zum Einsatz kommen. Generell wird zwischen Methoden der Luftkühlung sowie der Flüssigkeitskühlung (Wasserkühlung) unterschieden.\\n\\nfür\\n\\nsie\\n\\nIT-Equipment schreiben daher\\n\\nDa Kühlflüssigkeiten physikalisch dichtere Medien sind als Luft, können diese in der Regel auch deutlich höhere Wärmelasten abtransportieren. Aufgrund der stetig steigenden Leistungsdichten in den Racks und der damit steigenden Abwärmemenge wird es herausfordernder, Systeme besonders im HPC-Bereich effizient mit einer Luftkühlung zu kühlen. Laut der befragten Expert:innen ist die Luftkühlung eines Racks mit einer Leistungsdichte bis maximal 20 kW pro Rack umsetzbar. So entspricht 300 m³/h Luft einem Wasserdurchsatz von 90 l/h (Dürr, 2018).\\n\\nBei der von LEAM angestrebten Leistungsdichte von 36 kW pro Rack sind nur eine direct- to-chip Flüssigkeitskühlung oder flüssigkeitsgekühlte Racks geeignet. Flüssigkeiten sind ein effizienteres Wärmeübertragungsmedium als Luft und eignen sich somit besser bei hohen Leistungsdichten. Expert:innen geben jedoch an, dass bisher nur sehr wenige Rechenzentren mit einer Flüssigkeitskühlung oder hybriden Lösung aus Luft- und Flüssigkeitskühlung ausgestattet sind. Die Gebäudeinfrastruktur wird heute bei neuen Rechenzentren von Rechenzentrumsbetreibern und Collocation-Anbietern so geplant, dass die Installation einer Flüssigkeits-Kühlung möglich ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n132\\n\\nKühlmethode\\n\\nBeschreibung\\n\\nLuftkühlung\\n\\nDie gängigste Methode zur Luftkühlung ist die raumbasierte Kühlung mit Doppelboden in Kalt- und Warmgangkonfiguration. Hierbei wird die kühle Luft durch Auslassöffnungen im Doppelboden in den sog. Kaltgang vor die Racks geleitet und durch Ventilatoren der Server in den Serverschrank eingesaugt. Die erwärmte Luft wird auf der Rückseite des Racks über den Warmgang abgeleitet und einer erneuten Kühlung zugeführt. Des Weiteren finden sich auch Konzepte, bei denen auf einen Doppelboden verzichtet wird. In diesem Fall wird die kalte Luft von der Seite in den Serverraum eingeblasen und die erwärmte Abluft über eine spezielle „Doppelbodendecke” aus dem Raum abgeführt.\\n\\nDirect-to-Chip/Direct-to- Plate Flüssigkeitskühlung\\n\\nBei der Direct-to-Chip Kühlung wird eine Kühlflüssigkeit in kleinen Schläuchen oder Kupferrohren zu Kühlplatten geleitet, die sich direkt neben den zu kühlenden Komponenten (z.B. CPUs und GPUs) befinden. Die so erwärmte Flüssigkeit wird über einen Wärmetauscher und weitere nachgelagerte Kühlkreisläufe abgeführt.\\n\\nTauchkühlung/Immersion Cooling Flüssigkeitskühlung\\n\\nBeim sog. Immersion Cooling werden die elektronischen Bauteile, in der Regel die komplette Platine eines Servers, in einem Behälter mit einer speziellen elektrisch nichtleitenden Flüssigkeit eingetaucht, die das Kühlmittel darstellt. Die Flüssigkeit nimmt die Wärme auf und wird dann über einen Kühler abgeleitet, um die Wärme abzuführen.\\n\\nKühlung über Rücktüren Flüssigkeitskühlung\\n\\nWassergekühlte Rücktüren von Serverschränken bieten den Vorteil, dass konventionelle Servertechnik in den Schränken verbaut werden kann. Die von den Ventilatoren abtransportierte, erwärmte Abluft wird dabei durch Wärmetauscher in den rückseitigen Rack- Tür aufgenommen. Durch die angeschlossene Verrohrung wird die Wärme aus dem Serverraum abgeführt. Da bei dieser Technologie sowohl ein Luftstrom zum Abtransport der Wärme in die Wärmetauscher der Rücktüren als auch ein Wasserkreislauf zum weiteren Transport aus dem Serverraum zum Einsatz kommt, stellen diese Systeme quasi einen hybriden Lösungsansatz dar.\\n\\nTabelle 8: Übersicht über die Kühlmöglichkeiten in Rechenzentren\\n\\nGroße KI-Modelle für Deutschland\\n\\n133\\n\\nGebäudeinfrastruktur Die Anforderungen an die Gebäudeinfrastruktur eines KI-Rechenzentrums unterscheiden sich nicht grundlegend von den Anforderungen anderer Rechenzentren. Zu beachten ist jedoch, dass vor dem Bau eines neuen Rechenzentrums die Kühltechnologien geplant und die Bauweise des Gebäudes entsprechend angepasst wird. Die für das LEAM- Vorhaben benötigten Compute-Ressourcen und verwendeten Leistungsdichten von ca. 36 kW/Rack können nur durch eine Flüssigkeitskühlung gekühlt werden. Deshalb sollte die Gebäudeinfrastruktur für die Nutzung einer Flüssigkeitskühlung ausgestattet sein. Dies ist beim Bau eines Rechenzentrums sowie bei der Auswahl eines geeigneten Collocation/Housing Anbieters zu beachten.\\n\\nFlächenbedarf Entscheidenden Einfluss auf den benötigten Flächenbedarf hat die Leistungsdichte pro Rack. Wird bei einer vorgegebener Gesamtrechenkapazität die Leistungsdichte pro Rack erhöht, nimmt die Anzahl der benötigten Racks ab und der Flächenbedarf sinkt. Wird dagegen die Leistungsdichte pro Rack verringert, werden mehr Racks benötigt, um die angestrebte Gesamtleistung zu erreichen und somit steigt auch der Flächenbedarf. Laut der interviewten Expert:innen liegt der Flächenbedarf pro Rack (sog. Whitespace) in der Regel bei brutto 3,0-3,5 m² (die reine Standfläche eines Racks beträgt dabei ca. 1 m²). Für die von der LEAM-Initiative kalkulierten flüssigkeitsgekühlten 140 Racks (z.B. 4 NVIDIA DGX H 100 Knoten/Rack) mit einer Gesamtleistung von ca. 4,0 MW zur Berechnung und Training des KI-Foundation-Modells ergibt sich demnach ein Flächenbedarf von 345-525 m². Beim Einsatz von älteren Knoten (z.B. NVIDIA DGX A 100) in luftgekühlten Racks in ergibt sich eine höhere Anzahl von Racks und eine entsprechend größere Fläche, da weniger Knoten pro Rack verbaut werden können.\\n\\nStromversorgung Die grundsätzlichen Anforderungen von KI-Hochleistungsrechenzentren sind laut der interviewten Expert:innen mit den Anforderungen an die Stromversorgung anderer Rechenzentren gleichzusetzen. Gleichwohl zeichnen sich Höchstleistungsrechenzentren durch eine höhere Leistungsdichte (kW/Rack) und damit verbunden eine deutlich höhere Abwärmelast pro Serverraum aus. Die Stromversorgung eines Rechenzentrums besteht in der Regel aus mehreren Komponenten, die zusammenarbeiten, um sicherzustellen, dass das Rechenzentrum mit ausreichend Strom versorgt wird.\\n\\nGroße KI-Modelle für Deutschland\\n\\n134\\n\\nZu diesen Komponenten gehören in der Regel:\\n\\n1. Netzeinspeisung: Hier liegt der Übergabepunkt für den Strom, der für den Betrieb des Rechenzentrums benötigt wird. In der Regel wird der Strom aus dem bezogen. öffentlichen Kleinst- und Kleinrechenzentren werden meist über den normalen Hausanschluss mit 400 V Drehstrom versorgt. Bei größeren Abnahmemengen erfolgt die Einspeisung in der Regel mit 10 kV bzw. mit 20 kV (Mittelspannung). von MSHV\\n\\n2. Stromverteilung: Die Hauptstromversorgung wird über\\n\\nkaskadierende Abstufungen (Mittelspannungshauptverteilung) und NSHV (Niederspannungshauptverteilung) bis zu den jeweiligen Anschlusspunkten auf Netzteilebene im Rechenzentrum verteilt. Die Stromverteiler verteilen den Strom an die verschiedenen Bereiche des Rechenzentrums und sorgen dafür, dass der Strom zu den benötigten Stellen gelangt.\\n\\n3. Unterbrechungsfreie Stromversorgung (USV): Um sicherzustellen, dass das Rechenzentrum auch bei kurz- und langfristigen Stromausfällen weiter betrieben werden kann, muss nach INF.2 des BSI eine USV installiert werden. Die USV stellt über Batteriepuffer oder Schwungmassen-Systeme eine kontinuierliche Stromversorgung des Rechenzentrums sicher, bis die Hauptstromversorgung wiederhergestellt wird. Weitere Aufgaben einer USV sind das sog. Glätten von Spannungsstößen (Surge; <4ms), Abfedern von Oberschwingungen oder die galvanische Trennung des internen vom externen Stromkreislauf. Der Einsatz einer USV ist in Deutschland Pflicht.\\n\\nAuf das Thema Nachhaltigkeit in Bezug auf die Stromversorgung wird im Kapitel 8.6.3 eingegangen.\\n\\nEnergieverbrauch & Effizienzparameter Der PUE-Wert (Power Usage Effectiveness) ist eine vom Industriekonsortium The Green Grid eingeführte technische Kennzahl, welche die von der IT im Rechenzentrum verbrauchte Energie ins Verhältnis zum Gesamtenergieverbrauch setzt. In der Theorie beträgt der optimale PUE-Wert 1,0 (Gesamtstromverbrauch entspricht dem reinen IT- Verbrauch ohne sonstige energetische Aufwände wie z.B. für Kühlung oder für die USV- Verlustleistung). Der durchschnittliche PUE-Wert neu gebauter Rechenzentren betrug im Jahr 2015 unter 1,5, während der durchschnittliche PUE-Wert der luftgekühlten Bestandsrechenzentren in Deutschland im selben Jahr bei 1,8 lag. Im Jahr 2010 lag der durchschnittliche PUE-Wert der deutschen Rechenzentren noch bei 1,98 (Stobbe et al., 2015).\\n\\nEinfluss auf den PUE-Wert eines Rechenzentrums hat in erster Linie die Klimazone, in der das Rechenzentrum betrieben wird, sowie die eingesetzte Kühltechnologie, gefolgt von den energetischen Aufwänden zum Betrieb einer USV. In warmen Klimazonen (z.B. Mittelmeerraum) sind die energetischen Aufwände für die Kühlung naturgemäß höher als in kälteren Klimazonen (z.B. Skandinavien). Im Hinblick auf die Strompreisentwicklung ist die Senkung des PUE die vorrangige Möglichkeit, um die Betriebskosten zu senken (Lamonica, 2014). Rechenzentren mit einer Luftkühlung liegen durchschnittlich bei einem PUE-Wert zwischen 1,5 und 1,2 währenddessen flüssigkeitsgekühlte Rechenzentren einen\\n\\nGroße KI-Modelle für Deutschland\\n\\n135\\n\\nPUE-Wert von bis zu 1,06 erreichen können (PUE-Werte im Google-Rechenzentrum). Deutsche Rechenzentrumsbetreiber:innen bestätigten dies in der Befragung und gaben an, dass der PUE-Wert bei neuen, wassergekühlten Systemen Werte bereits bis zu 1,1 beträgt. Aktuell werde hier konzentriert die Nutzung der Abwärme optimiert, um insgesamt klimaneutral zu werden.\\n\\nDie vom Umweltbundesamt entwickelte Berechnungsmethode KPI4DCE (Key Performance Indicators for Data Center Efficiency) ist ein ganzheitlicher Ansatz für die Berechnung der Energieeffizienz von Rechenzentren, welcher auch den Lebenszyklus des IT-Equipment und der einbezieht. Diese Berechnungsmethode ist in der Theorie aussagekräftiger als der PUE-Wert allein, jedoch ist sie auch aufwendiger. Nicht allen Rechenzentren stellen die geforderten Messwerte zur Verfügung. Das Ziel ist eine automatisierte Messwertaufnahme (Schödwell et al., 2018). Auf EU-Ebene gibt es jedoch im Rahmen der Initiative Climate Neutral Data Centre Überlegungen, Rechenzentren mit diesen Kennzahlen zukünftig zu überwachen (Climate Neutral Data Centre Pact – The Green Deal Need Green Infrastructure, o.D.).\\n\\ntechnischen Versorgungsstruktur\\n\\nSkalierbarkeit / Modularer Aufbau Für die Skalierbarkeit eines KI-Hochleistungsrechenzentrums gelten die gleichen Gesetzmäßigkeiten wie für die Skalierbarkeit anderer Rechenzentren. Die Skalierbarkeit wird maßgeblich von drei Faktoren beeinflusst: ausbaubare Leerflächen, die maximale Leistungsdichte pro Rack und die maximal zugesicherte Stromleistung (im Gegensatz zum aktuellen Verbrauch). Hat ein Rechenzentrum beispielsweise die vorhandene Fläche bereits vollständig ausgeschöpft, kann die Leistungsdichte in den Racks unter Prüfung der vorhandenen Kühlung und Stromversorgung erhöht werden. Sind die zur Verfügung stehenden Serverräume bisher nicht vollständig genutzt, kann durch die Inbetriebnahme weiterer Räume die Gesamtleistung des Rechenzentrums erhöht werden. Seit 2021 ist eine leichte Abnahme der Gesamt-IT Flächen von 2,1 Mio. m² in Deutschland durch eine Erhöhung der Leistungsdichten festzustellen. Diese Konsolidierungsbestrebungen machen sich am stärksten durch die Flächenabnahmen im Bereich der traditionellen, häufig unternehmenseigenen, Rechenzentren erkennbar. Hier ist eine Migration der installierten IT-Leistung hin zu Cloud- und Edge-Betriebsmodellen erkennbar (Hintemann et al., 2022). Die IT-Fläche als Maß zur Beschreibung der Entwicklung der Rechenzentrumskapazitäten ist somit nur noch sehr bedingt aussagekräftig.\\n\\nBei entsprechender Grundstücksfläche ist laut der befragten Expert:innen auch ein modularer Rechenzentrumsaufbau denkbar. Modularer Aufbau bedeutet, dass zunächst nicht die gesamte Grundstücksfläche bebaut wird, sondern zunächst nur die Flächen, die aktuell gebraucht werden. Flächen für absehbares zukünftiges Wachstum des Rechenzentrums werden freigehalten, sofern dies in entsprechenden Bauanträgen vorgesehen ist.\\n\\nGroße KI-Modelle für Deutschland\\n\\n136\\n\\nLatenzen Der Begriff beschreibt das Zeitintervall zwischen dem Moment, in dem eine Anfrage an ein System gestellt wird, und dem Zeitpunkt, an dem die Antwort des Systems empfangen wird. Latenzen werden in Millisekunden oder Mikrosekunden gemessen. Den befragten Expert:innen zufolge sind die Latenzen für das Training eines KI- Foundation-Modells nicht von großer Bedeutung, da dies lokal auf einem geeigneten Rechencluster erfolgt. Für den Betrieb und die Entwicklung von Inference Anwendungen, die später auf Basis des Foundation-Modells entstehen, sollte ein Rechenzentrum je nach Use Case über eine Bandbreite von mindestens 100 Gbit/s verfügen und Latenzanforderungen von unter 10 ms RTD (Round Trip Delay) erfüllen.\\n\\nAnforderungen an Zertifizierungen, Datenschutz und Compliance KI-Hochleistungsrechenzentren sind in Bezug auf ihre Anforderungen im Bereich (DSGVO) mit anderen Zertifizierungen und der Datenschutzgrundverordnung laut Rechenzentren gleichzustellen. Die gängigsten Zertifizierungen sind u.a. Rechtsexpert:innen ISO 27001 (Zertifizierung auf der Basis von IT-Grundschutz), ISO 9001 (Qualitätsmanagementsystem) und (Bau und Betrieb sicherer Rechenzentren). Der Kriterienkatalog C5 des BSI beschreibt die Mindestanforderungen für sicheres Cloud Computing und muss berücksichtigt werden.\\n\\nISO EN 50600\\n\\n8.3 Nachhaltigkeitsaspekte\\n\\nAbwärme zu den wesentlichsten Die Nutzung der entstehenden Abwärme gehört in Rechenzentren und wird aufgrund verschiedener Nachhaltigkeitsaspekten Herausforderungen rege diskutiert. Bislang weitgehend ungenutztes Potenzial liegt in der Einspeisung CO2-freier Abwärme von Rechenzentren in Nah- und Fernwärmenetze. Die vorhandene Kühlungstechnologie ist dabei der ausschlaggebende Faktor, wie energieeffizient die entstandene Abwärme eines Rechenzentrums weiter genutzt werden kann. Ist eine Luftkühlung im Rechenzentrum installiert, erreicht die Abwärme laut Expert:innen Temperaturen von 30-35°C, in Spezialfällen auch bis zu 50°C. Diese Temperaturen sind jedoch zu niedrig, um die Abwärme direkt in ein Wärmenetz einspeisen zu können. Dies bedeutet, dass vor der Weiterleitung der Abwärme eine Wärmepumpe eingesetzt werden muss, um die Temperatur den Anforderungen des Wärmenetzes anzupassen. Ist eine Wasserkühlung im Rechenzentrum verbaut, kann die Abwärme mit Temperaturen von 60-70° C direkt einem Wärmenetz der vierten Generation zur Verfügung gestellt werden. Technologisch ältere Wärmenetze erfordern allerdings höhere Einspeisetemperaturen. Durch das Hochverdichten der zu niedrigen Abwärme entstehen hier zusätzlich energetische Aufwände durch den Betrieb der Wärmepumpen. Moderne Methoden der Wasserkühlung werden aufgrund ihrer Neuheit noch selten genutzt (vgl. Kühlung).\\n\\nGroße KI-Modelle für Deutschland\\n\\n137\\n\\nEine weitere Herausforderung lokale Abnahme der Abwärme über Nahwärmenetze. Auch wenn es möglich wäre, die Abwärme für die Beheizung umliegender Gebäude zu nutzen, fehlt es häufig vor Ort an Abnehmern (z.B. Wohn- und Büro- und Industriegebäuden), die genügend Abwärme aufnehmen können, sowie den politischen und regulatorischen Rahmenbedingungen (Bitkom e.V., 2022b). Es können deshalb bislang nur kleine Teile der Abwärme der Rechenzentren genutzt werden.\\n\\nist die\\n\\nStrom Ein weiterer Nachhaltigkeitsaspekt ist die Stromversorgung eines Rechenzentrums mit grünem Strom. Entscheidend ist hierbei eine konsequente und erfolgreiche Umsetzung der Energiewende, die den Ausbau und insbesondere die Verfügbarkeit von Strom aus erneuerbaren Energien in Deutschland beschleunigt (Bitkom e.V., 2022b). Grüner Strom wird aus erneuerbaren Energiequellen wie Sonne, Wind, Wasser oder Biomasse gewonnen.\\n\\nZukünftig wird das Forschungsprojekt ESCADE des Bundesministeriums für Wirtschaft und Klimaschutz (BMWK) durch weltweit führende Hard- und Software-Technologien prüfen, wie die Nachhaltigkeitsbilanz von KI-Anwendungen verbessert werden kann.\\n\\n8.4 Infrastrukturanforderungen im Detail\\n\\nDie kalkulierten Infrastrukturanforderungen zur Berechnung des KI-Foundation-Models erfolgten auf Basis der öffentlichen Informationen zu dem GPT-3 Sprachmodell von OpenAI.\\n\\nTabelle 9: Compute Anforderungen für die Berechnung eines Foundationmodells\\n\\nGroße KI-Modelle für Deutschland\\n\\n138\\n\\nNach Einschätzung der Expert:innen werden für das Training des LEAM Foundation- Modells folgende Compute-Ressourcen benötigt. Dabei wird exemplarisch für die Berechnung von einem Einsatz von 560 der leistungsfähigsten GPU-Knoten (z.B. NVIDIA DGX H 100, AMD Instinct MI200 oder Intel Ponte Vecchio Data Center GPU) ausgegangen. Für einen Trainingsdurchlauf benötigt das System 694 Stunden, insgesamt werden für das Training des Foundation-Modells vier Durchläufe und 2777 Stunden bzw. 115 Tage benötigt. Für die Installation des Systems würden 140 Racks benötigt, in denen jeweils 4 DGX-Knoten à 8 GPU verbaut sind. Pro DGX entsteht Abwärme von bis zu 9 kW pro Rack, sodass insgesamt Werte von 36 kW pro Rack erreicht werden können. Die benötigte Kühlung ist bei diesen Leistungsdichten nur durch Flüssigkeitskühlung bzw. direct-to-Chip Kühlung erreichbar. Bei der neuesten Generation von GPU-Systemen (z.B. NVIDIA DGX H100) ist die Möglichkeit zur direct-to-Chip Kühlung gegeben.\\n\\nAbb. 22: MLPerf hardware: accelarators (Zhang et al., 2022, S.18)\\n\\nDer Artificial Intelligence Index Report 2022 der Stanford University beschreibt die Entwicklung der Anzahl der genutzten GPU in den Top HPC-Systemen für das Training von Machine Learning Algorithmen und zeigt auf, dass für das Training die schnellsten KI- Algorithmen Rechencluster mit einer sehr hohen Anzahl von GPU eingesetzt werden (Zhang et al., 2022). Die maximale Zahl der genutzten GPU-Beschleuniger ist vermutlich seit der Erhebung im Januar 2021 nochmal gestiegen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n139\\n\\n8.5 Standortauswahl\\n\\nWas sind die Parameter für eine qualifizierte Auswahl eines Standortes für ein KI- Rechenzentrum? Neben der regionalen Verortung wird untersucht, was in Bezug auf die Energieversorgung sowie die Anbindung an vorhandene Infrastrukturen zu beachten ist.\\n\\n8.5.1 Vorhandene HPC-Kapazitäten in Deutschland\\n\\nForschungsbereich Das Angebot an High-Performance-Computing für die Wissenschaft wird in Deutschland entsprechend der Leistungsfähigkeit der HPC-Zentren auf den Ebenen 0-3 strukturiert. In Deutschland gibt es drei Hochleistungsrechenzentren der Ebene 1. Das Gauss Centre for Supercomputing (GCS) vereint die drei bedeutendsten Höchleistungsrechenzentren unter einem Namen. Die Gesamtleistung des Jülich Supercomputing Centre (JSC) in Nordrhein-Westfalen, dem Leibniz-Rechenzentrum (LRZ) in Garching bei München und dem Höchstleistungsrechenzentrum Stuttgart (HLRS) beträgt 130 Peta-FLOPS (Stand November 2021). Die HPC-Zentren haben unterschiedliche Ausrichtungen und können so unterschiedlichen Nutzeranforderungen gerecht werden. Außerdem können sie zusammen oder arbeitsteilig agieren.\\n\\nan Ebene Forschungseinrichtungen Nationales Hochschulen. Hochleistungsrechnen (NHR) haben sich acht der 12 universitären HPC-Zentren der Ebene 2 zusammengeschlossen. Dazu gehören:\\n\\n2\\n\\numfasst\\n\\n12\\n\\nüberregionale\\n\\nHochleistungsrechenzentren\\n\\nund\\n\\nZum\\n\\nVerbund\\n\\n\\n\\nIT-Center - RWTH Aachen\\n\\nZuse-Institut Berlin - Berlin University Alliance\\n\\nHochschulrechenzentrum (HRZ) - Technische Universität Darmstadt\\n\\nZentrum für\\n\\nInformationsdienste und Hochleistungsrechnen - Technische\\n\\nUniversität Dresden\\n\\nRegionales Rechenzentrum Erlangen - Universität Erlangen-Nürnberg\\n\\nGesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen - Universität Göttingen\\n\\nSteinbuch Centre for Computing (SCC) - Karlsruher Institut für Technologie\\n\\nPaderborn Center for Parallel Computing - Universität Paderborn\\n\\nZur Ebene 3 gehören regionale HPC-Zentren und Institutionen mit eigenen Clustern für Anwendungen, die eine geringere Leistungsfähigkeit benötigen. Exemplarisch werden hier zwei der leistungsfähigsten HPC-Zentren vorgestellt:\\n\\nGroße KI-Modelle für Deutschland\\n\\n140\\n\\nJülich Supercomputing Centre (JSC) Das Jülich Supercomputing Centre am Forschungszentrum Jülich gGmbH (JSC) ist ein Institut der Helmholtz-Gemeinschaft Deutscher Forschungszentren und ist durch den Bund (90 %) sowie das Land Nordrhein-Westfalen (NRW) (10 %) grundfinanziert. Das JSC verfügt u.a. über den Supercomputer JULES mit 86 Peta-FLOPS Leistung, einen der derzeit leistungsstärksten Rechner Europas. Der JUWELS Multi-Petaflop Supercomputer verfügt über ein Booster Modul, das mit NVIDIA Ampere GPUs ausgestattet ist und somit für die Berechnung von großen KI-Foundation-Modellen geeignet ist. Der Zugang zu HPC- Rechenressourcen erfolgt über halbjährliche Projektaufrufe, die anhand eines objektiven Peer-Review-Verfahrens ausgewählt werden. Das FZ Jülich ist eine gemeinnützige GmbH des öffentlichen Rechts und die Finanzierung erfolgt hier zu 50 % über die EU über die Organisation PRACE - Partnership for Advanced Computing in Europe und zu 50 % über das Bundesministerium für Bildung und Forschung (BMBF) und das Land NRW über den Verein Gauss Centre for Supercomputing (GCS). Für Projekte (üblicherweise Simulationen im Bereich Klimaforschung und Quantenphysik) gilt eine maximale Berechnungszeit von 24 Stunden. Nur in Ausnahmefällen und für größere Projekte sind Jobketten und Reservierung möglich. Ab 2023 wir hier mit der Installation des ersten europäischen Exascale-Rechners JUPITER begonnen, der unter anderem genutzt werden soll, um rechenintensive Machine-Learning-Algorithmen der neuesten Generation zu trainieren (Jülich Forschungszentrum, 2022). Das JSC richtet sich vornehmlich an die Wissenschaft und vergibt die Rechenzeit in einem kompetitiven Verfahren zweimal jährlich nach dem Peer-Review-Verfahren.\\n\\nDas FZ Jülich ist auch Partner des durch das Bundeswirtschaftsministerium geförderten Projekts OpenGPT-X, in dessen Rahmen ein Sprachmodell auf dem Supercomputer JUWELS trainiert wird. Aktuell nutzt das Projekt allerdings nur rund 320 GPUs. Dieser Wert wird sicherlich noch steigen, ist aber nicht mit den hier vorgeschlagenen 4480 GPUs vergleichbar.\\n\\nHöchstleistungsrechenzentrum Stuttgart (HLRS) Das Höchstleistungsrechenzentrum Stuttgart (HLRS) ist ein zentrales Institut der Universität Stuttgart, das u.a. den Supercomputer Hawk mit 26 Peta-FLOPS betreibt und seit 25 Jahren für Wissenschaft und Industrie zur Verfügung steht. Das HLRS ist Mitglied des deutschen Gauss Centre for Supercomputing (GCS), wodurch es eine teilweise Grundfinanzierung durch das Bundesministerium für Bildung und Forschung (BMBF) erhält. Der andere Teil der Grundfinanzierung wird durch das Land Baden-Württemberg bereitgestellt. Darüber hinaus finanziert sich das HLRS durch Forschungsmittel (Projektförderung) und Einnahmen aus der Nutzung der HLRS HPC-Rechenkapazitäten durch Unternehmen und die Industrie. Die Ressourcennutzung durch die Privatwirtschaft ist auf ca. 10 % der Rechenkapazität beschränkt und machte im Jahr 2021 rund 2 % der Drittmitteleinnahmen aus. Kennzeichnend für das HLRS sind die sogenannten Solution Center, die als externe Gesellschaften den Transfer in die Wissenschaft und Wirtschaft organisieren und den Zugang zu Höchstleistungsrechnern fördern.\\n\\nGroße KI-Modelle für Deutschland\\n\\n141\\n\\nNutzung von HPC-Rechenkapazitäten aus dem Bereich der Forschung am Beispiel HLRS *:\\n\\nDas HLRS kann zurzeit maximal 192 GPUs (24 GPU-Knoten) gleichzeitig für die Berechnung eines Foundation-Modells anbieten. Hier würden dann entsprechend 16.192 Knotenstunden für einen Trainingsdurchlauf anfallen. Das entspricht in etwa 675 Tagen Dauerbetrieb von 24 KI-Knoten mit jeweils 8 GPUs.\\n\\nEine exklusive Nutzung sämtlicher GPU-Knoten in dieser Form wäre am HLRS aktuell nicht realisierbar und nicht mit den zeitlichen Anforderungen an die Innovationszyklen bei der Entwicklung eines Foundation-Modells vereinbar (vgl. 1.3 Anforderungen an ein KI-Hochleistungsrechenzentrum).\\n\\nFür die GPU/CPU Rechenleistung fallen die folgenden Kosten für die Berechnung eines Foundation-Modells an:\\n\\n1. GPU-Nutzung Die 24 KI-Knoten des HLRS benötigen ca. 23 * 694 Knotenstunden für die Berechnung (560/24= ca. 23) --> 16.193 Knotenstunden = 4 Durchläufe ergeben dann 64.772 Knotenstunden.\\n\\n64.772 KI-Knotenstunden x 19,50 EUR pro KI-Knotenstunde entsprechen einem Preis von 1.263.054 EUR\\n\\nFür das Preprocessing der Daten, das bis zu 20.000 CPU-Cores und eine geschätzte maximale Laufzeit von 1.000 Stunden in Anspruch nehmen wird, würden folgende Zielkosten für die Nutzung der CPU-Cluster am HLRS entstehen:\\n\\n2. CPU-Nutzung 20.000 CPU-Cores für 840 Stunden Laufzeit --> 168.000.000 Core-Stunden Die aktuellen HAWK-Knoten am HLRS besitzen 128 CPU-Cores. Da hier pro Knotenstunde abrechnet wird, ergeben sich:\\n\\n131.250 Knotenstunden x Forschungspreis (Stand: 2022) in Höhe von 1,13 EUR/Knotenstunde = ca. 80.000 EUR\\n\\nBei der Nutzung durch die Industrie wird am HLRS zudem ein Zuschlag in Höhe von 10% bis 30% veranschlagt.\\n\\nDie Kostenabschätzung erfolgte auf Basis einer öffentlich zugänglichen Entgeltordnung mit Stand 2022.\\n\\nAn allen untersuchten Standorten mit HPC-Rechenkapazitäten stehen die benötigten GPU-Hardwareressourcen für LEAM nicht in ausreichendem Umfang zur Verfügung, sodass diese nicht als Bereitsteller von Infrastruktur in Frage kommen. Diese kommen eher als Nutzer von zukünftig verfügbaren spezialisierten KI-Recheninfrastrukturen in Frage. Kooperationen sind hier ebenfalls denkbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n142\\n\\nKommerzielle Anbieter Neben den Forschungseinrichtungen gibt es auch kommerzielle Anbieter, die HPC- Kapazitäten am Markt bereitstellen. Von diesen sind insbesondere die sogenannten Hyperscaler relevant. Als Hyperscaler werden Unternehmen bezeichnet, die sehr große Rechenzentren betreiben und ihren Kunden Clouddienste auf Basis von hochskalierbaren Infrastrukturen zur Verfügung stellen. Rechenleistung, Speicherkapazität und andere Ressourcen können hierbei nahezu verzögerungsfrei auf Anfrage bereitgestellt werden und die Abrechnung der verwendeten Ressourcen erfolgt in der Regel nach tatsächlicher Nutzung. Hyperscaler unterscheiden sich von anderen Anbietern durch die schiere Größe der jeweiligen Rechenzentren (teilweise deutlich über 100 MW Leistung) und ihre Fähigkeit, große Mengen an Daten und Rechenleistung schnell und effizient zu verarbeiten und die Dienste weltweit, hoch skalierend anbieten zu können. Um die gestiegenen Kundenanforderungen an niedrige Latenzen bedienen zu können, betreiben sie in der Regel mindestens ein Rechenzentrum auf den relevanten Kontinenten und bieten eine garantierte Datenspeicherung und -verarbeitung über verschiedene geografische Verfügbarkeitszonen an.\\n\\nDie größten und bekanntesten Hyperscaler sind Amazon Web Services (AWS), Microsoft Azure und Google Cloud Platform (GCP) aus dem amerikanischen Raum sowie zunehmend die chinesischen Unternehmen Tencent Cloud und Alibaba. Daneben gibt es auch noch weitere große Unternehmen wie IBM, Oracle oder HPE, die HPC-Kapazitäten bereitstellen und einige kleinere, hochspezialisierte KMUs und Start-ups wie z.B. Lambda. Als deutsche Unternehmen bieten bspw. IONOS, Northern Data und auch Aleph Alpha HPC-Lösungen an.\\n\\nGenerell stellen alle diese Unternehmen spezielle Systeme zur Verfügung, die für den gedachten Einsatzzweck der Verarbeitung von KI-Anwendungen konzipiert wurden und Zugriff auf leistungsfähige GPUs ermöglichen. Unterschiede gibt es jedoch bei der verwendeten Hardware und der Bereitstellung. Bei den meisten Anbietern sind z.B. NVIDIAs A100 GPUs verfügbar. Der Einsatz der neuesten Generation der H100 GPUs wird aktuell evaluiert und soll in Kürze z.B. bei Microsoft Azure zur Verfügung stehen. Ein weiterer Unterschied ergibt sich in der Art der Bereitstellung der gewünschten Ressourcen. Als Cloudspezialisten bieten naturgemäß alle Unternehmen die Möglichkeit von virtualisierten Systemen an. Hierbei laufen auf einem Hostsystem ein oder mehrere virtualisierte Systeme. Manche Anbieter wie IBM und Microsoft Azure bieten darüber hinaus auch den Zugriff auf dedizierte Systeme an, die sich noch flexibler konfigurieren lassen. Im Fall von Microsoft Azure kann sogar ein dedizierter Supercomputer des Herstellers Cray integriert werden.\\n\\nAuch wenn somit prinzipiell der Aufbau eines geeigneten Clusters möglich wäre, stehen zumindest in Europa nicht ausreichend Ressourcen in Form von GPUs zur Verfügung. Laut Aussage der befragten Expert:innen, gibt es keinen Anbieter, der die geforderte Anzahl von ca. 4.500 GPUs der neuesten Generation aus einem Rechenzentrum heraus zur Verfügung stellen kann. Die verteilte Nutzung von Ressourcen aus mehreren Rechenzentren scheitert aktuell an der notwendigen Bandbreite des internen Netzwerks. In diesem Zusammenhang muss jedoch erwähnt werden, dass die Anbieter vermutlich langfristigen Nutzungsverträgen bereit wären, entsprechende Kapazitäten bei\\n\\nGroße KI-Modelle für Deutschland\\n\\n143\\n\\naufzubauen. Die Kosten hierfür dürften jedoch höher ausfallen als bei den anderen hier betriebswirtschaftliche aufgezeigten Vergleichsrechnung für verschiedene HPC-Angebote für den Bezug von Rechenleistung aus der Cloud erfolgt in Kapitel 10.1.\\n\\nBetriebsmodellen.\\n\\nEine\\n\\nbeispielhafte\\n\\nDie Nutzung von Kapazitäten außerhalb Europas wäre zwar denkbar, steht aber dem Ziel des Aufbaus eines deutschen bzw. europäischen KI-Ökosystems zur Entwicklung, Bereitstellung, Betriebs sowie der Integration und Validierung besonders leistungsfähiger KI-Modelle entgegen.\\n\\nEin weiterer Punkt, der gegen den Aufbau eines cloudbasierten Systems unter Verwendung eines Hyperscalers spricht, ergibt sich aus der Zielgruppe der KMU. Auch wenn das Niveau und die Standards in Bezug auf Datensicherheit bei den Hyperscalern eher über dem Durchschnitt liegen, gibt es nach wie vor Bedenken bezüglich des Schutzes vor dem Zugriff Unbefugter, insbesondere staatlicher Stellen, auf die eigenen Daten. Je sensibler die eigenen Daten eingeschätzt werden, desto größer ist die Skepsis. Dies könnte unter Umständen dazu führen, dass notwendige Trainingsdaten nicht zur Verfügung gestellt werden. Hinzu kommt, dass viele Hyperscaler so genannte Lock-in Effekte zur Kundenbindung nutzen, die einen leichten Einstieg ermöglichen und einen späteren Wechsel zu einem anderen Anbieter erschweren.\\n\\nSPOTLIGHT Merantix Momentum GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDas Leadership-Team: Dr. Johannes Otterbach, Nicole Büttner-Thiel, Dr. Thomas Wollmann.\\n\\nMerantix Momentum ist ein in Berlin ansässiges KI- Startup, das auf die Einführung und Skalierung von KI- basierten Lösungen in verschiedenen Branchen spezialisiert ist. Mit einem erfahrenen Team lösen wir als KI-Service-Anbieter die Herausforderungen unserer Kunden durch maßgeschneiderte Machine-Learning- Lösungen und sichern so deren zukünftige Wettbewerbsfähigkeit in digitalen und datengetriebenen Märkten. Mit einer eigenen Forschungsabteilung unterstützen wir gleichzeitig aktiv den Transfer von Machine-Learning-Methoden in die produktive Anwendung bei Firmen und Organisationen in Deutschland und Europa.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Merantix Momentum spezialisiert sich auf die Implementierung und Umsetzung riskanter KI-Innovationsprojekte. Dabei begleiten wir unsere Kunden und Partner von der initialen Use Case Entwicklung, über die Datenstrategie bis hin zur Entwicklung und auch dem Produktionsbetrieb der KI-Lösungen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n144\\n\\nFoundation-Modelle bieten dabei den Vorteil, schnell und effizient auf limitierten Datensätzen unserer Kunden, neue Deep-Learning Lösungen zu entwickeln. Hierbei stellt es sich als vorteilhaft heraus, dass Foundation-Modelle auf einer breiten Datenbasis trainiert wurden, die mittelbar auf die Kundendaten übertragbar sind. Mit einer breiteren Verfügbarkeit verschiedener Foundation-Modelle, das heißt trainiert auf diversen Daten, lässt sich somit die Anwendung moderner KI-Methoden auf bisher unerschlossene Anwendungen realisieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? KI-Foundation-Modelle unterstützen uns bei der Projektentwicklung an mehreren Stellen. Zum einen helfen sie bei der Use Case Entwicklung, da sie ein schnelles Prototypisieren ermöglichen und damit die Kreativität unserer Kunden und Partner, aber auch unserer Entwickler:innen entfesseln. Zum anderen bieten sie aber auch die Möglichkeit, durch Distillation kleine und effiziente Modelle zu entwickeln, die durch das Trainieren eines neuen Modells von Grund auf gar nicht erst möglich gewesen wären, da die Datenlage oftmals nicht ausreichend ist. Damit schlagen KI- Foundation-Modelle gleich zweifach durch: In der Innovationsphase durch Unterstützung im Kreativprozess und in der anschließenden Entwicklung, die sonst nicht möglich wäre.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Zunächst einmal sollte klargestellt werden, dass bestimmte Biases gewollt sind. Hierbei handelt es sich um ethische und moralische Grundeinstellungen der Modelle, eben basierend auf europäischen Werten. Dies wird bedingt durch die verschiedenen Kulturkreise, in welchen die Modelle entwickelt werden. Abgesehen davon sind Europa-basierte Modelle hilfreich, um Anwendungen schneller in die Praxis bringen zu können. Der europäische Bias ermöglicht es uns, voraussichtlich schneller die Modelle anpassen zu können mit weniger Daten. Zum anderen müssen wir uns weniger mit dem Output der Modelle und deren Untersuchung auf ungewollten Output oder mögliche Schwachstellen beschäftigen, da eine gewisse minimale Operationslinie angenommen werden kann, die man bei nicht-europäischen Modellen so nicht annehmen darf.\\n\\nGroße KI-Modelle für Deutschland\\n\\n145\\n\\n8.5.2 Erforderliche Standortparameter\\n\\nGeopolitische und datenschutzrechtliche Verortung Im europäischen Kontext stellen die sogenannten FLAP-Märkte (Frankfurt, London, Amsterdam und Paris) – in den vergangenen Jahren häufig durch Dublin als FLAP-D ergänzt – die historisch gewachsenen Gravitätszentren für die großen Internet Hubs in Europa dar. Durch diese digitalen Internetzentren verlaufen, ähnlich der Entwicklung der historischen Seidenstraße, die weltweiten Datenübertragungsleitungen für den globalen Internetverkehr. Mit Ausnahme von London liegen diese Zentren im Geltungsbereich der Europäischen Union und stellen damit einen datenschutzrechtlich sicheren EU- konformen Rechtsrahmen für ihre Nutzer:innen dar.\\n\\nHistorisch betrachtet folgt diese Entwicklung dem Aufbau der internationalen Telekommunikationshubs in der ersten Hälfte des 20. Jahrhunderts bzw. schon den Entwicklungen der ersten industriellen Revolution ab der zweiten Hälfte des 18. Jahrhunderts. Diese verdichtete Ansiedlung digitaler Infrastrukturelemente ist aus volkswirtschaftlicher Sicht im Bereich der sogenannten „Blauen Banane”, einer dicht besiedelten Kette von Ballungsräumen angefangen von Manchester, dem Großraum London über die Amsterdamer „Randstad”, das Ruhrgebiet, die Rhein-Main-Region hin zu den Industriestandorten in Mannheim, Ludwigshafen und Basel bis schlussendlich in die Industriezentren Norditaliens mit Mailand und Turin zu verorten.\\n\\nMit einer Übertragungskapazität von mehr als 50 Tbit/s zählt allein die West-Ost- Datentrasse von Dublin über Amsterdam Richtung Frankfurt zu den größten transeuropäischen Trassenführungen für das IP-Routing, entlang derer sich die großen Volumina an Datenverkehr im Internet bewegen. Dies entspricht mehr als dem fünffachen Durchsatz des heute weltweit größten Internetknotens in Frankfurt (Simons & Frese, 2021). Ein weiterer Datenkorridor (Nord-Süd) verbindet die skandinavischen Rechenzentrumsansiedlungen mit den europäischen und amerikanischen Content- Anbietern via Stockholm, Kopenhagen, Düsseldorf, Frankfurt und Paris.\\n\\nEntlang dieser Trassen hat sich in den vergangenen 20 Jahren eine digitale Ökonomie mit allen Ausprägungen der Wertschöpfungskette herausgebildet. Um diese großen Rechenzentrumsansiedlungen haben sich häufig digitale Ökosysteme aus den Bereichen Software-Entwicklung, Content, KI oder IT-Dienste angesiedelt. Den Gravitätsanker für diese Ansiedlungen bilden häufig kurze Latenzzeiten, breitbandige Anbindung an die transkontinentalen Backbone-Trassen, verdichtete Metropolstrukturen mit der entsprechenden Anzahl potenzieller Nutzer:innen sowie ein ausreichend zur Verfügung stehender Markt an gut ausgebildeten Fachkräften.\\n\\nSowohl auf europäischer als auch auf nationaler Ebene ist der Wachstumstrend im Rechenzentrumsmarkt weiterhin ungebrochen. Zwischen 2016 und 2021 wuchsen die Kapazitäten gemessen in IT-Anschlussleistung um 30 % (Hintemann et al., 2022). Die Profiteure dieses Trends sind allen voran die Rechenzentrumsanbieter mit installierten Leistungen ab 5 MW und deutlich darüber hinaus. Kleinere Installationen unter 5 MW befinden sich aktuell in einem stagnierenden oder absteigenden Trend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n146\\n\\nGemessen an der Bedeutung nimmt die Rhein-Main-Region als Rechenzentrums-Standort in Deutschland unverändert eine Spitzenposition ein. Neben Frankfurt wird künftig Berlin für Rechenzentrums-Entwicklungen eine immer stärkere Rolle spielen. Gemessen an der IT-Anschlussleistung pro Einwohner kommt Hessen auf einen mehr als dreimal höheren Wert als die Stadtstaaten Hamburg oder Berlin. Neben Berlin werden künftig auch Standorte wie München, Hamburg sowie die Regionen Köln/Düsseldorf und Leipzig/Dresden für Rechenzentrumsentwicklungen immer wichtiger (Hintemann et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n147\\n\\nIm Bereich High Performance Computing (HPC) sind in Deutschland folgende bedeutende Standorte zu nennen:\\n\\nStadt\\n\\nEinrichtung\\n\\nPeak Performance\\n\\nMemory\\n\\nEggenstein- Leopoldshafen\\n\\nFrankfurt\\n\\nOffenbach\\n\\nHamburg\\n\\nHamburg\\n\\nKaiserslautern\\n\\nKaiserslautern\\n\\nKöln\\n\\nGöttingen\\n\\nDarmstadt\\n\\nStuttgart\\n\\nAachen\\n\\nJülich\\n\\nBerlin\\n\\nGarching\\n\\nGarching\\n\\nPaderborn\\n\\nMainz\\n\\nDresden\\n\\nErlangen\\n\\nSteinbuch Centre for Computing - ForHLR - bwUniCluster Center for Scientific Computing - LOEWE CSC - FUCHS Deutscher Wetterdienst - Cray XC40 - Cray CX40 II Deutsches Elektronen Synchrotron - Standort Zeuthen - Maxwell German Climate Computing Center Mistral (HLRE-3) Competence Center HPC - Beehive - Seislab - Ocean 1 Reg. Hochschulrechenzentrum - Elwetritsch - Elwetritsch II Reg. Rechenzentrum Uni Köln - CHEOPS Ges. f. wissen. Datenverarbeitung - Scientific Compute Cluster - Emmy Hochschulrechenzentrum - Lichtenberg II Höchstleistungsrechenzentrum - Hawk - Cray CS-Storm - vulcan IT Center RWTH - CLAIX-2016 - CLAIX-2018 Jülich Supercomputing Centre - JUWELS Konrad-Zuse Zentrum - Lise Leibniz Rechenzentrum - Super MUC - Super MUC NG Max Planck Computing & Data Facility - COBRA Paderborn Center for Parallel Comp. - XCL FPGA Cluster - Noctua 1 - Noctua 2 Zentrum für Datenverarbeitung - Mogon - Clover - Mogon 2 Center for Information Services / HPC - Taurus - Alpha Centauri Erlangen National Center for HPC - Emmy - Meggie - TinyGPU - TinyVec - TinyARM - TinyFAT - Alex - Fritz\\n\\n1171 TFlop/s 444 TFlop/s\\n\\n823 TFlop/s 41 TFlop/s\\n\\n1073 TFlop/s 1073 TFlop/s\\n\\n16 TFlop/s -\\n\\n3590 TFlop/s\\n\\n67 TFlop/s 35 TFlop/s -\\n\\n134 TFlop/s\\n\\n100 TFlop/s\\n\\n2883 TFlop/s 8261 TFlop/s\\n\\n3148 TFlop/s\\n\\n26000 TFlop/s - -\\n\\n678 TFlop/s 4965 TFlop/s\\n\\n12000 TFlop/s\\n\\n7907 TFlop/s\\n\\n3580 TFlop/s\\n\\n12720 TFlop/s\\n\\n835 TFlop/s 7100 TFlop/s\\n\\n379 TFlop/s 106 TFlop/s 3125 TFlop/s\\n\\n2621 TFlop/s -\\n\\n232 TFlop/s 511 TFlop/s - - - - - -\\n\\n136 TB 86 TB\\n\\n70 TB 18 TB\\n\\n125 TB 125 TB\\n\\n402 TB\\n\\n266 TB\\n\\n14 TB 6 TB 23 TB\\n\\n17 TB 53 TB\\n\\n36 TB\\n\\n92 TB 498 TB\\n\\n251 TB\\n\\n1 TB 9 TB 102 TB\\n\\n88 TB 251 TB\\n\\n286 TB\\n\\n455 TB\\n\\n197 TB\\n\\n530 TB\\n\\n512 GB 53 TB 355 TB\\n\\n90 TB 10 TB 194 TB\\n\\n279 TB 35 TB\\n\\n36 TB 47 TB 5 TB 96 TB 128 GB 22 TB 65 TB 242 TB\\n\\nTabelle 10: HPC-Standorte in Deutschland\\n\\nGroße KI-Modelle für Deutschland\\n\\nCPU-Cores\\n\\n34800 Cores 18304 Cores\\n\\n18960 Cores 6456 Cores\\n\\n29552 Cores 29952 Cores\\n\\n2288 Cores 26732 Cores\\n\\n101196 Cores\\n\\n3224 Cores 1584 Cores 11600 Cores\\n\\n5624 Cores 10520 Cores\\n\\n9712 Cores\\n\\n16640 Cores 116152 Cores\\n\\n61824 Cores\\n\\n702896 Cores 608 Cores 13856 Cores\\n\\n16152 Cores 62736 Cores\\n\\n123088 Cores\\n\\n110016 Cores\\n\\n86016 Cores\\n\\n136960 Cores\\n\\n32 Cores 10960 Cores 143488 Cores\\n\\n35760 Cores 5120 Cores 52248 Cores\\n\\n64536 Cores 1632 Cores\\n\\n11088 Cores 14560 Cores 1392 Cores 12 Cores 64 Cores 2484 Cores 8960 Cores 67968 Cores\\n\\n148\\n\\nDie oben aufgeführten Rechenzentrumsstandorte für HPC-Anwendungen lassen sich in unmittelbarer Nähe zu Universitäten und Forschungseinrichtungen verorten und folgen nicht zwingend der vorab beschriebenen Entwicklungslogik gewerblicher Collocation und Hyperscale-Rechenzentren Im wissenschaftlichen und universitären Bereich haben sich diese Rechenzentren in der Regel über eigene Netze (z.B. das Wissenschaftsnetz X-WiN, welches vom Deutschen Forschungsnetz DFN betrieben wird) untereinander verbunden. Die Anbindung des X-WiN an externe Netze erfolgt an dedizierten Standorten über lokale Internet-Knotenpunkte, wie z.B. am DE-CIX in Frankfurt am Main und Hamburg, am ECIX in Düsseldorf und am BCIX in Berlin.\\n\\nentlang historisch\\n\\nentwickelter Datentrassen.\\n\\nRegionale Verortung: Flächenbedarf, Kubatur, Gebäudeinfrastruktur Bei der regionalen und lokalen Verortung von Rechenzentren rücken andere Kriterien in den Vordergrund als bei einer geopolitischen oder nationalen Betrachtung. Als generelle Ansiedlungskriterien von Rechenzentren wären hier zu nennen:\\n\\nDie Grundstücke liegen nicht in direkter Nachbarschaft zu oder in Wohngebieten. • Eine einfache Erreichbarkeit, u.a. durch öffentlichen Personennahverkehr oder Straßenanbindung ist gegeben.\\n\\nAusschluss von ansiedlungsbehindernden Bedrohungs-/Gefahrenlagen (siehe auch Seveso II/III-Gebiete): Nähe zu Flughäfen, chemischer Industrie, Güterverkehrsstrecken, Elektromagnetische Exposition, Schwingungsquellen etc. Aber auch mögliche Gefahrenlagen durch Naturereignisse (Hochwasser, aktive seismische Zonen, Nähe zu Küstenlinien, etc.) sind zu vermeiden.\\n\\nEs existieren Ansätze für lokale Nutzungen des Rechenzentrums, u.a. ein vielfältiges Nutzerspektrum an datenzentrierten Unternehmen und Forschungsinstitutionen; idealerweise im Umkreis von 50 km.\\n\\nEine zuverlässige und redundant ausgebaute sowie skalierbare Stromversorgung wird bereitgestellt. Eine räumliche Nähe zum Umspannwerk wird hierbei angestrebt. Bei redundanter Versorgung über zwei Umspannwerke wird häufig die geographische Mitte zwischen zwei Umspannwerken bevorzugt.\\n\\nEine gute Anbindung an überregionale Glasfasertrassen ist gegeben. Idealerweise sind mindestens zwei überregionale (sog. Longhaul) Glasfasertrassen-Anbieter vorhanden, welche in Summe drei schleifen- und kreuzungsfreie Wegeführungen mit mindestens 20 nutzbaren Glasfaserpaaren je Weg zur Trassenanbindung realisieren können; die maximale Distanz zur Trasse beträgt 5 km, bezogen auf eine potenzielle Ansiedlungsfläche.\\n\\nEs ist ein Zugang zu einem Internetknoten-Anbieter vorhanden, der mit einer diskriminierungsfreien, verteilten Plattformkonzeption eine Vielzahl von Interconnection-Diensten auf Enterprise Niveau realisieren kann.\\n\\nEin zukünftig wichtiger werdender Faktor wird die Nähe zu möglichen Abnehmern von Abwärme (z.B. Quartiers-Konzepte, Schwimmbad, Vertical Farming etc.) oder die direkte Einspeisemöglichkeit in ein Nah- oder Fernwärmenetz bilden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n149\\n\\nFlächenbedarf Für Großrechenzentren und Hyperscaler spielt eine ausbaufähige Flächen-Skalierbarkeit eine entscheidende Rolle für das jeweilige Ansiedlungskonzept. Ansiedlungen von mittelständischen Betreibern gehen mit entsprechend kleineren Grundstücksgrößen ins Rennen. Grundsätzlich zu unterscheiden sind:\\n\\n\\n\\n„Solitär”-Rechenzentrum: Bau eines einzelnen in sich geschlossenen Rechenzentrums (häufig für unternehmensinterne Verwendung). Größenordnung: 0,5 bis ca. 3 ha Grundstücksfläche, Leistungsaufnahmen in der Regel bis 10 MW\\n\\nCampus Rechenzentrum: Es stehen mehrere jeweils für sich autark betriebene Rechenzentrums-Betriebseinheiten auf einem größeren Rechenzentrumscampus. Das Betriebsmodell ist häufig auf Collocation, Shell & Core oder Hyperscale ausgerichtet. Größenordnung: 10 ha Grundstücksfläche und mehr, Leistungsklasse: 50-120 MW\\n\\nGroßer Hyperscale Campus: Ab ca. 15 ha aufwärts. Das aktuell größte Ansiedlungsprojekt in Europa befindet sich in der Nähe von Esbjerg, Dänemark und umfasst ca. 200 ha Arealfläche (die Größe Monacos); Leistungsklassen: >100 MW bis hin zu 300 MW und darüber hinaus.\\n\\nKubatur / Baukennzahlen Das Maß der baulichen Nutzung Intensität einer Grundstücksausnutzung Bestandteil des öffentlichen Baurechts und stellt somit ein wichtiges städtebauliches Steuerungsinstrument des BauGB dar. Bei der Errichtung von Rechenzentren sind aus baurechtlicher Sicht verschiedene Vorgaben einzuhalten. Unter anderem gibt die Grundflächenzahl (GRZ) vor, wie groß der Flächenanteil eines Grundstücks sein darf, der überbaut werden darf. Die Kubatur beschreibt den umbauten Raum eines Gebäudes. Analog zur Erhöhung der Leistungsdichte in einem Serverrack lässt sich die Gesamtanzahl betreibbarer Server mit ansteigender Zahl der Geschosse (Geschossflächenzahl GFZ) eines Gebäudes erhöhen. Die Baumassenzahl (BMZ) schließlich gibt an, wie viele Kubikmeter Baumasse je Quadratmeter Fläche eines Grundstücks zulässig sind.\\n\\nist als Angabe über die\\n\\nDiese Vorgaben sind im lokalen Baurecht entsprechend verankert und können vom sind mehrgeschossige Errichter nicht Rechenzentrumsbauten jedoch sind Beispiele von Industriegebieten üblich, Rechenzentren in Hochhäusern, wie die 60 Hudson Street in Manhattan oder das Royal Bank of Canada Data Centre in Toronto in Deutschland bisher nicht anzutreffen.\\n\\nverändert werden.\\n\\nIn Deutschland\\n\\nin\\n\\nEnergieversorgung: Anbindung und Energieversorgungssicherheit Rechenzentren erfordern für einen (ausfall-)sicheren Betrieb ein redundantes und durchdachtes Stromversorgungskonzept. Um die erforderliche Redundanz zu erreichen, kann, sofern möglich, auf die getrennte Einspeisung über zwei Umspannwerke zurückgegriffen werden. Ist dies nicht der Fall, empfiehlt sich eine Ringeinspeisung über zwei getrennte Zuführungen eines Umspannwerkes.\\n\\nGroße KI-Modelle für Deutschland\\n\\n150\\n\\nWird das Rechenzentrum über eine Mittelspannung (10 kV oder 20 kV) versorgt, ist in der Regel auf dem Grundstück eine kundeneigene oder energieversorgereigene Mittelspannungsstation errichtet. Transformatoreneinspeisung und Messeinrichtungen sind in der Regel in diese Anlagen integriert. Um die Mittelspannung entsprechend in Rechenzentren heruntertransformieren Gießharztransformatoren herkömmlichen Trockentransformatoren wird hierbei eine höhere Teilentladungs-, Kurzschluss- und Spannungsfestigkeit erreicht und die Empfindlichkeit gegenüber Umweltbelastungen wie Feuchtigkeit und Staub ist ebenfalls geringer (Dürr, 2018). Vereinzelt kommen auch Öltransformatoren zum Einsatz. Diese sind bauartbedingt verlustarmer und weisen eine höhere andere Brandschutzerfordernisse auf, die ggf. einen größeren Raumbedarf mit sich bringen. Eine weitere notwendige Komponente zur Sicherstellung der Energieversorgung stellen die Unterbrechungsfreien Stromversorgungen (USV) sowie die Netzersatzanlagen (NEA; z.B. Dieselgeneratoren) dar.\\n\\nzu können, werden üblicherweise\\n\\neingesetzt.\\n\\nGegenüber\\n\\nEnergieeffizienz\\n\\nauf. Öltransformatoren weisen\\n\\njedoch\\n\\nDie Aufgabe einer USV besteht vorrangig in zwei Punkten: a) der kurzfristigen Überbrückung bei Stromausfällen durch Umschaltung auf Batteriespeicher oder Schwungmassenspeicher sowie b) der Eliminierung von Spannungsschwankungen und - spitzen sowie Frequenzabweichungen. Ausgehend vom Wirkungsprinzip lassen sich dabei drei grundsätzliche Typen von USV-Anlagen unterscheiden:\\n\\nVFD (Voltage and Frequency Dependent): Der Strom wird bei diesem USV-Typ direkt vom Eingang zum Ausgang durchgeleitet. Hierbei erfolgt keine „galvanische Entkopplung” mit der vorgeschalteten Last. Über den Eingang der USV wird kontinuierlich der Batteriespeicher mit Energie versorgt. Die Umschaltung im Falle eines Stromausfalls auf Batteriebetrieb ist jedoch unterbrechungsbehaftet und kann bis zu 10 ms benötigen. Dies ist ein Wert, der sich bei empfindlicher IT- Hardware u.U. bereits bemerkbar machen kann.\\n\\nVI (Voltage Independent): USV-Geräte vom Typ VI arbeiten mit einem AC/DC- Wandler als zentrale Komponente, der sowohl als Stromrichter als auch für die Aufladung der Batterien zuständig ist. Die USV läuft im Gegensatz zum vorgenannten Typ auch dann „aktiv\\'\\' mit, wenn der Strom über die Netzspannung zur Verfügung steht. Es werden jedoch Spannungsspitzen herausgefiltert, welche die IT-Hardware schädigen könnten. Die Umschaltzeit liegt hierbei zwischen 2,5 und 10 ms.\\n\\nVFI (Voltage and Frequency Independent): Im Gegensatz zu den beiden oben aufgeführten USV-Typen stellt eine USV vom Typ VFI zusätzlich noch sicher, dass es neben Spannungsspitzen und -schwankungen nicht auch zu unerwünschten Frequenzabweichungen kommt. Der Strom an der Ausgangsseite ist hier komplett entkoppelt vom Strom an der Eingangsseite. Da die USV dieses Typs im Dauerbetrieb arbeitet, fallen keine zusätzlichen Umschaltzeiten an.\\n\\nGroße KI-Modelle für Deutschland\\n\\n151\\n\\nBei der Auslastung der USV-Systeme ist darauf zu achten, dass mit sinkendem Auslastungsgrad der Wirkungsgrad der Anlage kontinuierlich abnimmt. Die erhöhte Verlustleistung macht sich so bei der Energieeffizienz negativ bemerkbar. Die Aufgabe eines Systems aus USV und Batteriespeicher besteht jedoch immer nur darin, eine relativ kurze Zeit (wenige Minuten) zu überbrücken, bis eine Netzersatzanlage (NEA; = Notstromdiesel) ist. längerfristigen Notstromersatzanlagen sind nach ISO 8528 genormt.\\n\\nfür\\n\\nden\\n\\nNotbetrieb\\n\\nangelaufen\\n\\nDieselbetriebene Notstromersatzanlagen müssen regelmäßigen Tests zur Sicherstellung des Betriebs unterzogen werden. Hierbei werden verschiedene Testbetriebsarten unterschieden:\\n\\nNetzparalleler Lastprobebetrieb: Die NEA wird gestartet und mit der Sinuswelle der Netzversorgung synchronisiert. Im Anschluss wird der Generatorschalter eingekuppelt und die NEA parallel zum Netz betrieben. Nach vorhergehender Rücksprache mit dem Energieversorger kann sogar Last in das Netz zurückgespeist werden.\\n\\nLastprobebetrieb im Inselbetrieb: Die NEA wird im Netzparallelbetrieb wie oben beschrieben gestartet. Nach Hochlaufen der Last wird jedoch der Netzschalter ausgekoppelt, sodass die NEA nun die volle Last für den Betrieb der Server erbringen kann. Diese Methode ist der vorangestellten vorzuziehen, da sie die reellen Bedingungen im Falle eines Netzausfalles besser abbilden kann.\\n\\nNetzausfalltest („Back Building Test”): Hierbei handelt es sich um einen „echten” Netzersatztest. Der zentrale Netzschalter wird vor dem Anlaufen der NEA ausgekoppelt, so dass USV und NEA spontan einspringen müssen. Viele Betreiber schrecken vor dieser Art des Netztests zurück, da sie das Risiko zu hoch einschätzen, dass Anlagen nicht anlaufen und es zu einem „echten” Ausfall im Rechenzentrum kommen kann.\\n\\nDie Kunden im Rechenzentrum werden in der Regel über anstehende Tests von USV und NEA im Vorfeld durch den Betreiber informiert. Testläufe von Dieselaggregaten müssen (u.a. auch aus emissionsrechtlichen Erfordernissen) bei den zuständigen Behörden genehmigt werden. In der Regel wird ein Stundenkontingent pro Jahr (z.B. 30 h/a) für den Test-Betrieb genehmigt. Die Dieselgeneratoren werden im Standby-Betrieb elektrisch vorgewärmt, um im Einsatzfall möglichst kurze Anlaufzeiten bis zum Erreichen der vollen Last gewährleisten zu können. Somit verbrauchen NEAs auch offline einen gewissen Betrag elektrischer (Heiz-)Energie.\\n\\nGroße KI-Modelle für Deutschland\\n\\n152\\n\\nVerfügbarkeitsklassen Rechenzentren werden nach dem Grad vorhandener redundanter Komponenten in sogenannte Verfügbarkeitsklassen (VK1 - VK4) unterteilt. Dabei unterscheidet man:\\n\\nVerfügbarkeitsklasse 1\\n\\n(N): Bezeichnet man einzelne Komponenten eines Rechenzentrums (z.B. eine NEA, eine USV, ein Klimaschrank) mit der Variablen „N”, so liegt bei diesem Konzept keine zusätzliche Redundanz vor. Bei Ausfall einer Komponente muss diese zuerst gewartet/repariert werden, bevor diese wieder in Betrieb geht.\\n\\nVerfügbarkeitsklasse 2 (N+1): Der Ausfall einer einzelnen Komponente führt hier nicht zum Ausfall des kompletten Versorgungspfads, da eine zusätzliche Ersatzkomponente (+1) einspringen kann. Beispiel: Zur Kühlung des Serverraums werden 5 Umluftklimageräte benötigt. Das sechste, im Raum verbaute Gerät springt im Fall des Ausfalls eines anderen Geräts ein.\\n\\nVerfügbarkeitsklasse 3 (2N): Bei dieser Redundanzkonzeption sind sämtliche Versorgungspfade„doppelt” ausgelegt. Beispiel: Sämtliche Server sind mit zwei Netzteilen ausgerüstet, die über zwei verschiedene Stromphasen mit Strom versorgt (z.B. bei werden. Durch Abschaltung einer Wartungsarbeiten) ist der operative Betrieb im Serverraum weiterhin gewährleistet. kompletten Stromphase\\n\\nVerfügbarkeitsklasse 4 (2N+1): Dies stellt die höchste Verfügbarkeitsklasse dar. Im Gegensatz zur 2N Konzeption kann bei Wartungsarbeiten eines kompletten Versorgungspfades zusätzlich noch die Ausfallsicherheit einer Einzelkomponente sichergestellt werden.\\n\\nDen Verfügbarkeitsklassen sind entsprechende maximale Ausfallzeiten pro zugeordnet:\\n\\nJahr\\n\\nVerfügbar- keitsklasse\\n\\nBezeichnung\\n\\nMindestver- fügbarkeit\\n\\nMax. Ausfallzeit pro Monat\\n\\nMax. Ausfallzeit pro Jahr\\n\\nVK 0\\n\\nOhne zugesicherte Verfügbarkeit\\n\\n--\\n\\n--\\n\\n--\\n\\nVK1\\n\\nNormale Verfügbarkeit\\n\\n99,0 %\\n\\n< 8 h\\n\\n< 88 h\\n\\nVK2\\n\\nErhöhte Verfügbarkeit\\n\\n99,9 %\\n\\n<44 min\\n\\n<9 h\\n\\nVK3\\n\\nHochverfügbarkeit\\n\\n99,99 %\\n\\n<5 min\\n\\n<53 min\\n\\nVK4\\n\\nHöchstverfügbarkeit\\n\\n99,999 %\\n\\n<26 sek.\\n\\n< 6 min\\n\\nTabelle 11: Verfügbarkeitsklassen (VK1 - VK4)\\n\\nverbundenen Mit Verfügbarkeitsklasse steigen auch die Investitionskosten an. Da die zusätzlichen Komponenten oftmals nicht situativ im Falle einer Wartung oder eines Ausfalls hinzugeschaltet werden, sondern vielmehr im sogenannten Halblastparallelbetrieb (mit)laufen, sind bei höherer Redundanzauslegung auch die Effizienzwerte geringer als im Betrieb ohne Redundanzauslegung. Dies wird von den Betreibern jedoch bewusst in Kauf\\n\\neiner\\n\\nhöheren\\n\\nRedundanzauslegung\\n\\nund\\n\\nder\\n\\ndamit\\n\\nGroße KI-Modelle für Deutschland\\n\\n153\\n\\ngenommen, um einen höheren Grad an Ausfallsicherheit zu gewährleisten. Für LEAM wird für die Inference Anwendungen eine Verfügbarkeit von 99 % benötigt.\\n\\nGlasfaserversorgung: Backbone-Netze, Redundanzen, diskriminierungsfreier und Carrier-neutraler Zugang, Nähe und Zugang zu Internetaustauschknoten Ähnlich wie die Stromversorgung von der externen Übertragungsinfrastruktur des Energieversorgers über Mittelspannungs- und Niederspannungsverteilung auf dem Rechenzentrumsgelände zum Server-Netzteil geleitet wird, erfolgt auch die Datenanbindung einer vergleichbaren Verteilstruktur.\\n\\nÜbergabepunkt: Über nationale und internationale Carrier-Anbindungen wird die externe Datenanbindung zu den Rechenzentren über interne Meet-Me Räume (MMR) sichergestellt. Meet-Me Räume sind hierbei der zentrale Ort innerhalb eines Collocation-Rechenzentrums, an dem sich Telekommunikationsunternehmen und Carrier sowie die Kunden des Collocation-Betreibers physisch miteinander verbinden und Daten austauschen können. Oftmals befinden sich die Hochleistungsrouter eines dezentral aufgestellten Internetknotenpunkts in den Meet-Me Räumen der Rechenzentrumspartners („Enabled Sites”) und ermöglichen so den Zugang zu den ‘Connected Networks’ des Knotenbetreibers.\\n\\nStandortverteilung (SV): Vom Meet-Me Raum als zentralen Übergabepunkt in das externe Netz gelangen die Daten über eine Primär/Campusverkabelung zu den einzelnen Stockwerken/Serverräumen.\\n\\nGebäudeverkabelung (GV): Ggf. unterteilt sich die Netzwerkverkabelung in einzelne Stockwerke über entsprechende Etagenverteiler (vertikale Verteilung).\\n\\nTertiärverkabelung (EV): Über eine Tertiär- oder Etagenverteilung erfolgt dann die Zuleitung der Verkabelung an die Serverschränke in den einzelnen Serverräumen.\\n\\nBei der Verteilung auf Serverschrankebene lassen sich zwei Konzepte voneinander unterscheiden:\\n\\nEnd of Row (EoR): Der erste und/oder der letzte Schrank einer Rackreihe ist mit dem Zugangsswitch für die Anbindung aller anderen Schränke der jeweiligen Rackreihe ausgestattet. Der Schrank mit dem EoR Switch muss dabei eine große Anzahl von Patchkabeln für die horizontale Verkabelung über seine Patchpanels unterbringen. Der Vorteil dieser Anordnung besteht im vereinfachten Change- Management, da sämtliche Patchkabel einer Rackreihe an diesem zentralen Ort zusammenlaufen.\\n\\nDas Middle of Row Konzept (MoR) ist mit dem des EoR vergleichbar, nur dass hierbei der zentrale Schrank mit den Zugangsswitchen in der Mitte der Rackreihe positioniert ist.\\n\\nTop of Row (ToR): Bei diesem Konzept befinden sich in jedem Schrank (zumeist oben) eigene Switche. So können die Patchkabel in der Regel sehr kurzgehalten werden. Bei hoher Rack Anzahl sind jedoch viele kleinere (Edge-)Switche erforderlich. Schrankübergreifende Change-Requests sind beim ToR Konzept nicht so leicht zu realisieren. Das Konzept ist kostenintensiver, da in der Regel mehr Switche benötigt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n154\\n\\nSchutzbedarf/Risiko-Analyse Viele Rechenzentren stellen systemrelevante Dienstleistungen zur Verfügung. Collocation- Cloud- und Hosting-Rechenzentren mit einer vertraglich vereinbarten Leistung von mehr als 3,5 MW fallen unter die Verordnung zur Bestimmung Kritischer Infrastrukturen (BSI-KritisV). Unabhängig davon haben alle Rechenzentrumsbetreiber mehr oder weniger strenge Sicherheitskonzepte für ihre Anlagen vorgesehen. Hierzu zählt die Etablierung eines Zonenkonzeptes. Die Gebäudeteile und Abschnitte eines Rechenzentrums(Campus) werden entsprechend ihrer Kritikalität in verschiedene Zonen und Sicherheitsbereiche eingeteilt. Von außen nach innen lassen sich nach einem „Zwiebelschalen-Prinzip” die folgenden fünf Zonen/Bereiche unterscheiden:\\n\\nZone I: Das Grundstück oder Firmengelände ist entweder frei zugänglich oder mit einem Zaun und einer Videoüberwachungsanlage gesichert.\\n\\nZone II: Halböffentlicher Bereich innerhalb des Betriebsgelände, z.B. mit normalen Büroarbeitsplätzen für Mitarbeiter:innen\\n\\nZone II: IT-Nebenräume und administrative Steuerung. Spätestens dieser Bereich ist nur noch einem bestimmten Personenkreis vorbehalten. Oftmals existieren hier Zugangspunkte in Form von Schleusen und Personenvereinzelungsanlagen.\\n\\nZone IV: Direkter Zugang zu technischen Anlagen des Rechenzentrums. Oftmals ist der Zugang in diesem Bereich ausschließlich für das technische Wartungspersonal vorgesehen.\\n\\nZone V: Zugang zu den Serverräumen, dem eigentlichen „Herz” des\\n\\nRechenzentrums. In Hochsicherheitsrechenzentren sind die Zugänge für die Serverräume sowie die Technikräume für Klima/Strom so ausgelegt, dass sich die entsprechenden Mitarbeiter:innen in separaten Gängen bewegen und eine direkte Begegnung ausgeschlossen wird. Dieser Aspekt ist besonders wichtig für den Fall, dass externes Wartungs- und Technikpersonal zum Einsatz kommt.\\n\\nErgänzend zum Zonenkonzept werden in der DIN EN 50600-1 vier unterschiedliche Schutzklassen definiert:\\n\\nSchutzart\\n\\nArt des Zugangs\\n\\nSchutzklasse 1 Öffentlicher oder halböffentlicher Bereich.\\n\\nSchutzklasse 2\\n\\nBereich, der allen autorisierten Personen (Mitarbeiter:innen und Besucher:innen) zugänglich ist.\\n\\nSchutzklasse 3\\n\\nBereich, der festgelegten Mitarbeiter:innen und Besucher:innen vorbehalten ist. Andere Personen mit Zugang zu Schutzklasse 2 müssen von Personen begleitet werden, die Zugang zu Bereichen der Schutzklasse 3 haben.\\n\\nSchutzklasse 4\\n\\nfestgelegten Mitarbeiter:innen, die einen Bereich, der nachgewiesenen Bedarf für den Zugang haben, vorbehalten ist. Andere Personen mit Zugang zu Schutzklasse 2 und 3 müssen von Personen begleitet werden, die Zugang zu Bereichen der Schutzklasse 4 haben.\\n\\nTabelle 12: Schutzklassen nach DIN EN 50600-1\\n\\nGroße KI-Modelle für Deutschland\\n\\n155\\n\\nDie unterschiedlichen Schutzklassen 1 – 4 sind dabei durch geeignete Maßnahmen und Prozesse entsprechend abzusichern. Die wichtigsten Punkte hierbei sind u.a.:\\n\\nSchutzklasse 1:\\n\\no Identifizierbare physische Sperre an der externen (Grundstücks)Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 2 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Physische Trennung des Zugangs von Fußgängern und Fahrzeugen der Schutzklasse 1\\n\\nund 2.\\n\\no Ausgewiesene Parkplätze für nicht autorisierte Fahrzeuge (Besucher:innen).\\n\\nSchutzklasse 2:\\n\\no Identifizierbare physische Barriere an der Grenze zur Schutzklasse 2. o Türen, Fenster, Gitter müssen der Widerstandsklasse 3 (DIN EN 1627:2011) entsprechen. Fenster und Türen müssen so konstruiert sein, dass sie im geschlossenen Zustand außerhalb der Schutzklasse 2 nicht geöffnet werden können. o Physische Trennung des Zugangs von Fußgängern und Fahrzeugen der Schutzklasse 2\\n\\nund 3.\\n\\no Maßnahmen zur Erkennung und Verhinderung unerwünschten und unnötigen\\n\\nZugangs.\\n\\no Jedes Öffnen einer Notausgangstüre muss einen Alarm auslösen, der eine geeignete\\n\\nReaktion auslöst.\\n\\nSchutzklasse 3:\\n\\no Identifizierbare physische Sperre an der externen Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 4 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Begrenzungen im Bereich der Schutzklasse 3 dürfen nicht mit denen der Schutzklasse\\n\\n1 örtlich zusammen angeordnet werden.\\n\\no Begrenzungen im Bereich der Schutzklasse 3, die mit Begrenzungen von Bereichen der Schutzklasse 2 zusammen angeordnet sind, müssen der Summe des Widerstands für Schutzklasse 2 und 3 entsprechen.\\n\\no Folgende Maßnahmen müssen vorhanden sein, um folgende Ereignisse zu erkennen und zu verhindern: Unerwünschter oder unnötiger Zugang zwischen Flächen der Schutzklasse 3 und 4; nicht autorisierter Zugang von einer Fläche der Schutzklasse 3 in Schutzklasse 4; Erkennung aller Personen sowie Materialien und Gerate (z.B. IT- Equipment), die Schutzklasse 3 betreten oder verlassen.\\n\\nSchutzklasse 4\\n\\no Identifizierbare physische Sperre an der externen Grenze. o Türen, Fenster, Gitter müssen der Widerstandsklasse 4 (DIN EN 1627:2011)\\n\\nentsprechen.\\n\\no Begrenzungen im Bereich der Schutzklasse 4 dürfen nicht mit denen der Schutzklasse\\n\\n1 örtlich zusammen angeordnet werden.\\n\\no Begrenzungen im Bereich der Schutzklasse 4, die mit Begrenzungen von Bereichen geringerer Schutzklassen zusammen angeordnet sind, müssen der Summe des Widerstands für alle Schutzklassen entsprechen.\\n\\no Alle Durchbrüche der physischen Begrenzung müssen den Zugang für nicht autorisierte Personen verhindern. Darin eingeschlossen sind z.B. auch Druckentlastungsklappen für Gaslöschanlagen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n156\\n\\nZugangskontrolle Um den kontrollierten Zugang in bzw. aus dem Rechenzentrumsbereich zu gewährleisten sind entsprechenden dahinterliegenden Prozessen installiert. Grundsätzlich lassen sich Online- und Offline- sind permanent mit einer Zugangssysteme unterscheiden. Online-Anlagen Zutrittskontrollzentrale verbunden. Alle Zutrittsbewegungen werden hierbei direkt an eine zentrale Stelle gesendet. Bei Offline-Anlagen ist die Zutrittsberechtigung auf entsprechenden das Zeiterfassungsterminal oder Onlineleser tagesaktuell übertragen werden (Dürr, 2018).\\n\\nin\\n\\nden\\n\\nRechenzentren\\n\\nZugangskontrollsysteme mit\\n\\nZutrittskarten\\n\\ngespeichert\\n\\nund\\n\\nkönnen\\n\\nz.B.\\n\\nüber\\n\\nZur Identifikation und Autorisierung sind unterschiedliche Systeme im Markt erhältlich. Folgende Grundtypen können unterschieden werden:\\n\\nPhysikalische Erkennung: mittels Leser und Ausweisen oder Schlüsselanhänger.\\n\\nLogische Erkennung: mittels Eingabe von Zahlencodes. (Diese Art des Zugangs ist nicht personalisiert und birgt die Gefahr, dass Codes unberechtigt weitergegeben werden können).\\n\\nBiometrische Erkennung: Eindeutige Identifikation einer zugangsberechtigten Iris-Scan, Handrückenerkennung, Person Venenerkennung.\\n\\nZur Erhöhung der Zugangssicherheit lassen sich die beschriebenen Methoden in der Regel auch miteinander kombinieren.\\n\\n8.6 Betrieb eines KI-Rechenzentrums\\n\\nFür den Betrieb eines KI-Rechenzentrums kommen generell drei Optionen in Betracht. Die Zusammenarbeit mit einem Collocation-Anbieter, der Aufbau eines eigenen HPC- Rechenzentrums sowie die Nutzung einer verteilten Infrastruktur. Die drei Optionen werden im folgenden näher beleuchtet.\\n\\nCollocation Ein Collocation-Betreiber ist ein Unternehmen, das Räume und Infrastrukturen für die Unterbringung von Rechenzentren und anderen IT-Anlagen bereitstellt. Collocation- Betreiber bieten ihren Kunden die Möglichkeit, ihre Rechenzentren und IT-Anlagen in Räumen unterzubringen, die speziell für den Betrieb von Rechenzentren ausgestattet sind (vgl. Tabelle 6). Diese Räume sind in der Regel mit Stromversorgungssystemen, Sicherheitsmaßnahmen Kühlungstechnologien, IT-Anlagen ausgestattet, um ordnungsgemäß betrieben werden können. Collocation-Betreiber bieten ihren Kunden auch Dienstleistungen wie Wartung und Support für IT-Anlagen an. Sie tragen somit dazu bei, dass Unternehmen ihre IT-Anlagen sicher und zuverlässig betreiben können, ohne sich um die notwendige Infrastruktur und die Wartung der Geräte zu kümmern. Der Normalfall ist allerdings, dass die Wartung und Installation der Racks durch den Kunden oder von in den Räumen des Collocation-Anbieters durchgeführt wird.\\n\\nNetzwerkverbindungen\\n\\nund\\n\\nsicherzustellen, dass die Rechenzentren und\\n\\nihm beauftragten Personal\\n\\nGroße KI-Modelle für Deutschland\\n\\n157\\n\\nBei der Auswahl eines Collocation Anbieters spielt auch die Erfüllung von Anforderungen an Kühlungssysteme eine wichtige Rolle. Weitere Kriterien sollten der Bezug von grünem Strom sowie ein schlüssiges Konzept zur Abwärmenutzung sein. Der Collocation Anbieter sollte demnach über entsprechende Infrastruktur zur Wasserkühlung verfügen. Die Anforderungen für das Training von Foundation-Modellen wurden bereits im Kapitel 2.2 erläutert.\\n\\n[GSI - Helmholtzzentrum für Schwerionenforschung]\\n\\nDas GSI Helmholtzzentrum für Schwerionenforschung in Darmstadt betreibt eine der weltweit führenden Teilchenbeschleunigeranlagen für die Forschung und den Green IT-Cube, ein fortschrittliches Rechenzentrum mit einer Kapazität von bis zu 12 MW. Dieses verfügt über hohe CPU Compute-Kapazitäten von mehr als 300.000 Cores und 400 GPUs. Es fungiert als Testrechenzentrum und verfügt auf sechs Etagen über eine Fläche von 4.645 m² und bietet Platz für 768 19” Racks à 2,2 Meter Höhe (4 MW und 256 Racks in der ersten Ausbaustufe). Dank eines speziellen Kühlsystems ist der Green Cube besonders energie- und kosteneffizient. Die Power Usage Effectiveness (PUE) beträgt hier weniger als 1,07 und wird über eine passive sowie Wärmetauscher- Wasserkühlung Verdunstungskühltürme erreicht. Dadurch, dass keine Raumluftkühlung nötig ist, sind hier hohe räumliche Leistungsdichten möglich, die für HPC-Systeme mit vielen GPU-Knoten benötigt werden. In einem KI-Cluster sind üblicherweise bis zu 4 NVIDIA DGX H100 Systeme pro Rack verbaut. Die entstehende Abwärme von 36 kW/Rack kann nach Aussage der Expert:innen des GSI mit dem Kühlsystem des Green Cube bewältigt werden.\\n\\nin den Rücktüren der Racks\\n\\nDer Green Cube verfügt über eine N+1 Redundanz und bietet die Möglichkeit der Bereitstellung von Rackspace und Dienstleistungen im Rechenzentrum (Collocation). Laut dem Betreiber könnten in den geplanten Ausbaustufen ausreichende Flächen für einen HPC-Supercomputer mit 4 MW Leistung und entsprechender Kühlung (Wasserkühlung im Rack und perspektivisch auch direct- to-Chip Kühlung) im Rahmen eines Collocation Modells bereitgestellt werden. Als zusätzliche Möglichkeit könnten hier im Rahmen des Testbetriebs des HPC- Clusters wertvolle Daten und wissenschaftliche Erkenntnisse zum Betrieb eines solchen großen, GPU basierten HPC-Systems gesammelt und veröffentlicht werden. Der Green Cube bezieht zu 100 % grünen Strom und wurde, neben anderen Preisen, 2020 von der Bundesregierung mit dem Umweltzeichen Blauer Engel ausgezeichnet. Die Abwärme der Racks wird zum Beheizen des Büro- und Kantinengebäudes auf dem Campus verwendet. Die technische Umsetzbarkeit eines Collocationbetriebs des LEAM KI-Rechenzentrums im Green Cube wurde vom Betreiber GSI bereits bestätigt.\\n\\nINFOBOX\\n\\nGroße KI-Modelle für Deutschland\\n\\n158\\n\\nDer europäische Collocation Markt lag 2021 bei einem Umsatzvolumen von ca. 10,7 Mrd. US-Dollar und wird von einer großen Anzahl global operierender Anbieter beherrscht, die europaweit bzw. weltweit eine Vielzahl von Rechenzentrumsstandorten unterhalten. Für den Zeitraum von 2021 bis 2027 wird aktuell ein jährliches Marktwachstum (CAGR) von 14.8 % prognostiziert (Research and Markets ltd, 2021). So hat sich z.B. in der Rhein-Main Region die Leistung der jährlich neu hinzugekommenen Collocation-Rechenzentren von 2020 mit 62 MW Leistung im Jahr 2021 auf 139 MW Leistung mehr als verdoppelt (Hintemann et al., 2021).\\n\\nZu den führenden Unternehmen in dieser Branche zählen u.a.:\\n\\nDigital Realty / Interxion unterhält an weltweit über 300 Standorten Collocation- Rechenzentren. In Europa betreibt Digital Realty in 15 Metropolregionen, verteilt auf 13 Länder insgesamt 114 Rechenzentren\\n\\nEquinix betreibt weltweit 240 Rechenzentren in 31 Ländern auf 6 Kontinenten. Insgesamt unterhält das Unternehmen 2,6 Mio. m² Fläche weltweit. Die durchschnittliche Verfügbarkeit der Rechenzentren liegt bei 99.9999 %.\\n\\nNTT Global Data Center unterhält 600.000 m² RZ-Fläche in über 20 Ländern mit einer IT-Leistung von 1.500 MW. In Europa ist das Unternehmen in 15 Metropolregionen mit Collocation-Dienstleistungen tätig.\\n\\nCyxtera betreibt mehr als 60 Rechenzentren in über 30 Märkten. In Europa ist das Unternehmen an den Standorten London, Amsterdam und Frankfurt tätig. • Cyrus One unterhält in Nordamerika und Europa 50 Collocation Standorte. In\\n\\nEuropa ist das Unternehmen in Deutschland, UK, Irland, den Niederlanden sowie Spanien mit 14 Collocation Rechenzentren vertreten.\\n\\nKDDI / Telehouse betreibt über 45 Collocation Rechenzentren weltweit. In Europa ist das Unternehmen an den Standorten London, Paris und Frankfurt mit 9 Rechenzentren präsent.\\n\\nVantage Data Centers betreibt auf 5 Kontinenten 24 Campus-Standorte. In Europa ist das Unternehmen an den Standorten Berlin, Frankfurt, Mailand, Warschau und Zürich und Cardiff mit 8 Lokationen vertreten.\\n\\nIron Mountain unterhält auf drei Kontinenten an insgesamt 21 Standorten über 370.000 m² Brutto RZ-Fläche. In Europa ist das Unternehmen an den Standorten Amsterdam, Frankfurt, London und Madrid präsent.\\n\\nGlobal Switch betreibt 13 Rechenzentren auf 2 Kontinenten. In Europa werden an den Standorten London, Amsterdam, Frankfurt, Paris und Madrid insgesamt 9 Rechenzentren betrieben.\\n\\nCOLT Data Center Services betreibt in Asien (Tokyo, Osaka und Mumbai) sowie in Europa (Frankfurt, London, Paris, Rotterdam) 14 Rechenzentrumsstandorte.\\n\\nPenta Infra betreibt Rechenzentren in den Niederlanden, Dänemark und Deutschland. In Deutschland ist das Unternehmen u.a. mit Rechenzentren in Berlin, Hamburg, Düsseldorf, Köln und Leipzig präsent.\\n\\nNorth C Datacenters betreibt Rechenzentren an insgesamt 14 Standorten in Deutschland, der Schweiz sowie in den Niederlanden. In Deutschland ist das Unternehmen in Nürnberg und München vertreten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n159\\n\\nZu den vorrangig im deutschsprachigen Raum tätigen Collocation-Betreibern zählen weiterhin (Auswahl):\\n\\nnoris network AG mit sieben Rechenzentren an fünf Standorten in Nürnberg, München und Hof.\\n\\nStackIT (Schwarz IT) mit zwei Standorten in Ellhofen sowie Ostermiething (AT). • Data Center One mit Standorten in Düsseldorf, Leverkusen und Stuttgart. • MyLoc Managed IT mit 3.500 m² Fläche an sechs Standorten in Düsseldorf. • Plusserver betreibt in Köln, Düsseldorf und Hamburg eigene Rechenzentren. • ScaleUp Technologies betreibt insgesamt sieben Hochleistungsrechenzentren an den Standorten Hamburg, Berlin und Düsseldorf.\\n\\nMaincubes One betreibt neben einem niederländischen Standort in Amsterdam in Deutschland drei Standorte in Frankfurt sowie einen weiteren Standort in Berlin. • Akquinet betreibt vier Rechenzentren an den Standorten Hamburg, Norderstedt und Itzehoe.\\n\\nCollocationIX betreibt am Standort Bremen ein Hochsicherheits-Collocation- Rechenzentrum.\\n\\nGrass Merkur betreibt am Standort Hannover 3.500 m² RZ-Fläche\\n\\nDarüber hinaus bieten viele Internet Services Provider ebenfalls Collocation Services neben ihrem klassischen IPS-Portfolio an (u.a. M-net, Pfalzkom, Telemaxx, NetCologne, Dokom, EnviaTel).\\n\\nDie Kosten liegen laut der befragten Expert:innen für Collocation Angebote in der geplanten Größenordnung zwischen 100-120 EUR/KW/Monat. Hinzu kommt der Strom, der im Beispiel mit einem Preis von 20 Cent/kWh berechnet wird. Dies entspricht der von LEAM benötigten Größenordnung von ca. 4 MW geschätzten monatlichen Kosten von ca. 400.000 bis 500.000 EUR für den laufenden Betrieb.\\n\\nBau und Betrieb eines eigenen HPC-Rechenzentrums Eine weitere Option ist der Bau und Betrieb eines eigenen HPC-Rechenzentrums mit entsprechender Gebäude-Infrastruktur. Die Kostenstruktur für den Bau und den Betrieb von Rechenzentren richtet sich vorrangig nach der erforderlichen Verfügbarkeitsklasse. Grundstückskosten stellen oftmals eine untergeordnete Rolle in der Total Cost of Ownership (TCO)-Betrachtung dar. Die folgende Beispielrechnung ist als grober Richtwert zu verstehen und kann aufgrund lokaler baulicher Gegebenheiten sowie besonderer technischer Erfordernisse abweichen:\\n\\nGroße KI-Modelle für Deutschland\\n\\n160\\n\\nAbb. 23: Beispielrechnung Bau und Betrieb eines eigenen HPC-Rechenzentrums\\n\\nFür die Inbetriebnahme eines Rechenzentrums mit eigener Gebäudeinfrastruktur rechnen die befragten Expert:innen mit zwei bis drei Jahren, je nachdem wie lange die baurechtlichen Genehmigungsprozesse dauern, die je nach zuständiger Kommune stark abweichen können. Die hier durchgeführte Betrachtung soll exemplarisch die verschiedenen Abhängigkeiten aufzeigen und eine Orientierung zur Planung eines Rechenzentrums geben.\\n\\nGroße KI-Modelle für Deutschland\\n\\n161\\n\\nNutzung einer verteilten Infrastruktur Fraglich ist, ob verteiltes Rechnen eines großen KI-Foundation-Modell mit mehreren zusammengeschalteten HPC-Rechenzentren möglich ist. Grundsätzlich ist verteiltes Rechnen bei der Erstellung eines großen KI-Foundation-Modells ein möglicher Ansatz, der aber noch weitgehend unerprobt ist. Außerdem stellt verteiltes Rechnen erhöhte Anforderungen an Infrastruktur, Netzwerk, Latenz (Ausfallsicherheit) und Sicherheit. Erste Studien zum Thema „Decentralized Training of Foundation-Models in Heterogeneous Environments\" kommen von der Stanford Universität (B. Yuan et al., 2022). Allerdings sind diese Systeme noch sehr neu und noch nicht ausreichend in der Praxis getestet. Somit würde ein solches verteiltes Rechenmodell für LEAM mit sehr hohem Risiko einhergehen, da beim Berechnen eines Foundation-Modells der Trainingsdurchlauf nicht unterbrochen werden sollte. Insbesondere scheitert das Rechnen auf verteilter Infrastruktur häufig noch an der nötigen Bandbreite bei der Vernetzung der einzelnen GPU-Knoten, die untereinander und mit dem Storage mit einer Bandbreite von bis zu 900 Gb/s verschaltet werden.\\n\\n8.6.1 Strompreisentwicklung und Vertragsgestaltung\\n\\nDie Stromkosten machen laut Béla Waldhauser CEO von Telehouse Deutschland GmbH sowie CEO von KDDI Deutschland GmbH mittlerweile in Deutschland 50 % der Kosten für Kunden der Rechenzentren aus (Weidmann & Krüger, 2020). Rechenzentren kaufen je nach Größe den notwendigen Strom entweder vom lokalen Energieversorger ein oder direkt an der Energiebörse. Hier sind die Preise aktuell volatil und betragen 0,20 EUR/KWh für den Normalbetrieb bis zu 0,50 EUR/KWh für den Einkauf bei Spitzenlasten. Im Vergleich dazu liegen die Strompreise für Anbieter in Norwegen, Finnland und Schweden bei ca. 0,05-0,10 EUR/KWh. Außerdem ergeben sich hier durch die niedrigeren nordischen Temperaturen weitere Vorteile im Bereich der Kühlung der Rechenzentren. Schwankungen des Strompreises können erheblichen Einfluss auf die Wirtschaftlichkeit des Geschäftsmodells haben. Dies wird auch durch die Ergebnisse einer Umfrage des Borderstep Instituts bestätigt, in dem die Befragten die Entwicklung des Strompreises als größtes Risiko für den Rechenzentrumsmarkt einstufen (Hintemann et al., 2022).\\n\\nGroße KI-Modelle für Deutschland\\n\\n162\\n\\nAbb. 24: Delphi-Befragung: Wie beurteilen Sie folgende Risiken für die Entwicklung des Rechenzentrumsmarktes in Deutschland? (Hintemann et al., 2022, S. 37)\\n\\nBei der Vertragsgestaltung zwischen Rechenzentren und ihren Kunden werden üblicherweise unterschiedliche Fristen für eine Strompreisbindung vereinbart. Dabei sichert der Rechenzentrumsbetreiber seinen Collocation-Kunden innerhalb der Laufzeit die Abgabe von Strom zu einem festen Kostensatz zu. Da der Rechenzentrumsbetreiber diesen Strom am Markt zu schwankenden Preisen einkaufen muss, stellen größere Schwankungen ein Risiko dar und gefährden unter Umständen die Wirtschaftlichkeit des Geschäftsmodells des Anbieters.\\n\\n8.6.2 Verfügbarkeiten und Beschaffungszeitraum der erforderlichen IT-\\n\\nRessourcen\\n\\nLaut der befragten Expert:innen liegen die Lieferzeiten beispielsweise für NVIDIA DGX H 100 Systeme momentan bei unter sechs Monaten bei größeren Systemen. Der Hersteller NVIDIA sieht Lieferengpässen zurzeit eher bei den Netzwerkkomponenten, die ca. drei bis sechs Monate betragen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n163\\n\\n8.6.3 Aspekte der Nachhaltigkeit\\n\\nDiese Studie soll zur strategischen Ausrichtung der Planung nicht zuletzt auch die Nachhaltigkeit von Rechenzentren anhand des Beispiels einer wichtigen Initiative differenzierter thematisieren. Im Rahmen des Climate Neutral Data Center Pact setzen sich im Rahmen einer Selbstregulierungsinitiative zahlreiche Verbände und ca. 80 Unternehmen auf europäischer Ebene für die Schaffung von Standards im Bereich des nachhaltigen Betriebs von Rechenzentren ein (Climate Neutral Data Centre Pact – The Green Deal Need Green Infrastructure, o.D.). Im Rahmen einer Selbstverpflichtung haben sich die Mitglieder auf folgende Ziele geeinigt:\\n\\nEnergie Effizienz\\n\\nBis 2025 sollen Rechenzentren mit 50 KW und mehr Energiebedarf in kühlen Klimazonen unter Volllast einen PUE-Wert von 1,3 erreichen. Rechenzentren haben in wärmeren Zonen einen Wert von 1,4. Außerdem sollen neue Effizienz-Metriken für Rechenzentren entwickelt werden.\\n\\nGrüne Energie\\n\\nDie Rechenzentren werden ihren Strombedarf künftig durch den Einkauf von grünem Strom decken. 75 % CO²-neutrale oder erneuerbare Energie bis Ende 2025; 100 % bis Ende 2030.\\n\\nWassernutzung\\n\\nBis 2025 werden sich alle Rechenzentren, die in kühlen Klimazonen in Gebieten mit Wassermangel unter voller Auslastung betrieben werden und Trinkwasser für die Kühlung nutzen, einen maximalen WUE (Water Usage Effectiveness) Wert von 0,4 L/kWh erfüllen. Existierende Rechenzentren, die neue Kühlsysteme installieren, werden bis 2040 die angestrebten WUE-Werte erfüllen.\\n\\nKreislaufwirtschaft\\n\\nDie Wiederverwendung, Reparatur und das Recycling von Servern, elektrischen Geräten und anderen elektrischen Komponenten hat für die Betreiber von Rechenzentren Priorität. Hier werden hohe Standards an das Recycling gesetzt und angestrebt, zukünftig 100 % des eingesetzten Server-Equipments wiederzuverwerten.\\n\\nAbwärmenutzung\\n\\nDie Rechenzentren planen den Ausbau der Einspeisung/Abgabe von Abwärme in die allgemeinen Energieversorgernetze und an andere Abnehmer voranzutreiben. Dies soll möglichst umweltfreundlich und kosteneffizient erfolgen.\\n\\nUm die Entwicklung der Nachhaltigkeit von Rechenzentren weiter voranzutreiben und gleichzeitig Kosten zu senken, beobachtet und evaluiert die Rechenzentrumsbranche momentan außerdem die Themenfelder Refurbished IT, Remanufacturing und Re-Use sowie die Verwendung CO²-armer Baustoffe oder die Wiederverwendung von Beton (Bitkom e.V., 2022b).\\n\\nGroße KI-Modelle für Deutschland\\n\\n164\\n\\n8.7 Zusammenfassung und Empfehlung\\n\\nZusammenfassend lässt sich feststellen, dass die zur Berechnung von großen Sprachmodellen benötigten HPC-Ressourcen in Deutschland/Europa derzeit kurzfristig nicht verfügbar sind. Die hohen Anforderungen an die GPU-Zahlen (ca. 4500) und die entsprechende schnelle Vernetzung der einzelnen GPUs untereinander können Stand heute nicht gewährleistet werden oder würden Berechnungszeiten erfordern, die die leistungsfähigsten deutschen HPC-Zentren für fast zwei Jahre komplett auslasten würden und außerdem die benötigten kurzen Innovationszyklen nicht gewährleisten könnten. Die einschlägigen Cloud-Dienste aus den USA und China können teilweise ausreichende KI- Rechenkapazitäten bereitstellen. Diese lassen sich aber nur schwerlich unter Wahrung der digitalen Souveränität und der europäischen Anforderungen an den Datenschutz nutzen. Zudem werden hier meist so genannte Lock-in Effekte wirksam, die einen späteren Wechsel zu einem anderen Anbieter erschweren.\\n\\nAufgrund der vielen genannten Faktoren kann eine Standortempfehlung nur auf den konkreten Use Case bezogen gegeben werden. Diese Einzelfallbetrachtung kann im Rahmen der Studie nicht geleistet werden. Gespräche mit Vertreter:innen verschiedener Bundesländer sowie Regionalinitiativen haben aber gezeigt, dass es in verschiedenen Bundesländern eine generelle Bereitschaft für den Aufbau eines Rechenzentrums gibt.\\n\\nDer Bau eines eigenen Rechenzentrums wäre generell deutlich teurer und würde einem schnellen Start der Entwicklungsaktivitäten entgegenwirken. Einschließlich der erforderlichen Planungs-, Genehmigungs- und Errichtungsphasen würden bis zu drei Jahre vergehen, bis ein entsprechendes KI-Rechenzentrum den operativen Betrieb aufnehmen könnte. Dies ist wegen des bereits beschriebenen Handlungsbedarfs zu lange. Von der Errichtung eines KI-Rechenzentrums mit eigener baulicher Infrastruktur für die sollte deshalb aus Zeit- und Kostengründen abgesehen werden.\\n\\nEine kurzfristig realisierbare Möglichkeit für den Betrieb eigener Rechenkapazitäten besteht im Rahmen eines Collocation Modells. Wie im Beispiel GSI Helmholtzzentrum für Schwerionenforschung aufgezeigt wurde, gibt es bereits Anbieter, die kurzfristig mit grünem Strom und nachhaltiger Abwärmenutzung betriebene Kapazitäten anbieten, die Anforderungen an die benötigte Wasserkühlung erfüllen und Skalierbarkeit ermöglichen. Die Angebote von Betreibern aus Norwegen, Finnland, Schweden und Island bieten hier zusätzlich über den Strompreis einen Betriebskostenvorteil wegen der niedrigeren durchschnittlichen insbesondere datenschutzrechtlich orientieren sich diese Länder zudem an den europäischen Datenschutzstandards (Schweden und Finnland sind EU-Mitgliedstaaten, Island und Norwegen sind Teil des Europäische Wirtschaftsraums (EWR)) und bekommen deshalb eine Empfehlung von den Autor:innen. Für die Inference-Anwendungen können je nach isländische und Use Case schwedische Anbieter wegen der großen geografischen Entfernungen ggf. nicht erfüllen können.\\n\\nAußentemperaturen.\\n\\nJuristisch\\n\\nund\\n\\njedoch Latenzen benötigt werden, die norwegische,\\n\\nGroße KI-Modelle für Deutschland\\n\\n165\\n\\nEine weitere Möglichkeit ist die Nutzung von neu zu schaffenden HPC-Kapazitäten, die von einem Anbieter, beispielsweise nach GPU-Stunden abgerechnet, bereitgestellt werden könnten. Auf diese Möglichkeit wird in Kapitel 10 im Rahmen der betriebswirtschaftlichen Betrachtungen näher eingegangen.\\n\\nNach Einschätzung der befragten Expert:innen evaluieren die einschlägigen, hier bereits genannten europäischen und internationalen Rechenzentren und Collocation Anbieter bereits größere Investitionen in HPC-Infrastrukturen in Europa, speziell in konzentrierte KI-geeignete Kapazitäten mit sehr großen Anzahlen von bis zu 20000 GPU. Hier werden parallel verschiedene Kooperationsmöglichkeiten evaluiert. Das Projekt Open GPT-X erforscht bereits heute die Entwicklung großer Sprachmodelle in Zusammenarbeit mit dem Jülich Supercomputing Centre (JSC). LEAM wird diese Ergebnisse im Rahmen der engen Zusammenarbeit mit dem Projekt in seine weitere Planung der Infrastruktur einfließen lassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n166\\n\\nDie organisatorische Struktur von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n167\\n\\n9. Die organisatorische Struktur von LEAM Die vorangegangenen Kapitel haben gezeigt, dass der Aufbau eines KI- Hochleistungsrechenzentrums entscheidend für den Technologie- und Industriestandort ist die Frage der Deutschland wirtschaftlichen Machbarkeit eines solchen Zentrums. Die wirtschaftliche Machbarkeit wird in dem folgenden Kapitel näher betrachtet. Auf Basis der Interviews mit Rechenzentrumsbetreibern und der Wirtschaft haben die Autor:innen die (LKS) entwickelt. Welche Organisationsstruktur konkret hinter der Idee des LKS steckt wird im Folgenden genauer erklärt.\\n\\nIdee eines LEAM KI-Servicezentrum\\n\\n9.1 Zielgruppen des LEAM KI-Servicezentrums\\n\\nDas in dieser Studie diskutierte LEAM-KI-Servicezentrum hat zum Ziel, an europäischen Werten orientierte KI-Foundation-Modelle zu entwickeln und insbesondere für die etablierte Industrie und junge Technologieunternehmen nutzbar zu machen. Dabei verfolgt das LKS zwei Zielsetzungen:\\n\\n(1) Kapazitäten des KI-Hochleistungsrechenzentrums bereitzustellen, die von\\n\\nWirtschaft und Wissenschaft für das Training eigener Modelle zur Verfügung stehen.\\n\\n(2) KI-Foundation-Modelle Open Source anzubieten, die mit entsprechend Beratungs- und Serviceleistungen auf die individuellen Bedürfnisse von Industrieunternehmen angepasst werden können (Tuning).\\n\\nDie Kapazitäten und Services sollen vier Zielgruppen angeboten werden. Die Zielgruppen sind:\\n\\nWirtschaft: Unter die Zielgruppe Wirtschaft fallen privatwirtschaftliche Organisationen, die die Services der LKS für die Entwicklung von Modellen, Anwendungen oder Produkten in Anspruch nehmen.\\n\\nKI-Start-ups: Wie bei der Zielgruppe Wirtschaftliche Anwendungen handelt es sich hier um privatwirtschaftliche Organisationen. Sie unterscheiden sich aber von ersterer in zwei Punkten: (1) KI-Startups arbeiten überwiegend an Künstlicher Intelligenz und\\n\\n(2) es handelt sich um junge Unternehmen.\\n\\nStart-ups sollten über spezielle Förderprojekte gezielt unterstützt werden. Ein Beispiel ist ein KI-Compute-Voucher, der Startups Zugang zu den Services des LKAS gewährleisten soll.\\n\\nGroße KI-Modelle für Deutschland\\n\\n168\\n\\nPublic: Öffentliche Institutionen, Behörden, Ministerien und Dienste können die Services des LKS nutzen, um die Verwaltung zu optimieren oder spezifische sicherheitsrelevante Insights auf Basis von großen Datenmengen zu erlangen. Für die Nutzung der LKS Services durch öffentliche Institutionen sind besondere Anforderungen hinsichtlich Datensicherheit, Datenschutz und allgemeine Richtlinien zur Verschwiegenheit zu erfüllen.\\n\\nWissenschaft: Unter die Zielgruppe Forschung und Entwicklung fallen Hochschulen, außeruniversitäre Forschungsinstitute und staatliche Forschungseinrichtungen, die die Services der LKS für die Forschung in Anspruch nehmen und vor allem die Erstellung von Foundation-Modellen unterstützen.\\n\\n9.2 Organisationseinheiten des LEAM KI-Servicezentrums\\n\\nDas Organisationseinheiten strukturiert (Abb. 25).\\n\\nLEAM KI-Servicezentrum\\n\\n(LKS)\\n\\nist\\n\\nin horizontalen und\\n\\nvertikalen\\n\\nDie horizontalen Organisationseinheiten bilden die infrastrukturelle und kapazitive Grundlage des LEAM KI-Servicezentrums. Sie stellen den Betrieb der Infrastruktur und die Bereitstellung der Kernservices sicher. Hierzu gehören:\\n\\nHousing • Infrastruktur-as-a-Service • Training-as-a-Service\\n\\nhorizontalen Die Organisationseinheiten auf und gliedern sich in die vier verschiedenen Kernservices für Kunden und Nutzer:innen. Hierzu gehören:\\n\\nvertikalen\\n\\nOrganisationseinheiten\\n\\nbauen\\n\\nauf\\n\\nden\\n\\nKI-Foundation-Model Development • KI-Model Tuning • Inference • Consulting\\n\\nDaneben „Koordination” als Managementeinheit verantwortlich für den Aufbau des LKS, hält engen Kontakt zum LEAM Board (s. Kapitel 9.3) und weiteren Stakeholdern aus Politik, Wissenschaft und Wirtschaft. Sie kann – je nach gewählter Gesellschaftsform – auch die Aufgaben eines gesellschaftsrechtlich erforderlichen Organs übernehmen, beispielsweise der Geschäftsführung oder des Vorstands.\\n\\nist die Organisationseinheit\\n\\nDie einzelnen Organisationseinheiten können innerhalb einer Gesellschaft abgebildet werden oder entsprechend eines Governance-Konzepts in unterschiedliche juristische Einheiten aufgeteilt werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n169\\n\\nAbb. 25: Organisationseinheiten des LEAM-KI-Servicezentrums\\n\\n9.2.1 Koordination\\n\\nDie OE (Organisationseinheit) Koordination dient als Managementeinheit für das gesamte LKS. Sie stellt den kontinuierlichen Aufbau und Betrieb des LKS sicher und erweitert es innerhalb Deutschlands und der EU.\\n\\nDie OE ist eine Anlaufstelle für interessierte Personen aus Wirtschaft und Wissenschaft und hält den Kontakt zur Politik. Sie koordiniert die Interessen der Stakeholder:innen, etabliert und steuert die horizontalen Organisationseinheiten, erstellt Marktstudien und akquiriert Förder- sowie Investitionsmittel. Als steuernde Einheit des LKS unterstützt sie die anderen Services und übernimmt die Vermarktung des LKS.\\n\\nDieser Bereich sollte unabhängig vom gewählten Organisationsszenario zusammen mit dem Bereich Consulting initial aufgebaut werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n170\\n\\n9.2.2 Housing\\n\\nDie OE Housing stellt die Basisinfrastruktur für den Betrieb des Rechenzentrums zur Verfügung. Hierzu gehören bspw. die Gebäudeinfrastruktur, die Strom- und Telekommunikations-Versorgung sowie ein Kühlungskonzept. Das Kapitel 8 fasst die besonderen Anforderungen für die Ausstattung zusammen.\\n\\nDa die Investitionen in die erforderliche Infrastruktur hoch sind und der Neuaufbau eines Rechenzentrums mehrere Jahre dauert, sollte zum jetzigen Zeitpunkt von einem Neubau abgesehen werden. Stattdessen bieten verschiedene Organisationen Housing Services an, die eingekauft oder angemietet werden können. Für den Aufbau des Rechenzentrums muss ein geeigneter Housing Partner gefunden werden.\\n\\nGovernance:\\n\\nAußerhalb LEAM\\n\\nOrganisatorische Schnittstellen:\\n\\nKoordination; Service\\n\\nInfrastruktur-as-a-\\n\\nKosten Collocation HW:\\n\\n1.344.000 EUR per annum\\n\\nTabelle 13: Übersicht über die OE Housing\\n\\n9.2.3 Infrastruktur-as-a-Service (IaaS)\\n\\nZentraler Bestandteil der OE IaaS ist der Aufbau und Betrieb eines KI-Supercomputers. Der Rechner benötigt zum Betrieb die Services der Housing-Infrastruktur. Die Anschaffung des Rechners ist einer der größten Posten des Gesamtbudgets und liegt im dreistelligen Millionen Euro Bereich (s. Kapitel 10). Nähere Infos zu den technischen Anforderungen und weiteren Aspekten des Rechners finden sich in Kapitel 8.\\n\\nFür die organisatorische Einordnung der OE IaaS ergeben sich grundsätzlich zwei Szenarien: Einerseits ist die Anschaffung und Betrieb des Rechners durch das LKS denkbar, andererseits kann die Infrastruktur des LKS auch als Service durch ein externes Unternehmen bereitgestellt werden.\\n\\nAufgrund der hohen Relevanz des gewählten Szenarios für Investitions- und Betriebskosten des LKS sowie organisatorischen Gegebenheiten werden diese beiden Szenarien im Folgenden näher betrachtet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n171\\n\\n9.2.4 IaaS innerhalb LEAM\\n\\nIm ersten Szenario ist das LKS verantwortlich für die Anschaffung des Computers sowie den Betrieb und die Bereitstellung der Infrastruktur als Service. Dies birgt die folgenden Vor- und Nachteile.\\n\\nVORTEILE EINER EIGENEN INFRASTRUKTUR\\n\\nUmfangreiche Kontrolle und Gestaltungsmöglichkeiten Bei der eigenen Anschaffung eines KI-Supercomputers können einzelne Komponenten, Bauteile sowie zugehörige Frameworks und Software selbstständig ausgewählt werden. Das macht eine flexiblere Auslegung und Ausrichtung auf die zukünftige Nutzung des Rechenzentrums möglich. Unterstützung eines eigenständigen Geschäftsmodells Die Investition in einen eigenen KI-Supercomputer macht es möglich, diesen nach der Abschreibungsdauer weiter zu nutzen. Die Abhängigkeit von eventuellen Kostenschwankungen und fremden Dienstleistern begrenzt sich dabei auf die zum Betrieb notwendigen Kosten. Keine Datenweitergabe an Drittanbieter Bei der Nutzung eines eigenen Rechenzentrums werden keine Daten über Nutzung etc. an Dritte weitergegeben. Das vereinfacht eventuelle Fragestellungen in Bezug auf Dritte (z.B. Sicherheitsdienste). Ebenso können auf diese Weise Anforderungen der Cybersicherheit (beispielsweise BSI-Anforderungen), deren Erfüllung für die Teilnahme an bestimmten öffentlichen Förderprogrammen oder öffentlichen Auftragsausschreibungen nachgewiesen werden müssen, bei der Nutzung eines eigenen Rechenzentrums besser nachgewiesen werden. Insbesondere bei einer staatlichen Finanzierung ist zudem Folgendes zu bedenken: Der Einkauf von Leistungen eines externen KI-Supercomputers bedarf einer öffentlichen Ausschreibung. Ein entsprechender Vertrag über den Einkauf kann zwar für einen längeren Zeitraum ausgestaltet sein, muss aber periodisch neu ausgeschrieben werden. Das kann die Investitionsbereitschaft eines externen Dienstleisters einschränken.\\n\\nNACHTEILE EINER EIGENEN INFRASTRUKTUR\\n\\nHohe Investitionskosten Die Anschaffung eines KI-Supercomputers ist mit einer sehr hohen initialen Investition verbunden. Als Abschreibungsdauer wird ein Zeitraum von vier Jahren angenommen. Komplexe Finanzierung Mit der Finanzierung und dem Aufbau des KI-Supercomputers ergeben sich komplexe Fragestellungen bezüglich der Finanzierung, der Besitzverhältnisse sowie den Nutzungsrechten. Aufbau einer eigenen Betriebseinheit Als Betreiber des Rechenzentrums ergeben sich hohe Kosten für Personal und Software. Die Verwaltung und Instandhaltung eines KI-Supercomputers ist komplex und muss durch entsprechendes Personal rund um die Uhr überwacht werden. Darüber hinaus muss man in der Lage sein, eine adäquate IT-Sicherheit herzustellen. Das bedeutet einen erheblichen administrativen Aufwand sowie hohe Kosten. Vor allem bei Förderprojekten oder Aufträgen aus der öffentlichen Verwaltung können zusätzliche Anforderungen und damit verbundene Aufwände beim Nachweis von Cybersicherheitsstandards entstehen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n172\\n\\n9.2.5 IaaS über einen externen Partner\\n\\nIn diesem Szenario kauft das LKS die Rechenkapazitäten von einem externen Partner ein. Der Partner übernimmt dabei die komplette Installation und den Betrieb der Compute Infrastruktur und nutzt seine Erfahrung im Bereich des Hostings aus bestehenden Geschäftsmodellen.\\n\\nVORTEILE EXTERNER INFRASTRUKTUR\\n\\nGeringe Investitionskosten für eigenes Personal, Hardware und Software Bei der Bereitstellung eines KI-Supercomputers über einen externen Partner ist dieser auch für die Verfügbarkeit und Funktionalität der Anlage verantwortlich. und Daher Softwarekosten, lediglich Unterstützungsfunktionen müssen übernommen werden. Flexible Skalierbarkeit des KI-Supercomputers Entsprechend der aktuellen Verfügbarkeit und des Bedarfs können die Kapazitäten, wenn vereinbart, flexibel dazu gebucht oder abgewählt werden. Das macht eine einfachere Anpassung auf den momentanen Bedarf möglich und fängt eventuelle Spitzen ab. Zugriff auf fachliches Know-How und etablierte Prozesse Expert:innen eines externen Partners können gegebenenfalls mittels fachlicher Expertise bei Fragestellungen unterstützen und die Nutzung der Services für Kunden so vereinfachen. Weiter kann bei der Abrechnung und dem Betrieb auf etablierte Prozesse und Erfahrungen - bspw. im Bereich IT-Sicherheitsstandards - des Drittanbieters zurückgegriffen werden.\\n\\nentstehen\\n\\nvergleichsweise\\n\\ngeringe\\n\\nPersonal-, Hardware-\\n\\nNACHTEILE EXTERNER INFRASTRUKTUR\\n\\nWeitergabe unternehmensbezogener Daten an Dritte Bei der Ausführung von Services auf der Infrastruktur eines externen Anbieters werden evtl. Nutzungsdaten an diese weitergegeben. Dabei müssen Abwägungen über die Datensicherheit getroffen und Bestimmungen zur Weitergabe von Daten genau geprüft werden. Abhängigkeit von externen Anbietern Die Nutzung von Infrastruktur externer Anbieter steht und fällt mit der Verfügbarkeit von Kapazitäten dieses Anbieters. Wenn diese, sei es auch nur kurzfristig, nicht gegeben ist, muss mit Umsatzeinbußen und Compliance-Schwierigkeiten gerechnet werden. Um dies zu umgehen, müssen externe Anbieter die Verfügbarkeit von Kapazitäten garantieren können und mit dem Kunden vertraglich vereinbaren. Rollierende, marktabhängige Kosten Infrastruktur eines externen Anbieters fallen Die Kosten zur Nutzung der kontinuierlich an und sind höher als die Betriebskosten eines eigenen Rechenzentrums. Die Kosten können, je nach vertraglichen Regelungen, aufgrund von Marktbewegungen variieren. Gefahr durch Übernahme oder Insolvenz Die Infrastruktur des externen Anbieters steht unter dessen Kontrolle und könnte entweder in ein anderes Land verlagert werden, was sich möglicherweise nachteilig auf Datensicherheitsaspekte auswirkt, oder aufgekauft werden. Ebenfalls besteht die Möglichkeit, dass die Infrastruktur bspw. nach einer Insolvenz nicht mehr zur Verfügung steht\\n\\nGroße KI-Modelle für Deutschland\\n\\n173\\n\\nEmpfehlung Die befragten Expert:innen aus Industrie und Wissenschaft halten es für erstrebenswert, eine eigene, leistungsstarke und flexibel nutzbare Supercomputing-Infrastruktur aufzubauen, da die bisher verfügbare private und wissenschaftliche Infrastruktur diese Kriterien nicht erfüllt. Dies haben die Interviews im Rahmen dieser Machbarkeitsstudie, aber auch die Empfehlungen von wissenschaftlichen Initiativen, bspw. des European Language Equality Projekts, deutlich gemacht. 19\\n\\nEine Entscheidungsgrundlage über die Finanzierung der Infrastruktur (eigene Hardware oder externe Infrastruktur) muss auch unter Berücksichtigung der hohen Investitionskosten weiter analysiert und vorbereitet werden.\\n\\n9.2.6 Training-as-a-Service\\n\\nAuf Basis der IaaS und damit bereitgestellten Compute-Kapazitäten werden Prozesse etabliert, um den Nutzer:innen und Kunden von LEAM umfangreiche Services für höher liegende Ebenen bereitzustellen. So ermöglicht das LKS potentiellen Nutzer:innen, die Infrastruktur zum Training ihrer Modelle zu nutzen.\\n\\nFür das Training von KI-Foundation-Modellen sind umfangreiche Maßnahmen zum Aufbau und Betrieb der entsprechenden Prozesse und Software-Infrastrukturen erforderlich (s. Kapitel 7).\\n\\nDie Aktivitäten können auf unterschiedlichen Ebenen (Layern) kategorisiert werden. Zu den Aktivitäten gehören u.a.:\\n\\nSystem Layer\\n\\nManagement der KI-Beschleuniger ○ Bereitstellung von (Open Source) Software und Softwarebibliotheken\\n\\nData Layer\\n\\nSicherstellung von effizienten und stress-resilienten Datenspeicher- und Laderoutinen\\n\\nBeachtung von gesetzlichen und regulatorischen Anforderungen an Datenschutz, Datenqualität und Datensicherheit\\n\\nTraining and Application Layer\\n\\nAufbau eines effektiven Optimierungssystems zur optimalen Auslastung der GPUs während des Trainingsprozesses\\n\\nAufbau eines Systems zum Management der Trainingsjobs ○ Implementation von Evaluations- und Benchmarkingprozessen\\n\\n19 “Current LT research also requires flexible access to High Performance Computing (HPC) facilities in the form of clusters of high capacity GPUs. There are many EU initiatives offerring HPC: EuroHPC JU, PRACE, national computing facilities, etc. However, it is unclear if these initiatives are ready to provide the computing support that the European LT research community currently needs for developing state-of-the-art language models for all languages, domains, tasks and modalities.” (ELE Consortium 2022, S. 23).\\n\\nGroße KI-Modelle für Deutschland\\n\\n174\\n\\nFramework & Service Layer\\n\\nNormalisierung der Trainingsumgebung durch Bereitstellung von Container- Umgebungen\\n\\nImplementierung von benutzerfreundlichen Interfaces für Entwickler:innen und Administrator:innen\\n\\nUm diesen Service anbieten zu können, muss ein fachkundiges Team bestehend aus Data Science, Data Engineering, Machine Learning Engineering und DevOps Expert:innen aufgebaut werden. Dieses wird unter dem Team Services vereint und bildet Schnittstellen zum Team Consulting.\\n\\nGovernance:\\n\\nInnerhalb LEAM\\n\\nOrganisatorische Schnittstellen:\\n\\nKoordination; Housing, Training-as-a- Service, externe Kunden\\n\\nKosten Büroräumlichkeiten:\\n\\n180.000 EUR per annum\\n\\nKosten Team Services:\\n\\n3.000.000 EUR per annum\\n\\nTabelle 14: Übersicht über die Training-as-a-Service\\n\\n9.2.7 KI-Foundation-Model Development\\n\\nDer Service KI-Foundation-Model Development ist der Kernservice des LKS. Unter diesem Service werden alle Aktivitäten zusammengefasst, die direkt mit der Entwicklung neuer KI-Foundation-Modelle zusammenhängen.\\n\\nLEAM wird den Service KI-Foundation-Model Development privatwirtschaftlichen, öffentlichen und wissenschaftlichen Einrichtungen anbieten. Es ist jedoch davon Investitionskosten das auszugehen, dass aufgrund der hohen Komplexität und grundlegende KI-Foundation-Modell Development überwiegend von Forschung- und Kooperationsprojekten aus Forschung und Wirtschaft genutzt wird. Die Interviews mit der Forschung haben gezeigt, dass ein großes Interesse darin besteht, neue und konkurrenzfähige KI-Foundation-Modelle zu entwickeln. Darüber hinaus werden sie den Service nutzen, um wissenschaftliche Fragestellungen zu beantworten.\\n\\nSollte die LKS (teil-)öffentlich finanziert werden, müssen mindestens 80 % der Modelle Open Source verfügbar gemacht werden. Maximal 20 % können von Unternehmen mit Exklusivrechten genutzt werden. Prinzipiell eine Chance, denn unsere Befragung von KI- Unternehmen hat gezeigt, dass der Mangel von Open Source KI-Foundation-Modellen eines der Haupthindernisse für KI-Unternehmen ist, diese produktiv nutzen zu können. Zusammen mit der Verfügbarkeit von Daten und hohen Kosten wurde dies von 58 % der befragten Unternehmen als Hindernis angegeben.\\n\\nForschungsprojekte können sich dann über ein noch zu definierendes Verfahren auf Rechenzeit bewerben. Dabei soll auf die Erfahrungen der im Gauss Centre for Supercomputing organisierten Rechenzentren zurückgegriffen werden. Um allerdings den Besonderheiten von LEAM gerecht zu werden, sollten einige Punkte beachtet werden:\\n\\nGroße KI-Modelle für Deutschland\\n\\n175\\n\\n\\uf0fc Interessierten Projekten soll durchgehend die Möglichkeit gegeben werden, sich auf Projekte zu bewerben. Dies bietet Forschungsprojekten die notwendige Flexibilität, um in der schnelllebigen KI-Forschung zeitschonend zu forschen. Darüber hinaus ist dies eine Möglichkeit, Spitzen in der Nutzung der Infrastruktur abzuschwächen, da nicht alle Projekte zur gleichen Zeit mit der Berechnung ihrer Modelle starten.\\n\\n\\uf0fc Die Bewerbungs- und Bewertungsverfahren sollen so einfach und flexibel wie\\n\\nmöglich gestaltet werden. Ein häufiger Kritikpunkt an der aktuellen HPC-Landschaft ist, dass die Antragsphase zu lange dauert und wichtige Ressourcen bindet.\\n\\n\\uf0fc Anwendungsbezogenen Forschungsprojekten soll ein Vorrang vor\\n\\nGrundlagenforschung gegeben werden.\\n\\nIm Bereich KI-Foundation-Model Development werden folgende Services angeboten:\\n\\nErstellung und Bereitstellung allgemeiner Trainingsdatensätze: Ein allgemeiner Trainingsdaten-Pool wird aufgebaut, entsprechend der Datenschutzvorgaben und Qualitätskriterien gepflegt und interessierten Organisationen zur Verfügung gestellt.\\n\\nBereitstellung von Basis-Algorithmen: In einem Repository werden Code-Basen existierender (Open Source) Programme, erforderliche Hilfs-Tools und weitere Frameworks zur Verfügung gestellt.\\n\\nVerwaltung und Bereitstellung von trainierten Foundation-Modellen: Die trainierten Foundation-Modelle werden zur weiteren Nutzung in einem Repository abgelegt und verwaltet.\\n\\nUm interessierten Organisationen den bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellung von Mitarbeiter:innen: Für die Begleitung und die Überwachung des Trainings werden Mitarbeiter:innen eingestellt und entsprechend ausgebildet.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Training von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es wird ein Abrechnungsmodell für das Training der Modelle entwickelt. Hierbei erfolgt eine Orientierung an bestehenden Services im HPC-Bereich oder im kommerziellen Cloud-Services Umfeld.\\n\\nVerwaltung des Trainingsdaten-Pools: Trainingsdatensätze werden in Repositorys gesammelt und Nutzer:innen zur Verfügung gestellt, um damit KI-Foundation- Modelle zu entwickeln. Mitarbeiter:innen des LKS unterstützen bei der Sammlung und Pflege von relevanten Datensätzen.\\n\\nSupport von Frameworks: Für das Training von KI-Foundation-Modellen werden relevante Frameworks in Repositorys gesammelt und den Nutzer:innen zur Verfügung gestellt. LEAM-Mitarbeiter:innen unterstützen die Nutzer:innen bei der\\n\\nAnwendung dieser im Zusammenhang mit dem KI-Supercomputer.\\n\\nGroße KI-Modelle für Deutschland\\n\\n176\\n\\n9.2.8 Model Tuning\\n\\nNeben der Entwicklung von KI-Foundation-Modellen werden Ressourcen und Infrastruktur für das Tuning von Modellen bereitgestellt. Dies ist nötig, um die allgemeinen KI-Foundation-Modelle um domänenspezifisches Wissen zu ergänzen und so für konkrete Anwendungen zu nutzen und zu optimieren.\\n\\nDer größte Teil der befragten Unternehmen ist an konkreten Anwendungen auf Basis von KI-Foundation-Modellen interessiert. 51 % der befragten KI-Unternehmen arbeiten bereits mit KI-Foundation-Modellen und 18 % planen die Nutzung von KI-Foundation- Modellen in der Zukunft (s. Kapitel 4).\\n\\nBereits vorhandene Modelle sollen durch Model-Tuning erweitert und für spezifische Zwecke nutzbar gemacht werden. Aktuell tun dies nur 27 % der befragten Unternehmen, die sich mit Foundation-Modellen auseinandersetzen. Somit ist es absehbar, dass die Nachfrage an Model-Tuning zeitnah steigen wird. LEAM kann dies der deutschen Industrie substantiell vereinfachen, indem es Expertise, Modelle, Daten und Infrastruktur bündelt und es Unternehmen erlaubt, Model-Tuning ohne großen Mehraufwand zu betreiben. Ein besonderer Fokus sollte hier darauf liegen, Start-ups aus dem Bereich KI, die ihre Modelle für Anwendungen in der Industrie weiterentwickeln möchten, Rechenkapazität zur Verfügung zu stellen.\\n\\nDaneben sind auch wissenschaftliche Institute daran interessiert, den Model-Tuning- Service zu nutzen. Entsprechend ist auch für die Wissenschaft die Kombination an Expertise, Modellen und Daten, die das LKS bietet, von Interesse.\\n\\nIm Bereich Tuning sollen folgende Services angeboten werden:\\n\\nTuning-as-a-Service: Kunden können das Tuning von Modellen beim Rechenzentrum in Auftrag geben. In Kooperation mit den Kunden passen die Mitarbeiter:innen des Rechenzentrums die Foundation-Modelle an.\\n\\nBeratung und fachliche Unterstützung bei der Auswahl von Modellen, Daten und Algorithmen.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellung von Mitarbeiter:innen: Zur Beratung und Unterstützung der Nutzer:innen in der Bedienung der Infrastruktur und zur Auswahl des Modells sowie der Datensätze stellt die LKS Mitarbeiter:innen ein.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Tuning von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es muss ein Abrechnungsmodell für das Tuning der Modelle entwickelt werden. Hierbei bietet sich beispielsweise ein Modell GPU/Stunde an.\\n\\nGroße KI-Modelle für Deutschland\\n\\n177\\n\\n\\n\\nVerwaltung des Trainingsdaten-Pools: Trainingsdatensätze werden in Reopsitorys gesammelt und Nutzer:innen zur Verfügung gestellt, um damit KI-Foundation- Modelle zu entwickeln. Mitarbeiter:innen von LEAM sammeln relevante Datensätze und halten diese instand.\\n\\nSPOTLIGHT SAP SE An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie SAP SE ist ein börsennotierter internationaler Softwarekonzern mit Sitz in Walldorf, Baden- Württemberg. Als ein Marktführer für Geschäftssoftware unterstützt SAP Unternehmen jeder Größe und Branche dabei, ihre Ziele bestmöglich zu erreichen: SAP-Kunden generieren 87 % des gesamten weltweiten Handels.\\n\\nDr. Feiyu Xu, Vizepräsidentin und Global Head of AI, SAP\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use- Case? Foundation-Modelle werden eine sehr wichtige Rolle in der Enterprise AI spielen und zu disruptiven Innovationen im Prozessmanagement führen. Dazu gehören neue Anwendungen für business process mining, business\\n\\nprocess prediction, business process composition und Prozessoptimierung.\\n\\nDaneben werden die angepassten großen Sprachmodelle auch zur besseren Mensch- Maschine-Interaktion via Chatbot oder Digital-Assistenten beitragen und auch die semantische Verarbeitung der Business-Dokumente, insbesondere Informationsextraktion und Entity-Linking, erheblich verbessern.\\n\\nEin weiteres Anwendungsfeld ist die Generierung von Programmcode durch die Foundation-Modelle. Das steigert die Effizienz der Programmierer. Weil sich dann auch die Anwender neue Werkzeuge oder Erweiterungen von Softwareprogrammen von der KI erzeugen lassen können, führt das auch zu einer Demokratisierung der Softwareentwicklung.\\n\\nDie potentiellen Use Cases kann man nach den jeweils benötigten Datenstrukturen ihrer I/Os klassifizieren:\\n\\n1. Text2Text oder Speech2Speech:\\n\\nChatbots und Digitale Assistenten sind wichtige Anwendungen für SAP, durch die Benutzer:innen, z.B. Angestellte einer Kunden-Firma, natürlichsprachliche Fragen stellen können und Antworten über Fakten oder Transaktionen erhalten. Digitalassistenten lassen sich für Kundendienste einsetzen.\\n\\nÜbersetzung und Lokalisierung der Software, Dienste und Business-Dokumente • Zusammenfassungen von Textdokumenten oder Meeting-Transkripten\\n\\nGroße KI-Modelle für Deutschland\\n\\n178\\n\\n2. Text2Prozess und Prozess2Text\\n\\nDie Integration der BPMN (Business Prozess Modelling Notation) in die Sprachmodelle ermöglicht die natürlichsprachliche Anfragen für die Generierung, Validierung und Ausführung der Business-Prozesse. In der anderen Richtung können zu bereits bestehenden Prozessmodellen textuelle Prozessbeschreibungen generiert werden, die z.B. für Schulungen oder Zertifizierungen benötigt werden.\\n\\n3. Prozess2Prozess\\n\\nHierzu gehören Prozesskomposition, Prozessmodifikation, Prozessvalidierung und Prozessoptimierung.\\n\\n4. Text2Code: low-code/no-code\\n\\nDie automatische Generierung von Programmcode steigert die Effizienz und Produktivität der Programmierung\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle führen einerseits zu großen inkrementellen Verbesserungen, da bessere Ergebnisse mit deutlich weniger (bis gar keinen) Trainingsdaten erzielt werden. Sie haben aber auch das Potential, neue und modularisierte End-to-End Geschäftsprozesse zu ermöglichen und so die Gesamtheit der Enterprise Resource Planning Landschaft zu revolutionieren. Wir geben ein erstes Beispiel aus dem Bereich Businessdokumentverarbeitung. Die Anpassung bestehender Deep Learning Modelle erfordert momentan einen erheblichen Aufwand. Kunden müssen für ihre spezifischen Dokumentformate große Mengen an Trainingsdaten bereitstellen. Auch die Lokalisierung der Modelle in weiteren Sprachen ist ohne Trainingsdaten nicht machbar. KI-Foundation-Modelle haben das Potential, den Aufwand in beiden Bereichen erheblich zu reduzieren. Durch Foundation-Modelle können wir eine neue Art von Angebot an Kunden machen: Verarbeitung ihrer spezifischen Formate in den für sie relevanten Sprachen, mit wenig Trainingsdaten oder sogar out-of- the-box.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Europa benötigt eine eigene KI-Computing-Infrastruktur, um nicht in Abhängigkeit zu geraten. Zudem ist der Zugang zu Daten, inklusive mehrsprachiger Inhalte und explizit kodiertem Wissen, essenziell. KI in Europa kann nur langfristig erfolgreich sein, wenn es uns gelingt, KI-Talente auszubilden und bei uns zu halten. Eine wichtige Rahmenbedingung für Europa ist außerdem eine KI-freundliche Policy.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Die Sprachabdeckung ist für uns essentiell, ebenso wie Transparenz zu verwendeten Daten. Nötig ist eine Infrastruktur, mit der KI-Modelle unter Gewährleistung der Transparenz und unter Einhaltung europäischer Datenschutzstandards generiert und Open Source bereitgestellt werden können. Bias ist in vielen Geschäftsbereichen ein kritisches Thema, etwa im Personalwesen und bei der Verarbeitung von Bewerbungen. Hier und auch in anderen KI-Anwendungen können KI-Modelle nur eingesetzt werden, wenn sie zu entsprechenden Regulierungen wie dem AI Act der Europäischen Union konform sind.\\n\\nGroße KI-Modelle für Deutschland\\n\\n179\\n\\n9.2.9 Inference\\n\\nDas LEAM-KI-Servicezentrum soll auch dafür genutzt werden, die Modelle für potentielle Kunden bereitzustellen. Der Service wird zu marktüblichen Preisen angeboten, rund 5-10 % der Compute-Infrastruktur sollen für das Bereitstellen der Modelle genutzt werden.\\n\\nUnternehmen sind vor allem am produktiven Einsatz von Anwendungen, die auf KI- Foundation-Modellen basieren, interessiert (s. Kapitel 4). Insbesondere Unternehmen, die keine eigenständige KI-Fachabteilung vorweisen und daher keine eigenen Modelle entwickeln bzw. anpassen können, werden auf diese Möglichkeit der Nutzung zurückgreifen. Da aktuell nur 50 % der befragten Unternehmen, die KI einsetzen, auch Foundation-Modelle nutzen, besteht hier eine relevante Zielgruppe für diesen Service. Für die Wissenschaft, die grundlegende Fragen zu KI-Foundation-Modellen beantworten möchte, ist dieser Service von geringerer Bedeutung.\\n\\nKI-Unternehmen können auf Basis der Inference-Services eigene KI-Produkte und Anwendung entwickeln und anbieten und diese ihren Kunden z.B. über eine API und spezifische Abrechnungsmodelle zur Verfügung stellen.\\n\\nIm Bereich Inference werden verschiedene Services angeboten:\\n\\nHosting-as-a-Service: Kunden können das Bereitstellen von Anwendungen beim Rechenzentrum in Auftrag geben. Rund 5 bis 10 % der gesamten Infrastruktur werden für diesen Service reserviert. Mitarbeiter:innen des Rechenzentrums unterstützen und koordinieren die Vorhaben.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nEinstellen von Mitarbeiter:innen: Für die Einrichtung und den Betrieb von Inference-APIs stellt die LKS Mitarbeiter:innen ein.\\n\\nAllokation von Compute-Ressourcen: Damit Nutzer:innen den KI-Supercomputer zum Tuning von Foundation-Modellen nutzen können, koordinieren Mitarbeiter:innen von LEAM die Verwaltung und optimale Distribution der Compute-Ressourcen an die Nutzer:innen.\\n\\nEntwicklung eines Abrechnungsmodells: Es muss ein Abrechnungsmodell für die Bereitstellung der Modelle entwickelt werden. Hierbei bietet eine Berechnungsmethode des Entgelts basierend auf der Menge an genutzten Tokens (einzelne Anfragen an das Modell) an.\\n\\nSchulung und Training von Nutzer:innen: Um die Infrastruktur nutzen zu können, müssen potentielle Nutzer:innen geschult werden. Dafür muss ein Training vorbereitet und angeboten werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n180\\n\\n9.2.10 Consulting\\n\\nIm Bereich Consulting erfolgt eine Beratung, die Kunden aus den verschiedenen Zielgruppen bei der Entwicklung, Optimierung sowie Implementierung von KI- Anwendungen unterstützt. Dieser Service erfolgt unabhängig von der Bereitstellung von Rechenzentrums-Infrastruktur-Leistungen.\\n\\nZielgruppe für Consulting Services sind alle wissenschaftlichen Institutionen, Start-ups und Unternehmen, die eigene Foundation-Modelle entwickeln oder existierende Foundation-Modelle für ihre spezifischen Anforderungen anpassen wollen.\\n\\nEine enge Zusammenarbeit mit Beratungs-Unternehmen aus der Wirtschaft und deren Befähigung, im Umfeld von KI-Foundation-Modellen Dienstleistungen anzubieten, gehört ebenfalls zu den Aktivitäten dieser OE.\\n\\nIm Bereich Consulting werden folgende Services angeboten:\\n\\nBeratung: An KI-Anwendungen interessierte Kunden können eine Beratung in Anspruch nehmen. Diese wird die Organisationen bei der Entwicklung, Optimierung sowie Implementierung von KI-Anwendungen auf Basis von KI- Foundation-Modellen unterstützen.\\n\\nSchulungen & Workshops: Interessierte Unternehmen sowie\\n\\nForschungseinrichtungen werden gezielt auf die Gegebenheiten der Entwicklung von Foundation-KI-Modellen mittels des LEAM-KI-Supercomputers vorbereitet.\\n\\nTraining von externen Beratungsunternehmen: Um externe Beratungen mit den spezifischen Gegebenheiten und der Technologie vertraut zu machen, müssen diese zunächst geschult werden. Dies wird durch enge Zusammenarbeit zwischen externer Beratung und dem internen Personal des LKS oder bereits beratender externer Unternehmen erreicht.\\n\\nUm Kunden einen bestmöglichen Service anzubieten, müssen verschiedene Voraussetzungen erfüllt werden:\\n\\nAufbau und Betrieb eines Beratungsteams: Um diesen Service anbieten zu können, muss ein Team aus fachkundigen Berater:innen angeboten werden. Dieses sollte aus Fachexpert:innen für Data Science und Machine Learning bestehen. Alternativ können externe Beratung herangezogen werden.\\n\\nPractice Work: Das Consulting-Team des LKS entwickelt eigene Fähigkeiten im Rahmen der Entwicklung von Foundation-KI-Modellen stetig weiter, um im Rahmen der Beratung stets die aktuell wichtigsten Bereiche und Technologien abdecken zu können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n181\\n\\n9.3 Das LEAM-Board\\n\\nAbb. 26: Das LEAM-Board als zentrale Governance-Einheit des LKS\\n\\nDas LEAM-Board überwacht und steuert die strategische Ausrichtung, Ziele sowie die Weiterentwicklung sich Entscheidungsträger:innen und Expert:innen aus Wissenschaft, Politik, Unternehmen und Start-ups. Weiterführend entscheidet das LEAM-Board über Investitionen sowie die Vergabe von Compute-Ressourcen an Forschung und Entwicklung.\\n\\ndes\\n\\nLKS.\\n\\nInnerhalb\\n\\ndes\\n\\nLEAM-Boards\\n\\nbefinden\\n\\nGroße KI-Modelle für Deutschland\\n\\n182\\n\\n9.4 Zusammenfassung\\n\\nDas LKS soll Kunden vier Kern-Services anbieten, die sich insbesondere an Unternehmen richten. Dabei kommt dem KI-Foundation-Modell Development die größte Bedeutung zu. Ziel muss es sein, Kunden optimal bei der Entwicklung und produktiven Nutzung von KI- Foundation-Modellen zu unterstützen. Dabei hilft der Service Consulting als Unterstützung bei der Entwicklung eines KI-Modells bis hin zum produktiven Einsatz mittels des Inference-Services. Interessierte Unternehmen sowie Forschung können jedoch auch einzelne Services wie Model Tuning, Inference oder Infrastruktur in Anspruch nehmen. Damit ist die Zielgruppe des KI-Rechenzentrums groß und der KI- Supercomputer wird flexibel genutzt. Ein Team aus LEAM-Mitarbeiter:innen steht dabei beratend und unterstützend zur Seite und begleitet Unternehmen auf ihrem Weg zur Entwicklung von KI-Foundation-Modellen.\\n\\nAls Grundlage für diese Services dienen die Organisationseinheiten Housing, Infrastruktur-as-a-Service sowie teilweise Training-as-a-Service. Die Einheit Housing wird dabei größtenteils an externe Partner übergeben. Dies spart der LKS hohe Investitionskosten und reduziert die Zeit bis zum Start des LKS. Die Einheit Koordination unterstützt die anderen Einheiten und koordiniert den Betrieb des LKS.\\n\\nGroße KI-Modelle für Deutschland\\n\\n183\\n\\nBetriebswirtschaftliche Aspekte\\n\\nGroße KI-Modelle für Deutschland\\n\\n184\\n\\n10. Betriebswirtschaftliche Aspekte Das LEAM-KI-Servicezentrum wird über die trainierten und bereitgestellten Foundation- Models einzigartige Wachstumsimpulse auslösen und branchenübergreifend enorme Effizienzgewinne in der Wirtschaft erreichen. Nach der anfänglich benötigten Investition, die entweder als öffentliche, private, gemischte Vollfinanzierung oder im Falle von externer IaaS mit Hilfe eines bestenfalls GAIA-X konformen Infrastrukturbetreibers erfolgt, wird die Recheninfrastruktur zu erheblichen Teilen der Wissenschaft und Unternehmen, die im Open-Source-Verfahren entwickeln, zur Entwicklung von Foundation-Modellen zur Verfügung gestellt. Die Entwickler:innen trainieren auf dieser Infrastruktur Foundation-Modelle der neuesten Generation und etablieren Wege zu standardisierten Fine-Tuning Prozessen. Die Modelle sowie die Verfahren und Prozesse zum Trainieren der Modelle werden entweder Open Source oder zu möglichst geringen Selbstkosten zur Verfügung gestellt.\\n\\nDie etablierte Wirtschaft sowie Start-ups können diese Technologien schnell in ihre Produkte einbauen, da sie von verlässlichen Partnern stammen und nach europäischen Standards entwickelt wurden. Durch die Bereitstellung zum Selbstkostenpreis sind Unternehmen aus der Wirtschaft damit unabhängig von amerikanischen HyperScalern. Infolgedessen wird für Wirtschaftsunternehmen die Barriere reduziert, diese Modelle in ihre Produkte einzubauen oder neue Produkte auf dieser Basis anzubieten. Zudem profitiert die Wirtschaft von den entwickelten standardisierten Blueprint-Prozessen in den Bereichen Training, Tuning und Bereitstellung. Durch die Bündelung von Know-How innerhalb des LKS können Wirtschaft und Start-ups von erfahrenen Expert:innen entlang (Entwicklung, Training, Tuning, des kompletten KI-Foundation-Model-Lifecycles Bereitstellung, Anwendung) lernen und sich kompetent beraten lassen.\\n\\nDamit leistet LEAM durch den Aufbau von Erfahrungswissen und den zu erwartenden Spillover-Effekten einen erheblichen Beitrag zur Gewinnung rarer Talente. Gleichzeitig erfolgt ein Wissenstransfer über die Schlüsseltechnologie „KI” in die Wirtschaft, die vor dem Aufbau eigener Teams LEAM Services im Finetuning und Beratungskompetenz bei der Implementierung im Bereich Inference nutzen kann.\\n\\n10.1 Kosten\\n\\nDer Finanzierungsbedarf für das LEAM-KI-Servicezentrum setzt sich zusammen aus\\n\\nInvestitionskosten (CapEx)\\n\\nBetriebskosten (OpEx)\\n\\nDie jährliche Kostensituation ist u.a. abhängig von der Abschreibungsdauer der Investitionskosten, die im Folgenden mit vier Jahren angenommen wird.\\n\\nIm Fall des Aufbaus der Infrastruktur über einen externen Partner und Einkauf von Rechenzentrumsleistung ist die Kostenkalkulation entsprechend anzupassen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n185\\n\\nDie anfallenden Kosten sind abhängig von einer Vielzahl von Parametern, die in dieser Machbarkeitsstudie lediglich abgeschätzt werden können.\\n\\nDie folgende Tabelle zeigt die für die Kostenkalkulation relevanten Parameter, die getroffenen Annahmen sowie die darüber hinaus gehenden möglichen Wertebereiche:\\n\\nUnit\\n\\nAnnahme Kommentare - Range\\n\\nHardware\\n\\nAbschreibungszeitraum\\n\\nJahre\\n\\n4\\n\\nGenerell 3-5 Jahre\\n\\nHardware (inkl. Speicher, Netw. Support, SW)\\n\\nkEUR\\n\\n260.000\\n\\nVorbild NVIDIA Selene\\n\\nAnzahl DGXA100\\n\\nAnzahl\\n\\n560\\n\\nAlternative HW: Cerebras, Graphcore, Intel, AMD, Nvidia H100\\n\\nDGXA100 per Rack\\n\\nAnzahl\\n\\n2\\n\\ngem. Info von NVIDIA\\n\\nBetriebskosten Infrastruktur\\n\\nMiete Kolokation HW\\n\\n[EUR/ Rack- Monate]\\n\\n400\\n\\nDurchschnittswert\\n\\nMiete Büroraum\\n\\n[EUR / qm] 30\\n\\n15 - 40 EUR/m2\\n\\nBüroraum\\n\\nqm\\n\\n600\\n\\nDurchschnittlich 10 qm / MA\\n\\nStromverbrauchs- Spitzenwert\\n\\nkw\\n\\n4.200\\n\\ngem. Info von NVIDIA\\n\\nStrompreis\\n\\nEUR/kwh\\n\\n0,35\\n\\naufgrund der aktuellen geopolitischen Lage ein hoher Unsicherheitsfaktor\\n\\nEffektivität Energienutzung\\n\\nder\\n\\nFaktor\\n\\n1,50\\n\\nrelativ hoher Wert, kann in einem Green Data Center niedriger sein\\n\\nDauerlast Stromaufnahme\\n\\nvon max\\n\\n%\\n\\n65\\n\\n50 %-90 %\\n\\nTeam Operationen\\n\\nFTE\\n\\n20\\n\\nAdministration Hardware und SW- Infrastruktur\\n\\nKosten Organisation und Services\\n\\nTeam Administration\\n\\nBusiness\\n\\nFTE\\n\\n20\\n\\nGovernance, Marketing\\n\\nVerwaltung,\\n\\nVertrieb,\\n\\nTeam Service\" & Consulting\\n\\n\"Training-as-a-\\n\\nFTE\\n\\n20\\n\\nImplementierung Prozessen, Dienstleistungen\\n\\nvon\\n\\nOperationen,\\n\\nPersonalkosten\\n\\nkEUR/FTE/ Jahr\\n\\n150\\n\\nDurchschnitt\\n\\nTabelle 15: Übersicht der Kosten des LEAM-KI-Servicezentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n186\\n\\nAnmerkung: Die in diesem Berechnungsmodell zugrunde liegende Architektur auf Basis von NVIDIA A100 wird durch die Hersteller in 2023 durch die neuere, leistungsfähigere Linie H100 ersetzt. Dadurch wird es auch zu einer Verbesserung des Performance- & Preisverhältnisses kommen.\\n\\nDamit ergibt sich folgende Gesamtkostenstruktur auf der Basis eines Betriebes und einer Abschreibungsdauer von vier Jahren:\\n\\nInvest\\n\\nInfrastruktur\\n\\nJahr 1 kEUR 88.732\\n\\nJahr 2 kEUR 88.732\\n\\nJahr 3 kEUR 88.732\\n\\nJahr 4 kEUR 88.732\\n\\nSumme kEUR 354.927\\n\\nHW-Abschreibung\\n\\n65.000\\n\\n65.000\\n\\n65.000\\n\\n65.000\\n\\n260.000\\n\\nHW Collocation\\n\\n1.344\\n\\n1.344\\n\\n1.344\\n\\n1.344\\n\\n5.376\\n\\nStromverbrauch\\n\\n19.316\\n\\n19.316\\n\\n19.316\\n\\n19.316\\n\\n77.263\\n\\nTeam Operationen\\n\\n3.000\\n\\n3.000\\n\\n3.000\\n\\n3.000\\n\\n12.000\\n\\nBüroraum 1/3\\n\\n72\\n\\n72\\n\\n72\\n\\n72\\n\\n288\\n\\nOrganisation und Services\\n\\n6.144\\n\\n6.144\\n\\n6.144\\n\\n6.144\\n\\n24.576\\n\\nPersonal\\n\\n6.000\\n\\n6.000\\n\\n6.000\\n\\n6.000\\n\\n24.000\\n\\nBüroraum2/3\\n\\n144\\n\\n144\\n\\n144\\n\\n144\\n\\n576\\n\\nTotal\\n\\n94.876\\n\\n94.876\\n\\n94.876\\n\\n94.876\\n\\n379.503\\n\\nTabelle 16: Gesamtkostenstruktur des LEAM-KI-Servicezentrums bei einer Abschreibungsdauer von vier Jahren\\n\\nInvestitionskosten Der zentrale Teil der Supercomputers in Anspruch genommen.\\n\\nInvestitionskosten wird durch die Anschaffung eines KI-\\n\\nFür die Berechnung dieser Infrastrukturkosten wird folgendes Szenario angenommen:\\n\\nAufbau Betrieb eines KI-Supercomputers in der Größenordnung des NVIDIA Selene (Wikipedia Contributors, 2022).\\n\\nDie Dauer eines Trainingslaufs für ein Modell der Größenordnung GPT-3 beträgt auf Selene ca. 1-1,5 Wochen.\\n\\nKernstück von Selene ist die NVIDIA Superpod-Architektur auf der Basis der DGX A100.\\n\\nDie Größenordnung einer NVIDIA Selene liegt bei 506 DGX A100 Nodes mit je 8 GPUs, in Summe.\\n\\nDie Anschaffungskosten liegen im Bereich von 260 Millionen Euro.\\n\\nAuf dieser Basis wird hier lediglich eine Beispielrechnung durchgeführt. Zum Zeitpunkt der Anschaffung der Infrastruktur bzw. deren Ausschreibungen werden alternative Lösungen (z.B. Graphcore, Cerebras, AMD, Intel) bzw. die neueste Architektur von NVIDIA (H100) analysiert und evaluiert.\\n\\nGroße KI-Modelle für Deutschland\\n\\n187\\n\\nBetriebskosten Die Betriebskosten des KI-Supercomputers setzen sich aus Collocation, Energiebedarf, Mietkosten und Personalkosten zusammen.\\n\\nMiete Collocation HW: Um den KI-Supercomputer zu betreiben, sind entsprechend ausgestattete Räumlichkeiten und Serverracks nötig. Diese müssen extern angemietet und mit der anzuschaffenden KI-Hardware ausgestattet werden. Pro Serverrack können zwei DGX A100 eingebaut werden. Bei Mietkosten in Höhe von 400 EUR pro Monat pro Rack ergeben sich jährliche Kosten in Höhe von 1.344.000 EUR.\\n\\nEnergiekosten: Der Energiebedarf der Anlage beläuft sich auf circa 4.200 kWh. Er ist stark abhängig von der Auslastung der Anlage. Es ist von einer durchschnittlichen Dauerlast der Anlage von 65 % auszugehen. In der aktuellen Situation sind die Stromkosten höchst volatil. Auch wenn der Strompreis für Industriekunden aktuell noch niedriger liegt, wird aktuell mit 0,35 EUR/kWh geplant. Dadurch ergeben sich jährliche Kosten in Höhe von 19.316.000 EUR.\\n\\nTeam Infrastruktur: Das Team Infrastruktur kümmert sich um den Betrieb der Hardware und SW-Infrastruktur. Dabei wird von einer Teamstärke von 20 FTE (Full- Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nMiete Büro: Um Mitarbeiter:innen von LEAM einen Arbeitsplatz zu bieten, müssen Büroräumlichkeiten mit entsprechender Ausstattung angemietet werden. Es wird aktuell mit einem 600 m² großen Büro zu 30 EUR pro Quadratmeter geplant. Dies entspricht jährlichen Kosten in Höhe von 60.000 EUR .\\n\\nTeam Koordination: Das Team Business Administration beschäftigt sich mit Governance, Sales, Marketing sowie administrativen Tätigkeiten rund um das Projekt LEAM. Dabei wird von einer Teamstärke von 20 FTE (Full-Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nTeam Services und Consulting: Dieses Team beschäftigt sich mit der Implementierung von Prozessen und Services rund um die Services, die LEAM anbietet. Dabei wird von einer Teamstärke von 20 FTE (Full-Time-Equivalents). Pro FTE sind pauschal 150.000 EUR Lohnkosten eingeplant.\\n\\nMiete Büro: Um Mitarbeiter:innen von LEAM einen Arbeitsplatz zu bieten, müssen Büroräumlichkeiten mit entsprechender Ausstattung angemietet werden. Es wird aktuell mit einem 600 m² großen Büro zu 30 EUR pro Quadratmeter geplant. Dies entspricht jährlichen Kosten in Höhe von 60.000 EUR.\\n\\nGroße KI-Modelle für Deutschland\\n\\n188\\n\\nAlternative Kostensituation bei Einkauf GPU-RZ-Leistungen Beim Einkauf von Rechenleistung fallen die initialen Investitionskosten für LKS durch die fehlende Notwendigkeit der Beschaffung von KI-Hardware fast vollständig weg. Voraussetzung hierfür ist die Bereitschaft eines Unternehmens aus dem Bereich Cloud- folgende Service-Providing, Rahmenbedingungen berücksichtigt werden müssen:\\n\\nin\\n\\ndie\\n\\nInfrastruktur\\n\\nzu\\n\\ninvestieren, wobei\\n\\nEs muss sichergestellt sein, dass ein ausreichend großes Compute-Cluster zur Verfügung gestellt wird (ca. 4500 GPUs).\\n\\nLKS wird über den Zeitraum von vier Jahren eine Mindest-Abnahmemenge von Rechenkapazität (z.B. 60 %) garantieren.\\n\\nDer Ankauf von Rechenleistung bzw. GPU-Stunden erfolgt nach marktüblichen Preisen (s.u.).\\n\\nEinzelheiten der Kalkulation und Vertragsgestaltung sind zu definieren.\\n\\nDie Auswahl eines Cloud-Service-Providers erfolgt evtl. im Rahmen einer öffentlichen Ausschreibung, deren Details zu definieren sind.\\n\\nIn diesem Szenario fallen folgende Kostenpositionen gemäß Tabelle 17 an:\\n\\nAnnahme\\n\\nJährl Kosten in kEUR\\n\\nAnkauf von Rechenzentrumskapazitäten / GPU Stunden\\n\\n60 % der für den eigenen Betrieb errechneten Kosten\\n\\n53.239\\n\\nOrganisation, Training-as-a- Service und Consulting\\n\\nEntsprechend der Kalkulation mit eigenem RZ\\n\\n6.144\\n\\nSumme\\n\\n59.383\\n\\nTabelle 17: Kosten des LEAM-KI-Servicezentrums bei einem Einkauf der GPU-RZ-Leistung\\n\\nGroße KI-Modelle für Deutschland\\n\\n189\\n\\n10.2 Einnahmen\\n\\nDurch die angebotenen Services des LKS lassen sich verschiedene Einnahmequellen definieren:\\n\\n\\uf0fc Verkauf von Rechenzentrums-Kapazität (GPU-Stunden) \\uf0fc Services für das Training von Foundation-Modellen (Training-as-a-Service) \\uf0fc Services für das Tuning von maßgeschneiderten Modellen (Training-as-a-Service) \\uf0fc \\uf0fc Allgemeine Beratungs-Tätigkeiten (Consulting)\\n\\nInference Service (GPU-Stunden)\\n\\nDie Möglichkeiten zur Generierung von Umsatz sind zielgruppenspezifisch zu differenzieren:\\n\\nWirtschaft: Corporates und KMU werden Leistungen zu marktüblichen Preisen angeboten, wenn sie Modelle für den privatwirtschaftlichen Betrieb entwickeln. Bei Forschungsprojekten, die im Open-Source-Verfahren arbeiten, erhält die Wirtschaft Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nWissenschaft: Projekte, die im Open-Source-Verfahren arbeiten, erhalten Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nPublic Sector: Öffentliche Einrichtungen erhalten Services kostenlos oder zu günstigen Preisen. Dies ist vor allem abhängig von der Governance und Finanzierung des LKS.\\n\\nStart-ups: Junge Technologieunternehmen können für Leistungen von der öffentlichen Hand bereitgestellte Kontingente (z.B. KI-Compute-Voucher) zur Nutzung der LKS-Services beantragen.\\n\\nVerkauf von Rechenzentrumskapazität Basis für die Services KI-Foundation-Model Development, Tuning und Inference ist die Nutzung von Rechenzentrumskapazität nach GPU-Stunde.\\n\\nDie Kosten für eine GPU-Stunde auf Basis des o.a. Kostenszenarios berechnet sich unter Annahme der Vollauslastung wie folgt:\\n\\nJährliche Kosten des Infrastrukturbetriebs gem. Tabelle 16: Anzahl GPUs Anzahl Stunden pro Jahr Auslastung\\n\\n88.732.000 EUR\\n\\n4.480 8.765 100 %\\n\\nKosten pro GPU-Stunde\\n\\n2,25 EUR\\n\\nDies liegt im Bereich der derzeit marktüblichen Preise (s. Anhang C).\\n\\nGroße KI-Modelle für Deutschland\\n\\n190\\n\\nUnter der Annahme, dass 20 % der Rechenkapazität zu diesen Preisen am Markt verkauft werden können, ergibt sich ein möglicher Jahresumsatz (kostendeckend, ohne Marge) von:\\n\\nPreis pro GPU-Stunde 20 % von 4480 GPUs * 8765 h\\n\\n2,25EUR 7.848.960h\\n\\nJährlicher Umsatz ca.\\n\\n14,464 Mio EUR\\n\\nDiese grobe Kalkulation muss bei einer detaillierten Ausgestaltung des Szenarios angepasst werden, vor allem hinsichtlich der Parameter-Auslastung, Verfügbarkeit, Marge sowie angepassten Infrastrukturkosten.\\n\\nKI-Foundation-Model-Training und -Tuning Die Services KI-Foundation-Model-Development und -Tuning stellen eine Kombination aus Beratung und der Nutzung der Computer-Kapazitäten dar. Die beratenden Tätigkeiten sowie die tatsächliche Entwicklung und das Tuning werden dabei durch das Team Services und Consulting des LKS erbracht. Abgerechnet werden dabei projektspezifisch übliche Tagessätze zwischen 1.200 EUR und 2.500 EUR pro Berater:in/Entwickler:in.\\n\\nAnnahme\\n\\nVerfügbare Kapazitäten\\n\\n10 FTE bei 80 % Chargeability & 200 Tagen\\n\\n1.600 Tage/Jahr\\n\\nDurchschnittlicher Tagessatz\\n\\nzwischen 1.200 & 2.500 Tagessatz\\n\\n1.600EUR\\n\\nSumme\\n\\n2,56 Mio EUR/Jahr\\n\\nTabelle 18: Übersicht der Einnahmen durch das Model-Training\\n\\nGroße KI-Modelle für Deutschland\\n\\n191\\n\\nBeratung Im Bereich Consulting sollen Beratungsleistungen nach marktüblichen Preisen abgerechnet werden. Diese werden anhand von Personentagen sowie der Seniorität und Expertise des Beratenden berechnet. Je nach der angefragten Leistung an das LKS wird dabei ein unterschiedlicher Umfang an Beratung nötig.\\n\\nAnnahme\\n\\nVerfügbare Kapazitäten\\n\\n10 FTE bei 80% Chargeability & 200 Tagen\\n\\n1.600 Tage/Jahr\\n\\nDurchschnittlicher Tagessatz\\n\\nzwischen 1.200 & 2.500 Tagessatz\\n\\n1.600EUR\\n\\nSumme\\n\\n2,56 Mio EUR/Jahr\\n\\nTabelle 19: Übersicht der Einnahmen durch die Beratung\\n\\nInference-Service Der Inference-Service bietet Unternehmen die Möglichkeit, KI-Modelle mit den Compute- Ressourcen des LKS zu nutzen. Abgerechnet wird hier nach der Menge genutzter Compute-Ressourcen, ähnlich der Vermietung von GPU-Stunden. Eventuell fallen weitere Kosten an, insofern das Team Services oder Consulting Modelle ein Modell zunächst für die Nutzung mittels Compute-Ressourcen des LKS vorbereiten muss. Diese zusätzlich nötigen Aktivitäten werden analog zum KI-Foundation-Model-Development abgerechnet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n192\\n\\nFinanzierungsmodelle von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n193\\n\\n11. Finanzierungsmodelle von LEAM Zur Finanzierung des Aufbaus und Betriebs des LKS bieten sich drei verschiedene Modelle an: die öffentliche und private Finanzierung sowie die Finanzierung als Public-Private- Partnership. Abhängig von möglichen Förderungen und damit einhergehenden rechtlichen Auflagen an LEAM muss die genaue Finanzierungsstruktur weiter evaluiert werden. Im Folgenden werden die möglichen Finanzierungsmodelle zunächst überblicksartig dargestellt. In Kapitel 11.5 findet sich eine Betrachtung der spezifischen rechtlichen Überlegungen denkbaren Finanzierungsmodelle. Public-Private- Partnership\\n\\nQuellen\\n\\nBund • Länder\\n\\nCorporates • Venture Capitals • Stiftungen\\n\\nKombination aus öffentlichen und privaten Quellen\\n\\nVehikel\\n\\nStaatliche Gesellschaft • Fördermittel (Förderung zur Gründung, Institutionelle Förderung, Projektförderung) • Garantierte Abnahme von Rechenleistung\\n\\nEigenkapital • Fremdkapital • Garantierte Abnahme von Rechenleistung\\n\\nKombination aus den Vorgenannten\\n\\n(Kredite) + maximale\\n\\nUnabhängigkeit + Konsens zwischen\\n\\n+ Flexibilität in der\\n\\nUnternehmensgestaltung\\n\\n+ Public als Ankerinvestor + Flexibleres operatives\\n\\nBewertung\\n\\nöffentl. Interesse und EU-konformer Entwicklung von KI- Modellen\\n\\n– Starre Strukturen und\\n\\n+ Einfaches und flexibles Anwerben von Personal – Komplexes Fundraising\\n\\nund eventuelle Governance\\n\\nGeschäft bei Einhaltung der Rahmenbedingungen für LEAM\\n\\n– Wettbewerbsrechtliche\\n\\nProzesse\\n\\n– Einschränkungen in\\n\\nAgilität, Personalaufbau etc.\\n\\n– Erschwerte Kooperation mit Wissenschaft durch hohe Kosten\\n\\nBeschränkungen\\n\\n– Kompliziertes Verfahren\\n\\nzur Gründung\\n\\nKommentar\\n\\nBerücksichtigung des EU-Beihilferechts mit Privilegierungen für Open-Source-KI- Projekte für Unternehmen und Wissenschaft\\n\\nBeispiel: DFKI\\n\\nTabelle 20: Gegenüberstellung der drei Finanzierungsszenarien für das LKS\\n\\nGroße KI-Modelle für Deutschland\\n\\n194\\n\\n11.1 Öffentliche Finanzierung\\n\\nIn der Wissenschaftslandschaft Deutschlands existiert eine Reihe von öffentlich finanzierten Rechenzentren, die zum Teil zur Weltspitze gehören (s. Kapitel 8). In der Regel teilen sich hier Bund und das zugehörige Bundesland die Finanzierung der Investitions- und laufenden Kosten. Es wird ein Grundstock an Personal zur Erhaltung des Betriebs finanziert sowie Planstellen zur Forschung. Üblicherweise erhalten diese Rechenzentren erhebliche Anteile ihrer Finanzierung über Drittmittelprojekte, also kompetitiv bei Förderern (DFG, Bund, Länder, Industrie) eingeworbene Personal- und Sachkosten für spezifische Forschungsprojekte. Zu unterscheiden sind hier in der Regel die institutionelle Förderung und die Projektförderung: Die institutionelle Förderung wird wiederkehrend jährlich gewährt; sie mag in der Höhe abhängig von verschiedenen Faktoren schwanken, bildet aber eine sichere Grundlage für den wirtschaftlichen Betrieb der Einrichtung. Daneben tritt die bereits zuvor erwähnte Projektförderung, auf die sich die Antragsteller:innen selbst (oft im kompetitiven Wettbewerb mit anderen Einrichtungen) bewerben können.\\n\\nVorteile öffentlicher Finanzierung Der Vorteil der öffentlichen Finanzierung für LEAM besteht in der engen Verzahnung zwischen öffentlichen Interessen eines an europäischen Werten orientierten Foundation- Models. Gleichzeitig bestehen nach erfolgter Finanzierungszusage eine hohe Verlässlichkeit und Planungssicherheit für die öffentlich geförderten Projekte. Damit könnte das Projekt über einen gewissen Zeitraum verlässlich wirtschaften und wäre unabhängig von der aktuellen Marktlage. Die entsprechenden Anreize für Start-ups könnten in dieser Finanzierungsform ebenfalls angeboten werden.\\n\\nNachteile öffentlicher Finanzierung finanzierte Rechenzentren aus EU- In der Regel unterliegen rein öffentlich steuerrechtlichen Gründen beihilferechtlichen, wettbewerbsrechtlichen Beschränkungen in der Nutzung durch privatwirtschaftliche Akteure. So stellt bspw. das HLRS der Uni Stuttgart, das dezidiert Rechenzeit für die Industrie zur Verfügung anbietet, nur einen Bruchteil der möglichen Rechenzeit zur Verfügung (s. Kapitel 8.5). Eine Nutzung einer öffentlich finanzierten LKS durch die eher wirtschaftlich organisierten LEAM Services würde also Beschränkungen unterliegen und es könnte womöglich aus rechtlichen Gründen nicht genügend Rechenzeit zur Verfügung gestellt werden. Dies gilt auch für Start-ups, die ebenfalls um die knappen Rechenressourcen mit den anderen wirtschaftlichen Einheiten konkurrieren. Dies könnte den Aufbau von wirtschaftlichen Applikationen auf den Foundation-Modellen hemmen. Gleichzeitig unterliegen rein öffentlich finanzierte Projekte umfangreichen Genehmigungen und Auflagen und somit in der Regel langwierigen Abstimmungsprozessen sowie bei wechselnden politischen Mehrheiten auch sich ändernden politischen Gegebenheiten. Öffentliche Unternehmen sind in der Regel tarifgebunden oder lehnen sich an Tarifverträge an, was die Flexibilität bei der Gewinnung der für den Betrieb und die Services erforderlichen hochqualifizierten Mitarbeiter:innen erschweren kann.\\n\\noder\\n\\nGroße KI-Modelle für Deutschland\\n\\n195\\n\\n11.2 Private Finanzierung\\n\\nFür eine privatwirtschaftliche Finanzierung von LEAM kommen vor allem zwei Szenarien in Betracht:\\n\\n\\n\\nJoint-Venture von großen Unternehmen (Cloud-Service-Provider und Anwender). Hierfür wurde in den geführten Interviews von verschiedenen Unternehmen eine generelle Bereitschaft und Interesse signalisiert.\\n\\nFinanzierung über Risikokapital durch klassische Venture Capital (VC)- Gesellschaften oder Private-Equity-Investoren.\\n\\nDa das Business-Modell einer rein privatwirtschaftlichen Gestaltung vom LKS mit hohen Risiken verbunden ist (u.a. auch durch die Dynamik der technologischen Entwicklungen und deren Auswirkungen auf den Markt), ist eine hundertprozentige Finanzierung durch private Unternehmen und Kapitalgeber nicht sehr wahrscheinlich.\\n\\nHier kann die öffentliche Hand durch Werkzeuge wie einer Anschubfinanzierung, Darlehen o.ä. helfen und das finanzielle Risiko abschwächen. Im Gegenzug kann vereinbart werden, dass z.B. ein gewisser Teil der Infrastruktur für nicht-wirtschaftliche Open-Source-Projekte von Unternehmen oder der Wissenschaft reserviert wird. Inwieweit diese Variante mit einer teilweisen oder gar überwiegenden wirtschaftlichen Nutzung auch in subventionsrechtlicher Hinsicht kompatibel ist, ist zu prüfen.\\n\\nDie Wissenschaft und kooperierende Unternehmen, die bereit sind, im Open-Source- im Rahmen von Verfahren zusammenzuarbeiten, würden Auftragsforschung oder einer Projektförderung die Rechenzeit nutzen, um die Open- Source Modelle im LEAM KI-Servicezentrum zu trainieren. Im Falle der Auftragsforschung kämen als Auftraggeber für die Wissenschaft dabei sowohl LEAM selbst als auch die öffentliche Hand in Betracht.\\n\\nin diesem Szenario\\n\\nVorteile privater Finanzierung Der Vorteil einer privaten Finanzierung liegt darin, dass Training und Verwertung ohne die regulatorischen Erfordernisse einer öffentlichen Finanzierung in einer Gesellschaft organisiert werden können. Die Investitionskosten könnten je nach Finanzierungsart auf viele Schultern verteilt werden, was das individuelle Risiko der Gesellschafter:innen minimieren würde.\\n\\nDie Gesellschaft könnte zudem unbegrenzte Gewinne erwirtschaften und wäre unter Berücksichtigung der Interessen der Kapitalgeber frei bei der Wahl der Investitionen. Die Gesellschaft wäre durch frei wählbare Vergütungen auch flexibler darin, entsprechendes Personal anzuwerben und könnte so einen Vorteil beim Know-How Aufbau erlangen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n196\\n\\nNachteile privater Finanzierung Die private Finanzierung birgt die Herausforderung, Geldgeber für einen erheblichen Investitionsbetrag zu finden. Die Wahrscheinlichkeit, dass ein einzelnes Unternehmen ins Risiko geht, die hohe Finanzierungslast auf sich zu nehmen, ist gering.\\n\\nEs müssten Joint-Ventures aus mehreren Unternehmen gebildet werden, wodurch die Gestaltung der Gesellschaft und deren Governance komplex und langwierig werden kann.\\n\\nIm Bereich der Venture Capital-Finanzierung übersteigen die erforderlichen Finanzierungssummen die zum Teil erheblich die Finanzierungspraxis der meisten deutschen und europäischen Fonds. So stellt der High-Tech-Gründerfonds (HTGF) nur Anschubfinanzierungen in einstelliger Millionenhöhe bereit.\\n\\nEin weiterer Nachteil privater Finanzierung in der Kooperation mit den wissenschaftlichen Partnern, die in diesem Modell nicht direkt, sondern nur im Rahmen von Aufträgen und Projekten an LEAM beteiligt sind. Das Training der Foundation- Modelle, die selbst keinen Profit generieren, aber viel Rechenzeit benötigen, stünde in diesem Fall in enger Konkurrenz mit dem Kerngeschäft der Betreibergesellschaft Inference und Tuning. Rechenzeit würde somit prioritär für die kommerziellen Produkte von LEAM verwendet werden, wodurch das Training der Foundation-Modelle eher keinen experimentellen Charakter hätte. Zudem müssten Unternehmen und/oder die Wissenschaft, die bereit sind, im Open-Source-Modell zu arbeiten, Marktpreise für die Nutzung zahlen.\\n\\nliegt\\n\\nDer größte Nachteil ist jedoch, dass die LEAM Zielstellung, KI-Foundation-Modelle im öffentlichen Interesse bereitzustellen, mit dem Gewinnstreben der Organisation im Wettbewerb stehen würde. So würden Trainingsdaten für die Foundation-Modelle in diesem Szenario auch stärker unter der Perspektive einer ökonomischen Verwertbarkeit ausgewählt, weshalb seltene Sprachen auf Grund der schlechten Skalierbarkeit am Markt und des höheren Aufwands bei der Beschaffung von Trainingsdaten ähnlich schlecht repräsentiert würden, wie bei den bisher am Markt angebotenen Modellen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n197\\n\\n11.3 Public-Private-Partnership\\n\\nBei einer Public-Private-Partnership erfolgt die Finanzierung durch die öffentliche Hand und private Unternehmen in jeweils zu definierenden Anteilen.\\n\\nEin Beispiel für eine erfolgreiche PPP im Bereich der Künstlichen Intelligenz ist das Deutsche Forschungszentrum für Künstliche Intelligenz (DFKI). Das DFKI wurde 1988 als gemeinnützige Public-Private Partnership (PPP) gegründet. Es unterhält Standorte in Kaiserslautern, Saarbrücken, Bremen, Niedersachsen, Labore in Berlin und Darmstadt sowie Außenstellen in Lübeck und Trier. Die Finanzierung erfolgt über Zuwendungen öffentlicher Fördermittelgeber wie der Europäischen Union, dem Bundesministerium für Bildung und Forschung (BMBF), dem Bundesministerium für Wirtschaft und Klimaschutz (BMWK), den Bundesländern und der Deutschen Forschungsgemeinschaft (DFG) sowie durch Entwicklungsaufträge aus der Industrie.\\n\\nWichtig bei der Gestaltung der PPP ist, die jeweiligen Vorteile von öffentlicher/privater Finanzierung zu maximieren und deren Nachteile zu minimieren.\\n\\nDie Herausforderung bei der Realisierung von LEAM als PPP steckt dabei im Charakter LEAMs als Infrastruktureinrichtung und den entsprechend hohen Investitionskosten, die für den Aufbau notwendig sind. Die Gesellschaft würde in diesem Szenario aus öffentlichen Mitteln (Bund und ggf. Land) die Infrastruktur beschaffen. Es müssten Mittel und Wege gefunden werden, die Mittel für die notwendige Infrastruktur zwischen den öffentlichen und privaten Partnern aufzuteilen. Grundsätzlich lassen sich in diesem Szenario auch die eher wirtschaftlich orientierten Services des Trainings-as-a-Service, Consulting und Inference/Tuning besser zusammen in einer Gesellschaft mit der Infrastruktur abbilden.\\n\\nVorteile einer Public-Private-Partnership Eine PPP ist ein denkbares Modell, in dem die Interessen der Wirtschaft nach optimalen Verwertungsmöglichkeiten eines Open Source Foundation-Modells sowie die Interessen der Politik und Gesellschaft nach einem digital souveränen Europa miteinander in Einklang gebracht werden, da beide Seiten an einer Gesellschaft beteiligt wären. Darüber hinaus wird das Projekt finanziell leichter zu realisieren, da die Investitionskosten geteilt werden. Gleichzeitig wäre der öffentliche Haushalt entlastet, da die Privatwirtschaft ebenfalls für einen Teil der Investitionskosten für die Infrastruktur aufkommt. Die entsprechenden Anreize für Start-ups könnten in dieser Finanzierungsform ebenfalls angeboten werden.\\n\\nNachteile einer Public-Private-Partnership Nachteil eines PPP besteht darin, dass die Gesellschaft nicht in derselben Weise privatwirtschaftlich agieren könnte, wie ein Wirtschaftsunternehmen, da der Bund bei seinen Beteiligungen wettbewerbsrechtlichen Beschränkungen unterworfen ist. Darüber hinaus ist sowohl mit Blick auf die genaue organisationstechnische Ausgestaltung der PPP als auch mit Blick auf die Spezifikationen der Foundation-Modelle mit einem intensiven Aushandlungsprozess zwischen den Vertretern der Privatwirtschaft und der öffentlichen Hand zu rechnen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n198\\n\\nSPOTLIGHT TUI Deutschland An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie TUI Group ist einer der weltweit führenden Touristikkonzerne. Zum Konzern gehören über 400 Hotels und Resorts und 16 eigene Kreuzfahrtschiffe, außerdem europaweit führende Veranstaltermarken und Online-Vermarktungsplattformen, fünf Fluggesellschaften und über 1.000 Reisebüros. Neben dem Ausbau des Kerngeschäfts mit Hotels, Kreuzfahrten über erfolgreiche Joint Ventures und Aktivitäten in den Urlaubsdestinationen setzt die TUI verstärkt auf den Ausbau digitaler Plattformen.\\n\\nHenning von Roon, Common Analytics Capabilities Lead, TUI Deutschland\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use-Case? Bei der TUI kommen vortrainierte Modell bei der\\n\\nBilderkennung und Textklassifizierung zum Einsatz. Mittels dieser Modelle können wir beispielsweise die Produktpräsentation optimieren.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Durch den Einsatz von KI-Foundation-Modellen können viele Geschäftsprozesse leichter automatisiert, erweitert oder verbessert werden. Gerade bei der Personalisierung von Angeboten für unsere Millionen von Kunden pro Jahr können wir mit KI-Foundation-Modellen einen großen Mehrwert schaffen, ohne die Entwicklungskosten massiv zu steigern. Personalisierung auf dieser Größenordnung wäre ansonsten kaum umzusetzen.\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Unsere hohen Ansprüche in Europa an Datenschutz und Datensicherheit werden oftmals durch außerhalb der EU bereitgestellte Modelle nicht erfüllt. Um keine Abstriche bei Datenschutz und Datensicherheit zu machen und gleichzeitig von den wirtschaftlichen Vorteilen des Einsatzes von KI-Foundation-Modellen zu profitieren, muss Europa deshalb eigene KI-Foundation-Modelle entwickeln. Zusätzlich ist die Übertragbarkeit häufig nicht gegeben, da die\\n\\nTrainingsdaten nicht die europäischen Sprachen und Verhaltensweisen widerspiegeln. Außerdem könnten Abhängigkeiten entstehen, welche sich in einer ungünstigen Preis- und Lizenzgestaltung abbilden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n199\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Open Source Modelle sorgen für Transparenz und erlauben eine bessere Interpretation der Ergebnisse. Zudem haben sie den Vorteil, dass wir sie entsprechend unserer spezifischen Bedürfnisse weiterentwickeln können. Europäische Modelle unterliegen von Anfang an den hiesigen Standards und bilden die lokalen Gegebenheiten deutlich besser ab, welches den Einsatz für uns attraktiver macht.\\n\\n11.4 Rechtliche Rahmenbedingungen\\n\\nDie Umsetzung der Finanzierung des LEAM KI-Supercomputers wirft zahlreiche Rechtsfragen auf, die im Rahmen dieser Machbarkeitsstudie weder abschließend noch tatsächlichen Gegebenheiten und vollständig behandelt werden können. Die Maßnahmen im Bezug auf rechtliche Rahmenbedingungen müssen daher im Nachgang tiefergehend evaluiert werden. Für die weitere Betrachtung sind jedoch grundsätzlich drei besondere Rechtsbereiche zu unterscheiden, die die geplante Struktur und den Betrieb des LKS mitprägen und bei der Strukturplanung mit in den Blick genommen werden müssen. Dabei handelt es sich hierbei um das EU-Beihilferecht, das (EU-)Vergaberecht sowie das öffentliche Dienst- und Vergütungsrecht. Nachfolgend sollen diese Rechtsmaterien zunächst überblicksmäßig dargestellt werden Anschließend sollen die Rechtsmaterien und ihre Auswirkungen auf die drei grundsätzlich zu unterscheidenden Finanzierungsmodelle im Hinblick auf die Besonderheiten des LEAM-Projekts dargestellt werden, um hieraus in einer vergleichenden Rechtsbetrachtung die Vor- und Nachteile zusammenzufassen.\\n\\nDie Bedeutung des EU-Beihilferechts Eine staatliche Förderung für das Projekt LEAM sowie der Betrieb des LKS müssen mit dem EU-Beihilferecht vereinbar sein. Dieses ist bindend und steht dem deutschen Recht vor. Die Vorschriften über staatliche Beihilfen – Art. 107 bis 109 des Vertrages über die Arbeitsweise der Europäischen Union (AEUV) – zählen zu den wichtigsten europäischen Wettbewerbsregeln. Die beihilferechtlichen Regelungen verfolgen das Ziel, Wettbewerbsverzerrungen innerhalb des europäischen Binnenmarkts durch Beihilfen, die Mitgliedstaaten Unternehmen gewähren, zu verhindern. Daher normiert Art. 107 Abs. 1 AEUV ein grundsätzliches Beihilfeverbot, von dem jedoch Ausnahmemöglichkeiten bestehen.\\n\\nDas EU-Beihilferecht wird – vereinfacht – wie folgt geprüft:\\n\\nAuf der ersten Ebene wird geprüft, ob überhaupt begrifflich eine – nach dem soeben Gesagten grundsätzlich unzulässige – Beihilfe vorliegt. Häufig ist dies durch eine geschickte Ausgestaltung einer staatlichen Förderung bereits nicht der Fall – mit sehr günstigen Rechtsfolgen. Liegt begrifflich eine staatliche Beihilfe vor, so muss auf einer zweiten Ebene geprüft werden, ob diese aufgrund bestimmter Regeln der EU zulässig ist, ohne dass die EU der Beihilfe im Einzelnen ausdrücklich zustimmen muss. Eine Beihilfe Allgemeinen kann\\n\\ninsbesondere\\n\\nzulässig\\n\\nsein, wenn\\n\\ndie\\n\\nRegeln\\n\\nder\\n\\nGroße KI-Modelle für Deutschland\\n\\n200\\n\\nGruppenfreistellungsverordnung (AGVO) greifen. Wenn solche generell geregelten Ausnahmen nicht einschlägig sind, muss auf der dritten Stufe eine Genehmigung der EU- Kommission für die Beihilfe eingeholt werden (sog. Notifizierung). Das ist oft langwierig und komplex und sollte für Projekte, die in besonderem Maße auf Schnelligkeit ausgerichtet sind, daher vermieden werden. Nicht mit dem Europarecht konforme Beihilfen darf der deutsche Staat nicht vergeben und ist verpflichtet, dennoch gewährte Gelder zurückzufordern. Daher muss das EU-Beihilferecht zwingend eingehalten werden.\\n\\nWichtig ist für das LEAM Projekt zunächst die Prüfung, ob überhaupt „begrifflich\" eine staatliche Beihilfe vorliegt. Eine solche ist gegeben, wenn die folgenden vier Merkmale kumulativ erfüllt sind. Eine Beihilfe ist\\n\\neine staatliche Maßnahme,\\n\\ndie eine Begünstigung,\\n\\neines bestimmten Unternehmens darstellt und\\n\\ndadurch zum Eintritt einer (jedenfalls potenziellen) Wettbewerbsverfälschung und Beeinträchtigung des innergemeinschaftlichen Handels führt.\\n\\nEine staatliche Förderung der Infrastruktur des Projekts LEAM stellt, ohne dass hierauf im Einzelnen eingegangen werden soll, eine staatliche Maßnahme dar, die zu einer Begünstigung führen und auch Auswirkungen auf den europäischen Markt haben kann, da einer solchen großen Infrastruktur schon ihrem Sinn und Zweck nach europaweite Bedeutung zukommen kann.\\n\\nAllerdings stellt sich – unabhängig von der Rechtsform – die Frage, ob das Projekt LEAM auch ein „Unternehmen\" im Sinne des EU-Beihilferechts darstellt. Der Begriff des Unternehmens im EU-Beihilferecht ist dabei nicht von der Rechtsform einer Einheit abhängig, er ist tätigkeitsbezogen. Maßgeblich ist immer die Frage, ob die Einheit eine wirtschaftliche Tätigkeit im Sinne des Beihilferechts ausübt. Nach ständiger EuGH- Rechtsprechung ist eine Tätigkeit wirtschaftlich, wenn sie darin besteht, Güter oder Dienstleistungen auf einem bestimmten Markt anzubieten, 20 unabhängig von ihrer Rechtsform, der Art ihrer Finanzierung sowie einer Gewinnerzielungsabsicht. 21 Irrelevant ist daher, ob das Unternehmen in privater oder öffentlicher Trägerschaft betrieben wird. Ein Beispiel sind Forschungsinfrastrukturen: Wenn sie – vereinfacht ausgedrückt – für die Allgemeinheit forschen, sind sie kein Unternehmen im Sinne des EU-Beihilferechts. Wenn sie dagegen Auftragsforschung für andere Unternehmen als Auftraggeber erbringen, sind sie beihilferechtlich ein Unternehmen und es gelten die Beschränkungen des EU- Beihilferechts. Ein und dieselbe Einheit kann daher sowohl „Unternehmen\" wie auch „Nicht-Unternehmen\" im Sinne des EU-Beihilferechts sein, abhängig von der jeweils ausgeübten Tätigkeit.\\n\\n20 Vgl. EuGH, Urteil vom 16.06.1987, Rs. 118/85, Rn. 7 – Kommission/Italien; vom 12.09.2000, verbundene Rsen. C-180/98 bis C-184/98, Rn. 75 – Pavlov u.a. und vom 01.07.2006, Rs. C-49/07, Rn. 22 – MOTOE. 21 EuGH, Urteil vom 23.04.1991, C-41/90, Slg. 1991, I-1979 Rn. 21 – Höfer u. Elser/Macroton; von Wallenberg/Schütte, in: Grabitz/Hilf/Nettesheim, Das Recht der Europäischen Union, Werkstand: 67. EL Juni 2019, Art. 107 AEUV, Rn. 39.\\n\\nGroße KI-Modelle für Deutschland\\n\\n201\\n\\nZusammengefasst bedeutet dies auch für das LKS:\\n\\n\\n\\nSoweit dieses nicht-wirtschaftliche Tätigkeiten im Sinne des EU-Beihilferechts ausübt, stellt die staatliche Förderung bzw. die Verwendung der staatlich geförderten Mittel keine Beihilfe dar.\\n\\nWenn das LKS wirtschaftliche Tätigkeiten im Sinne des EU-Beihilferechts ausübt, ist es ein Unternehmen und die Beschränkungen des EU-Beihilferechts gelten für die staatliche Förderung bzw. die Verwendung der staatlich geförderten Mittel.\\n\\nFür die Abgrenzung von nichtwirtschaftlichen und wirtschaftlichen Tätigkeiten, die, wie hier, einen Bezug zur Wissenschaft haben, ergeben sich Besonderheiten bei dieser Abgrenzung. Günstig ist, dass das Projekt LEAM einen großen Teil der Kapazitäten für die Entwicklung von KI-Foundation-Modellen zur Verfügung stellen will, die im Wege des Open-Source-Verfahrens der Allgemeinheit (bzw. diskriminierungsfrei der Wirtschaft) zur Verfügung gestellt werden sollen. Insbesondere ist bei der Auslegung und Anwendung des EU-Beihilferechts auf dem Gebiet der Entwicklung von KI-Modellen zur freien Nutzung der „Unionsrahmen für staatliche Beihilfen zur Förderung von Forschung, Entwicklung und Innovation“ vom 19.10.2022, 2022/C 7388 (Aktualisierung der Vorgängerversion vom 21.05.2014, 2014/C 198/01; nachfolgend „FuE-Rahmen“) maßgeblich. Dieser enthält die wichtigsten Unterscheidungen zwischen wirtschaftlichen und nichtwirtschaftlichen Tätigkeiten im Wissenschaftskontext sowie auch Privilegierungen für den Einsatz von Infrastruktur in der Wissenschaft, die dem LEAM-Projekt zugutekommen können.\\n\\nVereinfacht kann folgende Unterscheidung vorgenommen werden:\\n\\nSoweit das LKS mit der Wissenschaft, im Open-Source-Verfahren arbeitenden Unternehmen oder Konsortien aus beiden Bereichen zusammenarbeitet und insbesondere KI-Foundation-Modelle entwickelt, die im Open-Source-Verfahren wiederum allgemein der Wissenschaft und Wirtschaft zur Verfügung gestellt werden, können solche allgemein die (europäische) Wissenschaft und Wirtschaft bereichernde Tätigkeiten und damit nicht-wirtschaftliche Tätigkeiten im Sinne des FuE-Rahmens sein (vgl. Rz. 20 FuE-Rahmen). KI hat dabei im neuen FuE-Rahmen 2022 eine ausdrückliche Erwähnung gefunden: Der Begriff der experimentellen Entwicklung wird in Rz. 16 lit. k) definiert als (verkürzt) den Erwerb, die Kombination, Gestaltung und Nutzung vorhandener wissenschaftlicher, technischer, wirtschaftlicher und sonstiger einschlägiger Kenntnisse und Fertigkeiten mit dem Ziel, in beliebigen Bereichen, Technologien, Branchen oder Wirtschaftszweigen neue oder verbesserte Produkte, Verfahren oder Dienstleistungen einschließlich digitaler Produkte, Verfahren oder Dienstleistungen zu entwickeln. Hierbei wird die Entwicklung Künstlicher Intelligenz ausdrücklich als ein Anwendungsbeispiel genannt. Damit dürften die KI-Foundation-Modelle (sowie sonstige KI-Modelle) unter den Begriff der Entwicklung im Rahmen des FuE-Rahmens zu subsumieren sein. Zudem wird die Entwicklung an Software mit dem Ziel, diese als Open- Source-Software zur Verfügung zu stellen, ausdrücklich als nicht-wirtschaftliche und damit nicht beihilferelevante Tätigkeit eingestuft (Ziff. 20 FuE-Rahmen).\\n\\nWie aus der deutschen Förderlandschaft bekannt, können sich Unternehmen auf solche Projekte bewerben, auch in Verbund mit der Wissenschaft, wenn sie bereit sind, die Software im Open-Source-Verfahren am Ende zur Verfügung zu stellen. Das LEAM-Projekt kann hierbei mehrere Wege wählen, um eine entsprechende beihilfekonforme Ausgestaltung zu erreichen; so kann sie beispielsweise die Software im Open-Source-\\n\\nGroße KI-Modelle für Deutschland\\n\\n202\\n\\nVerfahren zusammen mit Unternehmen oder der Wissenschaft entwickeln, wobei ihr wesentlicher Beitrag die Zurverfügungstellung der Infrastruktur und der technische Support sein kann (siehe beispielsweise Rz. 20, lit. a) ii) des FuE-Rahmens, \"wirksame Zusammenarbeit\"), oder eben allgemein die Entwicklung solcher KI-Foundations-Modelle im Wege einer Ausschreibung/diskriminierungsfreien Vergabe auch durch Unternehmen ermöglichen (siehe die Logiken in Rz. 20, lit. a) iii) und lit. b) des FuE-Rahmens, wonach die öffentliche Ausschreibung sogar von Dienstleistungen durch Unternehmen im Rahmen solcher Projekte den nichtwirtschaftlichen Charakter nicht berührt). Unternehmen können also an der Entwicklung von KI-Foundations-Modelle zusammen mit dem Projekt LEAM beteiligt werden, soweit sie im Open-Source-Verfahren arbeiten und sich damit bereit erklären, entwickelte Software offen und diskriminierungsfrei (jedenfalls im EU- Raum) zur Verfügung zu stellen. Wie dies letztendlich konkret ausgestaltet ist, hängt von der gewünschten Organisationsform ab.\\n\\nFür das Projekt LEAM und seine Ziele kann dies eine erhebliche beihilferechtliche Privilegierung darstellen und eine öffentliche Finanzierung des Projekts begünstigen. Das bedeutet vereinfacht zusammengefasst: Eine staatliche Finanzierung der Infrastruktur des LEAM-Projekts ist jedenfalls dann beihilferechtlich möglich, wenn die Infrastruktur im Open-Source- weit überwiegend der Wissenschaft und/oder kooperierenden, Verfahren arbeitenden Unternehmen, in nicht-diskriminierender Weise zur Verfügung gestellt wird. Abzuklären wäre dabei im Rahmen der Umsetzung noch, ob eine zwingende Verpflichtung für die nutzende Wissenschaft bestehen muss, entwickelte KI-Modelle bzw.- Lösungen im Open Source-Verfahren wiederum der Allgemeinheit zur Verfügung zu stellen - dies wäre wohl jedenfalls beihilferechtlich der „sicherste\" Weg. Bei entwickelnden Unternehmen dürfte nach dem Beihilferahmen viel dafür sprechen, dass eine Open- Source-Nutzung verpflichtend ist, wobei Einzelheiten bei der konkreten Ausgestaltung festzulegen wären.\\n\\nSelbst wenn beispielsweise generierte Modelle oder Wissen im Rahmen des Wissenstransfers in die Wirtschaft veräußert werden, kann dies eine nichtwirtschaftliche Tätigkeit sein, sofern der Gewinn aus der Veräußerung dem nichtwirtschaftlichen Betrieb zugute kommt (ebenfalls Rz. 20 FuE-Rahmen). Das bedeutet also, dass beispielsweise die Weitergabe von Lizenzen für entwickelte Software oder KI-Modelle durch das LEAM- Projekt gegen eine entsprechende Gebühr die öffentliche Förderung im Sinne des EU- Beihilferechts nicht gefährdet, wenn die Gewinne aus diesen Lizenzen wiederum dem nicht-wirtschaftlichen Bereich des LEAM-Projekts zugutekommen (also etwa mehr Rechenzeit oder bessere Infrastruktur für die Wissenschaft bzw. im Open-Source-Modell arbeitende Unternehmen zur Verfügung zu stellen). Insoweit ergibt sich dann kein beihilferechtliches Problem.\\n\\nAndere geplante Tätigkeiten des Projekts LEAM, wie zum Beispiel die Consulting-Angebote oder die Vermietung der Infrastruktur, sind dagegen wirtschaftliche Tätigkeiten. Sie unterliegen den Beschränkungen des EU-Beihilferechts.\\n\\nSofern bei der öffentlichen Finanzierung des LEAM-Projekts danach doch das EU- Beihilferecht eingreift, kommt es zweiten Ebene auf Ausnahmetatbestände an, die ausdrücklich geregelt sind und bei denen keine Genehmigung der EU-Kommission notwendig ist. Welche Ausnahmetatbestände dies sind, lässt sich nicht im Einzelnen vorab festlegen, sondern hängt von der konkreten\\n\\nzunächst auf der\\n\\nGroße KI-Modelle für Deutschland\\n\\n203\\n\\nAusgestaltung des Projekts und der gewünschten Finanzierung ab. Während die De- minimis-Verordnung keine Rolle spielen wird (die Fördergrenzen sind mit max. 200.000 Euro zu niedrig, um für das Projekt relevant zu werden), kann die sogenannte Allgemeine Gruppenfreistellungsverordnung eine Rolle spielen.\\n\\nDie Allgemeinen Gruppenfreistellungsverordnung (Verordnung Nr. 652/2014 der EU- Kommission vom 17.06.2014, ABl. L 187/1 vom 26.06.2014; „AGVO“) enthält in größerem Umfang Tatbestände, bei denen eine begrifflich vorliegende Beihilfe an ein Unternehmen auch ohne Notifizierung (Genehmigung) durch die EU-Kommission beihilferechtskonform ist. Die Regeln sind dabei sehr dezidiert und müssen eingehalten werden, damit keine Notifizierung erforderlich ist. Sie sollen nachfolgend, soweit sie in Betracht kommen, bei den einzelnen Modellen vorgestellt werden.\\n\\nDaneben können Maßnahmen zugunsten von Unternehmen, die mit sogenannten Interesse“ befasst sind, auf „Dienstleistungen von allgemeinem wirtschaftlichen des des Grundlage Freistellungsbeschlusses der EU-Kommission (Beschluss der Kommission (2012/21/EU) vom 20.12.2011, ABl. Nr. L 7/3 vom 11.01.2012) vom EU-Beihilfeverbot ausgenommen sein. Es ist jedoch unwahrscheinlich, dass das LEAM-Projekt (trotz seiner möglichen Bedeutung für die deutsche Wirtschaft) hierunter fallen kann, da diese Rechtsprechung sich eher auf soziale Bedürfnisse (Öffentlicher Nahverkehr, Gesundheitsversorgung o.ä.) bezieht.\\n\\nder\\n\\nAltmark-Trans-Rechtsprechung\\n\\nEuGH\\n\\nbzw.\\n\\nSofern dann auch Regeln wie die AGVO nicht greifen, um die Beihilfe zu rechtfertigen, kommt es auf eine Notifizierung (mithin ein Genehmigungsverfahren) für die Beihilfe bei der EU-Kommission an. Dieses ist rechtlich komplex und kann eine längere Zeit in Anspruch nehmen, wobei die EU-Kommission in der Regel auch Änderungswünsche an der Finanzierungsstruktur bzw. an sonstigen Aspekten der Verwirklichung der geplanten Struktur hat. Einzelheiten eines solchen Verfahrens können an dieser Stelle nicht dargestellt werden und würden den Umfang dieser Studie erheblich überschreiten. Zudem ist die Frage, nach welchen Regeln die EU-Kommission notifiziert, auch maßgeblich davon abhängig, wie die Finanzierungsstruktur des Projekts ausgestaltet wird und welche Zwecke es im Kern verfolgen soll. Sofern die notifizierungspflichtige Beihilfe nach der Zweckrichtung des LEAM-Projekts unter den FuE-Rahmen fallen kann, prüft die EU- Kommission, ob die staatliche Beihilfe zur Förderung von Forschung und Entwicklung als mit dem Binnenmarkt vereinbar angesehen werden kann. Dazu untersucht die Kommission, ob die Beihilfemaßnahme die Entwicklung eines bestimmten Wirtschaftszweigs fördert und ob sie die Handelsbedingungen in einer Weise verändert, die dem gemeinsamen Interesse zuwiderläuft (Rz. 38 FuE-Rahmen). Entscheidend ist hierbei, ob eine Beihilfe eine wesentliche Verbesserung bewirkt, die der Markt selbst nicht herbeiführen kann, insbesondere wenn es im Übrigen ein Marktversagen gibt. Dabei verlangt die Kommission andererseits wiederum, dass die Höhe und die Intensität der Beihilfe auf ein Minimum begrenzt sein muss, was in der Praxis regelmäßig umfangreich geprüft wird. Dies sind nur einige – überblicksmäßige – Zusammenfassungen des Prüfungsprogramms der EU-Kommission im Rahmen eines Notifizierungsverfahrens.\\n\\nGroße KI-Modelle für Deutschland\\n\\n204\\n\\nDie Bedeutung des Vergaberechts Wenn die öffentliche Hand sich Güter und Dienstleistungen beschafft, ist sie in ihrer Auswahl, anders als private Unternehmen, nicht frei, sondern unterliegt den Beschränkungen des Vergaberechts. Dieses dient der Wirtschaftlichkeit und Transparenz staatlichen Handelns sowie einem fairen Wettbewerb um öffentliche Aufträge. Hierzu gibt das Vergaberecht vor, welches Verfahren bei der Auftragsvergabe einzuhalten ist und wie die Auswahl zwischen möglichen Vertragspartnern zu erfolgen hat.\\n\\ninsbesondere öffentliche Verpflichtet, öffentliche Aufträge auszuschreiben, sind Auftraggeber im Sinne von §§ 98, 99 des Gesetzes gegen Wettbewerbsbeschränkungen (GWB). Hierunter fallen nach § 99 Nr. 2 lit. a) GWB unter anderem juristische Personen des öffentlichen und des privaten Rechts, die zu dem besonderen Zweck gegründet wurden, im Allgemeininteresse liegende Aufgaben nichtgewerblicher Art zu erfüllen, sofern sie überwiegend von Gebietskörperschaften, das heißt, etwa von Bund und Ländern, durch Beteiligung oder in sonstiger Weise finanziert werden. Je nach konkreter Ausgestaltung der Finanzierung kann dies auf das LEAM-Projekt zutreffen. Diese Frage wird unten jeweils bezogen auf die einzelnen Finanzierungsmodelle erörtert.\\n\\nAbgesehen von Bagatellgeschäften mit Wert von (je nach Bundesland bzw. Bund) 500 EUR bzw. 1000 EUR (netto) müssen Aufträge öffentlicher Auftraggeber ausgeschrieben werden, und zwar je nach geschätztem Auftragsvolumen entweder bundes- oder – bei liegt der Überschreitung bestimmter Schwellenwerte – europaweit. Derzeit Schwellenwert für Liefer- und Dienstleistungen für öffentliche Auftraggeber bei 215.000 EUR (netto). Es hat eine Veröffentlichung der Ausschreibung zu erfolgen und Bieter können ein Angebot abgeben. Nach der Angebotsauswahl, die bestimmten Vorgaben unterliegt, wird der Auftrag einem Bieter zugeschlagen.\\n\\nKommt das Vergaberecht zur Anwendung, sind seine Vorgaben unbedingt einzuhalten, da Verstöße hiergegen erhebliche finanzielle Konsequenzen verursachen und den zeitlichen Ablauf von Projekten stark verzögern können. Durch die der staatlichen Kontrolle unterliegenden ist die Güter- und Leistungsbeschaffung durch öffentliche Auftraggeber zeitlich und inhaltlich deutlich weniger frei als für rein private Unternehmen.\\n\\nvergaberechtlichen Vorgaben\\n\\nDie Bedeutung des öffentlichen Dienst- und Vergütungsrecht Bei der Entscheidung für ein Finanzierungsmodell sind auch Belange des öffentlichen Dienst- und Vergütungsrechts zu beachten. Im öffentlichen Dienst existieren spezielle Anforderungen an die Personalgewinnung sowie vergütungsrechtliche Bestimmungen. Aber auch für private Unternehmen, die staatliche Förderungen in Anspruch nehmen, können in diesem Bereich Einschränkungen gelten.\\n\\nFür die Personalgewinnung im öffentlichen Dienst ist Art. 33 Abs. 2 Grundgesetz (GG) maßgeblich, der einen gleichen Zugang zu öffentlichen Ämtern garantiert. Daher hat die Stellenbesetzung im öffentlichen Dienst nach dem Prinzip der Bestenauslese zu erfolgen, was zu besonderen Anforderungen an die Ausgestaltung von Bewerbungsverfahren und die Personalauswahl führt. Auch bei der Personalgewinnung sind öffentliche Arbeitgeber daher weniger frei als private Unternehmen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n205\\n\\nWeiter gilt für beim Bund Beschäftigte der Tarifvertrag für den öffentlichen Dienst (TVöD), für Beschäftigte der Länder außerhalb von Branchen mit eigenen Tarifverträgen der Tarifvertrag für hochqualifizierte Kräfte nach diesen Tarifverträgen bleiben weit hinter ihren Verdienstmöglichkeiten in der freien Wirtschaft zurück und sind damit finanziell eher unattraktiv.\\n\\nfür den öffentlichen Dienst der Länder\\n\\n(TV-L). Die Gehälter\\n\\nFür Empfänger hochvolumiger Beihilfen greift oftmals das sog. Besserstellungsverbot. Dabei wird der Bewilligungsbescheid für die Zuwendung mit einer Auflage versehen, die besagt, dass der Zuwendungsempfänger seine Arbeitnehmer:innen nicht besser entlohnen darf als vergleichbare Arbeitnehmer:innen des Zuwendungsgebers. Beispielhaft darf der Empfänger einer Beihilfe des Bundes in einem solchen Fall seine Angestellten nicht besser entlohnen, als es nach dem TVöD der Fall wäre.\\n\\n11.5 Auswirkungen der Rechtsmaterien auf die Finanzierungsmodelle\\n\\nNachfolgend werden die Auswirkungen dieser Rechtsmaterien auf die einzelnen drei grundsätzlichen Finanzierungsmodelle dargestellt.\\n\\nDas Modell der öffentlichen Finanzierung Vorab ein Hinweis zur grundsätzlichen Zulässigkeit: Grundsätzlich ist der Bund befugt, Forschungsinfrastrukturen in eigener Verantwortung zu errichten oder sich an ihrer Finanzierung zu beteiligen. Neben der notwendigen Haushaltsermächtigung durch den Bundestag kommt es auf die konkrete Ausgestaltung der Struktur an, um dahinter liegende verfassungsrechtliche Fragen entsprechend beantworten zu können.\\n\\nIn Art. 91b des Grundgesetzes (GG) wurde verfassungsrechtlich abgesichert, dass Bund und Länder in Fällen überregionaler Bedeutung bei der Förderung von Wissenschaft und insbesondere für die Forschung zusammenwirken können. Die Regelung wurde Finanzierung großer Forschungsinfrastrukturen geschaffen und bietet dem Bund – dann in Zusammenwirken mit einem Sitzland – auch die Möglichkeit, das LEAM-Projekt öffentlich zu finanzieren.\\n\\nBei einem Modell der (rein) öffentlichen Finanzierung des LEAM-Projekts finden alle drei zuvor dargestellten Rechtsmaterien die stärkste Wirkung. Im Einzelnen:\\n\\nEU-Beihilferecht Bei einer rein oder jedenfalls weit überwiegenden öffentlichen Finanzierung gelten die oben genannten Darstellungen zum EU-Beihilferecht. Wie dort dargelegt, kann auf Basis des FuE-Rahmens voraussichtlich argumentiert werden, dass wesentliche geplante Einsätze des KI-Supercomputers nicht-wirtschaftliche Tätigkeiten darstellen und damit die öffentliche insoweit nicht den Beihilfetatbestand erfüllt. Das wäre insoweit vorteilhaft, als einerseits ein komplexes Notifizierungsverfahren bei der EU-Kommission entfällt und andererseits auch die Beschränkungen der AGVO, die in der Regel keine volle staatliche „Durchfinanzierung“ zulässt, nicht greifen würden. In einer öffentlichen Grundfinanzierung könnte, soweit es auch politisch gewollt ist, der Bund zusammen mit einem Sitzland damit grundsätzlich\\n\\nFinanzierung der\\n\\ngeplanten\\n\\nInfrastruktur\\n\\nGroße KI-Modelle für Deutschland\\n\\n206\\n\\nFinanzmittel für den Aufbau der Infrastruktur sowie deren Betrieb (einschließlich Personal) zur Verfügung stellen.\\n\\nDabei sind auch Tätigkeiten aus dieser Infrastruktur heraus, die wirtschaftlicher Natur im Sinne des EU-Beihilferechts sind (z.B. das geplante Consulting-Angebot), nicht von vornherein ausgeschlossen. Der FuE-Rahmen erlaubt auch solche Tätigkeiten, wenngleich auch mit gewissen Anforderungen. Diese sollen nachfolgend überblicksmäßig dargestellt werden:\\n\\nDas wesentlichste Gebot ist die Einführung einer sogenannten Trennungsrechnung. Das bedeutet Folgendes: Übt das LEAM-Projekt sowohl wirtschaftliche als auch fällt die öffentliche Förderung der nichtwirtschaftliche Tätigkeiten aus, so nichtwirtschaftlichen Tätigkeiten nicht unter das Beihilfeverbot, wenn die nichtwirtschaftlichen und die wirtschaftlichen Tätigkeiten und die Kosten, Finanzierung und Erlöse klar voneinander getrennt werden können, sodass keine Gefahr der Quersubventionierung der wirtschaftlichen Tätigkeiten besteht. Das Geld, das vom Bund bzw. einem Land stammt, darf nicht verwendet werden, um Verluste des wirtschaftlichen Bereichs auszugleichen (Verbot der Quersubventionierung und zugleich Gebot der sog. Trennungsrechnung, Ziff. 19 FuE-Rahmen). Eine solche Trennungsrechnung kann im Jahresabschluss durchgeführt werden, bedeutet aber auch, dass Einnahmen und Aufwand für nichtwirtschaftliche und wirtschaftliche Tätigkeiten in der Buchhaltung streng voneinander getrennt werden müssen und stets klar sein muss, ob ein Aufwand (gleich, ob Personal oder Infrastrukturnutzung) dem einen oder dem anderen Bereich zugeordnet werden kann. Das erfordert in der Praxis einiges an Verwaltungsaufwand, ist aber lösbar.\\n\\nDeshalb darf der wirtschaftliche Bereich (also z.B. das Consulting) keinen Verlust erwirtschaften. Ebenso darf der wirtschaftliche Bereich nicht eine Hauptanwendung der öffentlich geförderten Infrastruktur werden, bei der der nichtwirtschaftliche Bereich in den Hintergrund gedrängt wird. Hierzu enthält der FuE-Rahmen noch eine Privilegierung für Forschungseinrichtungen bzw. -infrastrukturen: Wenn die Forschungseinrichtung oder Forschungsinfrastruktur fast ausschließlich für eine nichtwirtschaftliche Tätigkeit genutzt wird, kann ihre Förderung ganz aus dem Anwendungsbereich des Beihilferechts herausfallen, sofern die wirtschaftliche Nutzung eine reine Nebentätigkeit darstellt, die mit dem Betrieb der Infrastruktur unmittelbar verbunden und dafür erforderlich ist oder die in untrennbaren Zusammenhang mit der nichtwirtschaftlichen Haupttätigkeit steht und ihr Umfang begrenzt ist. Das ist der Fall, wenn für die wirtschaftlichen Tätigkeiten dieselben Inputs (wie Material, Ausstattung, Personal und Anlagekapital) eingesetzt werden, wie für die nichtwirtschaftlichen Tätigkeiten und wenn die für die betreffende wirtschaftliche Tätigkeit jährlich zugewiesene Kapazität nicht mehr als 20 % der jährlichen Kapazität der betreffenden Einrichtung bzw. Infrastruktur beträgt (Ziff. 21 FuE-Rahmen). Das würde, sehr grob vereinfacht, für das LEAM KI-Projekt beispielsweise bedeuten, dass 80 % der Rechenkapazität des KI-Supercomputers für nichtwirtschaftliche Aktivitäten benutzt werden dürfen (z.B. für die Entwicklung großer KI Foundation-Modelle mit Open Access für die Wirtschaft bzw. Wissenschaft), 20 % dann für wirtschaftliche Zwecke. Einzelheiten sind dann für den tatsächlichen Betrieb auszuarbeiten. Es zeigt sich aber, dass auch bei einer rein öffentlichen Finanzierung wirtschaftliche Tätigkeiten dem Projekt LEAM nicht untersagt sind und im Geschäftsmodell weiter eingeplant werden können. In\\n\\nGroße KI-Modelle für Deutschland\\n\\n207\\n\\nder Praxis gibt es solche Fälle zum Beispiel bei Hochschulen, deren Infrastruktur öffentlich finanziert ist, die aber gleichwohl wirtschaftliche Angebote wie Auftragsforschung oder forschungsnahe Dienstleistungen haben.\\n\\nNachdem eine Abgrenzung von wirtschaftlicher zu nichtwirtschaftlicher Tätigkeit vorgenommen wurde, muss sichergestellt werden, dass wirtschaftliche Tätigkeiten nicht zu einer Quersubventionierung führen. Dies bemisst sich am sog. Private-Investor-Test. Dabei wird das wirtschaftliche Handeln der staatlichen Stelle mit dem hypothetischen Verhalten eines Privatinvestors verglichen. Würde ein solcher den Vorteil nicht oder zu ungünstigeren Konditionen anbieten, liegt eine Beihilfe nach Art. 107 Abs. 1 AEUV vor. Der FuE-Rahmen enthält insoweit dezidierte Regelungen (vgl. Ziff. 26f. FuE-Rahmen), wonach die Preisermittlung sich entweder nach dem Marktpreis oder nach einer Vollkostenkalkulation mit Gewinnaufschlag richten muss. Gleichwohl können die Kosten, die das LEAM-Projekt für die Wirtschaft für solche wirtschaftlichen Tätigkeiten berechnet, betriebswirtschaftlich deutlich niedriger liegen als vergleichbare Angebote aus den USA, da sich auch beihilferechtlich das LEAM-Projekt bei diesen wirtschaftlichen Aktivitäten nicht an dem Gedanken der Profitmaximierung orientieren muss, sondern an dem Gedanken Vollkosten + angemessener Gewinnaufschlag. Letzterer dürfte deutlich liegen als bei Angeboten von US-Anbietern, die gegebenenfalls eine niedriger monopolähnliche Stellung haben.\\n\\nZusammengefasst bedeutet dies mithin, dass eine öffentliche Finanzierung des Projekts LEAM und deren Infrastruktur beihilferechtlich vor allem dann denkbar ist, wenn die geschaffene Infrastruktur überwiegend für nichtwirtschaftliche Zwecke genutzt und der Wissenschaft bzw. kooperierenden Unternehmen zur Verfügung gestellt wird. Eine gewisse Kapazität des Supercomputers und des Personals kann gleichwohl auch für wirtschaftliche Tätigkeiten zur Verfügung gestellt werden, wobei diese wirtschaftlichen Tätigkeiten in der Regel keinen Verlust erwirtschaften und zu einem Marktpreis angeboten werden müssen. Eine Trennungsrechnung muss buchhalterisch und im Jahresabschluss klarstellen, dass keine Quersubventionierung wirtschaftlicher Tätigkeiten erfolgt.\\n\\nVergaberecht Das Vergaberecht ist bei einer rein öffentlichen Finanzierung voll einschlägig.\\n\\nWird das LEAM-Projekt nach dem ersten in Betracht kommenden Modell vollständig öffentlich finanziert, dann handelt es sich bei dem für das LEAM-Projekt zu schaffenden Rechtsträger um einen öffentlichen Auftraggeber im Sinne von §§ 98, 99 GWB. In diesem Fall sind die in Kapitel 11.4 aufgeführten Voraussetzungen von § 99 Nr. 2 lit. a) GWB erfüllt, denn es handelt sich in diesem Fall um eine juristische Person, die zu dem besonderen Zweck gegründet wurden, im Allgemeininteresse liegende Aufgaben nichtgewerblicher Art zu erfüllen und die mindestens überwiegend – hier: vollständig – von Gebietskörperschaften finanziert wird.\\n\\nDabei ist das Merkmal der Aufgaben nichtgewerblicher Art selbst dann erfüllt, wenn neben der Kernaufgabe der KI-Forschung und -Entwicklung im Rahmen des LEAM- Projekts auch Consulting oder die Vermietung von Rechenzeit angeboten werden, da es für die Frage der Nichtgewerblichkeit auf eine Gesamtschau ankommt, die Faktoren wie\\n\\nGroße KI-Modelle für Deutschland\\n\\n208\\n\\neine Entkoppelung von Wettbewerb und Marktmechanismen und das Vorliegen einer treten Nebenbetätigungen wie Gewinnerzielungsabsicht berücksichtigt. Consulting und Rechenzeitvermietung hinter der weit überwiegenden, nicht auf Gewinnerzielung ausgerichteten Haupttätigkeit zurück.\\n\\nInsoweit\\n\\nÖffentliches Dienst- und Vergütungsrecht Beim Modell der rein öffentlichen Finanzierung gelten Art. 33 Abs. 2 GG und die Einschränkungen des öffentlichen Vergütungsrechts uneingeschränkt.\\n\\nDie Geltung des verfassungsrechtlichen Grundsatzes der Bestenauslese führt dazu, dass eine rechtssichere Gestaltung von Bewerbungsverfahren einen gewissen Mehraufwand bedeutet. So muss beispielsweise für zu besetzende Stellen ein Anforderungsprofil festgelegt und Fragen im Bewerbungsgespräch hieran ausgerichtet werden und es gelten Dokumentationspflichten. Stellenausschreibungen müssen zudem fristgebunden sein.\\n\\nJe nach Struktur des Projekts kommen zudem TVöD bzw. TVL zur Anwendung mit der Folge, nur vergleichbar unattraktiv entlohnen zu können. Für Angestellte bei rein oder überwiegend staatlich finanzierten Unternehmen, auch wenn diese eine privatrechtliche ist einzuhalten bei einer Form haben, greift das Besserstellungsverbot. Dieses institutionellen Förderung (Zuwendungen zur Deckung der gesamten oder eines nicht abgegrenzten Teils der Ausgaben des Zuwendungsempfängers) sowie bei Projektförderung, wenn die Gesamtausgaben des Zuwendungsempfängers überwiegend aus Zuwendungen der öffentlichen Hand bestritten werden (vgl. § 8 Haushaltsgesetz 2021). Der Bundesfinanzminister kann bei Vorliegen zwingender Gründe zwar Ausnahmen erlassen, dies wird jedoch restriktiv gehandhabt.\\n\\nFür das EU-Beihilferecht und das öffentliche Dienst- und Vergütungsrecht kann jedoch noch ein gemeinsamer Hinweis gegeben werden: Selbst bei einer rein oder überwiegend ist es möglich, dass neben der öffentlichen Finanzierung des LEAM-Projekts Trägerstruktur des LEAM-Projekts auch eine Service-GmbH errichtet wird, die sich auf wirtschaftliche Aktivitäten fokussiert und zu deren Erbringung Infrastruktur des LKS anmietet (unter Berücksichtigung der oben dargestellten Kapazitätsbeschränkungen). Die Ausgliederung von wirtschaftlichen Aktivitäten im Sinne des EU-Beihilferechts in eine wirtschaftliche Einheit ist beihilferechtlich bei Einhaltung der Regeln zum Verbot der Quersubventionierung und der Kapazitätsbeschränkungen für die wirtschaftliche Nutzung geförderter Infrastrukturen grundsätzlich zulässig.\\n\\nGroße KI-Modelle für Deutschland\\n\\n209\\n\\nSie ist in der deutschen Wissenschaftslandschaft erprobt und bietet verschiedene Vorteile:\\n\\nDie Trennungsrechnung, die die geförderte Einrichtung führen muss, wird deutlich vereinfacht, wenn alle wirtschaftlichen Tätigkeiten in eine dafür eingerichtete GmbH ausgegliedert sind.\\n\\nDie rechtlich von der geförderten Struktur getrennte wirtschaftliche Einheit (also beispielsweise eine gesonderte Service-GmbH) genießt größere Handlungsspielräume in Fragen der Vergütung und Preisbestimmung. Insbesondere kann sie Beschäftigte der geförderten nichtwirtschaftlichen Einheit beispielsweise in Nebentätigkeit beschäftigen und vergüten, um auf diese Weise im Rahmen des rechtlich Zulässigen ein interessantes Vergütungspaket zu bieten. Da zudem die Beschäftigten dann mit der Nebentätigkeit noch eigene Anstrengungen unternehmen müssen, um diese Zusatzvergütung zu erreichen, ist in der Regel auch in der Außendarstellung und in der Politik eine Akzeptanz solcher Lösungen erreichbar. Diese Vorgehensweise ist insbesondere in der Wissenschaft erprobt. Rechtliche Parameter müssen im Einzelnen ausgearbeitet werden.\\n\\n\\n\\nIn der Umsetzung würden Infrastruktur/Kapazitäten der wirtschaftlich orientierten Einheit/GmbH von der geförderten Einheit angemietet, was beihilferechtlich grundsätzlich zulässig ist und eine klare Trennung zwischen wirtschaftlichen und nichtwirtschaftlichen Bereichen ermöglicht.\\n\\nIm Rahmen der konkreten Ausgestaltung kann es vor diesem Hintergrund sinnvoll sein, die Trennung von nichtwirtschaftlichen und wirtschaftlichen Aktivitäten des Projekts LEAM in zwei rechtlich getrennten Strukturen zu untersuchen.\\n\\nGroße KI-Modelle für Deutschland\\n\\n210\\n\\nSPOTLIGHT Ubermetrics Technologies GmbH An verschiedenen Stellen in der Studie stellen wir ausgewählte deutsche Startups und Unternehmen vor, die KI-Foundation-Modelle bereits nutzen. Sie berichten über ihre Anwendungsfälle, Herausforderungen und die Vorteile europäischer Modelle.\\n\\nDie Ubermetrics Content Intelligence-Plattform basiert auf einer für die Bedürfnisse von Kommunikatoren speziell entwickelten und trainierten Künstlichen Intelligenz, welche öffentlich verfügbare Inhalte und Insights für effektive Daten Kommunikationsstrategien der umwandelt. Mit Verarbeitung von über 50.000 Artikeln pro Minute und ist Inhalte von mehr als 460 Millionen Quellen Ubermetrics die Intelligence Plattform für Marketing- und PR-Experten.\\n\\nin aussagekräftige\\n\\nführende Content\\n\\nPatrick Bunk, Founder und CEO von Ubermetrics.\\n\\nWo setzt ihr Foundation-Modelle ein? Was ist euer Use- Case?\\n\\nUbermetrics extrahiert Texte aus Millionen von Internetquellen und wertet diese unter dem Einsatz menschlicher Experten und automatisiert durch KI-Verfahren für Kunden aus. Wir setzen Foundation-Modelle bislang begrenzt in einigen Teilen der Ergebnisdarstellung ein. Beispielsweise setzen wir ein RoBERTa Modell ein, um automatisiert Entitäten (bspw. Personen, Orte, Produkte etc.) in Texten zu erkennen, um diese für die weitere Analyse nutzbar zu machen (sog. Named Entity Recognition / -Linking). Allerdings schöpfen wir damit die Möglichkeiten der Modelle für unsere Produkte erst ansatzweise aus.\\n\\nWelchen Einfluss haben KI-Foundation-Modelle auf euer Geschäftsmodell bzw. eure Projekte? Foundation-Modelle sind von überragender Bedeutung für die Wettbewerbsfähigkeit und den Erfolg von Ubermetrics, bzw. UNICEPTA. Bislang mussten wir aufwändig für jeden einzelnen Aspekt in unserer Produktionskette und teilweise für jede Sprache ein neues KI-Verfahren implementieren oder selbst erforschen (bspw. Sentimentanalyse). Diese könnten zu einem großen Teil durch auf Foundation-Modellen basierten Verfahren ersetzt werden- bei gleichzeitiger Verbesserung der Treffsicherheit. Das betrifft (bspw. sowohl die Textsammlung als auch die Aufbereitung Spracherkennung) und die Auswertung. Foundation-Modelle erlauben uns nun eine umfassende Basis, auf der wir die Entwicklung besserer Verfahren und die Entwicklung neuer Produkte aufsetzen können. Für uns sinkt also einerseits der Aufwand in der Produktion und wir werden andererseits in die Lage versetzt, bessere bzw. für uns gänzlich neue Produkte und Features anbieten zu können. Bspw. könnten wir Summaries über mehrere Artikel hinweg und Abstracts von Artikeln teilautomatisieren, um unsere Wissensarbeiter:innen bei weniger anspruchsvollen Tätigkeiten maschinell zu unterstützen. Wir schätzen den wirtschaftlichen Wert durch die Implementation auf 60 % unseres Umsatzes ein.\\n\\nGroße KI-Modelle für Deutschland\\n\\n211\\n\\nWelche Schwierigkeiten und Probleme siehst du, dass nur USA und China derzeit KI- Foundation-Modelle umfassend bereitstellen? Die größten Schwierigkeiten und Probleme liegen in dem Verlust digitaler Souveränität und in der schwächeren Unterstützung relevanter europäischer Sprachen. Sollten wir gezwungen sein, außereuropäische Modelle über APIs zu nutzen, müssten wir dazu relevante und zum Teil sensible Geschäftsdaten preisgeben. Dies ist nur schwer vereinbar mit europäischen Datenschutzstandards und ein Hauptgrund für unseren bisher nur sporadischen Einsatz solcher Modelle.\\n\\nWie würden euch europäische Modelle - Open Source, alle europäischen Sprachen abdeckend, mit hohen Datenschutzstandards und minimalem Bias - helfen? Von europäischen Open-Source-Modellen erhoffen wir uns neben der besseren Sprachabdeckung u.a. durch die europäischen Datenschutzstandards eine größere Akzeptanz unserer Kunden. Gleichzeitig erwarten wir eine bessere Planbarkeit in Bezug auf Weiterentwicklungen.\\n\\nDas Modell der privaten Finanzierung Im Gegensatz zur rein öffentlichen Kapitalbeschaffung steht grundsätzlich das Modell der privaten Finanzierung bzw. der überwiegenden privaten Beherrschung der gewählten Struktur.\\n\\nAngenommen, es werden keine staatlichen Mittel in Anspruch genommen, sondern das Modell wird rein privat finanziert und betrieben, so gelten die oben genannten Beschränkungen nicht. Das LEAM-Projekt muss zwar in diesem Fall letztlich „sich selbst tragen\" bzw. (wenn nicht die gemeinnützige Form gewählt wird) und zum wirtschaftlichen Überleben sogar Gewinne erwirtschaften, doch greifen in diesem Fall weder beihilferechtliche noch vergaberechtlich noch tarifliche Beschränkungen. Das LEAM- Projekt kann (und muss) dann „frei am Markt\" agieren. Aufträge können schnell vergeben werden, am Markt übliche Gehälter bezahlt werden. Das gilt selbst dann, wenn aus steuerrechtlichen Gründen die Wahl auf eine gemeinnützige Gesellschaft fällt (mit bestimmten Einschränkungen des Gemeinnützigkeitsrechts, die hier nicht abschließend dargestellt werden können).\\n\\nEU-Beihilfenrecht Einer solchen privat strukturierten bzw. überwiegend privat finanzierten Einrichtung ist es dann wiederum möglich, sich allgemein auf Fördermittel für wissenschaftliche Infrastruktur und Projekte, im Wege der Projektförderung zum Beispiel bei der DFG, bei dem Bundesministerium für Forschung und Bildung oder bei der EU zu bewerben – so wie jeder anderen öffentlichen oder privaten Einrichtung auch. Diese Projektförderung wird jedoch nicht nur kompetitiv für begrenzte Laufzeiten vergeben (das heißt die Mittel sind zeitlich beschränkt und ihr Erhalt aufgrund des Wettbewerbscharakters öffentlicher Ausschreibungen nicht gesichert), sondern ist dann auch gegebenenfalls wieder mit öffentlich-rechtlichen Pflichten verbunden. Das können beispielsweise wiederum Beschränkungen in der Vergütung sein (gegebenenfalls gilt das Besserstellungsverbot, s. oben) oder es sind bei der Vergabe der Mittel an Unternehmen durch die geförderte Infrastruktur (also hier dann das LEAM-Projekt, das beispielsweise Rechenkapazitäten einkaufen will) öffentliche Ausschreibungen durchzuführen. Das hängt von dem\\n\\nGroße KI-Modelle für Deutschland\\n\\n212\\n\\njeweiligen Fördertopf, der in Anspruch genommen werden soll, und der Förderintensität ab.\\n\\nEs ist jedoch nicht ausgeschlossen, dass auch der Bund oder ein Land dann wiederum Finanzierungshilfe leisten, wenn die zugrundeliegende Einrichtung ihrem Wesen nach privat finanziert und in der Gesellschafterstruktur auch betrieben werden soll. Deutschland kennt zahlreiche Förderinstrumente für Start-Ups und Unternehmen mit Technologiebezug, die in ihrer Bandbreite hier jedoch nicht alle dargestellt werden können. Auch der bekannte High-Tech-Gründerfonds gehört dazu, ebenso wie Kreditprogramme der staatlichen Förderbank KfW auf Bundesebene oder der Förderbanken auf Landesebene. Unabhängig davon, dass sich hieraus auch wiederum beihilferechtliche Prüfungen ergeben können (dazu sogleich), können solche Mittel oftmals nur eine „Anschubfinanzierung\" von beschränktem Umfang sein, neben die erhebliche Privatmittel treten müssen (siehe auch die Ausführungen zur AGVO und zum Public-Private-Partnership, dazu sogleich).\\n\\nSolche Finanzierungen müssen sich dann, wenn sie einen erheblichen Umfang haben sollen, voraussichtlich als „Beihilfen\" für ein Unternehmen vor allem an den Regelungen der AGVO messen lassen, soll ein langwieriges Notifizierungsverfahren vermieden werden. Beihilfen, die auf Grundlage der AGVO gewährt werden, müssen bestimmte Transparenzvorgaben erfüllen und einen Anreizeffekt haben. Außerdem müssen Informationen zu den Beihilfen auf einer „Beihilfe-Website“ des Mitgliedstaates bzw. seiner handelnden Körperschaft veröffentlicht und der Kommission mitgeteilt werden. Die Mitgliedstaaten sind zum Monitoring verpflichtet. Für die Förderung des LKS könnten insbesondere folgende Kategorien der AGVO relevant sein:\\n\\nArt. 17 AGVO: Investitionsbeihilfen für KMU Bis zu 20 % der Investitionskosten kleiner bzw. 10 % der Investitionskosten mittlerer Unternehmen für immaterielle und materielle Investitionsgüter. Darunter fallen u. a. die Errichtung einer neuen Produktionsstätte sowie die Deckung von Lohnkosten der durch das Investitionsvorhaben geschaffenen Arbeitsplätze.\\n\\nArt. 18 AGVO: Investitionsbeihilfen für KMU bei der Inanspruchnahme von Beratungsdiensten Mit einer Höhe von bis zu 50 % der Kosten dürfen Beratungsleistungen externer Berater bezuschusst werden, insofern es sich dabei nicht um gewöhnliche Werbungskosten und fortlaufende Dienstleistungen handelt.\\n\\nArt. 26 AGVO: Investitionsbeihilfen für Forschungsinfrastrukturen Bau und Ausbau von Forschungsinfrastrukturen können mit bis zu 50 % der Kosten als bevorzugte Beihilfe nach den Regeln der AGVO gefördert werden. Der Preis für Betrieb und Nutzung der so geförderten Infrastruktur muss dem Marktpreis entsprechen. Die Infrastruktur muss außerdem mehreren Nutzer:innen offenstehen und der Zugang muss transparenten und diskriminierungsfreien Bedingungen gewährt werden. zu Unternehmen, die mindestens 10 % der Investitionskosten der Infrastruktur finanziert haben, können einen bevorzugten Zugang erhalten. Wenn eine Forschungsinfrastruktur ist eine sowohl wirtschaftliche als auch nichtwirtschaftliche Tätigkeiten ausübt, Trennungsrechnung zwingend.\\n\\nGroße KI-Modelle für Deutschland\\n\\n213\\n\\nArt. 28 AGVO: Innovationsbeihilfen für KMU Bis zu 50 % der Kosten, die kleinen und mittleren Unternehmen u. a. für die Abordnung hochqualifizierten Personals für Innovationsberatungsdienste innovationsunterstützende Dienstleistungen entstehen, dürfen als AGVO-Beihilfen finanziert werden. Bis zu einem Betrag von 200.000 EUR in drei Jahren dürfen auch bis zu 100 % der Kosten für Innovationsberatungsdienste und innovationsunterstützende Dienstleistungen finanziert werden.\\n\\nfür Forschungs- und Wissensverbreitung oder oder\\n\\nVergaberecht Bei einer rein privaten Finanzierung ist das LEAM-Projekt kein öffentlicher Auftraggeber, sodass bei seiner Beschaffung von Leistungen das Vergaberecht nicht zur Anwendung kommt. Das führt zu deutlich freieren Beschaffungsvorgängen.\\n\\nGegebenenfalls kann es im Rahmen einer von einer privaten Einheit beantragten Projektfinanzierung dazu kommen, dass die Fördermittelbedingungen Vorschriften zur öffentlichen Vergabe anzuschaffender Infrastruktur enthalten. Das kann, muss aber nicht der Fall sein.\\n\\nÖffentliches Dienstrecht Das öffentliche Dienstrecht kommt in diesem Fall in Bezug auf die Personalgewinnung nicht zur Anwendung. Da das Besserstellungsverbot nur bei überwiegend öffentlicher Finanzierung greift, entfällt dieses bei einer (überwiegend) privaten Finanzierung. In Bezug auf die Arbeitnehmer:innengewinnung ist eine rein private Finanzierung insgesamt deutlich freier und eine höhere Vergütung für Arbeitnehmer:innen attraktiver.\\n\\nDas Modell der Public-Private-Partnership Die Mischform zu den vorhergehenden Modellen stellt das Konzept der Public-Private- Partnership, also der (gesellschafts-)rechtlich statuierten Zusammenarbeit zwischen Privaten und dem Staat dar. In der Praxis sind hier verschiedenste Ausprägungen denkbar.\\n\\nfinanziellen und/oder\\n\\nBevor die Anwendbarkeit und Bedeutung von Beihilfen- Vergabe- und öffentlichem Dienstrecht auf das Modell der Public-Private-Partnership dargestellt werden kann, ist zunächst zu erklären, was eine solche ist, welche Formen möglich sind und inwieweit der Staat sich an einer solchen beteiligen darf.\\n\\nBegriffserklärung Eine sog. Public-Private-Partnership (nachfolgend auch „PPP\"), zu Deutsch Öffentlich- private-Partnerschaft, kurz ÖPP, stellt eine Mischform zwischen einer rein öffentlichen Finanzierung und Struktur und einer ausschließlich privaten Ausgestaltung dar.\\n\\nEs handelt sich bei einer PPP um eine partnerschaftliche Zusammenarbeit zwischen der öffentlichen Hand und privaten Unternehmen zur Realisierung eines öffentlichen Projekts oder Erbringung von Leistungen, die der Erfüllung einer öffentlich-rechtlichen Aufgabe dienen.\\n\\nFür die Betrachtung der rechtlichen Möglichkeiten, Anforderungen und einschlägigen Vorschriften kommt es auf die konkrete Ausgestaltung der jeweiligen Kooperation an. Für\\n\\nGroße KI-Modelle für Deutschland\\n\\n214\\n\\nPPP existieren nur lückenhafte Regelungen in verschiedenen Gesetzen, aber keine umfassenden gesetzlichen Vorgaben.\\n\\nAusgestaltungsmodelle Für die konkrete Ausgestaltung von PPP kommen zahlreiche verschiedene Organisationsmodelle in Frage. Wiederum sind diese nicht gesetzlich definiert, sondern haben sich in der Praxis herausgebildet. Dabei erfolgt die Kategorisierung von Organisationsmodellen häufig uneinheitlich.\\n\\nAnzahl, Namen und genaue Beschreibung der Organisationsformen unterscheiden sich je nach Betrachtung. Im Groben kann aber folgende Kategorisierung erfolgen:\\n\\nKonzessionsmodell Beim Konzessionsmodell überträgt die öffentliche Hand einem privaten Unternehmen eine öffentliche Aufgabe. Der Private betreibt die betroffene Einrichtung dabei in eigenem Namen, auf eigene Rechnung und auf eigenes wirtschaftliches Risiko. Nur er tritt hierbei gegenüber Dritten auf. Er generiert Einnahmen dadurch, dass er von Dritten für die Nutzung einer Sache bzw. die Inanspruchnahme einer Leistung ein Entgelt verlangt. Im Falle von LEAM wäre insoweit die Vermietung von Rechenzeit sowie Consulting möglich. Zum Teil wird bei Anwendung des Konzessionsmodells zudem vereinbart, dass die öffentliche Hand als Konzessionär an den Privaten eine Konzessionsabgabe entrichtet. Bei diesem Modell ist allerdings anzumerken, dass für eine solche Konzessionsvergabe eine europaweite Ausschreibung nötig ist.\\n\\nDie Problematik besteht bei einem Konzessionsmodell jedoch darin, dass es sich bei dem Betrieb des KI-Hochleistungsrechenzentrums denklogisch um eine (zwingende) öffentliche Aufgabe handeln muss, die dem Staat obliegt (im Gegensatz zu einer staatlichen Förderung von wünschenswerten Aktivitäten, zum Beispiel in der Wissenschaft). Das erscheint derzeit nur schwer darstellbar, bedarf jedoch auch einer Erörterung mit politisch Verantwortlichen, ob sie gegebenenfalls einen solchen Tatbestand durch Verordnung oder Gesetz schaffen wollen. Bis dahin sind Zweifel an der Anwendbarkeit des Konzessionsmodells gegeben.\\n\\nLeasingmodell Beim Leasingmodell beauftragt die öffentliche Hand eine Privatperson oder eine Leasinggesellschaft mit der Planung, Errichtung und Finanzierung eines Vorhabens gegen Zahlung einer vereinbarten Leasingrate. Die Privatperson wird Eigentümer:in bzw. Inhaber:in des Objekts, aber gewährt der öffentlichen Hand das Nutzungsrecht während der vereinbarten Leasingdauer. Hierzu erfolgt gegebenenfalls ein Widmungsakt, mit dem das Objekt zu einer öffentlichen Sache wird.\\n\\nBetreibermodell Beim Betreibermodell verwirklicht eine private Organisation vertraglich festgelegte Teilaufgaben in Bezug auf Planung, Entwicklung, Betrieb und Finanzierung und erhält hierfür ein vereinbartes Entgelt. Dabei ist sowohl möglich, dass der Private eher als technischer Erfüllungsgehilfe nur im Innenverhältnis zum Verwaltungsträger auftritt, als auch, dass er, näher am Konzessionsmodell, auch gegenüber Dritten agiert und von diesen ein Entgelt erhebt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n215\\n\\nBetriebsführungsmodell Beim Betriebsführungsmodell betreibt eine private Organisation eine öffentliche Einrichtung namens und im Auftrag der öffentlichen Hand auf deren Rechnung und deren Risiko. Er tritt gegenüber Dritten also nur in fremden Namen auf.\\n\\nDie vorgenannten Modelle können unter dem Oberbegriff der „Vertragsmodelle\" zusammengefasst werden. Ihnen ist gemein, dass öffentliche Hand und Private hier auf vertraglicher Basis miteinander kooperieren, es jedoch nicht zur Schaffung eines gemeinsamen Rechtssubjektes kommt.\\n\\nBeteiligungs- oder Kooperationsmodell Anders liegt es bei dem sog. Beteiligungs- oder Kooperationsmodell. Bei diesem gründen die öffentliche Hand und private Akteure eine gemeinsame Gesellschaft. An dieser beteiligen sich sowohl öffentliche Stellen (z.B. Bund und/oder Bundesländer) als auch private Unternehmen als Gesellschafter:innen. Aufgrund dieser Schaffung einer neuen Gesellschaft als von ihren Gesellschaftern zu unterscheidende eigenständige Rechtspersönlichkeit wird dieses Modell auch als „institutionelle PPP\" bezeichnet, aufgrund der gemeinsamen Inhaberschaft von öffentlichen und privaten Rechtsträgern auch als „gemischtwirtschaftliches Unternehmen\". Es wird ein Gesellschaftsvertrag abgeschlossen. Neben der Rechtsform der Gesellschaft (s. Kapitel 12) festgelegt, wie groß der Anteil der Gesellschaftsanteile ist, den die jeweiligen Gesellschafter:innen tragen. Dabei kann der Anteil der öffentlichen Hand unterschiedlich groß sein. Sie kann, was in der Praxis häufig vorkommt, Mehrheitsanteilseignerin sein; möglich ist aber auch eine Beteiligung zu einem Anteil von weniger als 50 %.\\n\\nist\\n\\nin diesem zunächst\\n\\nDer Gesellschaftsvertrag enthält weitere Regelungen zur Geschäftsführung und Vertretung der Gesellschaft, zu Rechten und Pflichten der Gesellschafter:innen, zur Verteilung von Gewinn und Verlust, zur Besetzung von Gesellschaftsorganen. Von den gesellschaftsrechtlichen Einflussnahmemöglichkeiten verspricht man sich auch eine verbesserte Steuerungs- und Kontrollmöglichkeiten durch die öffentliche Hand Zu Vor- und Nachteilen verschiedener Gesellschaftsformen (s. Kapitel 12).\\n\\nNach dem Verständnis von LEAM als auf Dauer angelegtes Projekt erscheint die Gründung einer den möglichen Ausgestaltungsformen von PPPs. Daher fokussieren sich die folgenden Ausführungen auf die rechtlichen Auswirkungen einer solchen Ausgestaltung.\\n\\ngemeinsamen Gesellschaft\\n\\nals\\n\\npassendste\\n\\nunter\\n\\nVoraussetzungen für öffentlich-private Kooperationen Bei der Erfüllung seiner Aufgaben hat der Staat grundsätzlich einen weiten Gestaltungsspielraum. Das betrifft auch die Frage, ob er in privatrechtlicher Form tätig wird und inwieweit er private Akteure zur Wahrnehmung staatlicher Aufgaben heranziehen kann. Abgesehen von für LEAM nicht einschlägigen Ausnahmebereichen ist daher auch die Beteiligung an Public-Private-Partnerships möglich.\\n\\nAuch weitere haushaltsrechtliche Regeln für die Beteiligung der öffentlichen Hand an Unternehmen – etwa die Begrenzung der Einzahlungsverpflichtung auf einen bestimmten in den Betrag und die Sicherung eines angemessenen staatlichen Einflusses Überwachungsorganen der Gesellschaft können durch das LEAM-Projekt eingehalten werden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n216\\n\\nEU-Beihilfenrecht Im Hinblick auf das EU-Beihilfenrecht ergeben sich für Public-Private-Partnerships keine speziellen Rechtsfragen. Durch die Beteiligung des Staates und die Finanzierung des Projekts mit staatlichen Mitteln bleibt das Beihilferecht grundsätzlich anwendbar, wobei sich jedoch für die Finanzierung nichtwirtschaftlicher Aktivitäten die gleichen Maßstäbe ergeben wie bei der rein öffentlichen Finanzierung.\\n\\nFür die Unterscheidung zwischen wirtschaftlichen und nichtwirtschaftlichen Tätigkeiten – die entscheidend für die Frage des Vorliegens einer staatlichen Beihilfe ist – gelten, da es insoweit nicht auf die Rechtsform und die Art der beteiligten Gesellschafter:innen ankommt, sondern die Abgrenzung tätigkeitsbezogen erfolgt, die in Kapitel 11.4 ausgeführten Grundsätze. Auch insoweit ist also der FuE-Rahmen maßgeblich. Dessen Anforderungen, insb. das Gebot der Einführung einer Trennungsrechnung und das Verbot einer Quersubventionierung wirtschaftlicher Tätigkeiten sind auf PPP zu übertragen. Möglich ist allerdings, die Struktur hier bei einer Finanzierung Privater so auszugestalten, dass im stärkeren Umfang wirtschaftliche Aktivitäten möglich sind; dies insbesondere dann, wenn die privaten Investoren sich bereit erklären, etwaige Verluste der wirtschaftlichen Tätigkeiten selbst auszugleichen (so dass aus staatlichen Mitteln keine Quersubventionierung erfolgt). Einzelheiten müssen dann bei der konkreten Ausgestaltung evaluiert werden.\\n\\nSoweit es sich begrifflich um eine Beihilfe handelt, kann auch beim PPP- Finanzierungsmodell eine Ausnahmevorschrift greifen. Insoweit kommen insb. die oben aufgeführten AGVO-Beihilfen in Betracht. Liegt eine nicht unter Ausnahmevorschriften fallende staatliche Beihilfe vor, ist ein Notifizierungsverfahren durchzuführen.\\n\\nVergaberecht Für die Frage, inwieweit das Vergaberecht auf PPPs Anwendung findet, sind verschiedene Themenkomplexe voneinander abzugrenzen: Zu unterscheiden sind einerseits eine Zusammenarbeit, die einem der oben unter dargestellten Vertragsmodelle (Konzessions- , Leasing-, Betreiber- oder Betriebsführungsmodell) zuzuordnen ist, andererseits PPPs nach dem Beteiligungsmodell, bei dem eine gemeinsame Gesellschaft gegründet wird. Bei diesen wiederum ist danach zu differenzieren, ob es um die Anwendbarkeit von Vergaberecht auf die Gesellschaftsgründung oder um die Beschaffung von Leistungen durch die bestehende Gesellschaft geht.\\n\\nFür Kooperationen nach einem der Vertragsmodelle kann aufgrund der Vielfältigkeit möglicher Modelle im derzeitigen Stadium keine vollständige vergaberechtliche Bewertung erfolgen. Da ihnen aber gemein ist, dass der Staat hier alle oder bestimmte Teile der Projektführung auf Private überträgt, wird es sich bei dem Projektvertrag häufig um einen öffentlichen Auftrag im Sinne von § 103 Abs. 1 GWB oder eine Konzession nach § 105 Abs. 1 GWB handeln. Daher muss dem Abschluss eines Projektvertrags mit einem privaten Kooperationspartner in aller Regel ein formelles Vergabeverfahren vorausgehen, aufgrund des hohen Projektvolumens mit europaweiter Bekanntmachung.\\n\\nFür die Gründung einer Joint Venture (Beteiligungsmodell) stellt sich die Rechtslage folgendermaßen dar: Die Gründung der gemischtwirtschaftlichen Gesellschaft selbst stellt grundsätzlichen keinen beschaffungsrelevanten Vorgang dar, unterliegt also nicht\\n\\nGroße KI-Modelle für Deutschland\\n\\n217\\n\\ndem Vergaberecht. Anders liegt es jedoch, wenn mit dem Gründungsakt zugleich eine unmittelbare Übertragung von Aufgaben von der öffentlichen Hand an die Gesellschaft einhergeht. In diesem Fall wird das beteiligte private Unternehmen nicht nur Gesellschafter, sondern zugleich auch Leistungserbringer gegenüber dem Staat. Im Rahmen einer vorzunehmenden Gesamtbetrachtung stellt sich dieses Gesamtkonstrukt insgesamt als ausschreibungspflichtiger Vorgang in Form eines öffentlichen Auftrags im Sinne von § 103 Abs. 1 GWB bzw. als Konzession nach § 105 Abs. 1 GWB dar, da Gesellschaftsgründung und Aufgabenwahrnehmung ein unteilbares Ganzes sind. Dasselbe gilt, wenn ein gemischtwirtschaftliches Unternehmen durch die öffentliche Hand und private Unternehmen gegründet wird, um eigene Aufgaben in einem entsprechend definierten Bereich zu übernehmen und die Leistungen im Anschluss durch die Gesellschafter erbracht werden.\\n\\nWerden später – von der Gesellschaftsgründung zu unterscheiden – durch die Gesellschaft für sie erforderliche Aufträge in einem nachgelagerten Schritt an Dritte vergeben, so ist die Gesellschaft in der Regel selbst als öffentlicher Auftraggeber nach § 99 Nr. 2 lit. a) GWB einzuordnen. Beschafft sie sich Leistungen am Markt, so unterliegt dieser Beschaffungsakt in aller Regel den vergaberechtlichen Vorgaben. Die oben in Kapitel 11.4 dargestellten Voraussetzungen dieser Vorschrift liegen vor. Insbesondere folgende sind gegeben: Wie auch im Rahmen der Ausführungen zum Vergaberecht bei rein öffentlicher Finanzierung dargestellt ist bei den im LEAM-Projekt geplanten im Aktivitäten Allgemeininteresse liegenden Aufgaben nichtgewerblicher Art auszugehen. Insoweit ergeben sich keine Unterschiede. Die weitere Voraussetzung des § 99 Nr. 2 lit. a) GWB, dass die Gesellschaft überwiegend von Gebietskörperschaften – hier voraussichtlich Bund und Standortland – finanziert wird, liegt vor, wenn – wie zu erwarten – die Finanzierung zu mehr als 50 % staatlich ist.\\n\\nim Rahmen der vorzunehmenden Gesamtschau\\n\\ninsgesamt von\\n\\nÖffentliches Dienst- und Vergütungsrecht Auch bei von der öffentlichen Hand beherrschten Public-Private-Partnerships (also solchen mit staatlicher Beteiligung von mehr als 50 %) muss die Stellenbesetzung nach Art. 33 Abs. 2 GG erfolgen. Der bisherigen Rechtsprechung des Bundesarbeitsgerichts zu öffentlichen Betrieben in privater Rechtsform ist zu entnehmen, dass es auch für öffentlich beherrschte gemischtwirtschaftliche Unternehmen von einer Geltung der Vorschrift ausgehen wird.\\n\\nVergütungsrechtliche Einschränkungen gelten nur, wenn aufgrund der Beihilfehöhe das Besserstellungsverbot zur Anwendung kommt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n218\\n\\n11.6 Abschließende Übersicht\\n\\nDie nachfolgende Tabelle stellt überblicksmäßig die rechtlichen Vor- und Nachteile der einzelnen Modelle in Bezug auf die drei grundsätzlichen Finanzierungsarten dar:\\n\\nÖffentlich finanziert\\n\\nPrivat finanziert\\n\\nPublic-Private- Partnership\\n\\n– mögliche\\n\\nEU-Beihilferecht\\n\\n– voll anwendbar – Trennungsrechnung – Beschränkung\\n\\nwirtschaftlicher Tätigkeiten\\n\\n– FuE-Rahmen 2022\\n\\nmit vielen Privilegien\\n\\n– nicht anwendbar\\n\\noder nur eingeschränkt\\n\\n– Beihilfen nach AGVO\\n\\ndenkbar\\n\\nBeschränkung wirtschaftlicher Tätigkeit und Trennungsrech- nung; FuE-Rahmen\\n\\n– im Übrigen Beihilfen nach AGVO denkbar\\n\\n– anwendbar auf den\\n\\nVergaberecht\\n\\n– voll anwendbar – öffentliche\\n\\nAusschreibungen notwendig\\n\\n– grundsätzlich nicht\\n\\nanwendbar\\n\\nGründungsakt, wenn damit zugleich Aufgabenübertragu ng auf den privaten Partner und bei Konzessionierung\\n\\n– in aller Regel\\n\\nanwendbar auf Leistungsbeschaffu ng durch die gemischtwirtschaftl iche Gesellschaft\\n\\n– Mitarbeitergewinn-\\n\\nÖffentliches Dienst- und Vergütungs- recht\\n\\n– voll anwendbar – Besserstellungs- verbot greift ein\\n\\n– in der\\n\\nMitarbeiterauswahl und Vergütung frei (ggf. steuerliche Einschränkungen bei Gemeinnützigkeit) – Marktvergütun-gen\\n\\nmöglich\\n\\nung unterliegt Vorgaben der Bestenauslese, wenn staatliche Beteiligung über 50 %\\n\\n– Besserstellungs- verbot je nach Beihilfenhöhe möglich\\n\\nTabelle 21: Übersicht der Vor- und Nachteile der drei Finanzierungsmodelle mit Hinblick auf die rechtlichen Rahmenbedingungen\\n\\nGroße KI-Modelle für Deutschland\\n\\n219\\n\\nGesellschaftsstruktur von LEAM\\n\\nGroße KI-Modelle für Deutschland\\n\\n220\\n\\n12. Gesellschaftsstruktur von LEAM\\n\\nDie Gesellschaftsstruktur von LEAM ist stark abhängig von der Finanzierungsart des Projektes. Hier bieten verschiedene Gesellschaftsformen an. Im Rahmen der Finanzierung sollten diese genauer evaluiert und bewertet werden. Generell ist aufgrund der Struktur des Projektes die Gründung einer Projektentwicklungsgesellschaft denkbar.\\n\\nsich\\n\\nje nach\\n\\nFinanzierungsstruktur\\n\\nGrundsätzlich können verschiedene Gesellschaftsformen unterschieden werden. Diese sollen anhand der Finanzierungsform kurz beleuchtet werden.\\n\\n12.1 Öffentliche Finanzierung\\n\\nSofern eine rein öffentliche Finanzierung gewählt wird, kommen neben den Rechtsformen des öffentlichen Rechts (vor allem der Anstalt des öffentlichen Rechts) grundsätzlich mehrere privatrechtliche Formen in Betracht. Auch in der deutschen Förder- und Wissenschaftspraxis gibt es Einrichtungen, die als e.V. organisiert sind (zum Beispiel Max-Planck-Gesellschaft oder Fraunhofer) oder auch als GmbHs. Insbesondere die GmbH – gegebenenfalls auch in Form der gemeinnützigen GmbH mit dem besonderen steuerrechtlichen Einschlag – kann in der Praxis heute eine Rolle für die Aufgabenerfüllung der öffentlichen Hand spielen. Dem Bund oder einem Land ist hierbei eine Gesellschaftsbeteiligung nach dem deutschen Haushaltsrecht im Umfang der Investition möglich (wenn also beispielsweise der Bund 90 % der Investition trägt, würde er auch 90 % der Geschäftsanteile an der GmbH enthalten). Dies folgt auch aus der Logik, dass das EU-Beihilferecht und das Vergaberecht sich mit ihren Möglichkeiten und Beschränkungen nicht an der Rechtsform, sondern vielmehr an der tatsächlich ausgeübten Tätigkeit orientieren. Das GmbH-Recht ist zudem hinreichend flexibel, um auch besondere Organstrukturen abzubilden und gleichzeitig eine gewisse Flexibilität der Handelnden, insbesondere der Geschäftsführung, zu erlauben. Bei einer GmbH- Konstruktion kann es sich zudem anbieten, eine eher nicht-wirtschaftlich orientierte Träger-GmbH für die Infrastruktur (ggf. als gGmbH) mit einer wirtschaftlich orientierten GmbH zu verbinden.\\n\\nDie Aktiengesellschaft und die Genossenschaft erscheinen dagegen in diesem Modell nicht sinnvoll, da sie haushaltsrechtlich für den Bund und ein beteiligtes Bundesland nicht darstellbar erscheinen und zudem auch in der rechtlichen Flexibilität Probleme auslösen können.\\n\\nGroße KI-Modelle für Deutschland\\n\\n221\\n\\n12.2 Private Finanzierung\\n\\nIm Rahmen einer privaten Finanzierung sind grundsätzlich alle Gesellschaftsformen, die oben dargestellt sind, denkbar.\\n\\nDie Rechtsform der AG ist im Hinblick auf die Übertragbarkeit von Anteilen und die – auch kurzfristige – Kapitalbeschaffung flexibel. Durch strenge gesetzliche Vorgaben wird indes der unternehmerische Gestaltungsspielraum reduziert. Zudem ist eine AG eher für ein kapitalmarktorientiertes Unternehmen sinnvoll. Übertragbarkeit und kurzfristige Kapitalbeschaffung sind bei einer GmbH im Vergleich dazu erschwert. Im Gegensatz zum Vorstand einer AG ist die Geschäftsführung einer GmbH an Weisungen der Gesellschafter:innen gebunden. Formale Vorgaben sind weniger streng. Die demokratischen Strukturen und Flexibilität der Genossenschaft können je nach Sichtweise als Vor- oder Nachteil begriffen werden. Nachteil einer Stiftung ist der Wegfall des Zugriffs auf das eingeflossene Vermögen. Der Betrieb eines wirtschaftlichen Unternehmens mit Gewinnerzielungsabsicht als eingetragener Verein ist so nicht möglich.\\n\\n12.3 Public-Private-Partnership\\n\\nAuch hier sind grundsätzlich verschiedene Gesellschaftsformen denkbar.\\n\\nFür PPPs wird häufig die Rechtsform der GmbH gewählt. Für diese spricht, dass sie aufgrund ihrer Satzungsfreiheit und der Weisungsbindung der Geschäftsführung gute Voraussetzungen bietet, um die haushaltsrechtlichen Pflichten der öffentlichen Hand zur Einwirkung und Kontrolle gegenüber der Geschäftsführung hinreichend Rechnung zu tragen. In Form der GmbH schützt die öffentliche Hand zudem vor untragbaren Haftungsrisiken.\\n\\nim der Der gemischtwirtschaftlichen Unternehmen auch die wichtige Bedeutung zu, der Gefahr einer Anfechtung von am Gemeinwohl orientierten Gesellschafterbeschlüssen durch überstimmte Dritte wegen Treuepflichtverstoßes oder der Verfolgung von Sondervorteilen zu begegnen.\\n\\nsatzungsmäßigen\\n\\nFestlegung\\n\\nöffentlichen\\n\\nAufgabe\\n\\nkommt\\n\\nVorteil der GmbH gegenüber der AG ist im Rahmen von Public-Private-Partnerships, dass sie mehr Steuerungsmöglichkeiten der Gesellschafter:innen zulässt. Das GmbH-Recht ist insoweit gestaltungsoffener, weil es die Regelung der gesellschaftsvertraglichen Rechtsverhältnisse weitgehend den Gesellschafter:innen überlässt.\\n\\nEine GmbH ermöglicht es bei einer PPP zudem, die Geschäftsanteile zwischen Bund, Land und Privatwirtschaft angemessen aufzuteilen und gleichzeitig der Geschäftsführung der GmbH gewisse Freiheiten und Flexibilität bei der Umsetzung der Ziele des Projekts LEAM zu gewährleisten.\\n\\nNicht ausgeschlossen ist es andererseits, gemischtwirtschaftliche Unternehmen in öffentlicher Rechtsform zu bilden. Hierbei sind allerdings erhöhte Anforderungen geboten, weil für öffentliche Rechtsformen das verfassungsrechtliche Legitimationsgebot umfassend gilt.\\n\\nGroße KI-Modelle für Deutschland\\n\\n222\\n\\nSzenario für ein LEAM KI-Servicezentrum\\n\\nGroße KI-Modelle für Deutschland\\n\\n223\\n\\n13. Szenario für ein LEAM KI-Servicezentrum Auf Basis der vorangegangenen Ausführungen lassen sich verschiedene Szenarien für die Gestaltung und Implementierung des LKS ableiten. Im Folgenden wird eine mögliche Konzeption entwickelt und erläutert. Hierbei ist zu berücksichtigen, dass es in allen Dimensionen noch Gestaltungsoptionen und Alternativen geben kann. Diese müssen dann im weiteren Verlauf der Konzeption und Verhandlungen detaillierter spezifiziert und zur Entscheidung gebracht werden.\\n\\nDie Governance-Struktur des LKS ist in Abbildung 27 schematisch dargestellt und besteht aus folgenden Komponenten:\\n\\nLEAM-Institut Das LEAM-Institut übernimmt folgende Aufgaben:\\n\\nGesamtkoordination der LEAM-Aktivitäten und des LKS\\n\\nSteuerung der Entwicklung von KI-Foundation-Modellen\\n\\nBereitstellung von Foundation-Modellen als Open Source\\n\\nEntwicklung und Umsetzung des Dienstes “Training-as-a-Service”\\n\\nDie Finanzierung erfolgt über öffentliche Mittel unter Berücksichtigung der Beihilfe- Konformität mit Gehaltsstrukturen des öffentlichen Dienstes (TVöD). Die Anmietung von Infrastrukturen (Housing, Office, evtl. Rechenkapazitäten) bzw. die Anschaffung eines KI- Supercomputer muss dafür öffentlich ausgeschrieben werden. Um dem EU-Beihilferecht gerecht zu werden, erfolgt die Nutzung der Kapazitäten des Supercomputers zu 80% für nicht-wirtschaftliche Zwecke - das heißt Forschung und Entwicklung durch Wirtschaft und Wissenschaft im Open-Source-Verfahren - und zu 20% für wirtschaftliche Zwecke.\\n\\nLEAM-Servicegesellschaft Die LEAM-Servicegesellschaft übernimmt folgende Aufgaben:\\n\\nAngebot eines Services für Model-Tuning mit Fokus auf Kunden aus der Wirtschaft\\n\\nBereitstellung von Kapazitäten für den Betrieb von KI-Anwendungen (Inference)\\n\\nBereitstellung von GPU Rechenkapazitäten\\n\\nBeratungsdienstleistungen (Consulting)\\n\\nDiese Services werden in der Regel kommerziell angeboten.\\n\\nMögliche Finanzierungsquellen für die Servicegesellschaft sind:\\n\\nFinanzinvestoren\\n\\nUnternehmen bzw. Joint Ventures\\n\\nEine Public-Private Partnership (PPP) mit Beteiligung des Bundes / eines Landes\\n\\nGroße KI-Modelle für Deutschland\\n\\n224\\n\\nDie Möglichkeit (z.B. Anschubfinanzierung) ist im weiteren Verlauf zu prüfen und die Rechtsform sowie die kommerzielle Ausrichtung (gewinnorientiert / gemeinnützig) zu definieren.\\n\\nfür eine weitere Unterstützung durch öffentliche Mittel\\n\\nDa die Gesellschaft nicht dem öffentlichen Dienst- und Vergütungsrecht unterliegt, kann sie marktübliche Gehälter bezahlen. Dadurch steigt die Attraktivität für hochqualifizierte Talente.\\n\\nInfrastruktur Die Bereitstellung der notwendigen Supercomputing-Infrastruktur folgenden zwei alternativen Ansätzen gestalten (s. Kapitel 9.2):\\n\\nlässt sich mit\\n\\nAnschaffung eines KI-Supercomputers\\n\\nEinkauf von GPU RZ-Leistungen bei einem externen Provider, der einen KI- Supercomputer bereitstellt\\n\\nHousing Die notwendige Housing Infrastruktur wird von einem externen Dienstleister angemietet (s. Kapitel 9.2).\\n\\nFörderprojekt Ein erstes Foundation-Modell wird im Rahmen eines öffentlich geförderten Ankerprojekts erstellt, an dem Wirtschaft und Wissenschaft gemeinsam arbeiten. Die Gestaltung dieses Projektes kann sich an dem bestehenden Projekt OpenGPT-X orientieren und die Ergebnisse von OpenGPT-X übernehmen und weiterentwickeln.\\n\\nIm Rahmen dieses Ankerprojekts werden folgende Zielsetzungen erreicht:\\n\\nAufbau und Pflege eines Daten-Korpus für Sprachmodelle\\n\\n\\n\\nImplementierung und Test der notwendigen Training-as-a-Service Prozesse\\n\\nBereitstellung des entwickelten Foundation-Modells als Open Source für die Wirtschaft\\n\\nKI-Compute-Voucher Start-ups sollen im Rahmen der LEAM-Initative die Möglichkeit erhalten, sich an der Entwicklung von Foundation-Modellen zu beteiligen sowie - vor allem - bereitgestellte Foundation-Modelle zu tunen und auf dieser Basis eigene KI-Anwendungen und Geschäftsmodelle zu entwickeln.\\n\\nUm diese Zielsetzung zu unterstützen, stellt die öffentliche Hand sog. KI-Compute- Voucher bereit. Hiermit können (auch kleinere) Entwicklungsprojekte für Start-ups gefordert werden. Die genaue Gestaltung und der Umfang dieses Programms sind im weiteren Verlauf zu definieren.\\n\\nGroße KI-Modelle für Deutschland\\n\\n225\\n\\nGroße KI-Modelle für Deutschland\\n\\ns m u r t n e z e c i v r e S - I K - M A E L\\n\\ns e d r u t k u r t S e d r ü f o i r a n e z S\\n\\ni\\n\\n:\\n\\n7 2\\n\\n.\\n\\nb b A\\n\\n226\\n\\nFazit LEAM Machbarkeitsstudie\\n\\nGroße KI-Modelle für Deutschland\\n\\n227\\n\\n14. Fazit Die Umfragen und Interviews mit Expert:innen aus Wirtschaft und Wissenschaft zeigen ein eindeutiges Bild: KI-Foundation-Modelle stellen die nächste Entwicklung in der Erfolgsgeschichte der Künstlichen Intelligenz dar. Dabei sind die aktuell populären Sprachmodelle nur der erste Schritt. In den nächsten Jahren werden noch weitaus performantere und auf noch vielfältigere Daten trainierte Modelle den Markt weiter revolutionieren.\\n\\nDie deutsche Wirtschaft hat diesen Trend erkannt und evaluiert bereits Möglichkeiten, die KI-Foundation-Modellen effektiv in internen Prozessen und als Produkte zu nutzen. Dabei ist sie aktuell aber weitgehend von proprietären, amerikanischen Foundation-Modellen abhängig. Dies stellt die Unternehmen vor große Herausforderungen in den Bereichen Datenschutz, Qualität und Zugriff auf die Modelle. Ein entscheidender Wettbewerbsnachteil gegenüber der ausländischen Konkurrenz droht. Europäischen Standards entsprechende, mit hochwertigen und vielfältigen Daten trainierte und Open Source verfügbare Foundation-Modelle würden diese Herausforderungen bewältigen und dazu beitragen, dass die deutsche Wirtschaft umfänglich von KI-Foundation- Modellen profitiert.\\n\\nDabei wurden Kernherausforderungen identifiziert:\\n\\nsowohl\\n\\nvon Wirtschaft als auch der Wissenschaft drei\\n\\n(1) Es bedarf einer Vielzahl an Expert:innen für das Thema KI-Foundation-Modelle.\\n\\n(2) Diese Expert:innen benötigen Zugriff auf qualitative hochwertige Daten\\n\\nverschiedener Arten sowie\\n\\n(3) Zugriff auf eine hinreichend mächtige Infrastruktur, die für die aktuelle KI-\\n\\nTechnologie und die Prozesse der KI-Entwicklung optimiert ist.\\n\\nEs gilt, diese Herausforderungen in einer gemeinsamen Anstrengung der Wirtschaft, der Wissenschaft und des Staates zu lösen.\\n\\n14.1 Beurteilung der Machbarkeit\\n\\nDiese Studie hat die Notwendigkeit, die Chancen und den Bedarf von KI-Foundation- Modellen in Deutschland untersucht. Dabei wurde vor allem die Machbarkeit für den Aufbau und den Betrieb eines dedizierten KI-Rechenzentrums beleuchtet. Dafür wurden in dieser Studie die Bereiche Software, Hardware, bauliche Infrastruktur, Personal, Organisationsstruktur sowie Finanzierung betrachtet. Die Machbarkeit in diesen Bereichen soll hier noch einmal abschließend beurteilt werden.\\n\\nSoftware Die für das Training und Entwicklung der KI-Foundation-Modelle benötigten Softwareframeworks und -tools stellen keine zentrale Herausforderung dar. Die notwendigen Technologien sind bereits vorhanden und überwiegend als Open-Source- Software verfügbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n228\\n\\nHardware Es gibt aktuell in Europa kein dediziertes KI-Rechenzentrum, das für die Entwicklung international kompetitiver Foundation-Modelle ausreicht. Um zum aktuellen Stand der amerikanischen Hyperscaler aufzuschließen, müssen rund 4.500 GPU im Rechenzentrum verbaut werden. Hersteller und Collocation-Anbieter haben signalisiert, dass der Aufbau eines solchen Clusters hardwareseitig zeitnah möglich ist.\\n\\nBauliche Infrastruktur Von dem Aufbau einer eigenen baulichen Infrastruktur sollte aufgrund der Kosten und der zu erwartenden Bauzeit abgesehen werden. Verschiedene Collocation-Anbieter haben aber bereits signalisiert, dass sie in der Lage wären, in ihrer Infrastruktur zu betreiben. Es sollte daher auf diese Möglichkeit zurückgegriffen werden.\\n\\nPersonal Für den Betrieb eines KI-Rechenzentrums wird ein Team aus hochspezialisierten Expert:innen benötigt. Der Aufbau des Teams für den Aufbau und den Betrieb der Services ist kurzfristig eine Herausforderung. Mittel- und langfristig eine große Chance, um talentierte Wissenschaftler zu halten.\\n\\nOrganisationsstruktur Für den Betrieb eines KI-Rechenzentrums schlagen die Autor:innen die Einrichtung einer eigenen Organisation, dem LEAM KI-Servicezentrum, vor. Dieses LKS wird den Aufbau der Infrastruktur begleiten und den identifizierten Zielgruppen spezialisierte Services bereitstellen. Der Fokus soll darauf liegen, der Wirtschaft KI-Foundation-Modelle für die Entwicklung von Anwendungen bereitzustellen.\\n\\nFinanzierung Die Autor:innen kalkulieren für den Aufbau und Betrieb eines KI-Rechenzentrums über vier Jahre einen Bedarf von rund 380 Millionen Euro. Für die Finanzierung wurde ein Modell entwickelt, das öffentliche und privatwirtschaftliche Mittel berücksichtigt.\\n\\nZusammenfassend lässt sich festhalten, dass der Aufbau von LEAM-Infrastruktur und -Services für die deutsche Wirtschaft ein entscheidender Wirtschaftsfaktor ist. Die Umsetzung ist mit der Beteiligung der öffentlichen Hand, der Forschung und Wissenschaft sowie der Wirtschaft realisierbar.\\n\\nGroße KI-Modelle für Deutschland\\n\\n229\\n\\n14.2 Ausblick\\n\\nIn engem Austausch mit der Politik, der Wirtschaft, potentiellen Standorten, Anbieter:innen und Anwender:innen sollte das Thema nun weiter vorangetrieben werden. Dabei gilt es vor allem, die Finanzierungsmöglichkeiten weiter zu präzisieren.\\n\\nAktuelle Entwicklungen wie der Hype um ChatGPT zeigen, wie dynamisch sich die Forschung und Anwendung von Foundation-Modellen entwickelt. Die Gefahr, dass sich in den USA monopolartige Strukturen - ähnlich Google bei Suchmaschinen - bilden, wird fortlaufend größer. Damit Deutschland nicht weiter zurückfällt, ist eine zeitnahe Umsetzung des Konzeptes für die LEAM-Infrastruktur nötig. Das Momentum, das im Rahmen der Erstellung dieser Machbarkeitsstudie gewonnen wurde - durch eine enge Zusammenarbeit zwischen Forschung, Wissenschaft, Wirtschaft und Start-ups - sollte genutzt werden, um die Zielsetzung weiter voranzutreiben.\\n\\nEine Möglichkeit ist die zeitnahe Gründung einer Projektentwicklungsgesellschaft, welche die Grundlagen für den Aufbau dieses strategischen KI-Leuchtturmprojekts erarbeitet.\\n\\nGroße KI-Modelle für Deutschland\\n\\n230\\n\\nVerzeichnisse & Methodik der LEAM Machbarkeitsstudie\\n\\nTitelseite\\n\\nGroße KI-Modelle für Deutschland\\n\\n231\\n\\nI. Quellenverzeichnis\\n\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,\\n\\nKudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Zheng, X. (2016). TensorFlow: A system for large-scale machine learning. Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, 265–283. https://dl.acm.org/doi/10.5555/3026877.3026899\\n\\nAbseil Python Common Libraries. (2022). [Python]. Abseil. https://github.com/abseil/abseil-py (Original work\\n\\npublished 2017)\\n\\nAgarwal, O., Ge, H., Shakeri, S., & Al-Rfou, R. (2021). Knowledge Graph Based Synthetic Corpus Generation for\\n\\nKnowledge-Enhanced Language Model Pre-training (arXiv:2010.12688). arXiv. https://doi.org/10.48550/arXiv.2010.12688\\n\\nAI accelerator. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=AI_accelerator&oldid=1123373022\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., … Simonyan, K. (2022). Flamingo: A Visual Language Model for Few-Shot Learning. In A. H. Oh, A. Agarwal, D. Belgrave, & K. Cho (Eds.), Advances in Neural Information Processing Systems. https://openreview.net/forum?id=EbMuimAbPbs\\n\\nAMD. (o.D.). AMD ROCmTM Open Ecosystem. Retrieved 28 November 2022, from\\n\\nhttps://www.amd.com/en/graphics/servers-solutions-rocm\\n\\nAMD. (2021, August 11). New AMD InstinctTM MI200 Series Accelerators Bring Leadership HPC and AI Performance\\n\\nto Power Exascale Systems and More. https://www.amd.com/en/press-releases/2021-11-08-new-amd- instinct-mi200-series-accelerators-bring-leadership-hpc-and-ai\\n\\nAn updated set of basic linear algebra subprograms (BLAS). (2002). ACM Transactions on Mathematical\\n\\nSoftware, 28(2), 135–151. https://doi.org/10.1145/567806.567807\\n\\nAn, W., Guo, Y., Bian, Y., Ma, H., Yang, J., Li, C., & Huang, J. (2022). MoDNA: Motif-oriented pre-training for\\n\\nDNA language model. Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, 1–5. https://doi.org/10.1145/3535508.3545512 Apache Flink. (2022). [Java]. The Apache Software Foundation. https://github.com/apache/flink (Original work\\n\\npublished 2014)\\n\\nApache Hadoop. (2022). [Java]. The Apache Software Foundation. https://github.com/apache/hadoop\\n\\n(Original work published 2014)\\n\\nApache Spark. (2022). [Scala]. The Apache Software Foundation. https://github.com/apache/spark (Original\\n\\nwork published 2014)\\n\\nArakelyan, G., Soghomonyan, G., & The Aim team. (2020). Aim (3.9.3) [TypeScript].\\n\\nhttps://doi.org/10.5281/zenodo.6536395\\n\\nASHRAE. (2022). Data Center Power Equipment Thermal Guidelines and Best Practices. ASHRAE Technical Committee (TC) 9.9 Mission Critical Facilities, Data Centers, Technology Spaces, and Electronic Equipment.\\n\\nBaek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G. R., Wang, J., Cong, Q., Kinch, L. N., Schaeffer, R. D., Millán, C., Park, H., Adams, C., Glassman, C. R., DeGiovanni, A., Pereira, J. H., Rodrigues, A. V., van Dijk, A. A., Ebrecht, A. C., … Baker, D. (2021). Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557), 871–876. https://doi.org/10.1126/science.abj8754\\n\\nBaevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning\\n\\nof speech representations. Proceedings of the 34th International Conference on Neural Information Processing Systems, 12449–12460.\\n\\nBannour, N., Ghannay, S., Névéol, A., & Ligozat, A.-L. (2021). Evaluating the carbon footprint of NLP methods: A\\n\\nsurvey and analysis of existing tools. 11–21. https://doi.org/10.18653/v1/2021.sustainlp-1.2\\n\\nBasic Linear Algebra Subprograms. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Basic_Linear_Algebra_Subprograms&oldid=1120747813\\n\\nBenaic, N., & Hogarth, I. (2022). State of AI 2022. https://www.stateof.ai\\n\\nGroße KI-Modelle für Deutschland\\n\\n232\\n\\nBitkom e.V. (2022a, January 3). IT-Fachkräftelücke wird größer: 96.000 offene Jobs | Bitkom e.V.\\n\\nhttps://www.bitkom.org/Presse/Presseinformation/IT-Fachkraefteluecke-wird-groesser\\n\\nBitkom e.V. (2022b). Bitkom Postionspapier: Perspektiven für eine nachhaltige Rechenzentren-Wirtschaft bis 2030.\\n\\nhttps://www.bitkom.org/sites/main/files/2022-10/2210-Positionspapier-Nachhaltige- Rechenzentren.pdf\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut,\\n\\nA., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., … Liang, P. (2021). On the Opportunities and Risks of Foundation Models (arXiv:2108.07258). arXiv. https://doi.org/10.48550/arXiv.2108.07258\\n\\nBoroditsky, L. (2012, March 15). Linguistik: Wie die Sprache das Denken formt. Spektrum.de.\\n\\nhttps://www.spektrum.de/news/linguistik-wie-die-sprache-das-denken-formt/1145804\\n\\nBranch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., Iglesias, D. del C., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples (arXiv:2209.02128). arXiv. https://doi.org/10.48550/arXiv.2209.02128\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\n\\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural Information Processing Systems, 33, 1877–1901. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a- Abstract.html\\n\\nBrundage, M., Mayer, K., Eloundou, T., Agarwal, S., Adler, S., Krueger, G., Leike, J., & Mishkin, P. (2022).\\n\\nLessons learned on language model safety and misuse. OpenAI. https://openai.com/blog/language- model-safety-and-misuse/\\n\\nBundesministerium für Digitales und Verkehr. (2022). Digitalstrategie Deutschland. https://digitalstrategie-\\n\\ndeutschland.de/medien/\\n\\nCampa, C., Kawalek, C., Vo, H., & Bessoudo, J. (2020, May 14). Defining AI Innovation with NVIDIA DGX A100. NVIDIA Technical Blog. https://developer.nvidia.com/blog/defining-ai-innovation-with-dgx-a100/\\n\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., &\\n\\nErlingsson, U. (2021). Extracting training data from large language models. 30th USENIX Security Symposium (USENIX Security 21), 2633–2650. https://www.usenix.org/system/files/sec21-carlini- extracting.pdf\\n\\nCarpintero, A. G. (2021, November 21). MLOps with Docker and Jenkins: Automating Machine Learning Pipelines.\\n\\nMedium. https://towardsdatascience.com/mlops-with-docker-and-jenkins-automating-machine- learning-pipelines-a3a4026c4487\\n\\nCen, S., & Shah, D. (2021). Regulating algorithmic filtering on social media. Advances in Neural Information\\n\\nProcessing Systems, 34, 6997–7011. https://proceedings.neurips.cc/paper/2021/hash/38b4f06e27fd4f6fdcceabc6f5c068ea-Abstract.html\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision Transformer: Reinforcement Learning via Sequence Modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems (Vol. 34, pp. 15084–15097). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\\n\\nBrockman, G., & others. (2021). Evaluating large language models trained on code. ArXiv Preprint ArXiv:2107.03374.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways (arXiv:2204.02311). arXiv. https://doi.org/10.48550/arXiv.2204.02311\\n\\nChowdhury, R., Bouatta, N., Biswas, S., Floristean, C., Kharkar, A., Roy, K., Rochereau, C., Ahdritz, G., Zhang, J.,\\n\\nChurch, G. M., Sorger, P. K., & AlQuraishi, M. (2022). Single-sequence protein structure prediction using a language model and deep learning. Nature Biotechnology, 40(11), Article 11. https://doi.org/10.1038/s41587-022-01432-w\\n\\nClimate Neutral Data Centre Pact – The Green Deal need Green Infrastructure. (o.D.). Retrieved 19 December\\n\\n2022, from https://www.climateneutraldatacentre.net/\\n\\nCodeCarbon. (2020). CodeCarbon—CodeCarbon 2.0.0 documentation. https://mlco2.github.io/codecarbon/\\n\\nGroße KI-Modelle für Deutschland\\n\\n233\\n\\nCremers, A. B., Englander, A., Gabriel, M., Hecker, D., Mock, M., Poretschkin, M., Julia Rosenzweig, J.,\\n\\nRostalski, F., Volmer, J., & Voosholz, J. (2019). Vertrauenswürdiger Einsatz von Künstlicher Intelligenz. Handlungsfelder aus philosophischer, ethischer, rechtlicher und technologischer Sicht als Grundlage für eine Zertifizierung von Künstlicher Intelligenz. Fraunhofer-Institut Für Intelligente Analyse-Und Informationssysteme (IAIS). https://www.iais.fraunhofer.de/content/dam/iais/KINRW/Whitepaper_KI- Zertifizierung.pdf\\n\\nCSTB Releases Report Fostering Responsible Computing Research: Foundations and Practices » CCC Blog. (2022, May 16). https://cccblog.org/2022/05/16/cstb-releases-report-fostering-computing-research- foundations-and-practices/\\n\\nDask. (2022). [Python]. dask. https://github.com/dask/dask (Original work published 2015) Data protection in the EU. (o.D.). [Text]. European Commission - European Commission. Retrieved 28\\n\\nNovember 2022, from https://ec.europa.eu/info/law/law-topic/data-protection/data-protection- eu_en\\n\\nDean, J., & Ghemawat, S. (2004). MapReduce: Simplified Data Processing on Large Clusters. OSDI’04: Sixth\\n\\nSymposium on Operating System Design and Implementation, 137–150.\\n\\nDeep Lake. (2022). [Python]. Activeloop. https://github.com/activeloopai/deeplake (Original work published\\n\\n2019)\\n\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers\\n\\nfor Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\\n\\nDodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., & Gardner, M. (2021). Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 1286–1305. https://doi.org/10.18653/v1/2021.emnlp-main.98\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\\n\\nM., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2022, March 23). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations. https://openreview.net/forum?id=YicbFdNTTy\\n\\nDürr, B. (2018). IT-Räume und Rechenzentren planen und betreiben: Handbuch der baulichen Maßnahmen und\\n\\nTechnischen Gebäudeausrüstung (Vol. 2). Verlag Bau+Technik.\\n\\nELE Consortium. (2022). Digital Language Equality in Europe by 2030: Strategic Agenda and Roadmap.\\n\\nhttps://european-language-equality.eu/agenda/\\n\\nFan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A., Goyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary, V., Goyal, N., Birch, T., Liptchinsky, V., Edunov, S., Grave, E., Auli, M., & Joulin, A. (2022). Beyond english-centric multilingual machine translation. The Journal of Machine Learning Research, 22(1), 4839–4886.\\n\\nFFCV. (2022). [Python]. FFCV. https://github.com/libffcv/ffcv (Original work published 2021) Frostig, R., Johnson, M., & Leary, C. (2018). Compiling machine learning programs via high-level tracing.\\n\\nhttps://mlsys.org/Conferences/doc/2018/146.pdf\\n\\nGehlhaus, D., & Koslosky, L. (2022). Training Tomorrow’s AI Workforce. Center for Security and Emerging Technology. https://cset.georgetown.edu/publication/training-tomorrows-ai-workforce/ Gehlhaus, D., Koslosky, L., Goode, K., & Perkins, C. (2021). U.S. AI Workforce: Policy Recommendations.\\n\\nCenter for Security and Emerging Technology. https://cset.georgetown.edu/publication/u-s-ai- workforce-policy-recommendations/\\n\\nGehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating Neural\\n\\nToxic Degeneration in Language Models (arXiv:2009.11462). arXiv. https://doi.org/10.48550/arXiv.2009.11462\\n\\nGlaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C., … Irving, G. (2022). Improving alignment of dialogue agents via targeted human judgements (arXiv:2209.14375). arXiv. https://doi.org/10.48550/arXiv.2209.14375 GlusterFS. (2022). [C]. Gluster.org. https://github.com/gluster/glusterfs (Original work published 2011) Gopani, A. (2021, July 16). JAX Vs TensorFlow Vs PyTorch: A Comparative Analysis. Analytics India Magazine.\\n\\nhttps://analyticsindiamag.com/jax-vs-tensorflow-vs-pytorch-a-comparative-analysis/\\n\\nHansell, S. (2002, April 8). Google’s Toughest Search Is for a Business Model. The New York Times.\\n\\nhttps://www.nytimes.com/2002/04/08/business/google-s-toughest-search-is-for-a-business- model.html\\n\\nGroße KI-Modelle für Deutschland\\n\\n234\\n\\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), Article 7825. https://doi.org/10.1038/s41586-020-2649-2\\n\\nHensel, M., & Ostler, U. (2020, November 22). Die beliebtesten Anbieter von Technik für das High-\\n\\nPerformance Computing. Datacenter Insider. https://www.datacenter-insider.de/die-beliebtesten- anbieter-vontechnik-fuer-das-high-performance-computing-a-980532/\\n\\nHickmann, H., & Koneberg, F. (2022). Die Berufe mit den aktuell größten Fachkräftelücken. 67.\\n\\nhttps://www.iwkoeln.de/studien/helen-hickmann-filiz-koneberg-die-berufe-mit-den-aktuell- groessten-fachkraefteluecken.html\\n\\nHintemann, Dr. R., Hinterholzer, S., Graß, M., & Grothey, T. (2022). Bitkom-Studie: Rechenzentren in\\n\\nDeutschland 2021 – Aktuelle Marktentwicklungen. Borderstep Institut. https://www.bitkom.org/sites/main/files/2022-02/10.02.22-studie-rechenzentren.pdf\\n\\nHintemann, Dr. R., Hinterholzer, S., & Grothey, T. (2021). Herausforderungen und Chancen durch den Boom\\n\\nbeim Neubau von Rechenzentren. Hessische Staatskanzlei, Ministerin für Digitale Strategie und Entwicklung. https://digitales.hessen.de/sites/digitales.hessen.de/files/2022- 05/rechenzentrumsmarkt_hessen.pdf\\n\\nHintemann, R. (2020). Data centers 2018. Efficiency gains are not enough: Data center energy consumption\\n\\ncontinues to rise significantly - Cloud computing boosts growth. https://doi.org/10.13140/RG.2.2.26033.40800\\n\\nHintemann, R., & Clausen, J. (2018). Bedeutung digitaler Infrastrukturen in Deutschland. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network (arXiv:1503.02531). arXiv.\\n\\nhttps://doi.org/10.48550/arXiv.1503.02531\\n\\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\\n\\n& Salimans, T. (2022). Imagen Video: High Definition Video Generation with Diffusion Models (arXiv:2210.02303). arXiv. https://doi.org/10.48550/arXiv.2210.02303\\n\\nHugging Face. (o.D.). Model Parallelism. Retrieved 28 November 2022, from\\n\\nhttps://huggingface.co/docs/transformers/v4.15.0/parallelism\\n\\nHydra. (2022). [Python]. Meta Research. https://github.com/facebookresearch/hydra (Original work\\n\\npublished 2019)\\n\\nIntel. (2022, June 29). Second-Gen Habana Gaudi2 Outperforms Nvidia A100. Intel.\\n\\nhttps://www.intel.com/content/www/us/en/newsroom/news/second-gen-habana-gaudi2- outperforms-nvidia-a100.html\\n\\nISO - ISO/IEC 27001 and related standards—Information security management. (o.D.). ISO. Retrieved 28 November 2022, from https://www.iso.org/isoiec-27001-information-security.html\\n\\nISO 27017 and ISO 27018 Certification | DEKRA. (o.D.). Retrieved 6 December 2022, from\\n\\nhttps://www.dekra.com/en/iso-27017-and-iso-27018-certification/\\n\\nIzacard, G., & Grave, E. (2021). Leveraging Passage Retrieval with Generative Models for Open Domain Question\\n\\nAnswering (arXiv:2007.01282). arXiv. https://doi.org/10.48550/arXiv.2007.01282\\n\\nJawahar, R. (2021, October 14). Teaching AI to perceive the world through your eyes. Meta AI.\\n\\nhttps://ai.facebook.com/blog/teaching-ai-to-perceive-the-world-through-your-eyes/ Joseph, E., Riddle, M., Sorensen, T., & Conway, S. (2022). The Economic and Societal Benefits of Linux\\n\\nSupercomputers. https://davidbader.net/publication/2022-hyperionresearch/\\n\\nJülich Forschungszentrum. (2022, June 15). Erster europäischer Exascale-Superrechner kommt nach Jülich. https://www.fz-juelich.de/de/aktuelles/news/pressemitteilungen/2022/exascale-standort- entscheidung\\n\\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R.,\\n\\nŽídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera- Paredes, B., Nikolov, S., Jain, R., Adler, J., … Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), Article 7873. https://doi.org/10.1038/s41586-021- 03819-2\\n\\nKahneman, D. (2011). Thinking, fast and slow. Macmillan. Kale, M., Siddhant, A., Al-Rfou, R., Xue, L., Constant, N., & Johnson, M. (2021). nmT5—Is parallel data still relevant for pre-training massively multilingual language models? Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\\n\\nGroße KI-Modelle für Deutschland\\n\\n235\\n\\nNatural Language Processing (Volume 2: Short Papers), 683–691. https://doi.org/10.18653/v1/2021.acl- short.87\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\\n\\nKeras. (2022). [Python]. Keras. https://github.com/keras-team/keras (Original work published 2015) Khan, S. M., & Mann, A. (2020, April). AI Chips: What They Are and Why They Matter. Center for Security and\\n\\nEmerging Technology. https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they- matter/\\n\\nKosson, A., Chiley, V., Venigalla, A., Hestness, J., & Köster, U. (2021). Pipelined Backpropagation at Scale:\\n\\nTraining Large Models without Batches (arXiv:2003.11666). arXiv. https://doi.org/10.48550/arXiv.2003.11666\\n\\nKubeflow. (2022). [Jsonnet]. Kubeflow. https://github.com/kubeflow/kubeflow (Original work published 2017) Kubernetes (K8s). (2022). [Go]. Kubernetes. https://github.com/kubernetes/kubernetes (Original work\\n\\npublished 2014)\\n\\nLamonica, M. (2014, June 11). HP’s Water-Cooled Supercomputer is Designed for the Hydrophobic. IEEE\\n\\nSpectrum. https://spectrum.ieee.org/a-watercooled-supercomputer-for-the-hydrophobic-\\n\\nLi, C. (2020, June 3). OpenAI’s GPT-3 Language Model: A Technical Overview.\\n\\nhttps://lambdalabs.com/blog/demystifying-gpt-3\\n\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta- Navas, D., Hudson, D. A., … Koreeda, Y. (2022). Holistic Evaluation of Language Models (arXiv:2211.09110). arXiv. https://doi.org/10.48550/arXiv.2211.09110\\n\\nLin, S., Hilton, J., & Evans, O. (2021). TruthfulQA: Measuring How Models Mimic Human Falsehoods\\n\\n(arXiv:2109.07958). arXiv. https://doi.org/10.48550/arXiv.2109.07958\\n\\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., Costa, A. dos S., Fazel-Zarandi, M., Sercu, T., Candido, S., & Rives, A. (2022). Evolutionary-scale prediction of atomic level protein structure with a language model (p. 2022.07.20.500902). bioRxiv. https://doi.org/10.1101/2022.07.20.500902\\n\\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L. (2020). Multilingual\\n\\nDenoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8, 726–742. https://doi.org/10.1162/tacl_a_00343\\n\\nLiu, Y., Liu, P., Radev, D., & Neubig, G. (2022). BRIO: Bringing Order to Abstractive Summarization.\\n\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2890–2903. https://doi.org/10.18653/v1/2022.acl-long.207\\n\\nMaszke, S. (2022). Torchdatasets [Python]. https://github.com/szymonmaszke/torchdatasets (Original work\\n\\npublished 2019)\\n\\nMerkel, D. (2014). Docker: Lightweight Linux containers for consistent development and deployment. Linux\\n\\nJournal, 2014(239), 2:2.\\n\\nMessage Passing Interface. (2022). In Wikipedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&oldid=1112449606 Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., & Sivic, J. (2019). HowTo100M: Learning a Text- Video Embedding by Watching Hundred Million Narrated Video Clips. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2630–2640. https://doi.org/10.1109/ICCV.2019.00272\\n\\nMishkin, P., Ahmad, L., Brundage, M., Krueger, G., & Sastry, G. (2022). DALL·E 2 Preview—Risks and Limitations.\\n\\n[https://github.com/openai/dalle-2-preview/blob/main/system- card.md](https://github.com/openai/dalle-2-preview/blob/main/system-card.md)\\n\\nMLflow: A Machine Learning Lifecycle Platform. (2022). [Python]. MLflow. https://github.com/mlflow/mlflow\\n\\n(Original work published 2018)\\n\\nMo, S., Fu, X., Hong, C., Chen, Y., Zheng, Y., Tang, X., Lan, Y., Shen, Z., & Xing, E. (2021, September 24). Multi-\\n\\nmodal Self-supervised Pre-training for Large-scale Genome Data. NeurIPS 2021 AI for Science Workshop. https://openreview.net/forum?id=fdV-GZ4LPfn\\n\\nMoritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., &\\n\\nStoica, I. (2018). Ray: A Distributed Framework for Emerging AI Applications (arXiv:1712.05889). arXiv. https://doi.org/10.48550/arXiv.1712.05889\\n\\nGroße KI-Modelle für Deutschland\\n\\n236\\n\\nMudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., Yang, J. A., Gao, L., Ivchenko, D., Basant, A., Hu, Y., Yang, J., Ardestani, E. K., Wang, X., … Rao, V. (2022). Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models (arXiv:2104.05158). arXiv. https://doi.org/10.48550/arXiv.2104.05158\\n\\nMujkanovic, N., Sivalingam, K., & Lazzaro, A. (2020). Optimising AI Training Deployments using Graph Compilers\\n\\nand Containers (arXiv:2008.11675). arXiv. https://doi.org/10.48550/arXiv.2008.11675\\n\\nNagrani, A., Seo, P. H., Seybold, B., Hauth, A., Manen, S., Sun, C., & Schmid, C. (2022). Learning Audio-Video\\n\\nModalities from Image Captions. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.), Computer Vision – ECCV 2022 (pp. 407–426). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-19781-9_24\\n\\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., & Schulman, J. (2021). WebGPT: Browser-assisted question-answering with human feedback (arXiv:2112.09332). arXiv. https://doi.org/10.48550/arXiv.2112.09332\\n\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V. A., Vainbrand, D.,\\n\\nKashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., & Zaharia, M. (2021). Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM (arXiv:2104.04473). arXiv. https://doi.org/10.48550/arXiv.2104.04473\\n\\nNguyen, T. T., Trahay, F., Domke, J., Drozd, A., Vatai, E., Liao, J., Wahib, M., & Gerofi, B. (2022). Why Globally\\n\\nRe-shuffle? Revisiting Data Shuffling in Large Scale Deep Learning. 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 1085–1096. https://doi.org/10.1109/IPDPS53621.2022.00109\\n\\nNichol, A. (2022, June 28). DALL·E 2 Pre-Training Mitigations. OpenAI. https://openai.com/blog/dall-e-2-pre-\\n\\ntraining-mitigations/\\n\\nNVIDIA. (o.D.). NCCL and MPI — NCCL 2.15.5 documentation. Retrieved 28 November 2022, from\\n\\nhttps://docs.nvidia.com/deeplearning/nccl/user-guide/docs/mpi.html\\n\\nNVIDIA Developer. (2013, July 2). CUDA Toolkit—Free Tools and Training. NVIDIA Developer.\\n\\nhttps://developer.nvidia.com/cuda-toolkit\\n\\nOfeidis, I., Kiedanski, D., & Tassiulas, L. (2022). An Overview of the Data-Loader Landscape: Comparative\\n\\nPerformance Analysis (arXiv:2209.13705). arXiv. https://doi.org/10.48550/arXiv.2209.13705\\n\\nOfer, D., Brandes, N., & Linial, M. (2021). The language of proteins: NLP, machine learning & protein\\n\\nsequences. Computational and Structural Biotechnology Journal, 19, 1750–1758. https://doi.org/10.1016/j.csbj.2021.03.022\\n\\nOpen MPI. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_MPI&oldid=1120683830 OpenAI. (2022a, July 18). Reducing Bias and Improving Safety in DALL·E 2. OpenAI.\\n\\nhttps://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/\\n\\nOpenAI. (2022b, November 30). ChatGPT: Optimizing Language Models for Dialogue. OpenAI.\\n\\nhttps://openai.com/blog/chatgpt/\\n\\nPaaß, G., & Giesselbach, S. (2023). Foundation Models for Natural Language Processing. Springer Cham.\\n\\nhttps://link.springer.com/book/9783031231896\\n\\nPapers with Code—Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (o.D.).\\n\\nRetrieved 10 January 2023, from https://paperswithcode.com/paper/exploring-the-limits-of-transfer- learning\\n\\nParliament, E. (2018). European Parliament resolution of 11 September 2018 on language equality in the digital\\n\\nage (2018/2028(INI)). https://www.europarl.europa.eu/doceo/document/TA-8-2018-0332_DE.html\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L.,\\n\\nDesmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library (arXiv:1912.01703). arXiv. https://doi.org/10.48550/arXiv.1912.01703\\n\\nPatterson, D., Gonzalez, J., Hölzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., &\\n\\nDean, J. (2022). The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. Computer, 55(7), 18–28. https://doi.org/10.1109/MC.2022.3148714\\n\\nPhilippe Lorenz & Kate Saslow. (2019). Demystifying AI & AI Companies. What foreign policy makers need to\\n\\nknow about the global AI industry. https://www.stiftung-nv.de/de/publikation/demystifying-ai-ai- companies-what-foreign-policy-makers-need-know-about-global-ai\\n\\nGroße KI-Modelle für Deutschland\\n\\n237\\n\\nPiloto, L. S., Weinstein, A., Battaglia, P., & Botvinick, M. (2022). Intuitive physics learning in a deep-learning\\n\\nmodel inspired by developmental psychology. Nature Human Behaviour, 6(9), Article 9. https://doi.org/10.1038/s41562-022-01394-8\\n\\nPoretschkin, M. (2022). ZERTIFIZIERTE KI | Qualität sichern. Fortschritt gestalten. ZERTIFIZIERTE KI.\\n\\nhttps://www.zertifizierte-ki.de/\\n\\nPoretschkin, M., Mock, M., & Wrobel, S. (2021). Zur Systematischen Bewertung der Vertrauenswürdigkeit von\\n\\nKI-Systemen. Regulierung Für Algorithmen Und Künstliche Intelligenz, 175–202. https://doi.org/10.5771/9783748927990\\n\\nPushkarna, M., Zaldivar, A., & Kjartansson, O. (2022). Data Cards: Purposeful and Transparent Dataset\\n\\nDocumentation for Responsible AI (arXiv:2204.01075). arXiv. https://doi.org/10.48550/arXiv.2204.01075\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\\n\\nKrueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 38th International Conference on Machine Learning, 8748–8763. https://proceedings.mlr.press/v139/radford21a.html\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring\\n\\nthe limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1–67.\\n\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation\\n\\nwith CLIP Latents (arXiv:2204.06125). arXiv. https://doi.org/10.48550/arXiv.2204.06125\\n\\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., & de Freitas, N. (2022). A Generalist Agent (arXiv:2205.06175). arXiv. https://doi.org/10.48550/arXiv.2205.06175\\n\\nRen, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T.-Y. (2022). FastSpeech 2: Fast and High-Quality End-to-\\n\\nEnd Text to Speech (arXiv:2006.04558). arXiv. https://doi.org/10.48550/arXiv.2006.04558\\n\\nResearch and Markets ltd. (2021, June). Europe Data Center Colocation Market: Market Size, Forecast, Insights,\\n\\nand Competitive Landscape. https://www.researchandmarkets.com/reports/5511065/europe-data- center-colocation-market-market\\n\\nReuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., & Kepner, J. (2022). AI and ML Accelerator Survey\\n\\nand Trends (arXiv:2210.04055). arXiv. https://doi.org/10.48550/arXiv.2210.04055\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models (arXiv:2112.10752). arXiv. https://doi.org/10.48550/arXiv.2112.10752\\n\\nSchödwell, B., Zarnekow, D. R., Liu, R., Gröger, J., & Wilkens, M. (2018). Kennzahlen und Indikatoren für die Beurteilung der Ressourceneffizienz von Rechenzentren und Prüfung der praktischen Anwendbarkeit.\\n\\nSchreiner, M. (2022, January 29). Meta’s AI chief: Three major challenges of artificial intelligence. THE DECODER.\\n\\nhttps://the-decoder.com/metas-ai-chief-three-major-challenges-of-artificial-intelligence/\\n\\nSchuhmann, C. (2021, August 8). LAION-400-MILLION OPEN DATASET | LAION. LAION.\\n\\nhttps://laion.ai/blog/laion-400-open-dataset\\n\\nSevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., & Villalobos, P. (2022). Compute Trends Across Three\\n\\nEras of Machine Learning (arXiv:2202.05924). arXiv. https://doi.org/10.48550/arXiv.2202.05924\\n\\nShuster, K., Poff, S., Chen, M., Kiela, D., & Weston, J. (2021). Retrieval Augmentation Reduces Hallucination in\\n\\nConversation (arXiv:2104.07567). arXiv. https://doi.org/10.48550/arXiv.2104.07567 Simons, G. J., & Frese, A. (2021). Zukunft regional – digital: Das Rheinische Revier; Machbarkeitsstudie\\n\\nDateninfrastrukturen im Rheinischen Revie. Ministerium für Wirtschaft, Innovation, Digitalisierung und Energie des Landes Nordrhein-Westfalen. https://www.wirtschaft.nrw/sites/default/files/documents/machbarkeitsstudie_dateninfrastrukturen _lang_de.pdf\\n\\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D.,\\n\\nGupta, S., & Taigman, Y. (2022). Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv:2209.14792). arXiv. https://doi.org/10.48550/arXiv.2209.14792\\n\\nSohofi, A., Yu, T., Aribal, A., Loetzsch, W., Team, S. D., & Wollmann, T. (2022). Squirrel [Python]. https://github.com/merantix-momentum/squirrel-core (Original work published 2022)\\n\\nSonnenburg, S., Braun, M. L., Ong, C. S., Bengio, S., Bottou, L., Holmes, G., LeCun, Y., Müller, K.-R., Pereira, F.,\\n\\nRasmussen, C. E., R&#228, G., tsch, Schölkopf, B., Smola, A., Vincent, P., Weston, J., & Williamson, R. (2007). The Need for Open Source Software in Machine Learning. Journal of Machine Learning Research, 8(81), 2443–2466.\\n\\nGroße KI-Modelle für Deutschland\\n\\n238\\n\\nStobbe, Dr. L., Proske, M., Zedel, H., Hintemann, Dr. R., Clausen, Dr. J., & Beucker, Dr. S. (2015). Entwicklung\\n\\ndes IKT-bedingten Strombedarfs in Deutschland [Abschlussbericht]. Fraunhofer-Institut für Zuverlässigkeit und Mikrointegration. https://www.bmwk.de/Redaktion/DE/Downloads/E/entwicklung-des-ikt-bedingten-strombedarfs-in- deutschland-abschlussbericht.pdf?__blob=publicationFile&v=3\\n\\nStöcker, C., & Dambeck, H. (2006, December 19). Deutsch-französische Suchmaschine: Quaero ist geplatzt. Der Spiegel. https://www.spiegel.de/netzwelt/web/deutsch-franzoesische-suchmaschine-quaero-ist- geplatzt-a-455558.html\\n\\nStreim, A. (2022, November 16). Trotz Krieg und Krisen: In Deutschland fehlen 137.000 IT-Fachkräfte |\\n\\nPresseinformation | Bitkom e.V. https://www.bitkom.org/Presse/Presseinformation/Deutschland- fehlen-137000-IT-Fachkraefte\\n\\nSuzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H.,\\n\\nZhou, D., & Wei, J. (2022). Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them (arXiv:2210.09261). arXiv. https://doi.org/10.48550/arXiv.2210.09261\\n\\nSystem and Organization Controls: SOC Suite of Services. (o.D.). AICPA. Retrieved 28 November 2022, from\\n\\nhttps://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html\\n\\nTachyum. (2022, October 4). Tachyum Unveils Details of Architecture and Prodigy Design in Overview White Paper | Tachyum. https://www.tachyum.com/media/press-releases/2022/10/04/tachyum-unveils-details-of- architecture-and-prodigy-design-in-overview-white-paper/\\n\\nTamkin, A., & Ganguli, D. (2021, February 5). How Large Language Models Will Transform Science, Society, and AI. Stanford HAI. https://hai.stanford.edu/news/how-large-language-models-will-transform-science- society-and-ai\\n\\nTerraform. (2022). [Go]. HashiCorp. https://github.com/hashicorp/terraform (Original work published 2014) THE NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH RESOURCE TASK FORCE (NAIRRTF). (o.D.). National Artificial\\n\\nIntelligence Initiative. Retrieved 10 January 2023, from https://www.ai.gov/nairrtf/\\n\\nThe role of data centers in an interconnected world—DECIX – Without You. (o.D.). DECIX – Without You.\\n\\nRetrieved 19 December 2022, from https://withoutyou.de-cix.net/the-role-of-data-centers/ Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., … Le, Q. (2022). LaMDA: Language Models for Dialog Applications (arXiv:2201.08239). arXiv. https://doi.org/10.48550/arXiv.2201.08239\\n\\nUng, M., Xu, J., & Boureau, Y.-L. (2022). SaFeRDialogues: Taking Feedback Gracefully after Conversational\\n\\nSafety Failures. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 6462–6481. https://doi.org/10.18653/v1/2022.acl-long.447 van Rossum, G. (1995). Python reference manual (R 9525). Article R 9525. https://ir.cwi.nl/pub/5008 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).\\n\\nAttention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems (Vol. 30, pp. 5998–6008). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\n\\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., & Yang, H. (2022). OFA: Unifying\\n\\nArchitectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework (arXiv:2202.03052). arXiv. https://doi.org/10.48550/arXiv.2202.03052\\n\\nWannemacher, K., & Bodmann, L. (2021). Künstliche Intelligenz an den Hochschulen. Potenziale und\\n\\nHerausforderungen in Forschung, Studium und Lehre sowie Curriculumentwicklung: Arbeitspapier Nr. 59. Berlin: Hochschulforum Digitalisierung. https://hochschulforumdigitalisierung.de/sites/default/files/dateien/HFD_AP_59_Kuenstliche_Intellige nz_Hochschulen_HIS-HE.pdf\\n\\nWeidmann, Dr. R. E., & Krüger, Dr. T. (2020, November 30). Dr. Béla Waldhauser, Telehouse Deutschland:\\n\\nDigitale Effizienz in Rechenzentren. https://detecon.com/de/journal/dr-bela-waldhauser-telehouse- deutschland-digitale-effizienz-rechenzentren\\n\\nWikipedia contributors. (2022). Selene (supercomputer)—Wikipedia, The Free Encyclopedia.\\n\\nhttps://en.wikipedia.org/w/index.php?title=Selene_(supercomputer)&oldid=1109224992\\n\\nWu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., & Duan, N. (2022). NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.),\\n\\nGroße KI-Modelle für Deutschland\\n\\n239\\n\\nComputer Vision – ECCV 2022 (pp. 720–736). Springer Nature Switzerland. https://doi.org/10.1007/978- 3-031-19787-1_41\\n\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2021). mT5: A\\n\\nmassively multilingual pre-trained text-to-text transformer (arXiv:2010.11934). arXiv. https://doi.org/10.48550/arXiv.2010.11934\\n\\nYazdani-Jahromi, M., Yousefi, N., Tayebi, A., Kolanthai, E., Neal, C. J., Seal, S., & Garibay, O. O. (2022).\\n\\nAttentionSiteDTI: an interpretable graph-based model for drug-target interaction prediction using NLP sentence-level relation classification. Briefings in Bioinformatics, 23(4). https://doi.org/10.1093/bib/bbac272\\n\\nYin, P., Neubig, G., Yih, W., & Riedel, S. (2020). TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8413–8426. https://doi.org/10.18653/v1/2020.acl-main.745\\n\\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple Linux Utility for Resource Management. In D. Feitelson, L. Rudolph, & U. Schwiegelshohn (Eds.), Job Scheduling Strategies for Parallel Processing (pp. 44–60). Springer. https://doi.org/10.1007/10968987_3\\n\\nYuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models.\\n\\n27th International Conference on Intelligent User Interfaces, 841–852. https://doi.org/10.1145/3490099.3511105\\n\\nYuan, B., He, Y., Davis, J. Q., Zhang, T., Dao, T., Chen, B., Liang, P., Re, C., & Zhang, C. (2022). Decentralized\\n\\nTraining of Foundation Models in Heterogeneous Environments (arXiv:2206.01288). arXiv. https://doi.org/10.48550/arXiv.2206.01288\\n\\nZellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., & Choi, Y. (2022).\\n\\nMERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound (arXiv:2201.02639). arXiv. https://doi.org/10.48550/arXiv.2201.02639\\n\\nZhang, D., Maslej, N., Brynjolfsson, E., Etchemendy, J., Lyons, T., Manyika, J., Ngo, H., Niebles, J. C., Sellitto, M., Sakhaee, E., Shoham, Y., Clark, J., & Perrault, R. (2022). Artificial Intelligence Index Report 2022. Stanford Institute for Human-Centered AI, Stanford University. https://aiindex.stanford.edu/wp- content/uploads/2022/03/2022-AI-Index-Report_Master.pdf\\n\\nZhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., & Chen, W. (2021). Poolingformer: Long document modeling with pooling attention. International Conference on Machine Learning, 12437–12446. https://arxiv.org/abs/2105.04371\\n\\nZhang, Y., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang, R., Le, Q. V., & Wu, Y. (2020). Pushing the Limits of Semi-\\n\\nSupervised Learning for Automatic Speech Recognition (arXiv:2010.10504). arXiv. https://doi.org/10.48550/arXiv.2010.10504\\n\\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2022). Automatic Chain of Thought Prompting in Large Language Models\\n\\n(arXiv:2210.03493). arXiv. https://doi.org/10.48550/arXiv.2210.03493\\n\\nGroße KI-Modelle für Deutschland\\n\\n240\\n\\nII. Abbildungsverzeichnis Abbildungs- nummer\\n\\nName\\n\\nAbb. 1\\n\\nAbb. 2\\n\\nAbb. 3\\n\\nAbb. 4\\n\\nAbb. 5\\n\\nAbb. 6\\n\\nAbb. 7\\n\\nAbb. 8\\n\\nAbb. 9\\n\\nAbb. 10\\n\\nAnzahl der Parameter großer KI-Sprachmodelle seit GPT-3 (Open Source Modelle rot markiert). Quelle: state of ai Report 2022 (Benaic & Hogarth, 2022) Verdeutlichung der Self-Attention am Satz \"Die Bank verleiht Geld\". Das Token Bank (unten) hat eine hohe Korrelation mit dem Token Geld (oben), wobei die Korrelation zu den anderen Token geringer ausfällt. Zentraler Bestandteil der Foundation-Modelle sind Schichten mit Self-Attention Blöcken (blau), die kontextsensitive Einbettungsvektoren (violett) von Eingabetoken (grün) berechnen. Die logistische Schicht L prognostiziert die Wahrscheinlichkeit der Ausgabetoken. Beim Training werden die Parameter so optimiert, dass die Wahrscheinlichkeiten der korrekten fehlenden bzw. nächsten Token (gelb) möglichst hoch sind. Ein Foundation-Modell kann Informationen aus verschiedenen Datenquellen verschiedener Modalitäten berücksichtigen. Dieses eine Modell kann dann eine Vielzahl von nachgelagerten Aufgaben lösen (Bommasani et al., 2021). Die Genauigkeit des „few-shot\"-Lernens von GPT-3 wird durch die Erweiterung der Modellgröße und der Anzahl der präsentierten Beispiele erhöht (Brown et al., 2020). Von einem Foundation-Modell mit Hilfe von Retrieval gefundene Antwort auf eine Frage im Natural Question Benchmark. Aktuelle Modelle erreichen eine Genauigkeit (F1) von 80% (Zhanag et al., 2021). Zusammenfassung eines Textes von 800 Wörtern durch das Modell BRIO im Vergleich zu der von Experten erstellten Zusammenfassung (Liu et al., 2022). Zu unterschiedlichen Texten von DALL-E 2 erzeugte Bilder (Ramesh et al., 2022) Zu unterschiedlichen Texten von CogVideo erzeugte Videos (Hong et al., 2022) Das Gato-Modell generiert aus den aktuellen Zuständen (hellblau) neue Aktionen (dunkelblau). Die Umgebung produziert daraus neue Zustände, usw. Das Modell kann Texte, Messwerte, Bilder, usw. verarbeiten. Um 2016 tauchte ein neuer Trend zu sehr großen Modellen auf, die von großen Internetfirmen trainiert wurden (rot). Diese waren in der Lage waren, die notwendigen Investitionen zu finanzieren. Die untere blaue Linie\\n\\nAbb. 11\\n\\nGroße KI-Modelle für Deutschland\\n\\nSeite\\n\\n19\\n\\n24\\n\\n27\\n\\n29\\n\\n31\\n\\n35\\n\\n36\\n\\n40\\n\\n41\\n\\n44\\n\\n55\\n\\nAbb. 12\\n\\nAbb. 13\\n\\nAbb. 14\\n\\nAbb. 15\\n\\nAbb. 16\\n\\nAbb. 17\\n\\nAbb. 18\\n\\nAbb. 19\\n\\nAbb. 20\\n\\nAbb. 21\\n\\nAbb. 22\\n\\nAbb. 23\\n\\nAbb. 24\\n\\nAbb. 25 Abb. 26 Abb. 27\\n\\n241\\n\\nveranschaulicht den Berechnungsaufwand der anderen Modelle, z.B. von Universitäten (Sevilla et al., 2022). Trainingsleistung (1 ExaFLOPs = 1018 FLOPs) unterschiedlicher Foundation-Modelle gegenüber Veröffentlichungsjahr nach Ländern Trainingsleistung (1018 FLOPs = 1 ExaFLOPs) gegenüber Veröffentlichungsjahr nach Ländern. Der Abbildung (intern)1.12: Großteil der Foundation-Modelle wurden in den USA entwickelt. Aus Deutschland und anderen EU Ländern wurden tendenziell kleinere Foundation-Modelle mit niedrigerem Trainingsaufwand veröffentlicht. Das BLOOM Modell stellt eine Ausnahme dar und wurde von einem Wissenschaftskollektiv bestehend aus über 250 Institutionen auf dem Jean Zay Supercomputer in Frankreich trainiert. Ergebnisse der Umfrage mit KMUs zu deren Einsatz von Foundation-Modellen Ergebnisse der Umfrage mit KMUs zu Hindernissen beim Einsatz von Foundation-Modellen Ergebnisse der Umfrage mit KMUs zur Bedeutung von unterschiedlichen Aspekten der Foundation-Modell- Entwicklung Ergebnisse der Umfrage mit KMUs zur Relevanz von unterschiedlichen Arten von KI-Modellen bei der Foundation-Modell-Entwicklung (Antworten mit einer Antwortrate von weniger als 20 % wurden ausgelassen. Die vollständigen Antworten befinden sich in Anhang A.2.) Simplifizierte Darstellung der Hard- und Software- Infrastruktur von HPCs Die Architektur des Trainings- & Applikations-Layers im Detail Die Architektur des System- und Data-Storage & Loading- Layers im Detail Die Architektur des Framework- & Service-Layers im Detail MLPerf hardware: accelarators (Zhang et al., 2022, S.18)\\n\\nBeispielrechnung Bau und Betrieb eines eigenen HPC- Rechenzentrums Delphi-Befragung: Wie beurteilen Sie folgende Risiken für die Entwicklung des Rechenzentrumsmarktes in Deutschland? (Hintemann et al., 2022, S. 37) Organisationseinheiten des LEAM-KI-Servicezentrums Das LEAM-Board als zentrale Governance-Einheit des LKS Szenario für die Struktur des LEAM-KI-Servicezentrums\\n\\nGroße KI-Modelle für Deutschland\\n\\n56\\n\\n57\\n\\n69\\n\\n70\\n\\n72\\n\\n73\\n\\n110\\n\\n113\\n\\n120\\n\\n122\\n\\n138\\n\\n160\\n\\n162\\n\\n169 181 225\\n\\n242\\n\\nIII. Tabellenverzeichnis\\n\\nTabellen- nummer\\n\\nName\\n\\nTabelle 1\\n\\nTabelle 2\\n\\nTabelle 3\\n\\nTabelle 4\\n\\nTabelle 5\\n\\nTabelle 6 Tabelle 7 Tabelle 8\\n\\nEine Auswahl möglicher Anwendungen auf Basis von Sprachmodellen Eine Auswahl möglicher Anwendungen auf Basis von multimodalen Modellen Übersicht der wichtigsten Information zu der Umfrage für die Wirtschaft Befragte Experten aus der Wirtschaft Im Betrieb von LEAM werden für Training, Tuning und Inference Tausende GPUs benötigt Beispiele für Rechenzentren Größen von Rechenzentren Übersicht über die Kühlmöglichkeiten in Rechenzentren Compute Anforderungen für die Berechnung eines Foundationmodells\\n\\nTabelle 9\\n\\nTabelle 10 HPC-Standorte in Deutschland Tabelle 11 Tabelle 12 Tabelle 13 Übersicht über die OE Housing Tabelle 14 Übersicht über die Training-as-a-Service Tabelle 15 Übersicht der Kosten des LEAM-KI-Servicezentrums\\n\\nVerfügbarkeitsklassen (VK1 - VK4) Schutzklassen nach DIN EN 50600-1\\n\\nTabelle 16\\n\\nTabelle 17\\n\\nGesamtkostenstruktur des LEAM-KI-Servicezentrums bei einer Abschreibungsdauer von vier Jahren Kosten des LEAM-KI-Servicezentrums bei einem Einkauf der GPU-RZ-Leistung\\n\\nTabelle 18 Übersicht der Einnahmen durch das Model-Training Tabelle 19 Übersicht der Einnahmen durch die Beratung\\n\\nTabelle 20\\n\\nTabelle 21\\n\\nGegenüberstellung der drei Finanzierungsszenarien für das LKS Übersicht der Vor- und Nachteile der drei Finanzierungsmodelle mit Hinblick auf die rechtlichen Rahmenbedingungen\\n\\nTabelle 22 Übersicht über die drei Online-Umfragen Tabelle 23 Übersicht der Interviewpartner:innen im Bereich Wirtschaft\\n\\nTabelle 24\\n\\nÜbersicht der Interviewpartner:innen im Bereich Rechenzentrum und Hardware\\n\\nTabelle 25 Übersicht der Interviewpartner:innen im Bereich Wissenschaft Tabelle 26 Übersicht der sonstigen Interviewpartner:innen\\n\\nTabelle 27\\n\\nKosten für eine GPU-Stunde auf einer NVIDIA A100 Tensor Core GPU 80 GB nach Anbieter. Preise in US-Dollar wurden in Euro umgerechnet zu einem Kurs von $1 = 0,948768EUR (Dollarkurs am 06.12.2022)\\n\\nGroße KI-Modelle für Deutschland\\n\\nSeite\\n\\n33\\n\\n39\\n\\n67\\n\\n68\\n\\n111\\n\\n128 129 132\\n\\n137\\n\\n142 152 154 170 174 185\\n\\n186\\n\\n188\\n\\n190 191\\n\\n193\\n\\n218\\n\\n320 321\\n\\n322\\n\\n323 324\\n\\n369\\n\\n243\\n\\nIV. Abkürzungsverzeichnis\\n\\nAbb\\n\\nAbbildung\\n\\nAbs\\n\\nAbsatz\\n\\nAEUV\\n\\nVertrag über die Arbeitsweise der Europäischen Union\\n\\nAI\\n\\nArtificial Intelligence - Künstliche Intelligenz\\n\\nAPI\\n\\nApplication Programming Interface\\n\\nArt\\n\\nArtikel\\n\\nB2B\\n\\nBusiness to Business\\n\\nBERT\\n\\nBidirectional Encoder Representations from Transformers\\n\\nBMWK\\n\\nBundesministerium für Wirtschaft und Klimaschutz\\n\\nbspw\\n\\nbeispielsweise\\n\\nbzgl\\n\\nbezüglich\\n\\nbzw\\n\\nbeziehungsweise\\n\\nca\\n\\ncirca\\n\\nDMZ\\n\\nDemilitarized Zone\\n\\nDNA\\n\\nDesoxyribonukleinsäure\\n\\nEU\\n\\nEuropäische Union\\n\\nF&E\\n\\nForschung und Entwicklung\\n\\nFLOP\\n\\noder FLOPs - Floating Point Operations\\n\\nFTE\\n\\nFull-Time-Equivalents\\n\\nGG\\n\\nGrundgesetz\\n\\nggf\\n\\nGegebenenfalls\\n\\nGPT\\n\\nGenerative Pre-trained Transformer\\n\\nGWB\\n\\nGesetz gegen Wettbewerbsbeschränkungen\\n\\nHPC\\n\\nHigh Performance Computing\\n\\nIaaS\\n\\nInfrastruktur-as-a-Service\\n\\nIT\\n\\nInformation Technologies\\n\\nGroße KI-Modelle für Deutschland\\n\\nKI\\n\\nKMU\\n\\nLEAM\\n\\nLKS\\n\\nMio\\n\\nML\\n\\nMrd\\n\\nNLP\\n\\nNLU\\n\\no.D.\\n\\nOE\\n\\nOSS\\n\\nPers\\n\\nPPP\\n\\nPUE\\n\\nRZ\\n\\nS\\n\\nSeq2Seq\\n\\nsog\\n\\nTVöD\\n\\nu.a.\\n\\nUSV\\n\\nusw\\n\\nvgl\\n\\nVPN\\n\\nz.B.\\n\\nZiff\\n\\n244\\n\\nKünstliche Intelligenz\\n\\nkleine und mittlere Unternehmen\\n\\nLarge European Artificial Intelligence Models\\n\\nLEAM KI-Servicezentrum\\n\\nMillionen\\n\\nMachine Learning\\n\\nMilliarden\\n\\nNatural Language Processing\\n\\nNatural Language Understanding\\n\\nohne Datum\\n\\nOrganisationseinheit\\n\\nOpen Source Software\\n\\nPerson\\n\\nPublic-Private-Partnership\\n\\nPower usage effectiveness\\n\\nRechenzentrum\\n\\nSeite\\n\\nSequence to Sequence\\n\\nsogenannte\\n\\nTarifvertrag für den öffentlichen Dienst\\n\\nunter anderem\\n\\nunterbrechungsfreie Stromversorgung\\n\\nUnd so weiter\\n\\nvergleiche\\n\\nVirtual Private Network\\n\\nzum Beispiel\\n\\nZiffer\\n\\nGroße KI-Modelle für Deutschland\\n\\n245\\n\\nV. Methodik der Machbarkeitsstudie\\n\\nIm Folgenden werden die bei der Durchführung der Machbarkeitsstudie angewandten Methoden beschrieben. Sie lassen sich in drei übergeordnete Bereiche einteilen: Literatur- und Internetrecherche, die Erhebung von Primärdaten in qualitativer und quantitativer Form sowie deren Analyse.\\n\\nZur besseren Nachvollziehbarkeit für den/die Leser:in können die einzelnen Schritte der methodischen Vorgehensweise wie folgt umrissen werden:\\n\\n1. Literatur-\\n\\nKI-Foundation-Modellen, Softwareanforderungen, Hochleistungsrechenzentren sowie organisatorische und finanzielle Rahmenbedingungen Identifikation relevanter Zielgruppen für die quantitative Online-Umfrage:\\n\\nund\\n\\nInternetrecherche\\n\\nzu\\n\\n2.\\n\\na. Kleine und mittlere KI-Unternehmen sowie KI-Initiativen b. Großunternehmen mit KI-Abteilungen c. KI-Forscher:innen\\n\\n3. Erstellung, Versendung und Auswertung einer Online-Umfrage pro Zielgruppe 4. a. Großunternehmen mit KI-Abteilungen b. Betreiber von Rechenzentren und Expert:innen von Hardware c. KI-Forscher:innen d. Sonstige (bspw. KI-Initiativen, Vertreter der Bundesländer)\\n\\n5. Erstellung mehrerer Leitfäden/ Fragenkataloge basierend auf den Zielgruppen 6. Durchführung und Auswertung der leitfadengestützten Interviews 7. Beurteilung der Machbarkeit anhand aller vorliegenden Informationen\\n\\nInternet- und Literaturrecherche Die Autor:innen der einzelnen Kapitel nutzten Internet- und Literaturrecherche als Ausgangspunkt für die Studie. Sie bietet in vielen Fällen einen geeigneten Einstieg in die Thematik. Die Erkenntnisse flossen außerdem in die Online-Umfrage sowie die Interviewleitfäden ein.\\n\\nIm Rahmen der Recherchen wurden relevante Quellen aus der Literatur genutzt, um grundlegendes Wissen ihren Eigenschaften und Besonderheiten sowie momentan existierenden Modellarten zu gewinnen. Außerdem wurde der aktuelle Entwicklungsstand und Einsatz von KI-Foundation-Modellen im internationalen Vergleich recherchiert.\\n\\nzu KI-Foundation-Modellen,\\n\\nIm Bereich der Softwareanforderungen wurden aktuelle Studien und das Expertenwissen der Autor:innen im Bereich der Foundation-Modellentwicklung herangezogen, um eine ausführliche Übersicht über die benötigte Software und das Personal zu erhalten. Darüber hinaus wurde die bestehende Literatur verwendet, um einen Überblick über bestehende Rechenzentren und die Herausforderungen für das KI-Training zu gewinnen. Schließlich bot die Recherche auch einen Einstieg in eine mögliche Governance, Gesellschaftsform sowie Finanzierungsmöglichkeiten.\\n\\nGroße KI-Modelle für Deutschland\\n\\n246\\n\\nDatenerhebung\\n\\nDie Datenerhebung bestand aus zwei Aspekten: quantitativen Online-Umfragen und qualitativen Experteninterviews. Die quantitativen Daten wurden in Form einer webbasierten Online-Umfrage erhoben und an drei Akteursgruppen versendet:\\n\\n(1) Kleine und mittlere KI-Unternehmen & KI-Start-Ups, (2) Großunternehmen und (3) KI-Wissenschaftler:innen.\\n\\nDas Ziel der qualitativen Datenerhebung war der Gewinn eines allgemeinen Überblicks über den wirtschaftlichen Bedarf von KI- Foundation-Modellen und deren technischen Voraussetzungen. Die qualitativen, durch wissenschaftlichen und leitfadengestützten Interviews wurden mit ausgewählten Expert:innen aus den Bereichen Wissenschaft, Wirtschaft, Politik und Recht durchgeführt. In den Interviews konnte tiefer auf Erkenntnisse aus der quantitativen Umfrage sowie neu aufkommende Themen eingegangen werden.\\n\\nOnline-Umfragen Die Online-Umfrage wurde maßgeblich aus der Internet- und Literaturrecherche abgeleitet und dient dazu, wesentliche Faktoren wie beispielsweise die Relevanz, Nutzung und Entwicklung verschiedener KI-Foundation-Modelle quantitativ messbar zu machen. Die Befragung fand vom 31. Oktober bis zum 23. November 2022 statt.\\n\\nDie drei Fragenkataloge wurden von den Projektpartnern der Machbarkeitsstudie unter Koordination des KI Bundesverbands ausgearbeitet. Sie bestehen aus 20 - 46 Fragen (s. Tabelle 22), die überwiegend in Multiple-Choice-Form aufgebaut sind. Bei der Auswertung ihrer Ergebnisse ist somit zu berücksichtigen, dass pro Frage meist mehrere Antwortmöglichkeiten gegeben waren. Zusätzlich zu den Multiple-Choice Fragen wurden demographische Angaben erfragt und zum Schluss durch ein offenes Kommentarfeld die Möglichkeit gegeben, weitere Anmerkungen zu den Anforderungen von KI-Foundation- Modellen oder zu der Machbarkeitsstudie zu machen. Im Anhang A.1 sind die Umfragen im Detail zu finden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n247\\n\\nUm eine hohe quantitative Fallzahl zu erlangen, wurden die drei Umfragen an ein breites Spektrum an Kontakten gesendet. Insgesamt gab es 71 Rückmeldungen von den kleinen und mittleren KI-Unternehmen und 21 Rückmeldungen von KI-Wissenschaftler:innen. Aufgrund der geringen Rücklaufquote bei den Großunternehmen wurden ausgewählte Antwortpartner:innen der Online-Umfrage zusätzlich um Interviews gebeten. Somit liegt bei dieser Auswertung der Fokus auf den Interviews.\\n\\nTabelle 22: Übersicht über die drei Online-Umfragen\\n\\nGroße KI-Modelle für Deutschland\\n\\n248\\n\\nExperteninterviews Den Interviews lagen Leitfäden zu Grunde, die eine offene Beantwortung der Fragen vorsahen. Dabei gab es für jede Zielgruppe eigene Leitfäden mit Fokus auf deren Expertise. Die Interviews wurden per Videokonferenz durchgeführt. Der Zeitraum der Befragung war vom 25. Oktober 2022 bis zum 12. Januar 2023.\\n\\nBei der Auswahl der Interviewpartner:innen wurde zunächst eine Identifikation und Einstellung relevanter Akteursgruppen vorgenommen. Daraus hervorgehend wurden die folgenden Gruppen bzw. Institutionen herausgestellt:\\n\\nGroßunternehmen\\n\\nKI-Forscher:innen\\n\\nRechenzentrums- und Hardwareexpert:innen\\n\\nSonstige Expert:innen (bspw. regionale Vertreter, KI-Initiativen, etc.)\\n\\nEs wurden insgesamt Interviews mit 71 Gesprächspartner:innen durchgeführt. Die vollständige Liste der Interviewpartner:innen befindet sich im Anhang B.1.\\n\\nDie Auswertung der unterschiedlichen Daten wurde von den verantwortlichen Projektpartner:innen unternommen und mit den Ergebnissen der Literatur- und Internetrecherche verbunden.\\n\\nGroße KI-Modelle für Deutschland\\n\\n249\\n\\nAuswertung der Online-Umfragen Zur Auswertung der quantitativen Datenerhebung wurden die Ergebnisse der Online- Umfrage von KI-Unternehmen/KI-Initiativen, auf der einen Seite, und KI-Forscher:innen, auf der anderen, jeweils gesammelt, ausgewertet und grafisch aufbereitet. Die offenen Fragen wurden zur Quantifizierbarkeit auf spezifische Schlüsselwörter durchsucht. Eine grafische Aufarbeitung der Multiple-Choice Fragen befindet sich in Anhang A.2. Aufgrund der geringen Teilnehmerzahl fand keine Auswertung der Umfrage an Großunternehmen statt.\\n\\nAuswertung der Experteninterviews Um die qualitativen Daten in Form der leitfadengestützten Interviews auszuwerten, wurden die Interviews transkribiert und in Ergebnisprotokollen zusammengefasst. Im Falle der KI-Forscher:innen wurden die Ergebnisse in einer Interviewmatrix für eine interne Auswertung zusammengefasst. Die Interviewten aus der Wirtschaft wurden gebeten, die Ergebnisprotokolle freizugeben. Die bis zum Redaktionsschluss freigegebenen Ergebnisprotokolle befinden sich im Anhang B.3.\\n\\nfür den Druck\\n\\nGroße KI-Modelle für Deutschland\\n\\n250\\n\\nD O W N L O A D D E R K O M P L E T T E N S T U D I E M I T A L L E N A N H Ä N G E N U N T E R:\\n\\nhttps://leam.ai/feasibility-study-leam-2023/\\n\\nLEAM Machbarkeitsstudie V1/01_2023\\n\\nGroße KI-Modelle für Deutschland\\n\\n251\\n\\nAnhang der LEAM Machbarkeitsstudie\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n252\\n\\nAnhang\\n\\nAnhang A - Zusätzliche Information zu den Umfragen\\n\\nAnhang A.1 - Die Umfragen\\n\\nUmfrage 1: Kleine und mittlere Unternehmen\\n\\nFoundation-Modelle\\n\\n1. Arbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n2. Inwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation-Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\na. Welche Voraussetzungen müssen erfüllt werden, damit Sie Foundation-\\n\\nModelle nutzen würden?\\n\\nb. Inwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation-\\n\\nModellen? Inwiefern passen Sie existierende Foundation-Modelle an (Tuning)?\\n\\nc. d. Inwiefern entwickeln Sie eigene Foundation-Modelle?\\n\\n3. In welchen Bereichen sind für Sie Foundation-Modelle in Nutzung und Entwicklung relevant?\\n\\nGesamtwirtschaftliche Bedeutung\\n\\n4. Welche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n5. Welche Bedeutung messen Sie den Foundation-Modellen für die KI und damit für gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n6. Wie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\nFoundation-Modell Entwicklung\\n\\n7. Welche Bedeutung hat für Sie die Verfügbarkeit von Foundation-Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n8. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenschaftlichen Institutionen?\\n\\n9. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Start- ups?\\n\\n10. Haben Sie weitere Anmerkungen zu den Themen Bedeutung von, Gefahren durch und wissenschaftliche Fragestellungen zu Foundation-Modellen?\\n\\nTechnologische Anforderungen\\n\\n11. Rechenleistung: Wie viele GPUs und andere Beschleuniger werden z.B. für Training und Inferenz von state-of-the-art Foundation-Modellen benötigt?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n253\\n\\n12. Verfügbarkeit: Welche Kriterien müssen Service Level Agreements (SLA) erfüllen, um die Entwicklung von Foundation-Modellen möglich zu machen?\\n\\n13. Anforderungen der Entwicklung von Foundation-Modellen an die Software- Infrastruktur\\n\\n14. Scheduling Infrastructure: Welche Scheduling Infrastructures sollen bei der Entwicklung eingesetzt werden?\\n\\n15. Deployment Infrastructure: Welche Deployment Infrastructures müssen bei der Entwicklung unterstützt werden?\\n\\n16. Access Control Levels & Datenhoheit: Wie soll das Laden der Daten in der Entwicklung angesteuert werden?\\n\\n17. Anforderungen der Entwicklung von Foundation-Modellen an die Trainings- Software\\n\\n18. Frameworks: Welche Frameworks sollen bei der Entwicklung von Foundation- Modellen eingesetzt werden?\\n\\n19. Inference: Welche Voraussetzungen sollen Service Level Agreements (SLAs) mindestens erfüllen?\\n\\na. Inference: Welche Voraussetzungen sollen Service Level Agreements\\n\\n(SLAs) bei der Verfügbarkeit mindestens erfüllen?\\n\\nb. Inference: Welche Voraussetzungen sollen Service Level Agreements\\n\\n(SLAs) bei der erlaubten Latenz mindestens erfüllen?\\n\\n20. Anforderungen der Entwicklung von Foundation-Modellen an den Data-Storage- Layer\\n\\n21. Internet- & Bandbreite: Wie schnell muss die Internetverbindung für die Entwicklung von Foundation-Modellen mindestens sein?\\n\\n22. Speicherplatz für Daten: Wie hoch ist der Speicherbedarf für die Entwicklung eines Foundation-Modells?\\n\\n23. Compliance: Welche regulatorischen Voraussetzungen müssen erfüllt werden, um Foundation-Modelle zu entwickeln?\\n\\na. Können sie die ISO/Normen benennen, die beachtet werden müssen?\\n\\nAnforderungen der Entwicklung von Foundation-Modellen an Daten\\n\\n24. Beschäftigen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n25. Wissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n26. Ist Ihnen das Konzept der Model Cards / Data Set Cards im Bezug zur Erstellung neuer Daten geläufig?\\n\\na. Ist das Konzept für Ihre Domäne relevant oder hilfreich?\\n\\nAnforderungen der Entwicklung von Foundation-Modellen an Mitarbeiter:innen\\n\\n27. Welche Personalrollen (oder äquivalent) sind für die Entwicklung von Foundation- Modellen erforderlich?\\n\\n28. In welchen dieser Personalrollen beobachten oder antizipieren Sie einen Mangel, um Foundation-Modelle zu entwickeln?\\n\\nWirtschaftliche Anforderungen\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n254\\n\\n29. Welche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n30. Welche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n31. In welchem Zeitraum erwarten Sie, diese Investitionskosten zu amortisieren? 32. Wie erwarten Sie, dass sich diese Investitionskosten amortisieren könnten?\\n\\nZivilgesellschaftliche Anforderungen\\n\\n33. Wie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n34. Empfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n35. Wie sollten Ihrer Meinung nach Datensätze für die Entwicklung von Foundation- Modellen erhoben werden?\\n\\n36. Wie hoch empfinden Sie Awareness, Relevanz und Akzeptanz bzgl. Foundation- Modellen innerhalb Ihrer Organisation?\\n\\n37. Haben Sie weitere Anmerkungen zu den technologischen, wirtschaftlichen und zivilgesellschaftlichen Anforderungen der Entwicklung von KI-Foundation- Modellen?\\n\\nMetadaten\\n\\n38. Was ist Ihr Name? 39. Was ist Ihre E-Mail Adresse? 40. Wie groß ist Ihr Unternehmen? 41. Welcher Branche gehört Ihr Unternehmen an? 42. Was ist Ihre Position im Unternehmen? 43. Stehen Sie für einen weiteren Austausch zu den Themen Foundation-Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\nEnde\\n\\n44. Abschließend möchten wir Ihnen die Gelegenheit bieten, uns Reaktionen, Ideen, Wünsche, Prioritäten oder Warnungen mitzuteilen, die für unsere weitere Arbeit wichtig sein könnten.\\n\\nUmfrage 2: Große Unternehmen\\n\\nFoundation-Modelle\\n\\n1. Inwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation-Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\nNutzung\\n\\n2. In welchen Bereichen sind für Sie Foundation-Modelle in Nutzung und Entwicklung relevant?\\n\\n3. Inwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation- Modellen?\\n\\n4. Inwiefern passen Sie existierende Foundation-Modelle an (Tuning)?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n255\\n\\n5. Inwiefern entwickeln Sie eigene Foundation-Modelle?\\n\\nGesamtwirtschaftliche Bedeutung\\n\\n6. Welche Bedeutung messen Sie der Datenanalyse und KI gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n7. Welche Bedeutung messen Sie den Foundation-Modellen für die KI und damit für gesamtwirtschaftliche Entwicklung in Deutschland zu?\\n\\n8. Wie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\nFragen der Entwicklung\\n\\n9. Welche Bedeutung hat für Sie die Verfügbarkeit von Foundation-Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n10. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation- Modellen, die Zusammenarbeit mit wissenschaftlichen Institutionen?\\n\\n11. Welche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation- Modellen, die Zusammenarbeit mit spezialisierten KMUs und Start-ups?\\n\\nLEAM Ziel der LEAM Initiative ist der Aufbau eines dedizierten KI-Servicezentrums für die Wissenschaft und Industrie zur Erstellung von Foundation-Modellen und der Entwicklung darauf aufbauender Anwendungen.\\n\\n12. Inwiefern würden Sie die Services von LEAM nutzen? 13. Inwiefern würden Sie bzw. Ihr Unternehmen in den Aufbau eines LEAM KI- Servicezentrums investieren (unter noch zu klärenden Voraussetzungen)?\\n\\nMetadaten\\n\\n14. Wie groß ist Ihr Unternehmen? 15. Welcher Branche gehört Ihr Unternehmen an? 16. Was ist Ihre Position im Unternehmen? 17. Was ist Ihr Name? 18. Was ist Ihre E-Mail Adresse? 19. Stehen Sie für einen weiteren Austausch zu den Themen Foundation-Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n20. Welche Anregungen, Ideen oder auch Kritikpunkte möchten Sie uns mitgeben?\\n\\nUmfrage 3: Wissenschaft\\n\\nPersönliche Daten\\n\\n1. Bitte füllen Sie Ihre persönlichen Daten aus\\n\\nFoundation-Modelle\\n\\n2. Wie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wissenschaft ein?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n256\\n\\n3. Wie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wirtschaft ein?\\n\\n4. Arbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n5. Arbeiten Sie bereits an oder wollen Sie mit Foundation-Modellen arbeiten?\\n\\nKönnen Sie uns darüber etwas erzählen? Welche Daten werden in Ihrer Organisation aktuell für das Training von Foundation-Modellen verwendet?\\n\\nWas hindert Sie daran, bereits heute Foundation-Modelle einzusetzen? ○ Wie hoch schätzen Sie die Wahrscheinlichkeit ein, Foundation-Modelle für Ihre Arbeit zu nutzen?\\n\\n6. Wie schätzen Sie die Bedeutung verschiedener Arten von Foundation-Modellen in Gegenwart und Zukunft ein?\\n\\n7. Gibt es weitere Arten von Foundation-Modellen, die Sie als bedeutend einschätzen?\\n\\n8. Wie schätzen Sie die Gefahren ein, die mitunter im Zusammenhang mit Foundation-Modellen genannt wurden?\\n\\n9. Sehen Sie zusätzliche Arten von potentiellen Gefahren? 10. Fast alle existierenden Foundation-Modelle, deren Architekturen und alle anderen wichtigen Neuerungen auf dem Gebiet kommen aus den USA und aus China. Warum? Was fehlt uns? Wo stehen wir in Deutschland im Vergleich in Bezug auf notwendige Voraussetzungen?\\n\\n11. Sehen Sie weitere Voraussetzungen, die uns fehlen? 12. Welche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zur Architektur?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Daten und Datenverarbeitung?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zum Pre-Training?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zum Fine-Tuning (Nachtraining)?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Multimedia?\\n\\nWelche Wichtigkeit haben Ihrer Meinung nach die folgenden wissenschaftlichen Fragestellungen zu Sonstigen Themen?\\n\\n13. Welche weiteren relevanten Fragestellungen sehen Sie für die Forschung - auch besonders in Ihrem eigenen Fachgebiet? (Siehe Beispiele)\\n\\n14. Wie schätzen Sie die Bedeutung von Foundation-Modellen für die folgenden Anwendungen ein?\\n\\n15. Sehen Sie weitere Anwendungen, für die Foundation-Modelle bedeutend sein werden?\\n\\n16. Haben Sie weitere Anmerkungen zu den Themen Bedeutung von, Gefahren durch und wissenschaftliche Fragestellungen zu Foundation-Modellen?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n257\\n\\nBias und Diskriminierung\\n\\n17. Beschäftigen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n18. Wissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\nWirtschaftliche Anforderungen\\n\\n19. Welche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n20. Welche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\nZivilgesellschaftliche Anforderungen\\n\\n21. Wie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n22. Empfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\nEnde\\n\\n23. Abschließend möchten wir Ihnen die Gelegenheit bieten, uns Reaktionen, Ideen, Wünsche, Prioritäten oder Warnungen mitzuteilen, die für unsere weitere Arbeit wichtig sein könnten.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n258\\n\\nAnhang A.2 - Graphische Auswertung der Umfragen\\n\\nDie detaillierte Auswertung der Umfragen sind zu finden unter:\\n\\nUmfrage 1: Kleine und Mittlere Unternehmen\\n\\nhttps://form.typeform.com/report/VoH1BI5d/WHsdAKz0nUHNr76M\\n\\nUmfrage 2: Große Unternehmen\\n\\nhttps://form.typeform.com/report/eDjpXmDx/RVEnkGRn1K7fhNja\\n\\nUmfrage 3: Wissenschaft\\n\\nhttps://form.typeform.com/report/G3MBzf8d/iG33eUmp26emWKiC\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n259\\n\\n259\\n\\nLEAM Umfrage KMU\\n\\n71 Antworten\\n\\nArbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n71\\n\\nWir setzen bereits KI bzw. darauf basierende Applikationen ein\\n\\n64 Antw. 90.1%\\n\\nWir planen aktuell, KI bzw. darauf basierende Applikationen einzusetzen\\n\\n4 Antw. 5.6%\\n\\nWir beabsichtigen aktuell nicht, KI bzw. darauf basierende Applikationen einzusetzen\\n\\n2 Antw. 2.8%\\n\\nWir haben angefangen, uns mit KI bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\n1 Antw. 1.4%\\n\\nWir kennen das Thema noch nicht\\n\\n0%\\n\\n0 Antw.\\n\\nInwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation Modellen oder darauf basierenden Applikationen auseinandergesetzt\\n\\n71 von 71 Personen haben diese Frage beantwortet\\n\\nGroße KI-Modelle für Deutschland\\n\\nWir setzen bereits Foundation Modelle bzw. darauf basierende Applikationen ein\\n\\nWir beabsichtigen aktuell nicht, Foundation Modelle bzw. darauf basierende Applikationen einzusetzen\\n\\nWir planen aktuell, Foundation Modellen bzw. darauf basierende Applikationen einzusetzen\\n\\nWir haben angefangen, uns mit Foundation Modellen bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\nWir kennen das Thema noch nicht\\n\\n36 Antw. 50.7%\\n\\n13 Antw. 18.3%\\n\\n11 Antw. 15.5%\\n\\n9 Antw. 12.7%\\n\\n2 Antw. 2.8%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n261\\n\\nWelche Voraussetzungen müssen erfüllt werden, damit Sie Foundation-Modelle nutzen würden?\\n\\n24\\n\\nNiedrige Kosten\\n\\n14 Antw. 58.3%\\n\\nOpen-Source Modelle\\n\\n14 Antw. 58.3%\\n\\nVerfügbare Daten\\n\\n14 Antw. 58.3%\\n\\nAusreichend Recheninfrastruktur\\n\\n9 Antw. 37.5%\\n\\nHoher Datenschutz beim Einsatz der Modelle\\n\\n8 Antw. 33.3%\\n\\nQualifizierte Mitarbeiter\\n\\n25%\\n\\n6 Antw.\\n\\nVerfügbare europäische Modelle\\n\\n25%\\n\\n6 Antw.\\n\\nOther\\n\\n5 Antw. 20.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n262\\n\\nInwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation Modellen?\\n\\n56\\n\\nIn der Entwicklung\\n\\n25 Antw. 44.6%\\n\\nBereits im produktiven Einsatz\\n\\n17 Antw. 30.4%\\n\\nIn Planung\\n\\n5 Antw. 8.9%\\n\\nKeine Planungen vorhanden\\n\\n5 Antw. 8.9%\\n\\nIn der Evaluation\\n\\n4 Antw. 7.1%\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n263\\n\\nInwiefern passen Sie existierende Foundation Modelle an (Tuning)?\\n\\n56\\n\\nIn der Entwicklung\\n\\n21 Antw. 37.5%\\n\\nBereits im produktiven Einsatz\\n\\n15 Antw. 26.8%\\n\\nIn der Evaluation\\n\\n8 Antw. 14.3%\\n\\nIn Planung\\n\\n6 Antw. 10.7%\\n\\nKeine Planungen vorhanden\\n\\n5 Antw. 8.9%\\n\\nGenerell nicht interessant\\n\\n1 Antw. 1.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n264\\n\\nInwiefern entwickeln Sie eigene Foundation Modelle?\\n\\n56\\n\\nKeine Planungen vorhanden\\n\\n25 Antw. 44.6%\\n\\nIn der Evaluation\\n\\n9 Antw. 16.1%\\n\\nBereits im produktiven Einsatz\\n\\n6 Antw. 10.7%\\n\\nGenerell nicht interessant\\n\\n6 Antw. 10.7%\\n\\nIn Planung\\n\\n6 Antw. 10.7%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nIn der Entwicklung\\n\\n4 Antw. 7.1%\\n\\nIn welchen Bereichen sind für Sie Foundation Modelle in Nutzung und Entwicklung relevant?\\n\\n56\\n\\nSprachmodelle\\n\\n40 Antw. 71.4%\\n\\nMultilinguale Sprachmodelle\\n\\n29 Antw. 51.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nMultimodale Modelle\\n\\nGeschä!s- und Fertigungsprozesse\\n\\nRobotik\\n\\nJura und Recht\\n\\nBiomed (Omiks)\\n\\nMaterialwissenscha!\\n\\nAusbildung/Pädagogik\\n\\nChemie\\n\\nKlima/Meteorologie\\n\\nAstronomie\\n\\nGeologie\\n\\nOther\\n\\n21 Antw. 37.5%\\n\\n19 Antw. 33.9%\\n\\n11 Antw. 19.6%\\n\\n8 Antw. 14.3%\\n\\n7 Antw. 12.5%\\n\\n4 Antw. 7.1%\\n\\n3 Antw. 5.4%\\n\\n3 Antw. 5.4%\\n\\n2 Antw. 3.6%\\n\\n1 Antw. 1.8%\\n\\n0 Antw.\\n\\n0%\\n\\n10 Antw. 17.9%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n266\\n\\nWelche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n71\\n\\nGroße Bedeutung\\n\\n64 Antw. 90.1%\\n\\nMittlere Bedeutung\\n\\n7 Antw. 9.9%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nNiedrige Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n267\\n\\nWelche Bedeutung messen Sie den Foundation Modellen für die KI und damit für gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n56\\n\\nGroße Bedeutung\\n\\n41 Antw. 73.2%\\n\\nMittlere Bedeutung\\n\\n13 Antw. 23.2%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 3.6%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n268\\n\\nWie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\n56\\n\\nGroße Bedeutung\\n\\n46 Antw. 82.1%\\n\\nMittlere Bedeutung\\n\\n6 Antw. 10.7%\\n\\nKeine Bedeutung\\n\\n2 Antw. 3.6%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 3.6%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n269\\n\\nWelche Bedeutung hat für Sie die Verfügbarkeit von Foundation Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n46\\n\\nGroße Bedeutung\\n\\n39 Antw. 84.8%\\n\\nMittlere Bedeutung\\n\\n4 Antw. 8.7%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 4.3%\\n\\nKeine Bedeutung\\n\\n1 Antw. 2.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n270\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenscha!lichen Institutionen?\\n\\n46\\n\\nGroße Bedeutung\\n\\n24 Antw. 52.2%\\n\\nMittlere Bedeutung\\n\\n18 Antw. 39.1%\\n\\nNiedrige Bedeutung\\n\\n3 Antw. 6.5%\\n\\nKeine Bedeutung\\n\\n1 Antw. 2.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n271\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Startups?\\n\\n46\\n\\nGroße Bedeutung\\n\\n31 Antw. 67.4%\\n\\nMittlere Bedeutung\\n\\n11 Antw. 23.9%\\n\\nKeine Bedeutung\\n\\n2 Antw. 4.3%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 4.3%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n272\\n\\nRechenleistung: Wie viele GPUs und andere Beschleuniger werden z.B. für Training und Inferenz von state-of-the-art Foundation-Modellen benötigt?\\n\\n15\\n\\n>100\\n\\n7 Antw. 46.7%\\n\\n5-10\\n\\n4 Antw. 26.7%\\n\\n1-4\\n\\n2 Antw. 13.3%\\n\\n10-49\\n\\n2 Antw. 13.3%\\n\\n50-100\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n273\\n\\nVerfügbarkeit: Welche Kriterien müssen Service Level Agreements (SLA) erfüllen, um die Entwicklung von Foundation-Modellen möglich zu machen?\\n\\n16\\n\\nOn-demand\\n\\n50%\\n\\n8 Antw.\\n\\nPre-ordered <24h\\n\\n6 Antw. 37.5%\\n\\nPre-ordered <60min\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n274\\n\\nScheduling Infrastructure: Welche Scheduling Infrastructures sollen bei der Entwicklung eingesetzt werden?\\n\\n15\\n\\nRay\\n\\n10 Antw. 66.7%\\n\\nkubeflow\\n\\n60%\\n\\n9 Antw.\\n\\nSLURM\\n\\n40%\\n\\n6 Antw.\\n\\nOther\\n\\n20%\\n\\n3 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n275\\n\\nDeployment Infrastructure: Welche Deployment Infrastructures müssen bei der Entwicklung unterstützt werden?\\n\\n16\\n\\nDocker\\n\\n15 Antw. 93.8%\\n\\nKubernetes\\n\\n10 Antw. 62.5%\\n\\nTerraform\\n\\n25%\\n\\n4 Antw.\\n\\nFlyter\\n\\n2 Antw. 12.5%\\n\\nSeldon\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n276\\n\\nAccess Control Levels & Datenhoheit: Wie soll das Laden der Daten in der Entwicklung angesteuert werden?\\n\\n16\\n\\nDMZ + VPN\\n\\n11 Antw. 68.8%\\n\\nZentral, temporär gelagerte Daten\\n\\n6 Antw. 37.5%\\n\\nEnclave computing\\n\\n3 Antw. 18.8%\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n277\\n\\nFrameworks: Welche Frameworks sollen bei der Entwicklung von Foundation-Modellen eingesetzt werden?\\n\\n16\\n\\nTensorFlow\\n\\n14 Antw. 87.5%\\n\\nPyTorch\\n\\n75%\\n\\n12 Antw.\\n\\nKeras\\n\\n9 Antw. 56.2%\\n\\nJax\\n\\n2 Antw. 12.5%\\n\\nMXNet\\n\\n1 Antw. 6.2%\\n\\nFlux\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n278\\n\\nInternet- & Bandbreite: Wie schnell muss die Internetverbindung für die Entwicklung von Foundation- Modellen mindestens sein?\\n\\n16\\n\\n>100 Gbit/s\\n\\n50%\\n\\n8 Antw.\\n\\n< 1 Gbit/s\\n\\n3 Antw. 18.8%\\n\\n< 100 Gbit/s\\n\\n3 Antw. 18.8%\\n\\n< 10 Gbit/s\\n\\n2 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n279\\n\\nSpeicherplatz für Daten: Wie hoch ist der Speicherbedarf für die Entwicklung eines Foundation- Modells?\\n\\n15\\n\\n10-100 TB\\n\\n5 Antw. 33.3%\\n\\n>1 PB\\n\\n4 Antw. 26.7%\\n\\n1-10 TB\\n\\n20%\\n\\n3 Antw.\\n\\n100-1000 TB\\n\\n20%\\n\\n3 Antw.\\n\\n< 1 TB\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n280\\n\\nCompliance: Welche regulatorischen Voraussetzungen müssen erfüllt werden, um Foundation- Modelle zu entwickeln?\\n\\n14\\n\\nISO/Normen\\n\\n9 Antw. 64.3%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nHIPAA\\n\\n4 Antw. 28.6%\\n\\nSOC 3\\n\\n4 Antw. 28.6%\\n\\nOther\\n\\n2 Antw. 14.3%\\n\\nBeschä!igen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n15\\n\\nJa\\n\\n8 Antw. 53.3%\\n\\nNein\\n\\n7 Antw. 46.7%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n281\\n\\nWissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n9\\n\\nJa\\n\\n6 Antw. 66.7%\\n\\nNein\\n\\n3 Antw. 33.3%\\n\\nIst Ihnen das Konzept der Model Cards / Data Set Cards im Bezug zur Erstellung neuer Daten geläufig?\\n\\n16\\n\\nJa\\n\\n9 Antw. 56.2%\\n\\nNein\\n\\n7 Antw. 43.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n282\\n\\nIst das Konzept für Ihre Domäne relevant oder hilfreich?\\n\\n9\\n\\nJa\\n\\n7 Antw. 77.8%\\n\\nNein\\n\\n2 Antw. 22.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n283\\n\\nWelche Personalrollen (oder äquivalent) sind für die Entwicklung von Foundation-Modellen erforderlich?\\n\\n16\\n\\nDevOps, MLOps\\n\\n14 Antw. 87.5%\\n\\nMachine Learning Engineer\\n\\n14 Antw. 87.5%\\n\\nMachine Learning Researcher\\n\\n13 Antw. 81.2%\\n\\nSo!ware Engineer\\n\\n9 Antw. 56.2%\\n\\nSysAdmin\\n\\n9 Antw. 56.2%\\n\\nSite Reliability Engineer\\n\\n5 Antw. 31.2%\\n\\nOther\\n\\n1 Antw. 6.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n284\\n\\nIn welchen dieser Personalrollen beobachten oder antizipieren Sie einen Mangel, um Foundation- Modelle zu entwickeln?\\n\\n13\\n\\nMachine Learning Engineer\\n\\n8 Antw. 61.5%\\n\\nDevOps, MLOps\\n\\n6 Antw. 46.2%\\n\\nMachine Learning Researcher\\n\\n6 Antw. 46.2%\\n\\nSo!ware Engineer\\n\\n6 Antw. 46.2%\\n\\nSite Reliability Engineer\\n\\n3 Antw. 23.1%\\n\\nSysAdmin\\n\\n3 Antw. 23.1%\\n\\nOther\\n\\n2 Antw. 15.4%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n285\\n\\nWelche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n16\\n\\n5-10 Mio. Euro\\n\\n5 Antw. 31.2%\\n\\n> 50 Mio. Euro\\n\\n5 Antw. 31.2%\\n\\n25-50 Mio. Euro\\n\\n25%\\n\\n4 Antw.\\n\\n1-5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n10-25 Mio. Euro\\n\\n0%\\n\\n0 Antw.\\n\\n<1 Mio. Euro\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n286\\n\\nWelche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n16\\n\\n> 5 Mio. Euro\\n\\n7 Antw. 43.8%\\n\\n1-3 Mio. Euro\\n\\n25%\\n\\n4 Antw.\\n\\n3-5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n<0.5 Mio. Euro\\n\\n2 Antw. 12.5%\\n\\n0,5-1 Mio. Euro\\n\\n1 Antw. 6.2%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n287\\n\\nIn welchem Zeitraum erwarten Sie, diese Investitionskosten zu amortisieren?\\n\\n16\\n\\n5-10 Jahre\\n\\n6 Antw. 37.5%\\n\\n1-3 Jahre\\n\\n5 Antw. 31.2%\\n\\n>10 Jahre\\n\\n3 Antw. 18.8%\\n\\n3-5 Jahre\\n\\n2 Antw. 12.5%\\n\\n< 1 Jahr\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n288\\n\\nWie erwarten Sie, dass sich diese Investitionskosten amortisieren könnten?\\n\\n16\\n\\nMaßgebliche Entwicklung eines neuen Marktsegments\\n\\n11 Antw. 68.8%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nLeasing des KI-Modells an andere Entwickler\\n\\n10 Antw. 62.5%\\n\\nEroberung eines maßgeblichen Marktanteils existierender Marktsegmenten\\n\\n9 Antw. 56.2%\\n\\nOther\\n\\n0%\\n\\n0 Antw.\\n\\nWie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n16\\n\\nNeutral, es gibt Unklarheiten\\n\\n7 Antw. 43.8%\\n\\nSchlecht\\n\\n5 Antw. 31.2%\\n\\nGut, keine Unklarheiten\\n\\n25%\\n\\n4 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n289\\n\\nEmpfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n10\\n\\nEnabler\\n\\n70%\\n\\n7 Antw.\\n\\nDisabler\\n\\n30%\\n\\n3 Antw.\\n\\nWie sollten Ihrer Meinung nach Datensätze für die Entwicklung von Foundation-Modellen erhoben werden?\\n\\n16\\n\\nDonation\\n\\n13 Antw. 81.2%\\n\\nScraping\\n\\n75%\\n\\n12 Antw.\\n\\nCentralized Third-Party collection\\n\\n50%\\n\\n8 Antw.\\n\\nOther\\n\\n3 Antw. 18.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n290\\n\\nWie hoch empfinden Sie Awareness, Relevanz und Akzeptanz bzgl. Foundation-Modellen innerhalb Ihrer Organisation?\\n\\n16\\n\\nHoch\\n\\n9 Antw. 56.2%\\n\\nNeutral\\n\\n6 Antw. 37.5%\\n\\nNiedrig\\n\\n1 Antw. 6.2%\\n\\nWeiß ich nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n291\\n\\nWie groß ist Ihr Unternehmen?\\n\\n71\\n\\n< 100 Mitarbeiter\\n\\n64 Antw. 90.1%\\n\\n100-1000 Mitarbeiter\\n\\n6 Antw. 8.5%\\n\\n1000-10.000 Mitarbeiter\\n\\n1 Antw. 1.4%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n10.000 -100.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\n> 100.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nWelcher Branche gehört Ihr Unternehmen an?\\n\\n71\\n\\nGroße KI-Modelle für Deutschland\\n\\nInformationstechnologie\\n\\nDienstleistung\\n\\nIndustrie\\n\\nMedien\\n\\nFinanzwirtscha!\\n\\nGesundheit\\n\\nHandel\\n\\nMedien\\n\\nAutomobil\\n\\nImmobilien\\n\\nTourismus\\n\\nOther\\n\\n56 Antw. 78.9%\\n\\n18 Antw. 25.4%\\n\\n8 Antw. 11.3%\\n\\n8 Antw. 11.3%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n3 Antw. 4.2%\\n\\n2 Antw. 2.8%\\n\\n1 Antw. 1.4%\\n\\n1 Antw. 1.4%\\n\\n8 Antw. 11.3%\\n\\nvon 71 Personen haben diese Frage beantwortet\\n\\n293\\n\\nStehen Sie für einen weiteren Austausch zu den Themen Foundation Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n71\\n\\nJa\\n\\n61 Antw. 85.9%\\n\\nNein\\n\\n10 Antw. 14.1%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n294\\n\\nLEAM Umfrage Große Unternehmen\\n\\n10 Antworten\\n\\nInwiefern haben Sie sich in Ihrem Unternehmen bereits mit Foundation Modellen oder darauf basierenden Applikationen auseinandergesetzt?\\n\\n10\\n\\nWir setzen bereits Foundation Modelle bzw. darauf basierende Applikationen ein\\n\\n50%\\n\\n5 Antw.\\n\\nWir haben angefangen, uns mit Foundation Modellen bzw. darauf basierenden Applikationen auseinanderzusetzen\\n\\n30%\\n\\n3 Antw.\\n\\nWir beabsichtigen aktuell nicht, Foundation Modelle bzw. darauf basierende Applikationen einzusetzen\\n\\n10%\\n\\n1 Antw.\\n\\nWir planen aktuell, Foundation Modellen bzw. darauf basierende Applikationen einzusetzen\\n\\n10%\\n\\n1 Antw.\\n\\nWir kennen das Thema noch nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n295\\n\\nIn welchen Bereichen sind für Sie Foundation Modelle in Nutzung und Entwicklung relevant?\\n\\n8\\n\\nGeschä!s- und Fertigungsprozesse\\n\\n75%\\n\\n6 Antw.\\n\\nMultilinguale Sprachmodelle\\n\\n75%\\n\\n6 Antw.\\n\\nSprachmodelle\\n\\n75%\\n\\n6 Antw.\\n\\nMultimodale Modelle\\n\\n50%\\n\\n4 Antw.\\n\\nRobotik\\n\\n25%\\n\\n2 Antw.\\n\\nBiomed (Omiks)\\n\\n0%\\n\\n0 Antw.\\n\\nChemie\\n\\n0%\\n\\n0 Antw.\\n\\nKlima/Meteorologie\\n\\n0%\\n\\n0 Antw.\\n\\nMaterialwissenscha!\\n\\n0%\\n\\n0 Antw.\\n\\nOther\\n\\n1 Antw. 12.5%\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n296\\n\\nInwiefern entwickeln Sie eigene Anwendungen auf Basis von Foundation Modellen?\\n\\n10\\n\\nBereits im produktiven Einsatz\\n\\n40%\\n\\n4 Antw.\\n\\nIn der Entwicklung\\n\\n30%\\n\\n3 Antw.\\n\\nKeine Planungen vorhanden\\n\\n20%\\n\\n2 Antw.\\n\\nIn Planung\\n\\n10%\\n\\n1 Antw.\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nIn der Evaluation\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n297\\n\\nInwiefern passen Sie existierende Foundation Modelle an (Tuning)?\\n\\n9\\n\\nBereits im produktiven Einsatz\\n\\n3 Antw. 33.3%\\n\\nIn der Entwicklung\\n\\n3 Antw. 33.3%\\n\\nKeine Planungen vorhanden\\n\\n2 Antw. 22.2%\\n\\nIn der Evaluation\\n\\n1 Antw. 11.1%\\n\\nGenerell nicht interessant\\n\\n0%\\n\\n0 Antw.\\n\\nIn Planung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n298\\n\\nInwiefern entwickeln Sie eigene Foundation Modelle?\\n\\n10\\n\\nKeine Planungen vorhanden\\n\\n50%\\n\\n5 Antw.\\n\\nIn der Entwicklung\\n\\n20%\\n\\n2 Antw.\\n\\nGenerell nicht interessant\\n\\n10%\\n\\n1 Antw.\\n\\nIn der Evaluation\\n\\n10%\\n\\n1 Antw.\\n\\nIn Planung\\n\\n10%\\n\\n1 Antw.\\n\\nBereits im produktiven Einsatz\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n299\\n\\nWelche Bedeutung messen Sie der Datenanalyse und KI für die gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n10\\n\\nGroße Bedeutung\\n\\n90%\\n\\n9 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nMittlere Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n300\\n\\nWelche Bedeutung messen Sie den Foundation Modellen für die KI und damit für gesamtwirtscha!liche Entwicklung in Deutschland zu?\\n\\n10\\n\\nGroße Bedeutung\\n\\n70%\\n\\n7 Antw.\\n\\nMittlere Bedeutung\\n\\n20%\\n\\n2 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n301\\n\\nWie wichtig ist aus Ihrer Sicht der Aufbau eines deutschen und europäischen Ökosystems rund um Foundation Models für die Digitale Souveränität und Wettbewerbsfähigkeit?\\n\\n9\\n\\nGroße Bedeutung\\n\\n7 Antw. 77.8%\\n\\nMittlere Bedeutung\\n\\n1 Antw. 11.1%\\n\\nNiedrige Bedeutung\\n\\n1 Antw. 11.1%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n302\\n\\nWelche Bedeutung hat für Sie die Verfügbarkeit von Foundation Modellen, die in Europa entwickelt wurden und Werte wie Transparenz, Reduktion von Bias und Nachhaltigkeit berücksichtigen?\\n\\n10\\n\\nGroße Bedeutung\\n\\n70%\\n\\n7 Antw.\\n\\nMittlere Bedeutung\\n\\n20%\\n\\n2 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n303\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit wissenscha!lichen Institutionen?\\n\\n10\\n\\nGroße Bedeutung\\n\\n60%\\n\\n6 Antw.\\n\\nMittlere Bedeutung\\n\\n30%\\n\\n3 Antw.\\n\\nNiedrige Bedeutung\\n\\n10%\\n\\n1 Antw.\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n304\\n\\nWelche Bedeutung hat für Sie, bzgl. der Entwicklung/Anwendung von Foundation-Modellen, die Zusammenarbeit mit spezialisierten KMUs und Startups?\\n\\n9\\n\\nGroße Bedeutung\\n\\n4 Antw. 44.4%\\n\\nMittlere Bedeutung\\n\\n3 Antw. 33.3%\\n\\nNiedrige Bedeutung\\n\\n2 Antw. 22.2%\\n\\nKeine Bedeutung\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n305\\n\\nInwiefern würden Sie die Services von LEAM nutzen?\\n\\n10\\n\\nZum Tuning von eigenen Modellen auf Basis von Foundation Modellen\\n\\n60%\\n\\n6 Antw.\\n\\nZum Trainieren von Foundation Modellen\\n\\n40%\\n\\n4 Antw.\\n\\nGar nicht\\n\\n30%\\n\\n3 Antw.\\n\\nZum Betrieb von Foundation Modellen\\n\\n20%\\n\\n2 Antw.\\n\\nOther\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n306\\n\\nInwiefern würden Sie bzw. Ihr Unternehmen in den Aufbau eines LEAM KI-Servicezentrums investieren (unter noch zu klärenden Voraussetzungen)?\\n\\n9\\n\\nMit geringer Wahrscheinlichkeit\\n\\n4 Antw. 44.4%\\n\\nMit mittlerer Wahrscheinlichkeit\\n\\n4 Antw. 44.4%\\n\\nMit hoher Wahrscheinlichkeit\\n\\n1 Antw. 11.1%\\n\\nDefinitiv nicht\\n\\n0%\\n\\n0 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n307\\n\\nWie groß ist Ihr Unternehmen?\\n\\n10\\n\\n10.000 -100.000 Mitarbeiter\\n\\n60%\\n\\n6 Antw.\\n\\n> 100.000 Mitarbeiter\\n\\n40%\\n\\n4 Antw.\\n\\n100-1000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n1000-10.000 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\n< 100 Mitarbeiter\\n\\n0%\\n\\n0 Antw.\\n\\nWelcher Branche gehört Ihr Unternehmen an?\\n\\n10\\n\\nGroße KI-Modelle für Deutschland\\n\\nAutomobil\\n\\nHandel\\n\\nFinanzwirtscha!\\n\\nIndustrie\\n\\nInformationstechnologie\\n\\nDienstleistung\\n\\nMedien\\n\\nMedien\\n\\nGesundheit\\n\\nImmobilien\\n\\nTourismus\\n\\nOther\\n\\n3 Antw.\\n\\n3 Antw.\\n\\n2 Antw.\\n\\n2 Antw.\\n\\n2 Antw.\\n\\n1 Antw.\\n\\n1 Antw.\\n\\n1 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n0 Antw.\\n\\n30%\\n\\n30%\\n\\n20%\\n\\n20%\\n\\n20%\\n\\n10%\\n\\n10%\\n\\n10%\\n\\n0%\\n\\n0%\\n\\n0%\\n\\n0%\\n\\nvon 10 Personen haben diese Frage beantwortet\\n\\n309\\n\\nStehen Sie für einen weiteren Austausch zu den Themen Foundation Modelle, digitale Souveränität sowie KI im allgemeinem zur Verfügung?\\n\\n10\\n\\nJa\\n\\n60%\\n\\n6 Antw.\\n\\nNein\\n\\n40%\\n\\n4 Antw.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n310\\n\\nLEAM Umfrage - KI-Forscher\\n\\n21 responses\\n\\nWie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wissenscha! ein?\\n\\n21\\n\\nHoch\\n\\n11 resp. 52.4%\\n\\nEher hoch\\n\\n9 resp. 42.9%\\n\\nEher niedrig\\n\\n1 resp. 4.8%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n0%\\n\\n0 resp.\\n\\nNiedrig\\n\\n0%\\n\\n0 resp.\\n\\nWeder hoch noch niedrig\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n311\\n\\nWie schätzen Sie die Bedeutung von KI-Foundation-Modellen für die Wirtscha! und andere Bereiche der Gesellscha! ein?\\n\\n19\\n\\nHoch\\n\\n12 resp. 63.2%\\n\\nEher hoch\\n\\n5 resp. 26.3%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 5.3%\\n\\nWeder hoch noch niedrig\\n\\n1 resp. 5.3%\\n\\nEher niedrig\\n\\n0%\\n\\n0 resp.\\n\\nNiedrig\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n312\\n\\nArbeiten Sie bereits an oder wollen Sie mit normalen KI-Modellen (nicht Foundation-Modellen) arbeiten?\\n\\n21\\n\\nBereits recht viel\\n\\n81%\\n\\n17 resp.\\n\\nSchon etwas\\n\\n2 resp. 9.5%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nNein\\n\\n1 resp. 4.8%\\n\\nEs ist geplant\\n\\n0%\\n\\n0 resp.\\n\\nVielleicht in der Zukun!\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n313\\n\\nArbeiten Sie bereits an oder wollen Sie mit Foundation-Modellen arbeiten?\\n\\n21\\n\\nBereits recht viel\\n\\n7 resp. 33.3%\\n\\nSchon etwas\\n\\n5 resp. 23.8%\\n\\nEs ist geplant\\n\\n3 resp. 14.3%\\n\\nVielleicht in der Zukun!\\n\\n3 resp. 14.3%\\n\\nNein\\n\\n2 resp. 9.5%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n314\\n\\nWas hindert Sie daran, bereits heute Foundation-Modelle einzusetzen?\\n\\n5\\n\\nFehlende Daten\\n\\n40%\\n\\n2 resp.\\n\\nFehlender Datenschutz beim Einsatz amerikanischer und chinesischer Modelle\\n\\n40%\\n\\n2 resp.\\n\\nInfrastrukturelle Hürden\\n\\n40%\\n\\n2 resp.\\n\\nTechnische Limitierungen: Breakthroughs benötigt, z.B. Interpretable AI\\n\\n40%\\n\\n2 resp.\\n\\nHohe Investitionskosten\\n\\n20%\\n\\n1 resp.\\n\\nKeine Anwendungsfälle\\n\\n20%\\n\\n1 resp.\\n\\nMangel an Talenten\\n\\n20%\\n\\n1 resp.\\n\\nUnklarheit in der Regulation\\n\\n20%\\n\\n1 resp.\\n\\nFehlende Unterstützung der deutschen Sprache in den Modellen\\n\\n0%\\n\\n0 resp.\\n\\nOther\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n315\\n\\nWie hoch schätzen Sie die Wahrscheinlichkeit ein, Foundation-Modelle (intensiver) für Ihre Arbeit zu nutzen?\\n\\n21\\n\\nHoch\\n\\n9 resp. 42.9%\\n\\nEher hoch\\n\\n5 resp. 23.8%\\n\\nWeder hoch noch niedrig\\n\\n5 resp. 23.8%\\n\\nIch kann oder möchte hierauf nicht antworten\\n\\n1 resp. 4.8%\\n\\nNiedrig\\n\\n1 resp. 4.8%\\n\\nout of 21 answered\\n\\nEher niedrig\\n\\n0%\\n\\n0 resp.\\n\\nBeschä!igen Sie sich mit möglichen Bias, Diskriminierung und Misrepresentation in Daten?\\n\\n21\\n\\nNein\\n\\n12 resp. 57.1%\\n\\nJa\\n\\n9 resp. 42.9%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n316\\n\\nWissen Sie, wie Sie Bias, Diskriminierung und Misrepresentation hinreichend quantifizieren können, um diese zu adressieren?\\n\\n7\\n\\nJa\\n\\n4 resp. 57.1%\\n\\nNein\\n\\n3 resp. 42.9%\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n317\\n\\nWelche Kosten halten Sie für die Entwicklung eines Foundation-Modells für realistisch?\\n\\n16\\n\\n10-25 Mio. Euro\\n\\n25%\\n\\n4 resp.\\n\\n5-10 Mio. Euro\\n\\n25%\\n\\n4 resp.\\n\\n1-5 Mio. Euro\\n\\n3 resp. 18.8%\\n\\n25-50 Mio. Euro\\n\\n3 resp. 18.8%\\n\\n> 50 Mio. Euro\\n\\n2 resp. 12.5%\\n\\n<1 Mio. Euro\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n318\\n\\nWelche Kosten halten Sie für das Training eines Foundation-Modells für realistisch?\\n\\n17\\n\\n3-5 Mio. Euro\\n\\n5 resp. 29.4%\\n\\n1-3 Mio. Euro\\n\\n4 resp. 23.5%\\n\\n> 5 Mio. Euro\\n\\n4 resp. 23.5%\\n\\nout of 21 answered\\n\\n0,5-1 Mio. Euro\\n\\n3 resp. 17.6%\\n\\n<0.5 Mio. Euro\\n\\n1 resp. 5.9%\\n\\nWie sind Sie über die Regulation der Entwicklung von Foundation-Modellen informiert?\\n\\n21\\n\\nNeutral, es gibt Unklarheiten\\n\\n14 resp. 66.7%\\n\\nSchlecht\\n\\n7 resp. 33.3%\\n\\nGut, keine Unklarheiten\\n\\n0%\\n\\n0 resp.\\n\\nGroße KI-Modelle für Deutschland\\n\\nout of 21 answered\\n\\n319\\n\\nEmpfinden Sie Ihr Wissen über die Regulation als Enabler oder Disabler für Ihren möglichen Einsatz von Foundation-Modellen?\\n\\n12\\n\\nEnabler\\n\\n8 resp. 66.7%\\n\\nDisabler\\n\\n4 resp. 33.3%\\n\\nGroße KI-Modelle für Deutschland\\n\\n320\\n\\nAnhang B - Zusätzliche Information zu den Interviews\\n\\nAnhang B.1 - Die Interviewpartner:innen\\n\\nWirtschaft\\n\\nWolfgang\\n\\nHauner\\n\\nAllianz SE\\n\\nDr. Maik\\n\\nFriedel\\n\\nBASF SE\\n\\nDr. Marion\\n\\nLegler\\n\\nBayer AG\\n\\nDr. Hans-Jörg\\n\\nVögel\\n\\nBMW Group\\n\\nDr. Michael\\n\\nFausten\\n\\nRobert Bosch GmbH\\n\\nJean-Paul\\n\\nSchmetz\\n\\nBurda Media\\n\\nMario\\n\\nDeng\\n\\nBWI GmbH\\n\\nDr.\\n\\nCorina\\n\\nApachiţe\\n\\nContinental AG\\n\\nDr. Matthias\\n\\nDorner\\n\\nDATEV eG\\n\\nStephan\\n\\nKaulbach\\n\\nDeutsche Bahn AG\\n\\nDr.\\n\\nFrank\\n\\nSäuberlich\\n\\nEnBW Energie Baden-Württemberg AG\\n\\nDr.\\n\\nSebastian\\n\\nKaiser\\n\\nErgo Group AG\\n\\nThomas\\n\\nWolf\\n\\nHugging Face, Inc.\\n\\nDr.\\n\\nSabine\\n\\nDonauer\\n\\nInfineon Technologies AG\\n\\nNico\\n\\nKelling\\n\\nInfineon Technologies AG\\n\\nRainer\\n\\nSträter\\n\\nIonos SE\\n\\nChristian\\n\\nSpannbauer\\n\\nLufthansa Group\\n\\nJochen\\n\\nKaiser\\n\\nMercedes-Benz Group AG\\n\\nDr.\\n\\nStephan\\n\\nMeyer\\n\\nMunich RE\\n\\nDr. Michael\\n\\nMüller-Wünsch\\n\\nOtto GmbH & Co KG\\n\\nDr.\\n\\nArmin\\n\\nKurrle\\n\\nPorsche AG\\n\\nDr.\\n\\nLorenz\\n\\nDetermann\\n\\nRewe Group\\n\\nDr.\\n\\nFeiyu\\n\\nXu\\n\\nSAP SE\\n\\nDr.\\n\\nAndreas\\n\\nWierse\\n\\nsicos BW GmbH\\n\\nDr. Michael\\n\\nMay\\n\\nSiemens AG\\n\\nDr. Dirk\\n\\nDr.\\n\\nSebastian\\n\\nDr.\\n\\nPatrick\\n\\nSchlesinger\\n\\nHallensleben\\n\\nvan der Smagt\\n\\nTÜV Süd AG VDE Verband der Elektrotechnik Elektronik Informationstechnik e. V. Volkswagen AG\\n\\nDr.\\n\\nAlexander\\n\\nBorek\\n\\nZalando SE\\n\\nTabelle 23: Übersicht der Interviewpartner:innen im Bereich Wirtschaft\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n321\\n\\nRechenzentrums- und Hardwareexpert:innen\\n\\nDr.\\n\\nWolfgang\\n\\nStefan\\n\\nChristmann\\n\\nRüping\\n\\nchristmann informationstechnik + medien GmbH & Co. KG Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme IAIS\\n\\nDr.\\n\\nStefan\\n\\nKesselheim\\n\\nForschungszentrum Jülich GmbH\\n\\nDr. Helmut\\n\\nKreiser\\n\\nGSI Helmholtzzentrum für Schwerionenforschung\\n\\nJan\\n\\nSeiler\\n\\nGigabyte Technology\\n\\nAndreas\\n\\nHerden\\n\\nGreen Mountain Datacenter GmbH\\n\\nDr. Bastian\\n\\nKoller\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nDennis\\n\\nHoppe\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nOleksandr\\n\\nShcherbakov\\n\\nHöchstleistungsrechenzentrum Stuttgart\\n\\nVolker\\n\\nLudwig\\n\\nInterxion Deutschland GmbH\\n\\nMarco\\n\\nMaslon\\n\\nNorthern Data AG\\n\\nBedrettin\\n\\nAltay\\n\\nNoya Group Holding GmbH\\n\\nVolker\\n\\nMeschonat\\n\\nNvidia Corporation\\n\\nMarkus\\n\\nHacker\\n\\nNvidia Corporation\\n\\nOlaf\\n\\nDalmer\\n\\nOneFiber Interconnect Germany GmbH\\n\\nWolfgang\\n\\nDreyer\\n\\nOracle Corporation\\n\\nDr.\\n\\nThorsten\\n\\nHennrich\\n\\nPlusServer GmbH\\n\\nMax\\n\\nSchulze\\n\\nSDIA - Sustainable Digital Infrastructure Alliance e.V.\\n\\nAlexander\\n\\nHauser\\n\\nTTSP HWP Planungsgesellschaft mbH\\n\\nTabelle 24: Übersicht der Interviewpartner:innen im Bereich Rechenzentrum und Hardware\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n322\\n\\nWissenschaft\\n\\nProf.\\n\\nAlexander\\n\\nLöser\\n\\nBeuth Hochschule für Technik Berlin\\n\\nProf.\\n\\nDr.\\n\\nProf.\\n\\nDr.\\n\\nDr.\\n\\nProf.\\n\\nSabine\\n\\nKirchmeier\\n\\nJoachim\\n\\nKöhler\\n\\nStefan\\n\\nWrobel\\n\\nNarges\\n\\nAhmidi\\n\\nHolger\\n\\nKarl\\n\\nAlexander Waibel\\n\\nEuropean Federation of National Institutions for Language (EFNIL) Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS) Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS) Fraunhofer-Institut für Kognitive Systeme (IKS). Hasso-Plattner-Institut Karlsruher Institut für Technologie (KIT) und Carnegie Mellon University, Pittsburgh King Abdullah University of Science and Technology (KAUST)\\n\\nProf.\\n\\nJürgen\\n\\nSchmidhuber\\n\\nProf.\\n\\nHinrich\\n\\nSchütze\\n\\nLudwig-Maximilians-Universität München\\n\\nProf.\\n\\nVolker\\n\\nTresp\\n\\nLudwig-Maximilians-Universität München\\n\\nDietmar\\n\\nHarhoff\\n\\nMax-Planck-Institut für Innovation und Wettbewerb\\n\\nDr.\\n\\nPeter\\n\\nNorvig\\n\\nStanford University, Google Inc., NASA\\n\\nProf.\\n\\nIryna\\n\\nGurevych\\n\\nTechnische Universität Darmstadt\\n\\nProf.\\n\\nKristian\\n\\nKersting\\n\\nTechnische Universität Darmstadt\\n\\nProf.\\n\\nAndreas\\n\\nDengel\\n\\nTechnische Universität Kaiserslautern\\n\\nProf.\\n\\nDaniel\\n\\nCremers\\n\\nTechnische Universität München\\n\\nProf.\\n\\nJosef\\n\\nvan Genabith\\n\\nUniversität des Saarlandes\\n\\nProf.\\n\\nAnette\\n\\nFrank\\n\\nUniversität Heidelberg\\n\\nProf.\\n\\nProf.\\n\\nSepp\\n\\nLeo\\n\\nHochreiter\\n\\nWanner\\n\\nUniversität Linz und Linz Institute of Technology (LIT) Universitat Pompeu Fabra Barcelona (UPF)\\n\\nProf.\\n\\nRalf\\n\\nHerbrich\\n\\nUniversität Potsdam\\n\\nTabelle 25: Übersicht der Interviewpartner:innen im Bereich Wissenschaft\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n323\\n\\nSonstige\\n\\nPhilipp\\n\\nDr. Daniel\\n\\nGünther\\n\\nGille\\n\\nBerlin Partner für Wirtschaft und Technologie GmbH Agentur für Innovation in der Cybersicherheit GmbH\\n\\nRisto\\n\\nUuk\\n\\nFuture of Life Institute\\n\\nDr.\\n\\nTina\\n\\nKlug\\n\\nHessische Staatzkanzlei Ministerin für Digitale Strategie und Entwicklung\\n\\nOlly\\n\\nSalzmann\\n\\nKI Park e.V.\\n\\nDr.\\n\\nPeter\\n\\nChristian\\n\\nSebastian\\n\\nPhilipp\\n\\nJörg\\n\\nMendler\\n\\nDinnus\\n\\nLey\\n\\nDenker\\n\\nSchaub\\n\\nMinisterium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen PD - Berater der öffentlichen Hand GmbH Wirtschaftsinitiative FrankfurtRheinMain e.V.\\n\\nTabelle 26: Übersicht der sonstigen Interviewpartner:innen\\n\\nAnhang B.2 - Die Leitfragen\\n\\nLeitfragen für die Interviews mit Wirtschaftsvertreter:innen:`\\n\\n1)\\n\\nInwiefern nutzen Sie bereits KI-Foundation-Modelle? Welche Modelle nutzen Sie und für welche Anwendungen?\\n\\n2) Welche Bedeutung messen Sie Foundation-Modellen aktuell und in der Zukunft zu? 3) Wie wichtig ist aus Ihrer Sicht der Aufbau eines europäischen Ökosystems rund um 4) Foundation-Modelle, inkl. eigener europäischer Modelle? 5) Würden Sie die Services der LEAM Initiative nutzen? In welchem Umfang?\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n324\\n\\nAnhang B.3 - Ergebnisprotokolle\\n\\nInterviewprotokoll Allianz\\n\\nInterviewter:\\n\\nWolfgang Hauner, Head of Group Data Analytics, Allianz SE Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Zukunft der KI Liegt in Foundation-Modellen. Die Initiative LEAM ist daher sehr wichtig.\\n\\nDie Möglichkeiten der KI und der Foundation Modelle werden Geschäftsmodelle\\n\\nverändern, ähnlich der Situation beim Aufkommen des Internets und e-\\n\\nCommerce.\\n\\nFür interne Anwendung (bspw. in der Kommunikation und dem Kundenservice)\\n\\nkönnen die bestehenden Modelle bereits gut eingesetzt werden.\\n\\nDie Nutzung von KI-Services über eine API (und der Transfer der Daten in die\\n\\nUSA) ist aufgrund des Einsatzes sensibler Daten keine Option.\\n\\nEs gibt wichtige Anforderungen für versicherungsspezifische Modelle, die es\\n\\naktuell aber noch nicht gibt, z.B. in den Bereichen:\\n\\nVerbesserung des Kundenservices, indem Fragen zur Police automatisch\\n\\nausgelesen werden oder\\n\\nGenerierung von individuellen Policen auf Basis der Foundation-Modellen (Hierfür müssen die Modelle aber verlässlicher und rechtlich abgesichert werden).\\n\\nOhne die Verfügbarkeit dieser Modelle wird es mittelfristig zu Nachteilen für Versicherungsunternehmen im internationalen Wettbewerb kommen..\\n\\nV.a. bei der Verwendung sensibler Versicherungsdaten braucht es lokale Modelle,\\n\\num den Datenschutz zu gewährleisten.\\n\\nDie Herausforderung liegt nicht im traditionellen Versicherungsmarkt, der stark reguliert ist. Stattdessen besteht die Gefahr, den Schritt bei neuen, digitalen Geschäftsmodellen zu verpassen, die keine nationalen Grenzen kennen. Ähnliche\\n\\nEntwicklungen gab es bspw. bei Amazon und Google.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n325\\n\\n2. Zusammenarbeit mit LEAM ● Einzelne Unternehmen in Deutschland sind nicht in der Lage die notwendigen Investitionen alleine zu tragen. Es braucht die Unterstützung der Politik.\\n\\nDer Möglichkeit einer PPP bzw. eines Joint Ventures steht die Allianz offen\\n\\ngegenüber. Eine Zusammenarbeit der DAX Unternehmen hierfür ist durchaus\\n\\nrealistisch.\\n\\nInterviewprotokoll BASF\\n\\nInterviewter:\\n\\nMaik Friedel, Principal Scientist for Artificial Intelligence Chemistry & Leiter der Initiative “Generative AI”, BASF Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● BASF erforscht bereits intensiv den Einsatz von Foundation-Modellen und hat hierfür kürzlich die Initiative „Generative AI“ ins Leben gerufen\\n\\nZiel ist, neben dem Model Tuning von bereitgestellten Modellen auch eigene\\n\\nFoundation Modelle zu tunen.\\n\\nHierzu gehören neben großen Sprachmodellen auch domänenspezifische\\n\\nModelle im Chemie Bereich\\n\\nBASF besitzt bereits eine eigene Supercomputing-Recheninfrastruktur (Quriosity) und plant diese weiter auszubauen. Der Vorteil einer eigenen Infrastruktur liegt in der Datensicherheit.\\n\\nMitarbeiter nutzen bereits privat Dienste wie GPT-3 oder ChatGPT. Das ist kaum kontrollierbar und kann ein potentielles Daten-Sicherheitsrisiko darstellen.\\n\\nDas Trainieren bzw. Tunen von Modellen durch BASF auf Basis von eigenen\\n\\nDaten ist eine strategisch, wichtige Capability. Hierfür können außereuropäische\\n\\nServices (z.T. auch Cloud-Services generell) aus Datensicherheitsgründen nur\\n\\nbedingt, oder gar nicht in Anspruch genommen werden.\\n\\nWenn bei BASF bzw. in Deutschland/Europa entsprechende Services und\\n\\nFoundation Modelle nicht bereitgestellt werden können, entwickelt sich hieraus\\n\\nein großer Wettbewerbsnachteil.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n326\\n\\n2. Zusammenarbeit mit LEAM ● Aus den oben genannten Gründen wäre ein LEAM Service eine gute Unterstützung um eigene Foundation-Modelle zu erstellen und existierende\\n\\nModelle zu tunen\\n\\nInterviewprotokoll Bayer Pharma\\n\\nInterviewter:\\n\\nMarion Legler, Head of Decision Science & Advanced Analytics at Bayer Pharma Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Bayer arbeitet intensiv mit Foundation-Modellen, v.a. im Bereich Sprache (Natural Language Processing) und der Bildverarbeitung (Computer Vision)\\n\\n\\n\\nIm Bereich Natural Language Processing greift Bayer auf den Medizinbereich\\n\\nspezialisierte Modelle wie SciBERT, BioBERT und PubMedBERT zurück.\\n\\nAber: Die aktuellen Modelle erfüllen noch nicht alle Erwartungen, bspw. ist nicht\\n\\ndie gesamte medizinische Terminologie eingebunden. Ebenso können die\\n\\naktuellen Language Models die Nuancen der medizinischen Inhalte, die für Laien\\n\\noft nicht erkennbar sind, noch nicht differenzieren. Größere Modelle im\\n\\nmedizinischen Bereich könnten dort weiterhelfen.\\n\\nEin europäisches Foundation-Modell für die Medizin wäre für Bayer sehr\\n\\ninteressant. Dieses könnte Bayer dann speziell für seine Ansprüche, d.h. konkrete\\n\\nIndikationsgebiete von Interesse, anpassen.\\n\\nAufgrund des Datenschutzes ist es für Bayer keine Option, Modelle von US-\\n\\nProvidern auf amerikanischen Servern zu nutzen bzw. anzupassen, wenn dabei\\n\\nwertvolle (Patienten-)daten geteilt werden.\\n\\nGleichzeitig besteht beim „finetuning“ die Gefahr, dass ein gewisser Bias, der aus dem Ursprungsmodell hervorgeht, bestehen bleibt. Insofern besteht hier ein großes Interesse an großen Sprachmodellen, die von der Basis aus auf\\n\\neuropäischen Daten trainiert wurden und somit europäische\\n\\nSprachgepflogenheiten aber auch unterschiedliche europäische Sprachen per se\\n\\nberücksichtigt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n327\\n\\nNeben Sprachmodellen spielen auch Foundation-Modelle im Bereich Computer\\n\\nVision eine wichtige Rolle.\\n\\nAuch hier lassen sich Modelle, die bspw. auf amerikanischen Patientendaten\\n\\ntrainiert wurden, nicht immer mit derselben Qualität auf europäische\\n\\nPatientendaten anwenden. Der Grund dafür liegt hier in den unterschiedlichen\\n\\nethnischen Zugehörigkeiten der PatientInnen, die sich dann auch in minimalen\\n\\nUnterschieden bspw. der CT-Scans oder anderen Bilddaten niederschlagen, die\\n\\njedoch für die Performance der Modelle ausschlaggebend sind. Modelle, die auf\\n\\neiner Population trainiert sind, sind somit meist nicht generalisierbar und auf\\n\\nandere PatientInnenpopulationen anwendbar. Aus diesem Grund besteht auch\\n\\nhier ein großes Interesse an europäischen Modellen.\\n\\nBayer kann sich gut vorstellen, das Serviceangebot von LEAM im Bereich Model- Tuning und evtl. auch in der Erstellung von Foundation Modellen zu nutzen.\\n\\n2. Zusammenarbeit mit LEAM ● Bayer könnt sich prinzipiell auch ein Engagement an LEAM im Rahmen eines Joint-Ventures / einer PPP vorstellen.\\n\\nEin anzustrebendes Ziel wäre, gemeinsam mit anderen Akteuren aus dem\\n\\nBereich Health ein Foundation-Modell speziell für Anwendungen in der Medizin\\n\\nzu entwickeln. Das Foundation Modell könnte dann als wertvolle Basis für\\n\\nfirmen-spezifische Domän-Modelle dienen.\\n\\nInterviewprotokoll Berlin Partner\\n\\nInterviewter:\\n\\nPhilipp Günther, Berlin Partner Interviewer: Vanessa Cann, KI Bundesverband Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 30. November 2022\\n\\nZusammenfassung\\n\\n1. Berlin Partner ●\\n\\nInformationen zu Berlin Partner\\n\\nDie Berlin Partner für Wirtschaft und Technologie GmbH ist die\\n\\nWirtschaftsförderung des Landes Berlin.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n328\\n\\nMit ca. 220 Mitarbeitenden unterstützt das Public-Private-Partnership\\n\\nUnternehmen, Wissenschaftseinrichtungen und NPOs am Standort.\\n\\n\\n\\nIm Innovationsfeld Künstliche Intelligenz unterstützt Berlin Partner\\n\\nAkteure in Förderfragen, vernetzt sie untereinander und in\\n\\nAnwendungsbranchen und sorgt für mehr Sichtbarkeit über Berlins\\n\\nGrenzen hinaus.\\n\\nEntwicklung KI-Foundation Modelle in Europa\\n\\nSie sehen die Entwicklung eigener Foundation-Modelle als entscheidende Grundlage für deutsche KI-Innovationen innerhalb der europäischen Wertegemeinschaft.\\n\\n\\n\\nIn ihrer Rolle als Wirtschaftsförderung unterstützen sie Akteure in der\\n\\nAnsiedlung, Identifikation von Fördermöglichkeiten, Stakeholder-Dialog,\\n\\nKommunikation und Transfer.\\n\\nBeispielprojekte sind:\\n\\n■ ResKriVer unter Leitung des Fraunhofer Fokus ■ WHO Hub for Pandemic and Epidemic Intelligence ■ EU Testing and Experimentation Facilities (TEF) Health unter\\n\\nLeitung der Charité Berlin\\n\\n■ www.ki-berlin.de: eine Plattform auf der News, Events und\\n\\nErfolgsgeschichten aus dem Berliner KI-Ökosystem dargestellt\\n\\nwerden.\\n\\nProjekte zu Foundation-Modellen wurden bisher nicht enger betreut.\\n\\nGovernance\\n\\nBerlin Partner ist selbst eine PPP. ○ aus dem Protokoll des vorherigen Gesprächs zum PPP-Modell: ■ Berlin Partner bestreitet öffentliche Aufgaben, die vom Land\\n\\nvergeben werden. Sie erhalten eine institutionelle Zuwendung des\\n\\nLandes und keine Finanzierung über eine Projektstruktur.\\n\\n■ Die Partner für Berlin Holding Gesellschaft für Hauptstadt-\\n\\nMarketing mbH ist zur Hälfte privat, zur Hälfte aus der Stadt\\n\\nfinanziert. Sie hält 28% an Berlin Partner.\\n\\nIBB (Investitionsbank Berlin) Unternehmensverwaltung AöR - 31,5%\\n\\n■ ■ Technologiestiftung Berlin - 30,0%\\n\\n■\\n\\njeweils zu 3,5%: Handwerkskammer Berlin, IHK Berlin, Vereinigung\\n\\nder Unternehmensverbände Berlin und Brandenburg e.V.\\n\\n■\\n\\nInsgesamt hält der öffentliche Sektor weniger als 50% der Anteile.\\n\\n\\n\\nJe mehr Partner im Projekt sind, desto mehr Kompetenzen gibt man auch\\n\\nab.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n329\\n\\nEine Möglichkeit ist die Finanzierung über eine Betreiberkonzession, wie\\n\\nes bspw. bei Messegeländen der Fall ist.\\n\\n2. Wie kann LEAM mit den Projekten zusammenarbeiten? ● Generelle Einschätzung zu LEAM\\n\\nLEAM kann eine Grundlage für KI-Innovationen in Berlin, Deutschland und\\n\\nEuropa sein.\\n\\nFür Berlin Partner zeigt das Interesse des Ökosystems, dass LEAM\\n\\nunterstützt werden sollte. Dabei ist vor allem die europäische Perspektive\\n\\ninteressant.\\n\\nNeben dem Interesse bestehender KI-Akteure in der Hauptstadtregion, gibt es auch Gespräche mit internationalen Akteuren, die nach Berlin kommen wollen, von LEAM profitieren könnten und sich vermutlich\\n\\neinbringen würden. LEAM würde auch die Attraktivität des Standorts\\n\\nweiter steigern.\\n\\nDer Standort Berlin\\n\\nBerlin bietet ein dynamisches und diverses KI-Ökosystem mit internationaler Strahlkraft sowie Akteuren, die auf Augenhöhe kooperieren und offen für neue Partner sind.\\n\\nBerlin ist Innovationsstandort. Auch Unternehmen, deren Hauptstandort woanders liegt, haben häufig Innovation Labs oder Entwicklungsteams in der Hauptstadt.\\n\\n\\n\\nIn Berlin findet exzellente KI-Forschung statt – in den Grundlagen, sowie\\n\\nanwendungsnah. Schwerpunkte sind u.a. NLP sowie erklärbare und\\n\\nvertrauenswürdige KI.\\n\\nBerlin ist KI-Startup-Hauptstadt. ○ Berlin zieht als internationaler Hub viele internationale Talente an, die auch für LEAM benötigt würden.\\n\\nMögliche Standorte und Rechenzentren:\\n\\n■ Mit NTT und Penta Infra gibt es Akteure in Berlin, die ohne\\n\\nZeitverzug ein LEAM Rechenzentrum integrieren und hosten\\n\\nkönnten.\\n\\nDie 11 Zukunftsorte Berlins sowie Innovationsparks genießen\\n\\nFörderprivilegien, haben bereits relevante Infrastruktur und könnten\\n\\nebenfalls interessante Partner und Standorte sein.\\n\\nFinanzierung:\\n\\n■ Die Gesprächsbereitschaft der Berliner Landesregierung wird aus\\n\\nSicht von Berlin Partner als hoch eingeschätzt. Ein enger Austausch\\n\\nmit der Senatsverwaltung für Wirtschaft, Energie und Betriebe zu\\n\\ndiesem Thema besteht bereits.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n330\\n\\n■ Generell gilt, dass eine Bundesförderung, an der sich ein Land\\n\\nbeteiligen kann, die Möglichkeit einer schnelleren Förderung durch\\n\\ndas Land deutlich begünstigt.\\n\\n■ Beispiel: AI Quality & Testing Hub / TEF Health\\n\\nBerlin hat als erstes der involvierten Bundesländer\\n\\nFördermittel für die bundesweite Initiative zugesagt.\\n\\nBerlin Partner konnte außerdem EU-Förderung sowie Partner vermitteln, um ein EU-weites Konsortium zur Bewerbung für Testing and Experimentations Facilities (TEF)\\n\\naufzustellen. Dabei stemmen im Erfolgsfall das Land Berlin,\\n\\nsowie die Charité Berlin die nationale Ko-Finanzierung.\\n\\nZusammenarbeit mit Berlin Partner\\n\\nBerlin Partner kann selbst keine finanzielle Unterstützung leisten. ○ Sie unterstützt aber bei: ■ der Suche nach geeigneten Standorten und entsprechenden\\n\\nPartnern (KI Park, Penta-Infra, Zukunftsorte uvm.).\\n\\n■ der Initiierung und Begleitung der Kommunikation mit\\n\\nMinisterialverwaltungen auf Landesebene.\\n\\n■ der Identifikation von weiteren Fördermöglichkeiten (Landes-,\\n\\nBundes- und EU-Mittel).\\n\\n■ der Vernetzung mit weiteren potentiellen Partnern und Kunden\\n\\naus ihrem Netzwerk.\\n\\n■ durch einen Erfahrungsaustausch zu einer möglichen\\n\\nGovernancestruktur und dem Finanzierungsmodell eines PPP.\\n\\n■ Stakeholder-Dialog auf regionaler Ebene. ■ Kommunikation & Transfer, über Area Managements und\\n\\nAuslandsbüros auch weltweit.\\n\\nInterviewprotokoll BMW\\n\\nInterviewter:\\n\\nHans-Jörg Vögel, Manager AI, Robotics, and Cognitive Systems, BMW Group Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n331\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Zielsetzung von LEAM ist sinnvoll und sehr plausibel. ● BMW ist als Associate Partner in OpenGPT-X involviert. ● Es gibt sehr viele Anwendungsfälle für Foundation-Models z.B. im Bereich Benutzerhandbücher, Kunden- und Fahrerdialoge.\\n\\nMultimodale Modelle sind ebenfalls in Planung. Hier gibt es Forschung im Bereich Innenraumüberwachung und Sprachdialogsysteme. Die Fragestellung, inwiefern Foundation-Modelle / Transformer-Modelle zu Durchbrüchen auch beim\\n\\nAutonomen Fahren führen könnten, ist offen.\\n\\nWichtig ist die Berücksichtigung von Datenschutz, Datensicherheit und\\n\\nInformationsschutz, GDPR-Compliance, Unterstützung von Internationalisierung\\n\\n(zweistellige Anzahl von Sprachen für Märkte weltweit) und die Bereitstellung\\n\\neiner mandantenfähigen Plattform. All dies kann durch US-amerikanische\\n\\ngeneral-purpose Services (derzeit) nicht gewährleistet werden.\\n\\nDie Services von ChatGPT sind bereits jetzt schon kritisch, da Nutzung durch\\n\\nMitarbeiter mit Unternehmensdaten schwer kontrollierbar.\\n\\nFür die digitale Souveränität in Deutschland ist es wichtig, die Modelle zu\\n\\nverstehen und über die Bereitstellung von entsprechender Infrastruktur und\\n\\nMöglichkeiten Experten und Talente im Land zu halten. Forschung und Transfer\\n\\nder Forschungsergebnisse in die Wirtschaft ist dabei essenziell.\\n\\nDie intensive Nutzung von KI und die Möglichkeit, Foundation Modelle zu nutzen, ist sehr wettbewerbsrelevant. Im Moment gibt es hierfür in Deutschland nicht die erforderliche Infrastruktur.\\n\\n2. Zusammenarbeit mit LEAM ● BMW ist auf jeden Fall ein potenzieller Nutzer von LEAM. Betriebssicherheit, Qualität und wirtschaftliche Wettbewerbsfähigkeit der Services sind dafür eine\\n\\nVoraussetzung.\\n\\nDie Beteiligung von BMW an einen Joint Venture / einer PPP kann diskutiert\\n\\nwerden, passende Rahmenbedingungen vorausgesetzt.\\n\\nInterviewprotokoll Bosch Center for AI\\n\\nInterviewter:\\n\\nMichael Fausten, SVP AI and Systems at Robert Bosch GmbH, Bosch Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n332\\n\\nAlex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● KI-Foundation-Modelle sind ein relevantes Thema bei Bosch. Aktuell klärt Bosch die Potenziale der Technologie.\\n\\nNeben Sprach- und Bilddaten sind auch weitere Industriedaten spannend. ● Bei der Nutzung amerikanischer Modelle kann es zu Abhängigkeiten von den USA kommen. Darüber hinaus gibt es das Problem, dass aufgrund geopolitischer Entwicklungen die amerikanischen Modellen eventuell in anderen Märkten -\\n\\nbspw. China - nicht mehr nutzbar sind.\\n\\nEuropäische Modelle müssen genauso leistungsstark sein wie die\\n\\namerikanischen.\\n\\n2. Zusammenarbeit mit LEAM ● Bosch betreibt selbst ein Rechenzentrum. Für die Entwicklung von Foundation Modellen ist ein LEAM Service durchaus attraktiv. Daneben besteht vor allem Interesse an der Möglichkeit des Modell Tunings.\\n\\nBosch steht einer finanziellen Beteiligung bei LEAM grundsätzlich offen\\n\\ngegenüber. Dafür müssten jedoch noch einige offene, insbes. kommerzielle\\n\\nFragen geklärt werden. Ein Konsortium aus mehreren Unternehmen klingt nach\\n\\neinem denkbaren Weg.\\n\\nEine zeitnahe Realisierung ist entscheidend. In fünf Jahren sind wir bereits zu\\n\\nspät.\\n\\nInterviewprotokoll BWI GmbH\\n\\nInterviewter:\\n\\nMario Deng, Lead Service Manager Data Analytics, BWI GmbH Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 10. Januar 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n333\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Services, die auf Basis von KI-Foundation-Modellen entwickelt werden, haben einen Mehrwert. Die BWI möchte diese Anwendungen in ihr Portfolio\\n\\nübernehmen.\\n\\nDie Nutzung von US-amerikanischen AI-Services über API ist für die BWI nicht realistisch, da bei sicherheitskritischen Daten besondere Schutzvorkehrungen gelten. Die Technologie dahinter ist aber nutzungswert.\\n\\nEs müssen Use Cases entwickelt werden, die über das Thema Chatbot\\n\\nhinausgehen.\\n\\nEs ist ineffizient, wenn jede Organisation ihr eigenes Rechenzentrum baut und KI-\\n\\nFoundation-Modelle berechnet. Stattdessen sollten gemeinsam wenige\\n\\nFoundation-Modelle berechnet werden, die dann individuell angepasst werden\\n\\nkönnen.\\n\\n2. Zusammenarbeit mit LEAM ● Aufgrund hoher Sicherheitsanforderungen darf die BWI viele Anbieter nicht nutzen.\\n\\nDie BWI hat Bedarf an den Modellen und den von LEAM geplanten Services. ● Entscheidend sind die Sicherheitsvorkehrungen der Infrastruktur sowie die Sicherheit beim Transfer der Daten.\\n\\nEs muss ausgearbeitet werden, wie diese Sicherheitsvorkehrungen\\n\\nauszusehen haben.\\n\\nDaneben benötigt das BWI für eine Nutzung der Infrastruktur folgende\\n\\nVoraussetzung:\\n\\nEine saubere Dokumentation bei der Entwicklung der Modelle ○ Transparenz bei den für Modelle genutzten Daten und den beteiligten Personen\\n\\nEin Verrechnungsmodell, das eine einfache Kostenplanung ermöglicht.\\n\\nInterviewprotokoll Continental\\n\\nInterviewter:\\n\\nDr. Corina Apachiţe, Programm-Leiterin für Künstliche Intelligenz und Daten Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n334\\n\\n14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Foundation-Modelle ● Die Fähigkeiten von großen Sprachmodellen wie GPT-3 sind faszinierend. ● Diese Fähigkeiten sind auch für Continental wichtig, allerdings müsste hierzu ein domainspezifisches Tuning auf Basis von „Ingenieursprache und spezifischen Inhalten“ erfolgen. Dies kann derzeit nicht über GPT-3 oder ähnliche Modelle\\n\\nabgewickelt werden.\\n\\nDie Befähigung ein eigenes große Sprachmodell zu erstellen bzw. zu tunen ist\\n\\nsehr wichtig für die Zukunftsfähigkeit der deutschen und europäischen\\n\\nDigitalindustrie.\\n\\n2. Multimodale Modelle ● Es besteht ein großer Bedarf auch an neuen multimodalen Modellen u.a. für das autonome Fahren.\\n\\nAuf Basis bestehender Modelle sind derzeit eher inkrementelle Verbesserungen\\n\\nmöglich.\\n\\nWenn ein Anbieter in der Lage ist, auf Basis von großen Foundation Modellen ein sehr viel leistungsfähigeres autonomes Fahren anzubieten, würde dies den Markt stark verändern. Gerade in Deutschland müssen wir uns für diese Disruption\\n\\nstärken und die nutzbare Datenbasis vergrößern.\\n\\n3. Gemeinsame Aktivitäten ● Die Industrie muss zusammenarbeiten und befähigt werden, unter Einhaltung von Regulierung und Gesetzen Foundation Modelle zu entwickeln bzw.\\n\\nbereitgestellte Foundation Modelle zu tunen.\\n\\nEin einzelnes Unternehmen kann diese Herausforderungen aufgrund der unzureichenden Datenmenge, Mangel an Personal und Erfahrung nicht stemmen.\\n\\nDie Erfahrungen aus bestehenden Projekten (bspw. Catena-X) muss genutzt\\n\\nwerden.\\n\\nWir brauchen in Deutschland eine neu aufgebaute „Supply-Chain“ für Modelle\\n\\nund Daten.\\n\\nDie Möglichkeit, große Modelle zu erstellen, wird auch ein Katalysator für den\\n\\nAufbau von Datenpools darstellen.\\n\\nEbenfalls kann durch die Befähigung, große Modelle zu erstellen und durch\\n\\nführende Forschungsaktivitäten in diesem Bereich ein attraktives Betätigungsfeld\\n\\nfür Top-Talente gestaltet werden und so einem „Brain-Drain“ entgegen gewirkt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n335\\n\\nAll diese Punkte sind enorm wichtig für das geostrategische Setup und die\\n\\nWettbewerbsfähigkeit der deutschen Wirtschaft und Gesellschaft.\\n\\nInterviewprotokoll Cyberagentur\\n\\nInterviewter:\\n\\nDaniel Gille, Leiter Sicherheit durch KI und Sicherheit für KI, Cyberagentur Michael Lindner, Pressesprecher, Cyberagentur Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 09.12.2022\\n\\nZusammenfassung\\n\\n1. Cyberagentur ● Die Cyberagentur ist eine GmbH des Bundes. Alleinige Gesellschafterin ist die Bundesrepublik Deutschland, vertreten durch das Bundesministerium des Innern\\n\\nund für Heimat sowie das Bundesministerium der Verteidigung.\\n\\nSie forscht nicht selbst, sondern beauftragt Grundlagenforschung im Bereich disruptiver Technologieansätze mit Bezug zu Innerer und/oder Äußerer Cybersicherheit.\\n\\nKünstliche Intelligenz ist aus Sicht der Cyberagentur eine Schlüsseltechnologie und wird entsprechend in der Forschungsstrategie als eines der relevanten Themenfelder behandelt.\\n\\nDie SPRIND, die im Auftrag des Bundesministeriums für Wirtschaft und\\n\\nKlimasschutz sowie des Bundesministeriums für Bildung und Forschung arbeitet,\\n\\nist die “Schwesteragentur” der Cyberagentur.\\n\\n2. Nutzung von LEAM ● Es lässt sich aktuell noch nicht sagen, ob die Cyberagentur selbst die Kapazitäten von LEAM bräuchte, insbesondere da sie selbst nicht im KI-Bereich forscht und entwickelt. Für Forschungs- und Entwicklungsaufgaben mit Sicherheitsbezug\\n\\nmüssten in jedem Fall hohe Security-Anforderungen erfüllt werden.\\n\\nInsbesondere müssten LEAM-Infrastrukturen relevante VS-Kriterien erfüllen.\\n\\nEine zukünftige Nutzung hochperformanter KI-Infrastrukturen durch forschende Auftragnehmer:innen im Rahmen der Umsetzung ihres Forschungsvorhabens ist\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n336\\n\\ndurchaus vorstellbar. Allerdings kann die Cyberagentur ihren\\n\\nAuftragnehmer:innen hinsichtlich zu nutzender externer Ressourcen keine\\n\\nkonkreten Empfehlungen aussprechen oder Vorgaben machen.\\n\\nDas Thema KI-Foundation-Modelle ist in seiner Relevanz erkannt und auf der\\n\\nForschungsroadmap entsprechend hoch priorisiert. Seine Bedeutung wird in der\\n\\nZukunft noch steigen.\\n\\nFür die Cyberagentur ist bei der Betrachtung von Security-Fragestellungen jeder\\n\\nSchritt im ML-Lebenszyklusrelevant.\\n\\nDie Nutzung nicht-europäischer Modelle kann unter Umständen ein\\n\\nSicherheitsrisiko für die Bundesrepublik Deutschland darstellen. Aktuell stehen\\n\\nwir noch am Anfang der Entwicklung, aber bereits in wenigen Jahren sind\\n\\nmöglicherweise viele Anwendungsbereiche, Wertschöpfungsprozesse und\\n\\nGeschäftsmodelle zumindest in Teilen auf Foundation Models angewiesen, über\\n\\nderen Funktionsweise, Entwicklung, Trainingsdaten etc. nur unzureichende\\n\\nTransparenz besteht. Als mahnendes Beispiel sei auf die Hintertüren-Diskussion\\n\\num 5G und Huawei verwiesen.\\n\\n3. Infrastruktur ● Der Bedarf nach einer übergreifenden KI-Infrastruktur ist nach persönlicher Einschätzung DG gegeben, um die technologische Souveränität bei der Entwicklung und Anwendung großer KI-Modelle sicherzustellen.\\n\\nInterviewprotokoll Deutsche Bahn\\n\\nInterviewter:\\n\\nStephan Kaulbach, Head of Data Intelligence Center, Deutsche Bahn AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Bedeutung des Themas Foundation-Modelle wird in der Zukunft wachsen. ● Aktuell nutzt DB KI-Modelle v.a. für Optimierungen. Es gibt aber erste Bestrebungen, Foundation-Modelle bspw. bei der Angleichung von Handbüchern\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n337\\n\\nfür Lokführer und die Instandhaltung. Im Bereich Customer Support (bspw.\\n\\nChatbots) wird das Thema ebenfalls wichtiger werden.\\n\\nAktuell kann niemand abschätzen, was mit den Daten passiert, die für die Nutzung der amerikanischen Modelle über API in deren Cloud-Services übertragen werden. geschieht. Das stellt die Deutsche Bahn vor\\n\\nHerausforderungen, da sie nicht verhindern können, dass die Mitarbeiter bereits\\n\\njetzt Anwendungen wie ChatGPT nutzen.\\n\\nDie DB arbeitet bereits mit dem deutschen Startup Deepl zusammen. Sie haben in ihren Sprachendienst das Bahnlexikon integriert. Bei dieser Kooperation wurde vor allem Wert auf die Einhaltung von Datenschutzstandards gelegt..\\n\\nGenerell sind europäische Modelle aufgrund des Datenschutzes einfacher zu\\n\\nintegrieren als amerikanische Modelle.\\n\\n2. Zusammenarbeit mit LEAM ● Die Initiative LEAM ist unterstützenswert.Die DB wäre sehr an der Nutzung von LEAM-Modellen interessiert.\\n\\nDie Beteiligung an einem Joint Venture bzw. einer PPP ist grundsätzlich sinnvoll.\\n\\nDie Umsetzung müsste, vor allem hinsichtlich der Struktur der DB, geklärt\\n\\nwerden.\\n\\nEntscheidend ist, dass beim Thema KI-Foundation-Modelle zeitnah etwas\\n\\npassiert.\\n\\nInterviewprotokoll EnBW\\n\\nInterviewter:\\n\\nDr. Frank Säuberlich, Chief Data Officer, EnBW Energie Baden-Württemberg AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Aktuell werden KI-Modelle bereits intensiv in klassischen Vorhersagen wie Predictive Maintenance etc. eingesetzt.\\n\\nGroße Sprachmodelle könnten zukünftig ein wichtiges Thema u.a. zur\\n\\nAnalyse/Verarbeitung von unstrukturierten Daten werden.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n338\\n\\nEs gibt auch erste Ansätze, wie mehr Rechenkapazität bzw. Quantencomputing die aktuell genutzten Modelle verbessern könnten, hier denken wir vor allem in Kontext von Optimierungsmodellen, bspw. für Layout von Offshore Windparks.\\n\\nHierfür sind leistungsfähige Rechner erforderlich. Als Anbieter kritischer\\n\\nInfrastruktur ist die Nutzung von amerikanischen Cloud-Service Providern hier\\n\\nmit entsprechenden Risiken / Unwägbarkeiten verbunden. Das Thema\\n\\nDatensicherheit ist wichtig. Die EnBW schaut sehr genau auf diese Themen, wenn\\n\\nModelle außerhalb Europas gehostet werden.\\n\\nEine europäische Alternativen würde hierbei sehr helfen, daher wird die LEAM\\n\\nInitiative begrüßt.\\n\\nNeben Rechenkapazität spielen auch hochwertige Daten und ausreichend\\n\\nPersonal eine Rolle.\\n\\n2. Zusammenarbeit mit LEAM ● Einer Beteiligung am Projekt LEAM steht die EnBW offen gegenüber. Für eine Zusage müssten allerdings weitere Personen involviert und die praktische Umsetzung konkretisiert werden.\\n\\nInterviewprotokoll Ergo\\n\\nInterviewter:\\n\\nSebastian Kaiser, Head of Machine Learning, ERGO Group AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 16. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Das Thema KI-Foundation-Modelle ist von strategischer Bedeutung. Die hierdurch realisierbaren Produktivitätssteigerungen und Service-Verbesserungen\\n\\nkönnen zu Wettbewerbsvorteilen führen.\\n\\nDie größte Herausforderung bei der Nutzung amerikanischer Modelle ist der Datenschutz. Der Zugang zu GPT-3 über API auf einen Rechner in den USA erlaubt keine Verarbeitung von sensiblen Versicherungsdaten .\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n339\\n\\nEin großer Nachteil der aktuellen Foundation-Modelle ist, dass sie nicht auf\\n\\nversicherungsspezifischen Daten trainiert sind. Andere Sprachmodelle (z.B. Wu\\n\\nDao) enthalten mehr versicherungsspezifische Dokumente und sind daher\\n\\nbesser auf die Domäne anwendbar.\\n\\n2. Zusammenarbeit mit LEAM ● Das Serviceangebot von LEAM trifft genau die Bedürfnisse der Ergo hinsichtlich der Nutzung von großen KI-Modellen.\\n\\nWichtig ist aber auch der Vergleich der Kosten ggü. Anbietern (auch aus den\\n\\nUSA).\\n\\nEin kompetitives europäisches Foundation-Modell würde die Diskussionen rund\\n\\num den Datenschutz enorm vereinfachen.\\n\\nEine mögliche finanzielle Beteiligung der ERGO im Rahmen eines Joint Ventures /\\n\\nPublic Private Partnership ist prinzipiell denkbar\\n\\nDie Muttergesellschaft der Ergo, Munich Re, ist der Zusammenarbeit mit anderen Organisationen in diesem Bereich grundsätzlich aufgeschlossen. Allerdings liegt die Entscheidung hier bei der Munich Re.\\n\\nInterviewprotokoll Forschungszentrum Jülich\\n\\nInterviewter:\\n\\nStefan Kesselheim, Head of AI Consultants Team, FZ Jülich Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 14.11.2022\\n\\nZusammenfassung\\n\\n1. Governance des FZ Jülich ● Finanzierung:\\n\\nDie Finanzierung erfolgte über zwei Stränge:\\n\\n■ zu ca. 50% über die EU über die Organisation PRACE - Partnership\\n\\nfor Advanced Computing in Europe\\n\\n■ zu ca. 50% über das BMBF und das Land NRW über den Verein\\n\\nGauss Centre for Supercomputing (GCS)\\n\\n■ PRACE wird von EuroHPC abgelöst\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n340\\n\\n■ weitere Communities (z.b. Helmholtz-Gemeinschaft) sind z.T. an den Rechnern beteiligt und können Ressourcen frei nutzen\\n\\nOrganisationsform:\\n\\nDas FZ Jülich ist eine GmbH des öffentlichen Rechts. ○ Sie wirtschaftet daher nicht wie eine standardmäßige GmbH.\\n\\nDie Recheninfrastruktur ist Eigentum des FZ Jülich\\n\\nAber das FZ Jülich muss den größten Teil der Rechenzeit für externe\\n\\nProjekte zur Verfügung stellen.\\n\\nUnd nur ein kleiner Teil steht dem FZ Jülich zur freien Verfügung.\\n\\nZugang zu Rechenpower:\\n\\nHalbjährlich werden Projektaufrufe auf Rechenzeit veröffentlicht. ○ Projekte werden anhand eines objektiven, peer-review Verfahrens bewertet.\\n\\nBei kleineren Tier 2 Zentren (bspw. NHR-Verbund) gibt es rollierende\\n\\nAufrufe.\\n\\n● Personal:\\n\\nIn der Regel laufen die Jobs max 24 Stunden\\n\\nEine Aussage zur Anzahl des Personals ist schwierig, da es sich in viele\\n\\nverschiedene Bereiche zergliedert.\\n\\n\\n\\nInsgesamt arbeiten am JSC ca. 300 Personen. Ein kleiner Teil ist direkt mit\\n\\ndem Rechnerbetrieb befasst. Ein Großteil betreibt angewandte Forschung\\n\\nz.B. zu großskaligen Simulation und künstlicher Intelligenz. Diese Expertise\\n\\nist ein Schlüsselfaktor für den Erfolg.\\n\\nAlle Mitarbeiter sind über den öffentlichen Dienst eingestellt. Das bringt\\n\\neinige Herausforderungen mit sich:\\n\\n■ Es gibt wenig Flexibilität beim Gehalt, ■ Kündigungen sind so gut wie ausgeschlossen und ■ es ist schwierig, Dauerstellen zu schaffen.\\n\\nWichtig ist, dass auch das Betreibermodell KI-Expertise besitzen muss.\\n\\nKunden:\\n\\n\\n\\nIm FZ Jülich werden v.a. Simulation für wissenschaftliche Domänen wie\\n\\nKlimaforschung und Quantenphysik berechnet.\\n\\n\\n\\nIndustriepartner spielen nur eine kleine Rolle. Sie zahlen für die Nutzung.\\n\\n2. Technik ● Nutzung\\n\\n\\n\\nIn der Regel liegt die Auslastung über 90%. Die Verfügbarkeit ist\\n\\ntypischerweise über 80%. Dies ist auf geplante Wartungen sowie\\n\\ngelegentlich auftretende Schwierigkeiten mit der Hardware\\n\\nzurückzuführen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n341\\n\\nZeitweise liegt die Auslastung bei weniger als 90%. Dies ist vor allem\\n\\ndarauf zurückzuführen, dass Projekte gleichzeitig starten, aber zum Start\\n\\noft noch keine Rechenzeit benötigen.\\n\\nLimitierung:\\n\\nEs gibt eine maximale Berechnungszeit von 24 Stunden. Dies ist für eine gleichmäßig hohe Auslastung sehr entscheidend. In Ausnahmefällen und größeren Projekten sind Jobketten und Reservierung möglich.\\n\\nManagementsoftware\\n\\nDie Jobvergabe erfolgt über das Queuing-System SLURM. ○ Die verwendete Managementsoftware ist eine Eigenentwicklung des FZ Jülich.\\n\\nInterviewprotokoll Future of Life Institute\\n\\nInterviewter:\\n\\nRisto Uuk, Policy Researcher, Future of Life Institute Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 05.12.2022\\n\\nZusammenfassung\\n\\nHinweis: Das Interview wurde auf Englisch geführt.\\n\\n1. Thema KI-Foundation-Modelle ● Das Future of Life Institute hat einen Artikel veröffentlicht zum Thema: Emerging Non-European Monopolies in the Global AI Market\\n\\nAktuell entwickelt lediglich AlephAlpha Foundation-Modelle in Europa. Sie können\\n\\naber nicht mit den amerikanischen Modellen konkurrieren.\\n\\nEuropa braucht eigene Modelle. Die Hoffnung ist, dass diese vertrauenswürdiger sind, da sie auf GDPR-konformen Daten und weiteren vertrauenswürdigen EU Gesetzen und Guidelines beruhen.\\n\\nDas Rennen um die besten Modelle hat negative Auswirkungen auf die\\n\\nSicherheit. Es geht den Unternehmen darum, schnellstmöglich neue Modelle zu\\n\\nproduzieren und nicht das sicherste Modell.\\n\\nDie Sicherheitsvorkehrungen bei GPT-3 sind bspw. einfach zu umgehen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n342\\n\\n\\n\\nIm AI Act gibt es aktuell keine Gründe für US-Unternehem, ethische Modelle zu\\n\\nentwickeln.\\n\\nAuch Open Source Modelle haben Probleme. Sie sollten nicht ungetestet auf die Welt losgelassen werden. Es bringt auch niemand Autos ohne Bremsen auf die Straße und sagt, probiert mal aus. Der Provider muss sicherstellen, dass jedes\\n\\nModell so sicher wie möglich ist.\\n\\nLediglich große (amerikanische) Unternehmen haben die Ressourcen, um\\n\\nFoundation-Modelle zu entwickeln. Selbst bei einer Zusammenarbeit der Open-\\n\\nSource Community, dem Mittelstand und anderen Unternehmen, wird es\\n\\nschwierig, diese Vormachtstellung zu brechen.\\n\\nInterviewprotokoll Höchstleistungsrechenzentrum Stuttgart\\n\\nInterviewter:\\n\\nBastian Koller, Geschäftsführer, HLRS Dennis Hoppe, Leiter Künstliche Intelligenz und Quantum Computing, HLRS Interviewer: Jörg Bienert, Alexander Thamm GmbH Jakob Tesch, Ubermetrics Patrick Bunk, Ubermetrics Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. November 2022\\n\\nZusammenfassung\\n\\n1. Governance\\n\\nOrganisationsform\\n\\no Das HLRS wurde 1996 als Bundeshöchstleistungsrechenzentrum\\n\\ngegründet und ist seit mehr als 25 Jahren eine zentrale Einrichtung\\n\\nder Universität Stuttgart. Das HLRS bietet ein umfassendes Paket\\n\\nan Ressourcen und Services für Hochleistungsrechnen (HPC),\\n\\nDatenanalyse, Künstliche Intelligenz, Visualisierung und verwandte\\n\\nTechnologien. Das HLRS unterstützt Wissenschaftler, Ingenieure\\n\\nund Nutzer:innen aus vielen Forschungs- und\\n\\nAnwendungsbereichen durch die Bereitstellung von HPC-Tools und\\n\\nFachwissen, die für Forschung, Entwicklung besserer Produkte,\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n343\\n\\nLösung komplexer und sehr rechenintensiver Probleme oder die\\n\\nUmsetzung neuer Ideen erforderlich sind.\\n\\no Das Gauss Centre for Supercomputing e.V. (GCS) dient dem\\n\\nZusammenschluss der drei Bundeshöchstleistungsrechenzentren\\n\\nin Deutschland: HLRS (Höchstleistungsrechenzentrum der\\n\\nUniversität Stuttgart), FZJ (Forschungszentrum Jülich) und LRZ\\n\\n(Leibniz Rechenzentrum, Garching bei München). Die drei Zentren\\n\\nstimmen sich im Rahmen des GCS ab, agieren aber im operativen Bereich autonom.\\n\\nKunden\\n\\nDas HLRS unterstützt sowohl akademische Nutzer:innen als auch\\n\\nKund:innen aus der Industrie.\\n\\nKunden aus der Industrie sind in Anzahl und Nutzung der Systeme der Bundeshöchstleistungsrechenzentren ein Alleinstellungsmerkmal des HLRS. Hier können, im Schnitt, bis zu 10% der Rechenkapazität durch\\n\\nIndustriekunden, auf Basis kostendeckender Preise, genutzt werden.\\n\\nUm die Nutzung der Rechenressourcen für Produktionsläufe durch die Industrie zu ermöglichen wurde 1995 eine Public Private Partnership mit den Industriepartnern Daimler und Porsche gegründet. Diese PPP,\\n\\ndie HWW GmbH, ist auch heute noch aktiv; aktuelle Gesellschafter sind\\n\\nPorsche, T-Systems, das Land Baden-Württemberg, die Universität\\n\\nStuttgart sowie das Karlsruher Institut für Technologie. Die HWW agiert\\n\\nprimär als Vermittler von Rechenzeit.\\n\\nAktuell rechnen zahlreiche KMUs und Großunternehmen regelmäßig\\n\\nam HLRS.\\n\\nAkquise erfolgt entweder direkt oder bspw. über die SICOS BW GmbH, die potenzielle Kund:innen anspricht. Oft kommt es am Anfang zu Testläufen, Proof-of-Concepts und dann im Idealfall zur Gewinnung\\n\\nneuer Kunden.\\n\\nDas HLRS bietet seine Rechen- und Speicherressourcen seinen Kunden\\n\\nzu kostendeckenden Preisen an.\\n\\nUm Industriekunden zu gewinnen, bietet das HLRS umfangreichen Support, Sicherheit (über ISO-Zertifizierung) und einen Fokus auf Nachhaltigkeit (bspw. Zertifizierung über den Blauen Engel). Dies sind\\n\\nAspekte, die ein gängiger Cloud-Anbieter nicht bietet und Vertrauen\\n\\nschaffen.\\n\\nPersonal\\n\\nDas HLRS hat feste Stellen im Budget der Universität Stuttgart, der\\n\\nRest der Mitarbeitenden sind auf Drittmittel eingestellt.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n344\\n\\n\\n\\nIm Rahmen des deutschen SiVeGCS Projektes (das Projekt, das u.a. die\\n\\nRechner der drei Gauss Zentren finanziert), werden u.a. zwei weitere\\n\\nStellen zur Interaktion mit der Industrie auf Landesebene gefördert.\\n\\nDie Förderung von SiVeGCS erfolgt durch den Bund (BMBF), sowie das\\n\\nLand Baden-Württemberg.\\n\\nBei der Beschaffung neuer Flaggschiff-Systeme liegt oft ein Augenmerk\\n\\nauf eine weitergehende Kooperation mit dem Anbieter, der den\\n\\nZuschlag bekommt. Innerhalb solcher Kooperationen ist auch Personal\\n\\ndes Herstellers vor Ort anzustreben, sowie Mitarbeitende, die in der\\n\\nKooperation mit dem HLRS zusammenarbeiten, um die HPC-Services\\n\\nzu stärken. Diese Personen unterstützen dann das HLRS und den\\n\\nProduktionsbetrieb.\\n\\nZugang zu Rechenressourcen\\n\\nDie Wissenschaft bewirbt sich bei Großprojekten zentral über das GCS\\n\\nund deren durch einen Lenkungsausschuss begutachtete peer-\\n\\nreviewte Calls.\\n\\nBeim HLRS sind jederzeit Anfragen für Projekte auf den Systemen möglich, die von einem wissenschaftlichen Lenkungsausschuss begutachtet werden. Ein Test-Zugang kann individuell nach Absprache\\n\\nzügig gewährt werden.\\n\\nFür Entwicklungsprojekte der Wirtschaft gibt es einen klaren\\n\\nZugangsprozess und gegebenenfalls die Möglichkeit der Priorisierung\\n\\nvon Rechenjobs.\\n\\n2. Ausgewählte Projekte mit Industriebezug ● Fortissimo - Factories of the Future Resources, Technology, Infrastructure and Services for Simulation and Modelling\\n\\nDas aktuelle Projekt (FF4EuroHPC) wird über die EuroHPC Joint\\n\\nUndertaking gefördert.\\n\\nEs ermöglicht KMUs den Zugang zu HPC-Systemen in Europa, wie dem\\n\\nHLRS, um sogenannte Businessexperimente durchzuführen.\\n\\nFür die Nutzung müssen Projektanträge geschrieben werden, die von\\n\\nExperten begutachtet werden.\\n\\nErgebnisse sind Case Studies, die veröffentlicht werden.\\n\\nCATALYST\\n\\nDas Projekt läuft nach 5 Jahren Ende des Jahres 2022 aus; es wurde durch\\n\\ndas MWK Baden-Württemberg gefördert.\\n\\nDas Projekt stellt Kontingente (Personal und Rechenbudget) sowohl für\\n\\nWissenschaft und Industrie zur Verfügung, um Technologien und\\n\\nPotentiale zu validieren.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n345\\n\\nEine geringe Einstiegshürde (bspw. gibt es keine lange Antragsphase)\\n\\nermöglicht eine hohe Akzeptanz des Förderprojektes.\\n\\nProjekte haben in der Regel eine Laufzeit zwischen drei und zwölf Monaten. Ergebnis ist eine Success Story oder mindestens eine gemeinsame wissenschaftliche Publikation, die veröffentlicht wird.\\n\\nDas HLRS hat darüber auch neue Kunden gewonnen.\\n\\nSolution Center\\n\\nDie Solution Center sind eigenständige Vereine, die den Mitgliedern als\\n\\nWissensplattform dienen.\\n\\nAnschubfinanzierung erhalten sie vom Land Baden-Württemberg. Mittelfristig müssen sie sich aber über Mitgliedsbeiträge und Förderprojekte finanzieren.\\n\\nFür das HLRS ermöglichen sie den Kontakt in verschiedene Branchen und\\n\\nsind eine wichtige Säule im Wissens- und Technologietransfer.\\n\\nSatellitenprojekte der Solution Center werden unter anderem auf den\\n\\nHLRS-Systemen gerechnet.\\n\\n3. Technik\\n\\nVerlässlichkeit ist für das HLRS wichtig. Daher:\\n\\nbeschafft das HLRS seine neuen Systeme meist basierend auf ausgereiften Konzepten und Technologien, um Stabilität und Sicherheit zu garantieren; dies schließt jedoch den Einsatz neuester\\n\\nTechnologien nicht aus, die als Teil des Gesamtsystems eingebunden\\n\\nwerden können\\n\\nbietet das HLRS einen vollumfänglichen Service für Kunden an. ○ unterzieht sich das HLRS regelmäßig relevanten Zertifizierungen wie EMAS, Blauer Engel oder Sicherheitsstandards wie TISAX oder im ersten Halbjahr 2023 der ISO 27001 Zertifizierung.\\n\\n4. LEAM ● Bei LEAM müssen Wissenschaft und Industrie klar getrennt werden. ● Die 10% Nutzung für die Industrie ist eine Sonderregelung, die andere HPC- Zentren in Deutschland nicht so einfach nutzen können.\\n\\nEine individuelle Kooperation zwischen LEAM und dem HLRS ist zu begrüßen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n346\\n\\nInterviewprotokoll Höchstleistungsrechenzentrum Stuttgart\\n\\nInterviewter:\\n\\nOleksandr Shcherbakov, Wissenschaftlicher Mitarbeiter, HLRS Interviewer: Jörg Bienert, Alexander Thamm Jakob Tesch, Ubermetrics Hauke Timmermann, eco Verband Protokollant: Alex Dickmann, KI Bundesverband Kim Lambers, eco Verband Datum: 02. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Hardware ●\\n\\nIn Stuttgart sind insgesamt 24 KI-Knoten mit jeweils 8 NVIDIA A100 GPUs\\n\\nvorhanden (= 192 GPUs). Die KI-Knoten sind in das HPC-System integriert, um\\n\\nhybride HPC/KI Workflows zu ermöglichen. Neben diesen KI-Knoten existiert\\n\\nnoch ein weiteres KI-System (CS-Storm) mit insgesamt 64 NVIDIA V100 GPUs und\\n\\nlokalen SSDs. Das reicht für LEAM eher nicht aus.\\n\\n\\n\\nIm Jahr 2024 soll ein neues HPC-System installiert werden, welches aller\\n\\nVoraussicht nach ebenfalls über Beschleunigertechnologie verfügen wird.\\n\\nWeitere Informationen können erst Ende 2023/Anfang 2024 bekannt gegeben\\n\\nwerden. Das System wird öffentlich ausgeschrieben.\\n\\nDer HAWK-Cluster wird wassergekühlt; allgemein geht der Trend aufgrund von\\n\\nEnergieeffizienz und Abwärmenutzung in Richtung Wasserkühlung.\\n\\nGekühlt wird direkt auf der CPU und dem Arbeitsspeicherriegel. ● Co-Location ist zunächst am HLRS nicht vorgesehen; über kleinere Systeme kann individuell diskutiert werden.\\n\\nLinkliste:\\n\\nEntgeltordnung ○ https://www.hlrs.de/solutions/systems/hpe-apollo-hawk ○ https://www.hlrs.de/solutions/systems/cray-cs-storm ○ https://kb.hlrs.de/platforms/index.php/Batch_System_PBSPro_(vulcan)#N ode_types\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n347\\n\\nInterviewprotokoll Hubert Burda Media\\n\\nInterviewter:\\n\\nJean-Paul Schmetz, Chief Scientist, Hubert Burda Media Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle\\n\\nBurda sieht eine hohe Bedeutung von Sprachmodellen. ● Derzeit wird u.a. in Vietnam ein eigenes Sprachmodell entwickelt. ● Ein deutsches/europäisches Foundation-Model würde von Burda zum Tuning und Erstellung unterschiedlicher Anwendungen genutzt werden.\\n\\n2. Daten-Problematik\\n\\nProblematisch ist vor allem die Verfügbarkeit von qualitativ hochwertigen Daten.\\n\\nEs reicht nicht aus, einen “Sumpf an Daten” zu haben, diese müssen auch\\n\\ngerankt, kommentiert und überprüft werden. Das kostet viel Arbeitszeit.\\n\\nInvestitionen sind neben HW vor allem in die Aufarbeitung der Daten erforderlich\\n\\n● Burda besitzt qualitativ hochwertige Datensätze, die sie Kunden anbieten.\\n\\n3. Zusammenarbeit mit LEAM ● Burda könnte LEAM prinzipiell einen gut kuratierten Datensatz gegen Lizenzgebühren zur Verfügung stellen..\\n\\nEs ist unbestritten, dass jeder Anwender ein europäisches LEAM Modell benutzen\\n\\nwürde, wenn es kompetitiv zu den amerikanischen Modellen ist.\\n\\nInterviewprotokoll Hugging Face\\n\\nInterviewter:\\n\\nThomas Wolf, Co-Founder Hugging Face Carlos Munoz Ferrandis, Hugging Face Interviewer:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n348\\n\\nJörg Bienert, Alexander Thamm GmbH Alexander Löser, Berliner Hochschule für Technik (BHT) Protokollant: Alex Dickmann, KI Bundesverband Datum: 20. Dezember 2022\\n\\nZusammenfassung\\n\\nHinweis: Das Interview wurde auf Englisch geführt.\\n\\nAI Foundation Models\\n\\nThe Project LEAM is a good initiative. There is a need for a data center that\\n\\nspecializes in AI.\\n\\nThe danger is that in the future all foundation models will come from American\\n\\nprivate companies.\\n\\nHugging Face has trained their model on the French computer Jean Zay. Their\\n\\nexperience shows that it is optimized for other workflows and might be\\n\\nchallenging to train an AI foundation model with the standardized workflows of\\n\\nthe Jean Zay.\\n\\nAt Hugging Face, there are already initial considerations to build their own\\n\\ncomputing cluster. However, the goal is not economic profit, but to support the\\n\\nmission of open models.\\n\\n\\n\\nIn addition, high quality data is critical. There needs to be public support for\\n\\nlabeling data.\\n\\nHugging Face is currently investigating what makes a good dataset. The second\\n\\nstep is to produce these datasets on a large scale.\\n\\nCollaboration with LEAM\\n\\nHugging Face would be interested in using the LEAM computing center.\\n\\nInterviewprotokoll Ionos\\n\\nInterviewter:\\n\\nRainer Sträter, SVP Cloud Services and Global Platform Hosting, Ionos Interviewer: Jörg Bienert, Alexander Thamm GmbH Hauke Timmermann, eco Verband Jakob Tesch, Ubermetrics Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n349\\n\\nAlex Dickmann, KI Bundesverband Datum: 14. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Verfügbarkeiten Ionos ●\\n\\nIonos betreibt aktuell drei Rechenzentren in Deutschland.\\n\\n\\n\\nIn Frankfurt wird aktuell ein neues Rechenzentrum errichtet. Der Bau beginnt\\n\\n2023. Das Rechenzentrum wird 2024/2025 betriebsbereit sein. Das Ziel ist es,\\n\\ncarbon negative zu sein.\\n\\nEnergiekosten machen aktuell bereits ein Drittel aller Kosten aus. Die Kosten\\n\\nwerden in Zukunft sicherlich weiter steigen. Energieeinsparung ist daher höchst\\n\\nrelevant.\\n\\n2. Zusammenarbeit mit LEAM ● Die Kalkulation für den Aufbau und Betrieb eines Supercomputing RZ für LEAM sieht realistisch aus. Unter den angegebenen Parametern lässt sich das Projekt realisieren.\\n\\nBeschaffung und Aufbau der Infrastruktur stellt für Ionos kein Problem dar. Die Lieferung und der Aufbau der Boxen alleine dauert aber sicherlich ein halbes Jahr.\\n\\nEntscheidend ist für Ionos eine Zusage zur Abnahme von Rechenzeit über 24\\n\\nMonate. Die genaue Höhe muss im operativen Betrieb geklärt werden.\\n\\nV.a. das neue Rechenzentrum in Frankfurt bietet sich für LEAM an. Die anderen\\n\\nRechenzentren in Berlin und Karlsruhe eher weniger.\\n\\nDer Preis richtet sich vor allem nach den Kosten für den Aufbau und Betrieb. ● Eine genauere Ausgestaltung kann im zweiten Schritt diskutiert werden.\\n\\n3. Finanzierung ● Für Ionos ist die Unterstützung von LEAM eine ernsthafte Option. ● Die Finanzierung könnte u.a. erfolgen durch\\n\\nAufbau und Betrieb eines eigenen RZ und Vereinbarung einer garantierten\\n\\nAbnahmemenge (z.B. 60%-70%) der Kapazitäten durch den Bund\\n\\nAufbau und Betrieb eines eigenen RZ mit Anschubfinanzierung durch die\\n\\nöffentliche Hand und Bereitstellung von Rechenkapazität für\\n\\nWissenschaft/Startups/Public.\\n\\nIonos hat umfangreiche Erfahrungen mit öffentlichen Ausschreibungen.\\n\\n● Bei beiden Vorgehen gilt, dass das Konsortium so klein wie möglich gehalten werden sollte. Ansonsten läuft das Projekt Gefahr, in Abstimmungsschleifen zu\\n\\nverlaufen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n350\\n\\n4. Kühlung ● Die A100 besitzen serienmäßig keine Liquidkühlung. Es gibt aber Anbieter, die diese anpassen. Die neue H100 Generation hat eine Standardmäßige\\n\\nLiquidkühlung. Deren Nutzung wird daher empfohlen.\\n\\nDie Power Usage Effectiveness (PUE) liegt in neuen Rechenzentren bei unter 1,1.\\n\\nEs wird also wenig Energie für andere Zwecke als den Betrieb der Server benötigt.\\n\\nWirkliche Effizienz entsteht aber erst, wenn die Abwärme auch großflächig\\n\\ngenutzt und nicht “in die Umwelt gepustet” wird. Hier lassen sich auch neue\\n\\nGeschäfsfelder erschließen.\\n\\nInterviewprotokoll KI Park\\n\\nInterviewter: Olly Salzmann, Stellvertretender Vorstandsvorsitzender Interviewer & Protokollant:\\n\\nVanessa Cann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Warum unterstützt der KI Park eine Initiative wie LEAM? ● KI Park Mission: Wir wollen einen Beitrag leisten, um KI-Innovationen “made in Germany and Europe” zu beschleunigen und damit Deutschland und die EU bis 2030 zu einem global führenden Innovationsstandort für KI zu machen\\n\\nZiel der Aktivitäten des KI Parks ist die Schaffung von Voraussetzungen für die erfolgreiche Erforschung und Entwicklung zukunftweisender KI-Technologien. Der Schwerpunkt des KI Parks liegt auf Deutschland bzw. Europa und in klarer\\n\\nAbgrenzung zu den USA und China. Daraus abgeleitet die Notwendigkeit einer\\n\\ndeutschen bzw. europäischen KI-Souveränität anerkennt und ermöglichen\\n\\nmöchte.\\n\\nLEAM bietet hier eine gute Möglichkeit den Technologiestandort Deutschland\\n\\nbzw. Europa wieder auf die Weltkarte zu bringen. Diese Ambition wird durch das\\n\\nÖkosystem des KI Park in der Hauptstadt Berlin unterstützt.\\n\\nDer KI Park existiert und definiert sich über seine Mitglieder und repräsentiert eine ausgewählte Gruppe an global führenden Unternehmen wie z.B. Deloitte, VW, Schaeffler oder Celonis und Forschungseinrichtungen wie das ZUSE Institute,\\n\\ndie Friedrich-Alexander Universität, Humboldt Innovation oder ISST Fraunhofer.\\n\\nDie Mitglieder des KI Parks können sowohl von den neuesten Technologien und\\n\\nLösungen profitieren, als auch vom kuratierten Zugang zu Wissen, Fähigkeiten\\n\\nund Erfahrungen. Das liefert auch einen Beitrag zum Gelingen von Initiativen wie\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n351\\n\\nzB LEAM.\\n\\n2. Warum eignet sich der KI Park als Standort für LEAM? ● Im September 2022 wurde das neuste und modernste Data Center in\\n\\nDeutschland direkt neben der KI Park-Geschäftsstelle im Marienpark eröffnet\\n\\nund wird in den nächsten Monaten noch kontinuierlich erweitert. Es besteht\\n\\ndaher die Möglichkeit für LEAM auf modernste aber bereits bestehende\\n\\nInfrastruktur zurückzugreifen zu können. Das bestehende Data Center zeichnet\\n\\nsich insbesondere auch durch seine Nachhaltigkeit aus, so wird die Abwärme des\\n\\nData Center zum Heizen des Marienparkcampus verwendet und der Campus hat\\n\\nseinen eigenen Solarpark zur Stromproduktion. Neben der bestehenden bzw. im\\n\\nBau befindlichen (Server) Infrastruktur bietet die Örtlichkeit des Marienpark\\n\\nCampus in Berlin auch die Möglichkeit, weitere Firmen und andere\\n\\nOrganisationen vor Ort anzusiedeln und somit die Infrastruktur und Ergebnisse\\n\\nvon LEAM zu konsumieren bzw. darauf aufzubauen weitere Produkte entwickeln\\n\\nzu können.\\n\\nDer Community Gedanke wird allein durch physische Nähe des KI Park zu den\\n\\nSchaltzentralen des deutschen Staates (u.a. Regierung, Parlament),\\n\\nKompetenzzentren der Industrie (z.B. Siemens City, AWS Research,\\n\\nInnovationshubs, etc.) und Forschungseinrichtungen (drei Universitäten, DFKI,\\n\\nFraunhofer, etc) weiter unterstütz und ausgebaut. Es kommt so also zu einer\\n\\nVerknüpfung von Infrastruktur und Innovations-Community in nächster\\n\\nUmgebung des deutschen Startups und Innovationszentrums Berlin im Rahmen\\n\\ndes Marienpark Technologie- und Innovationscampus.\\n\\nGanz unabhängig vom KI Park in Berlin, bietet sich die Umsetzung eines Data Centers für Projekte wie LEAM allein aus Gesichtspunkten des Energiebedarfs und den damit verbundenen Nachhaltigkeitsaspekten im Norden von\\n\\nDeutschland und insbesondere in Küstennähe an. Da hier zum einen\\n\\nausreichend Strom erzeugt wird und zumindest für den Norden bereits ein\\n\\nausreichend großes Verteilernetz existiert, wohingegen die Leitungen in Richtung\\n\\nSüden noch fehlen.\\n\\n3. Welche Anknüpfungspunkte gibt es zu bestehenden Aktivitäten des KI\\n\\nPark?\\n\\nEs gibt bereits verschiedene Initiativen im Rahmen des KI Park, die sowohl als\\n\\nGrundlage für LEAM dienen können, bzw. sich gut mit LEAM ergänzen. Zum einen\\n\\nist der KI Park physisch Teil des im Aufbau befindlichem Marienparks, ein in\\n\\nBerlin Tempelhof-Schöneberg gelegener, rund 360.000 m2 großer Gewerbepark,\\n\\nin dem ein innovatives Ökosystem mit Unternehmen und Start-ups aus\\n\\nzukunftsweisenden Bereichen wie KI, Critical Infrastructure, Additiver Fertigung\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n352\\n\\noder auch Laboren für die physische Testung neuer Technologie z.B. im Bauhaus\\n\\nder Erde. Es besteht also noch ausreichend Platz und Infrastruktur für die\\n\\nAnsiedlung weiterer Unternehmen, Initiativen für den Aufbau eines Innovations-\\n\\nCampus und einer (physischen) KI-Community.\\n\\nDie Notwendigkeit für viele kleine wie großen Unternehmen einen rechtlichen Rahmen für schnelles Experimentieren und Forschung auch mit externen Partnern zur Verfügung zu haben ist ein Kern des KI Park Ökosystems. Kleine\\n\\nUnternehmen scheuen oft die notwendigen jedoch meist riskanten Investitionen\\n\\nin Technologie und Infrastruktur; bei großen Unternehmen steht meist\\n\\norganisatorische Komplexität und generelle Widerstände, die mit zunehmender\\n\\nUnternehmensgröße entstehen, im Wege neuer Innovationen durch schnelles\\n\\nExperimentieren und Entwicklung Raum zu geben. In Bezug auf LEAM bietet der\\n\\nKI Park eine rechtliche und organisatorische Umgebung für die erfolgreiche\\n\\nRealisierung des LEAM Projektes, sowie eine möglichst geringe Barriere bzw. eine\\n\\nVielzahl von Anknüpfungspunkten für die anschließende (kommerzielle) Nutzung\\n\\nvon riesigen Sprachmodellen.\\n\\nEiner der zentralen Säulen des KI Park ist KI-Ethik oder verantwortungsvolle KI bzw. Wert getriebener Einsatz von Technologie. Da das LEAM Projekt in der Projektbeschreibung auch in Abgrenzung an andere, vergleichbare Initiativen den\\n\\nDatenschutz, europäische Werte und Open-Source in den Vordergrund stellt,\\n\\nbietet sich hier für den KI Park eine Reihe von Anknüpfungspunkten. Ergänzend\\n\\nist hier auch die Unterstützung eines 60 Millionen Euros schweren, EU- weiten\\n\\nKonsortium zur Erforschung von bildgebenden Verfahren und\\n\\nvertrauensspendender KI im Gesundheitswesen durch den KI Park zu erwähnen\\n\\n(TEF Health via FAU).\\n\\nDas LEAM Projekt mit der dazugehörenden Infrastruktur ergänzt sich perfekt mit\\n\\nden bereits angelaufenen Initiativen des KI Parks im Marienpark seinen\\n\\nMitgliedern Zugang zu 5G bzw. 6G Mobilfunknetzwerke und Quanten Computer\\n\\nzu Testzwecken bzw. in Zukunft bei entsprechendem Erfolg auch im Regelbetrieb\\n\\nzur Verfügung zu stellen.\\n\\n4. Wie ließe sich LEAM im Rahmen des KI Park organisatorisch umsetzen und\\n\\nfinanzieren?\\n\\nDie Betreibergesellschaft für LEAM und dessen Kommerzialisierung könnte als Teil des wirtschaftlichen Geschäftsbetriebs den KI Parks realisiert werden. Idee: ohne Profitmaximierung aber kostendeckend bzw. zur Finanzierung weiter\\n\\nForschung bzw. Weiterentwicklung\\n\\nMitglieder des KI Parks können die Umsetzung des LEAM Projektes sowohl finanziell wie auch organisatorisch unterstützen. Der KI Park kann hier als Koordinierungsstelle und erster Ansprechpartner zur Verfügung stehen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n353\\n\\nUnter den an LEAM interessierten Mitgliedern und Freunden des KI Park sind neben DAX 30 Unternehmen, deutschen Unicorns auch die weltweit größte Professional Service Firm (Deloitte), der wichtigste KI-Infrastruktur Anbieter für\\n\\nriesige KI-Modelle schlechthin (NVIDIA), ein hochinnovativer Projektentwickler mit\\n\\nSchwerpunkt Technologie und Innovation (Investa), sowie einer der global\\n\\nführenden Data Center Entwickler und Betreiber (NTT). So besteht hier sowohl\\n\\ndie Bereitschaft mit eigenen Kräften, Technologie und Wissen zur Verfügung zu\\n\\nstellen, als auch ein Interesse sich für die deutsche bzw. europäische KI- Souveränität zu engagieren.\\n\\nInterviewprotokoll Lufthansa\\n\\nInterviewter:\\n\\nChristian Spannbauer, CTO, Lufthansa Group Digital Hangar Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Aktuell arbeitet Lufthansa im Servicebereich mit Foundation-Modellen. ● Generell sind die Bereiche Optimierung, Sustainability, Efficiency u.ä. interessanter. Dort sollen spezifische Anwendungsfälle identifiziert werden\\n\\nBei sensiblen Daten ist die Nutzung amerikanischer Modelle schwierig. Bei\\n\\noperativen Daten (bspw. Flugdaten, Wetterdaten, u.ä.) ist die Problematik nicht\\n\\nso groß.\\n\\nDie Herausforderung ist aktuell nicht die Technologie, sondern die internen Möglichkeiten. Ohne externe Unterstützung ist eine Implementierung nicht möglich.\\n\\n2. Zusammenarbeit mit LEAM ● Bereich zu engagieren.\\n\\n2. Zusammenarbeit mit LEAM ● Bereich zu engagieren.\\n\\nEine Beteiligung wäre im Prinzip möglich, ist aber unter den aktuellen\\n\\ngesamtwirtschaftlichen Rahmenbedingungen zu prüfen und müsste sich für die\\n\\nLufthansa aber betriebswirtschaftlich rechnen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n354\\n\\nInterviewprotokoll Mercedes-Benz\\n\\nInterviewter:\\n\\nJochen Kaiser, Chief Data Officer, Mercedes-Benz Group AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. Allgemeines ● Die Entwicklung und Nutzung von KI ist vor allem dann wichtig, wenn sich hieraus ein Wettbewerb differenzierender Faktor ergibt.\\n\\nChina ist derzeit dominierend (56% der Patente) gefolgt von USA (~20%), Europa\\n\\nliegt mit ca. 8% schon sehr weit hinten.\\n\\nEinsatzgebiet von KI sind neben dem autonomen Fahren vor allem\\n\\nFahrzeugsteuerung und Fahrzeugintegration.\\n\\nDerzeit läuft ein Programm, um mehrere hundert Mitarbeiter im Bereich Data&AI\\n\\nauszubilden.\\n\\n2. KI-Foundation-Modelle ● Die Services von OpenAI werden aktuell genutzt. Prinzipiell ist es sinnvoller, große Modelle selbst zu entwickeln oder auf Basis großer Modelle eigene Modelle zu tunen (auch unter Gesichtspunkten von Bias, Ethik und GDPR)\\n\\nFoundation-Modelle sollten idealerweise Open Source und über die gesamte\\n\\nIndustrie angeboten werden.\\n\\nMercedes-Benz betreibt bereits ein eigenes Rechenzentrum in Norwegen. Das\\n\\nThema Nachhaltigkeit ist dabei ein treibender Faktor.\\n\\nNeben der Rechenzeit sind vor allem die Themen Personal und Daten eine große Herausforderung für die Entwicklung von Foundation Modellen. Der Data Act der EU kann dabei helfen, diese frei zugänglich zu machen.\\n\\n3. Zusammenarbeit mit LEAM ● Mercedes-Benz kann die geplanten LEAM Services nutzen. Bei amerikanischen Services behindern juristische Vorgaben oft die Anpassung und Nutzung bzw. verlangsamen den Start von Projekten durch längliche Klärung von juristischen\\n\\nKlauseln.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n355\\n\\nSynergien mit anderen Unternehmen u.a. auch im Rahmen von Catena-X sind\\n\\nsinnvoll.\\n\\nOpendata und der European Data Act können sich mit der Entwicklung von\\n\\nFoundation Modellen gegenseitig positiv beeinflussen.\\n\\nDie Beteiligung in einem Konsortium zur Finanzierung von LEAM ist gut\\n\\nvorstellbar und sinnvoll. Dabei sollte auch auf das Thema Daten geachtet\\n\\nwerden. Die Herausforderung ist die Geschwindigkeit, mit der das Projekt\\n\\nrealisiert wird.\\n\\nDie Initiative ist v.a. auch für kleinere Akteure wie Zulieferer entscheidend. Die werden das Thema Nutzung von Foundation-Modelle nicht alleine umsetzen können.\\n\\nInterviewprotokoll Ministerium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg\\n\\nInterviewter:\\n\\nDr. Peter Mendler, Leitung des Referats „Industrie- und Technologiepolitik, Digitalisierung“, Stv. Abteilungsleiter im Ministerium für Wirtschaft, Arbeit und Tourismus Baden-Württemberg Interviewer: Jörg Bienert, Alexander Thamm Vanessa Cann, KI Bundesverband Protokollant: Alex Dickmann, KI Bundesverband Datum: 30. November 2022\\n\\nZusammenfassung\\n\\n1. Aktivitäten in Baden-Württemberg ● Aleph Alpha als KI Champion 2021 war der erste öffentlich sichtbare Kontaktpunkt mit dem Thema KI-Foundation Modelle.\\n\\nIPAI & KI-Exzellenzzentren sind weitere Anknüpfungspunkte mit dem Thema.\\n\\n● Es gibt verschiedene Förderprojekte und –programme zum Thema KI, aber Foundation Modelle laufen unter anderen Dimensionen.\\n\\nZiel des Landes Baden-Württemberg ist es, Ökosysteme zu schaffen, die auch\\n\\nphysisch angesetzt sind.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n356\\n\\n2. Warum ist LEAM für Baden-Württemberg interessant? ● Es passt in die KI-Strategie der Landesregierung. Generell ist das Thema KI hoch auf der Agenda des Landes angesetzt.\\n\\nBaden-Württemberg hat seine Aktivitäten bei KI in den letzten Jahren mit hohem\\n\\nTempo entwickelt. Es gibt Synergien mit:\\n\\ndem IPAI und den KI-Exzellenzzentren, ○ den Universitäten und Forschungseinrichtungen im Land, insbesondere auch mit dem CyberValley,\\n\\nden zahlreichen Unternehmen mit KI-Aktivitäten einschließlich der\\n\\nzahlreichen KI-Startups\\n\\ndem bestehenden KI-Ökosystem und ○ verschiedenen Wirtschaftsinitiativen wie bspw. die europäische Testing and Experimentation Facility (TEF Manufacturing) oder die Vanguard Initiative.\\n\\nAleph Alpha gilt als Leuchtturm für das Thema. ● LEAM würde dem rasch wachsenden KI-Ökosystem in Baden-Württemberg zusätzliche Dynamik geben und Baden-Württemberg könnte ggf. mit hoher Wahrscheinlichkeit rasch Flächen für LEAM mobilisieren. Dadurch könnten sich\\n\\nzusätzliche nationale und europäische Kooperationen entwickeln.\\n\\n3. LEAM ● Die Initiative wird ein Erfolg, wenn es eine Zusammenarbeit zwischen privaten und öffentlichen Akteuren ist. Dafür müssen beihilferechtliche Fragestellungen geklärt werden.\\n\\nFür die Finanzierung über ein IPCEI-Projekt müssten zuerst die beihilferechtlichen\\n\\nVoraussetzungen geschaffen werden.\\n\\nEs braucht ein tragfähiges Finanzierungskonzept und ein nachhaltiges\\n\\nGeschäftsmodell, damit das Projekt wirtschaftlich erfolgreich sein wird.\\n\\n4. Finanzierung: ● Die Landesregierung Baden-Württemberg möchte bei KI im internationalen Innovationswettbewerb vorne dabei sein.\\n\\nDas Land würde sich dafür einsetzen, dass LEAM nach Baden-Württemberg\\n\\nkommt.\\n\\nPrivate Finanziers müssen in das Projekt eingebunden werden. Hier stellt das\\n\\nLand bei Bedarf gerne Kontakte her.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n357\\n\\nInterviewprotokoll Ministerium für Wirtschaft, Industrie, Klimaschutz und Energie des Landes Nordrhein-Westfalen\\n\\nInterviewter:\\n\\nChristian Dinnus, Referatsleiter Digitale Wirtschaft, Digitale Geschäftsmodelle (MWIKE NRW) Sebastian Ley, Digitale Wirtschaft, Digitale Geschäftsmodelle (MWIKE NRW) Dr. Dirk Hecker, Managing Director Fraunhofer Allianz Big Data, Fraunhofer IAIS Interviewer: Vanessa Cann, KI Bundesverband Protokollant: Alex Dickmann, KI Bundesverband Datum: 06.12.2022\\n\\nZusammenfassung\\n\\nWichtiger Hinweis: Die Interviewten äußern ihre fachliche Einschätzung, die im Wesentlichen auf öffentlich verfügbaren Informationen beruhen. Zusagen oder ähnliches sind mit den Äußerungen nicht verbunden.\\n\\n1. Thema KI-Foundation-Modelle ● Die Größe der Modelle und Recheneinheiten spielt eine Rolle. Je größer diese sind, desto mehr Wirkung lässt sich erzielen. Um Basismodelle entstehen Ökosysteme, die darauf Zugriff haben möchten. Diese Ökosysteme sind das, was\\n\\nwir in Deutschland und Europa erreichen sollten. Wichtig ist entsprechend eine\\n\\nAnwendbarkeit der Basismodelle (Anpassen für konkrete Anwendungsfälle).\\n\\n2. Standort NRW ● Das Rheinland liegt ideal zwischen den Welt-Internetknoten in Frankfurt und Amsterdam, bei hoher Stromversorgungssicherheit und eingebettet zwischen\\n\\nden Städten Köln, Düsseldorf, Aachen, Bonn, mit hohem Potenzial an Fachkräften\\n\\n(vgl. Machbarkeitsstudie Dateninfrastrukturen im Rheinischen Revier unter\\n\\nwww.dateninfrastruktur.nrw).\\n\\nDie Region ist auch ein starker Forschungsstandort mit relevanten\\n\\nForschungsfeldern, bspw. RWTH Aachen, Universität Bonn, Universität Köln,\\n\\nFraunhofer IAIS, FZ Jülich (Quanten- und Super-Computing)\\n\\nNordrhein-Westfalen verfügt über ein stark aufstrebendes Start-up Ökosystem, dass gerade im Rheinland (Aachen, Bonn, Köln, Düsseldorf) besonders stark ausgeprägt ist (vgl. www.wirtschaft.nrw/sites/default/files/documents/nrw_start-\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n358\\n\\nup-report_2020.pdf). Im speziellen gibt es im Umfeld von KI und Sprachmodellen\\n\\nbesonders hervorzuhebende Start-ups, wie bspw. DeepL, das erste NRW-Unicorn\\n\\nmit Sitz in Köln oder Cognigy aus Düsseldorf.\\n\\nNordrhein-Westfalen hat die eigene Kompetenzplattform KI.NRW als zentrale\\n\\nAnlaufstelle für Künstliche Intelligenz in Nordrhein-Westfalen. Neben konkreten\\n\\nProjekten zum Transfer aus der Forschung in die Wirtschaft ist das Thema „KI-\\n\\nZertifizierung“ ein wichtiges Thema in NRW.\\n\\n● Das Gaia-X-Projekt Open GPTX für die Entwicklung großer KI-Sprachmodelle hat mehrere starke Teilnehmer und Partner aus NRW, so Westdeutscher Rundfunk (WDR), Forschungszentrum Jülich, Fraunhofer IAIS (und mehr).\\n\\nIm Rheinischen Revier (Hürth) entsteht das Projekt AI Village.\\n\\nAuf Basis der genannten Voraussetzungen werden Maßnahmen umgesetzt, die\\n\\ndas Rheinland zur Digitalregion weiterentwickeln, insbesondere sollen\\n\\nDigitalparks entstehen.\\n\\nEin Digitalpark ist eine für Unternehmen der Digitalwirtschaft optimierte\\n\\nGewerbefläche, in räumlicher Nähe zu großen Rechenzentren und\\n\\nInternetknoten (Grund: sehr schnelle Reaktions- bzw. Latenzzeiten). Die\\n\\nFinanzierung erfolgt über private Investoren.\\n\\n\\n\\nIn einem Digitalpark steht immer auch ein Rechenzentrum. Hier werden\\n\\nSynergien und große Chancen zu dem Vorhaben „KI-Rechenzentrum“ gesehen.\\n\\n3. Governance ● Die Umsetzung im Rahmen eines PPP-Modells ist eine Möglichkeit, die im Detail zu prüfen wäre.\\n\\nDie Höhe des Finanzierungsbedarfs lässt den Bund als natürlichen Partner dieses Projekts erscheinen. Fachlich werden starke Anknüpfungspunkte zu nordrhein- westfälischen Initiativen gesehen (siehe „Standort NRW“, sowie „Housing“).\\n\\n4. Housing ● Standorte für Digitalparks gibt es in der Region, und damit auch für ein Rechenzentrum in der für das KI-Rechenzentrum angedachten Größenordnung.\\n\\nDie Anforderungen (Infrastruktur, Größe, Lage) an einen Standort und die Flächensuche sind sehr wichtig und sollten bei der Machbarkeitsstudie mit bedacht werden. Kompetenzen bei der Flächensuche für Rechenzentren liegen in\\n\\nNRW vor.\\n\\nEine gute Stromversorgung auf der einen Seite und eine gute Nutzung\\n\\nentstehender Abwärme auf der anderen Seite sind für Rechenzentren wesentlich.\\n\\nInsgesamt sollte das Thema „Nachhaltigkeit“ in all seinen Facetten berücksichtigt\\n\\nwerden (Energieeffizienz, erneuerbare Energien, Wassereffizienz, Recycling).\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n359\\n\\nInterviewprotokoll MPI\\n\\nInterviewter:\\n\\nProf. Dietmar Harhoff, Geschäftsführender Direktor, Max-Planck Institut für Innovation und Wettbewerb Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle\\n\\nHerr Prof. Harhoff stimmt der Analyse der LEAM-Initiative definitiv zu. Europa\\n\\ndarf bei diesem Thema nicht in die Abhängigkeit der Amerikaner und Chinesen\\n\\ngeraten.\\n\\nEs ist vor allem eine Frage der Ressourcen. Die Entwicklung von GPT-3 zu ChatGPT war bspw. eine Frage des Aufwands und kein technologischer Durchbruch.\\n\\nDie Frage nach verfügbaren Daten wird entscheidend sein. Zu viele europäische\\n\\nOrganisationen halten ihre Daten noch verdeckt.\\n\\n2. Next-Steps\\n\\nEr empfiehlt der LEAM- Initiative weitere Gespräche.\\n\\nInterviewprotokoll Otto\\n\\nInterviewter:\\n\\nDr. Michael Müller-Wünsch, CIO, Otto Gmbh & Co KG Interviewer & Protokollant: Jörg Bienert, Alexander Thamm GmbH Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n360\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Bis 2030 könnten 50% der IT-Anwendungen durch KI-Modelle geprägt sein. ● Die Gefahr ist, dass alle Daten, die in US-Cloud Services gespeichert und verarbeitet werden, sich nicht mehr im EU-Raum befinden. Letztendlich machen\\n\\nwir Amerika damit stark.\\n\\nOtto evaluiert derzeit KI-Modelle, u.a. neben Sprachmodellen auch im Bereich\\n\\nBild-und Videobearbeitung. Große KI Modelle werden in diesem Umfeld\\n\\nzunehmend an Bedeutung gewinnen.\\n\\nOtto muss prüfen, inwiefern sie US-Services aus Datenschutz und\\n\\nDatensicherheitsaspekten nutzen können.\\n\\n2. Zusammenarbeit mit LEAM ● Ein unabhängiger deutscher Service zur Erstellung und Nutzung von Foundation Modellen ist hilfreich und zu begrüßen. Eine Initiative wie LEAM ist politisch sinnvoll.\\n\\nDas Angebot muss allerdings wettbewerbsfähig im Hinblick auf Servicequalität\\n\\nund Kosten sein.\\n\\nEine Zusammenarbeit mit LEAM und eine Nutzung der Services wird gewünscht. ● Eine mögliche Investition in ein Joint-Venture wäre sinnvoll, ist aber kurz- und mittelfristig wegen der angespannten wirtschaftlichen Situation nicht realistisch.\\n\\nInterviewprotokoll REWE\\n\\nInterviewter:\\n\\nLorenz Determann, Bereichsleiter Analytics, REWE Group Interviewer & Protokollant: Jörg Bienert, Alexander Thamm GmbH Datum 15.12.2022\\n\\nAllgemeine Einschätzung\\n\\nAllgemein wird das LEAM Vorhaben begrüßt, vor allem unter dem Aspekt, alternative Angebote für General Purpose Modelle aus Deutschland zu bekommen\\n\\nEine Nutzung der LEAM Services bzw. Foundation Modelle wird stark abhängig sein, von dem Mehrwert und wirtschaftlichen Nutzen – vor allem hinsichtlich\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n361\\n\\no Den Kosten der bereitgestellten Services/Modelnutzung im\\n\\nWettbewerbsvergleich o Der Qualität der Modelle\\n\\nKI bei Rewe\\n\\nDerzeit werden in unterschiedlichen Bereichen eher kleine / mittelgroße KI-\\n\\nModelle selbst entwickelt (u.a. auch auf Basis von Bert)\\n\\nBei zentralen Modellen ist eine Eigenentwicklung wichtig, um mit eigenen Daten\\n\\nund Berechnungen Wettbewerbsvorteile erzielen zu können\\n\\n\\n\\nIn allgemeineren, unkritischen Bereichen ist auch eine Nutzung von allgemeinen\\n\\nModellen denkbar\\n\\nCloud Infrastruktur\\n\\nRewe greift u.a. auch im Data&Anaytics / AI Bereich auf die Cloud Services von\\n\\nGoogle zurück\\n\\nEine mögliche Abhängigkeit von amerikanischen Providern im Bereich der KI Applicationen / KI Foundation Modellen wird in diesem Kontext nicht als besonders kritisch gesehen.\\n\\nInterviewprotokoll SAP\\n\\nInterviewter:\\n\\nDr. Feiyu Xu, Senior Vice President, Global Head of Artificial Intelligence, SAP Interviewer: Dr. Sven Schmeier, DFKI Dr. Gerhard Paass, Fraunhofer IAIS Protokollant: Alex Dickmann, KI Bundesverband Datum: 13. November 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● SAP nutzt Foundation-Modelle bereits, v.a.in der Dokumentenverarbeitung. Dabei spielen auch multimodale Modelle eine Rolle, wenn bspw. Rechnungen per\\n\\nFoto geschickt werden und weiterverarbeitet werden müssen. Im Bereich\\n\\nProzessplanung machen wir auch Experimente mit Foundation-Modellen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n362\\n\\nEntscheidend für einen erfolgreichen Einsatz von Foundation-Modellen sind\\n\\nfolgende Aspekte:\\n\\n(1) Konzepte für den sicheren Zugang zu den Daten; (2) Eine Infrastruktur zur Datenhaltung und Rechenkapazität, die auf grüner Energie basiert; (3) Realisierung von „Data to Value“: Wie können wir domänenspezifische Modelle trainieren und damit wirtschaftlichen Mehrwert schaffen, z.B. für Geschäftsprozesse, Prozesse in Bereichen wie Manufacturing oder Supply Chain etc. bzw. auch für industriespezifische Anpassungen.\\n\\nWichtig in diesem Kontext ist, dass die deutsche Industrie die KI-Foundation- Modelle nutzen kann. Viele deutsche Firmen haben keinen Zugriff auf die notwendigen menschlichen Ressourcen, wie Data Scientists, und auf die\\n\\nnotwendige KI-Infrastruktur. Deshalb muss der Zugang zu KI-Technologien,\\n\\ninsbesondere zu Foundation-Modellen, auch möglichst einfach gestaltet werden.\\n\\nIn der Planung von LEAM muss man unterschiedliche Personas und Stakeholders\\n\\nfür die Entwicklung und Anwendung der Foundation-Modelle identifizieren und\\n\\nihre Rollen spezifizieren, als Beitrag zur Entwicklung einer holistischen Strategie\\n\\nfür die Deutsche Forschung, Wirtschaft und Industrie.\\n\\n2. Zusammenarbeit mit LEAM ● Ein Ansatz wie LEAM hilft, um ähnliche Modelle parallel an verschiedenen Orten und in verschiedenen Anwendungskontexten berechnen und nutzen zu können.\\n\\nDas Prä-Investment via LEAM ist notwendig, damit KI-Foundation-Modelle auch in\\n\\nder Praxis und mit wirtschaftlichem Mehrwert einsetzbar sind.\\n\\nMan braucht dazu ein Kollaborationsmodell zwischen der Wirtschaft und der\\n\\nWissenschaft.\\n\\nLEAM sollte auf jeden Fall auch die Möglichkeiten für Inferenzen über KI-\\n\\nFoundation-Modelle zur Verfügung stellen. Dadurch bekommen kleinere Akteure\\n\\ndie Ressourcen, um mit Foundation-Modellen gewinnbringend zu arbeiten.\\n\\nInterviewprotokoll Siemens\\n\\nInterviewter:\\n\\nMichael May, Head Company Core Technology Data Analytics & Artificial Intelligence, Siemens AG Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n363\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Foundation-Modelle sind ein wichtiges Thema. Mit GPT-3 wurde bereits experimentiert, bspw. für den Servicebereich.\\n\\nFür die Siemens-Geschäftsbereiche ist v.a. das Thema Industrial AI wichtig. Hier sind uns im Bereich Foundation-Modelle noch keine echten Anwendungen bekannt. Ein wichtiger Punkt ist, dass die Datensätze tendenziell in diesem\\n\\nBereich kleiner bzw. fragmentiert sind.\\n\\nSiemens arbeitet gemeinsam mit der LMU und TUM an\\n\\nGrundlagenforschungsthemen, z.B. im Rahmen von Doktorarbeiten, und\\n\\nevaluiert parallel konkrete Anwendungsfälle für Foundation Models.\\n\\nSiemens hat derzeit keine eigene hausinterne Infrastruktur zum Trainieren von\\n\\nFoundation Models.\\n\\n2. Zusammenarbeit mit LEAM ● Ein deutsches bzw. europäisches Angebot ist wünschenswert und hätte viele Vorteile.\\n\\nEs besteht ein hohes Interesse, die LEAM-Initiative zu unterstützen ● Projekte zur Erstellung von Foundation-Modellen können dabei unterstützen, das Thema Datenteilung (innerhalb und zwischen Unternehmen) neu zu beleben und ihm eine neue Relevanz zu verleihen.\\n\\nDas wird v.a. kleineren Unternehmen helfen, die aufgrund fehlender finanzieller\\n\\nund personeller Ressourcen weniger Möglichkeiten haben als Großunternehmen.\\n\\nInterviewprotokoll TÜV Süd\\n\\nInterviewter:\\n\\nDirk Schlesinger, Chief Digital Officer, TÜV Süd Interviewer: Jörg Bienert, Alexander Thamm Protokollant: Alex Dickmann, KI Bundesverband Datum: 06.12.2022\\n\\nZusammenfassung\\n\\n1. Thema LEAM ● Wir brauchen eine gemeinsame Aktion!\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n364\\n\\nDas mögliche de-facto Monopol der Marktbegleiter aus den USA verträgt sich\\n\\nnicht mit unserem europäischen Wertekanon.\\n\\nWir müssen Basisfähigkeiten aufbauen, um nicht abhängig und abgehängt zu\\n\\nwerden.\\n\\nEin Gegengewicht aus geostrategischer Sicht ist notwendig – vgl. supply chain für\\n\\nGas, seltene Erden, usw.\\n\\n2. Rechenzentrum ● ● Mit den Gauss Centre for Supercomputing haben wir in Deutschland die Erfahrung, wie das Thema Rechenzentrum geht – wir wissen, wie Großforschung\\n\\ngeht (DESY, Jülich…).\\n\\nCapex ist besser als Opex, weil das Budget nicht jedes Jahr neu verhandelt\\n\\nwerden muss.\\n\\n3. Datenmanagement ● Es gibt offene Fragen zu Themen wie GDPR, homomorpher Verschlüsselung, Federated Learning und weiteren. Die Gefahr ist, dass viele Unternehmen ohne\\n\\nGarantien für ihre IP nicht bereit sind, ihre Daten zu teilen.\\n\\nGDPR-Konformität kann dabei ein Wettbewerbsvorteil ggü. USA und China sein. ● Federated Learning as a Service ist eine großartige Idee. Vor allem, wenn Partnern Rechtsunsicherheiten genommen werden können.\\n\\n4. Was macht der TÜV? ● Der TÜV SÜD nutzt KI selbst in verschiedenen, Projekten (bspw. Visual Analytics). ● Wir (TÜV SÜD) nutzen bspw. NLP für Konsistenzprüfungen in Dokumenten und bauen darauf aktuell ein Document Service Layer auf.\\n\\nDer TÜV verwendet große Modelle bisher nicht! Es ist aktuell auch nicht geplant, diese zu nutzen. Die größte Herausforderung ist dabei sicherlich die Einführung der Modelle innerhalb der Organisation.\\n\\nTÜV nutzt Explainability Verfahren. ● Kuratierte, große Datensätze werden aber ein wichtiges Thema, v.a. in Hinblick auf Marktzulassung von Produkten (benchmarks).\\n\\nCase: Simulation von einem autonom fahrenden Auto braucht Daten über die Straßenbeschaffenheit, das Wetter, die Lichtverhältnisse, Fahrphysik, etc.\\n\\nSobald der AI Act der Europäischen Union umgesetzt ist, wird sicherlich auch der\\n\\nTÜV auf dem Gebiet der Qualitätszertifizierung von KI werden.\\n\\n5. Governance ● AI Lab GmbH:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n365\\n\\nTÜV Gesellschaften arbeiten z.Zt, als Arbeitsgruppe des Verbandes\\n\\nvorwettbewerblich und gemeinschaftlich zusammen, um das Thema KI –\\n\\nQualitätskontrolle voranzutreiben, v.a. auf technischer Ebene.\\n\\nDie Gründung einer AI Lab GmbH ist angestrebt, vorbehaltlich einer\\n\\nGenehmigung durch das Bundeskartellamt. Pilotprojekte werden aber\\n\\nbereits abgehandelt.\\n\\nEin Vertrieb der entwickelten Anwendungen ist hier aber nicht ohne\\n\\nweiteres möglich, bzw. nicht angestrebt. Rolle der AI Lab GmbH ist die\\n\\neines internen R&D-Dienstleisters\\n\\nDie TÜVe haben über viele Projekte dazugelernt. Einen allgemeinverbindlichen\\n\\nBlueprint gibt es nicht.\\n\\nAI Quality und Testing Hub:\\n\\nEine Landesgesellschaft des Landes Hessen und des VDE als erste\\n\\nShareholder.\\n\\nHerausforderung: Zusammenarbeit mit dem Land, das anders plant und\\n\\narbeitet als eine Firma, der VDE oder der TÜV.\\n\\n○ AIQs sind komplementär zu den AI Labs. Die AI Labs sind interne R&D Stellen, die Quality und Testing Hub bieten Kunden Services an.\\n\\nIn NRW ist das AI Quality und Testing Hub ein klassisches Förderprojekt.\\n\\nAktuell gibt es die Hubs nur ‚auf dem Papier‘ sie sind noch nicht final\\n\\ngegründet, obwohl dies die nächsten Wochen / Monate geschehen dürfte.\\n\\nTesting & Experimentation Facility Healthcare:\\n\\nCharite als Konsortialführer. Läuft sehr gut. ○ Europäisches Projekt – 30 plus Partner ○ Ziel: Entwicklung und agile Zertifizierung von ‚echten‘ Medizinprodukten mit KI\\n\\nFSD GmbH als Analogie für AI Lab GmbH:\\n\\nentwickelt Prüfmittel für Hauptuntersuchung der Autos ○ Bund beleiht FSD und ist daher auch beteiligt. Nicht immer sind die Interessen der öffentlichen Hand dieselben wie von\\n\\nWirtschaftsunternehmen – Ausgleich und Absprache vorab wichtig.\\n\\nInterviewprotokoll VDE\\n\\nInterviewter:\\n\\nSebastian Hallensleben, Head of Digitalisation and AI, VDE Verband der Elektrotechnik Elektronik Informationstechnik e. V. Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant:\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n366\\n\\nAlex Dickmann, KI Bundesverband Datum: 22. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● KI-Foundation-Modelle sind eine Schlüsseltechnologie. Die europäische Wirtschaft und Gesellschaft braucht eigene Modelle.\\n\\nDie Auswirkungen auf Geschäftsmodelle und Jobs wird sehr groß, z.B.\\n\\nRedenschreiber, Analysten, Standard-Journalismus, Kundenservice, Briefings, etc.\\n\\nDas Training eigener Foundation-Modelle ist eine Frage der europäischen\\n\\ndigitalen Souveränität. Europa braucht die Infrastruktur und die Kompetenz, um\\n\\nbeim Thema nicht abgehängt zu werden.\\n\\nDer Bedarf an den Modellen steigt. ● Das Thema der Qualitätskontrolle dieser Modelle wird wichtiger. Eine Qualitätskontrolle ist aber nur möglich, wenn wir die Technologie selbst\\n\\nbeherrschen.\\n\\nDerjenige, der die Technologie beherrscht, wird auch die Standards setzen und die Regulierung steuern. Wenn wir nicht in der Lage sind, hier mitzuwirken, werden wir die digitale Souveränität auch in diesem Bereich verlieren.\\n\\nDie Verfügbarkeit von Rechenkapazitäten sollte Teil der staatlichen Infrastruktur /\\n\\nDaseinsvorsorge sein.\\n\\n2. AI Quality und Testing Hub ● Gemeinsam mit dem Land Hessen hat die VDE ein AI Quality und Testing Hub gegründet.\\n\\n\\n\\nInhaltlich beschäftigt es sich mit Qualitätsmanagement und KI. Dafür sollen\\n\\nPrüfwerkzeuge zu einem Toolkit zusammengeführt und die Trainingsdaten-\\n\\nQualität überprüft werden.\\n\\nDer Hub soll in den nächsten Jahren organisch wachsen. ● Für eine Zusammenarbeit mit dem Land Hessen mussten einige Punkte im Beihilferecht beachtet werden. Bspw. tritt Hessen als kommerzieller Investor auf,\\n\\nes gab Vorgaben für den Aufsichtsrat und das Finanzministerium muss den\\n\\nBusiness Plan absegnen.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n367\\n\\nInterviewprotokoll Volkswagen\\n\\nInterviewter:\\n\\nPatrick van der Smagt, Director of AI Research, Volkswagen Group Machine Learning Research Lab Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband e.V. Datum: 21. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Die Anwendung von KI-Foundation-Modellen bei VW befindet sich aktuell in der Evaluation. Dabei sind viele Anwendungen, auch im Bereich End-User\\n\\n(Kommunikation im Auto, Chatbots in der Kundenkommunikation, Digitalisierung\\n\\nvon Handbüchern, etc.) denkbar.\\n\\nDabei denkt VW sowohl über die Nutzung und Tuning bestehender Modelle als\\n\\nauch das Training eigener Foundation Modelle nach.\\n\\nVW besitzt kein eigenes Rechenzentrum, um Foundation-Modelle zu trainieren.\\n\\nSie müssten hier auf externe Anbieter zurückgreifen.\\n\\nFür die Entwicklung und Nutzung von Foundation Modellen sind für VW sind auch\\n\\ndie Themen Explainability, Trustworthy AI und die juristischen\\n\\nRahmenbedingungen wichtig.\\n\\nDie Umsetzung und Nutzung von Modellen auf Basis einer durch einen Cloud-\\n\\nService bereitgestellten API ist aus Gründen der Datensicherheit problematisch.\\n\\nVW arbeitet nur mit Modellen, die für sie kontrollierbar sind und idealerweise\\n\\nauch vom Unternehmen gehostet werden.\\n\\nDaneben ist das Thema Datenverfügbarkeit wichtig. VW besitzt eigene, spezielle\\n\\nDatensätze auf denen Modelle trainiert werden müssten.\\n\\n2. Zusammenarbeit mit LEAM ● VW begrüßt die Initiative LEAM und möchte gerne weiter unterstützen. ● LEAM Services würden im Bereich Modell Tuning, aber auch potentiell in der Erstellung von eigenen Foundation Modelle genutzt.\\n\\nDie Beteiligung an einem Joint Venture bzw. einer PPP ist grundsätzlich sinnvoll.\\n\\nVW steht der Idee offen gegenüber.\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n368\\n\\nInterviewprotokoll Zalando\\n\\nInterviewter:\\n\\nAlexander Borek, Director of Data Analytics, Zalando SE Interviewer: Jörg Bienert, Alexander Thamm GmbH Protokollant: Alex Dickmann, KI Bundesverband Datum: 15. Dezember 2022\\n\\nZusammenfassung\\n\\n1. KI-Foundation-Modelle ● Nachteil der amerikanischen Modelle ist, dass sie nicht öffentlich zugänglich sind und individuell angepasst werden können.\\n\\nDarüber hinaus spielt der Datenschutz eine wichtige Rolle. Vor allem\\n\\nKundendaten sind sehr sensibel und für Zalando auch wettbewerbskritisch.\\n\\nWenn bestimmte Foundation-Modelle aufgrund von Datenschutzbedenken nicht\\n\\ngenutzt werden können und es keine Alternativen gibt, ist das ein großer\\n\\nWettbewerbsnachteil für Zalando.\\n\\nFür Zalando sind vor allem auch die europäischen Sprachen relevant. ● Entscheidend ist, dass die Anwendungen auf Basis der Modelle beim Kunden funktionieren.\\n\\n2. Zusammenarbeit mit LEAM ● Die geplanten LEAM Services sind interessant für Zalando. Eine Nutzung ist wahrscheinlich.\\n\\nEs ist besser, Einfluss auf die Technologie zu haben bzw. die Technologie selber betreiben zu können, als über API auf Fremd-Services aus Übersee zugreifen zu müssen.\\n\\nMögliche Anwendungsfälle sind vor allem Chatbots, Verbesserungen in der\\n\\nSuche, aber (in geringerem Maße) auch die Image-Verarbeitung\\n\\nAnhang | Große KI-Modelle für Deutschland\\n\\n369\\n\\nAnhang C - Übersicht Cloud-GPU Anbieter\\n\\nDie aufgeführten Kosten haben keinen Anspruch, alle Anbieter abzudecken. Sie dienen lediglich als Orientierung für marktübliche Preise.\\n\\nTabelle 27 gibt einen groben Eindruck über die Preise anderer Cloud Computing Anbieter. Die Preise pro GPU Stunde liegen ungefähr zwischen 1,94 EUR und 3,73 EUR. Somit sind die geplanten 2,21 EUR pro GPU Stunde für das LEAM-Hochleistungsrechenzentrum ein wettbewerbsfähiger Preis. In der Tabelle wurden die Preise für eine Stunde Rechenleistung der NVIDIA A100 Tensor Core GPU 80 GB verglichen, eines der stärksten, marktrelevanten GPUs. Die Preise variieren stark, je nach Anzahl der GPUs, weitere Hardware-Kapazitäten (Anzahl CPUs, RAM etc.) und Dauer der Buchung. Zum Beispiel bieten Amazon Web Services acht A100 GPUs für 1,38EUR pro GPU Stunde an, wenn diese für drei Jahre reserviert werden. Da das Hochleistungsrechenzentrum hauptsächlich für das Training von einzelnen Modellen gebucht wird, sind Buchungszeiträume von mehreren Monaten am ehesten vergleichbar und werden somit als Referenzwert genutzt, falls vorhanden (entsprechende Anbieter sind markiert). Allerdings muss bedacht werden, dass sich die angegebenen Preise auf Kosten für einen GPU belaufen. Es ist möglich, dass manche Anbieter größeren Projekten Rabatte anbieten können. Außerdem bieten manche Anbieter Spot-Preise. Dies sind stark reduzierte Preise für Rechenleistungen, welche zu jeder Zeit abgebrochen werden können, wenn die Nachfrage zu stark steigt. Der Spot-Markt ist ungeeignet für das hochkomplexe Training von Foundation-Modellen und somit sind die Preise nicht vergleichbar.\\n\\nAnbieter\\n\\nPreis\\n\\nGoogle Cloud\\n\\n3,73 EUR\\n\\nMicrosoft Azure\\n\\n3,63 EUR\\n\\nPaperspace\\n\\n2,93 EUR\\n\\nAmazon AWS EC2\\n\\n2,90 EUR\\n\\nNorthern Data\\n\\n2,59 EUR\\n\\nVultr\\n\\n2,31 EUR*\\n\\nCoreweave\\n\\n2,10 EUR\\n\\nDatacrunch\\n\\n2,09 EUR\\n\\nRunPod\\n\\n1,98 EUR\\n\\nFluidStack\\n\\n1,94 EUR*\\n\\nTabelle 27: Kosten für eine GPU-Stunde auf einer NVIDIA A100 Tensor Core GPU 80 GB nach Anbieter. Preise in US-Dollar wurden in Euro umgerechnet zu einem Kurs von $1 = 0,948768EUR (Dollarkurs am 06.12.2022)\\n\\nMonatsraten ansonsten On-Demand-Preise\\n\\nAnhang | Große KI-Modelle für Deutschland', metadata={'source': 'data\\\\LEAM-MBS_KIBV_webversion_mitAnhang_V2_2023 (1).pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception in thread Thread-6:                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 196, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 495, in _make_request\n",
      "    conn.request(\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 398, in request\n",
      "    self.endheaders()\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 1277, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 1037, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 975, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 236, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 211, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001AF8A65B290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AF8A65B290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py\", line 164, in _process_emb_response\n",
      "    res = requests.post(\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AF8A65B290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 650, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\embeddings\\base.py\", line 26, in embed_text\n",
      "    embs = await self.embed_texts([text], is_async=is_async)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\embeddings\\base.py\", line 36, in embed_texts\n",
      "    return await aembed_documents_with_retry(texts)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\ragas\\embeddings\\base.py\", line 67, in aembed_documents\n",
      "    return await self.embeddings.aembed_documents(texts)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_core\\embeddings\\embeddings.py\", line 46, in aembed_documents\n",
      "    return await run_in_executor(None, self.embed_documents, texts)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py\", line 211, in embed_documents\n",
      "    embeddings = self._embed(instruction_pairs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py\", line 199, in _embed\n",
      "    return [self._process_emb_response(prompt) for prompt in iter_]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py\", line 199, in <listcomp>\n",
      "    return [self._process_emb_response(prompt) for prompt in iter_]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tobia\\Repositories\\tobiasoberrauch\\rag-showcase\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py\", line 170, in _process_emb_response\n",
      "    raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
      "ValueError: Error raised by inference endpoint: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AF8A65B290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "# Initialize the generator with OpenAI models\n",
    "generator_llm = ChatOllama(model=\"llama3\")\n",
    "critic_llm = ChatOllama(model=\"llama3\")\n",
    "embeddings = OllamaEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Generate testset without retry mechanism\n",
    "try:\n",
    "    distributions = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents, \n",
    "        test_size=10, \n",
    "        distributions=distributions\n",
    "    )\n",
    "    result = evaluate(\n",
    "        testset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-showcase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
